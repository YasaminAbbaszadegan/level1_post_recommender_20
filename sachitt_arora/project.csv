,Title,Category,Post Info,Comments
0,welcome to the mycroft community,general discussion,"
welcome to the mycroft community

wedidit.jpg1024×1024 177 kb
 
we did it! we funded on kickstarter, and now we’ve started this forum in order to have a place where we can build a community around mycroft and promote participation in the project. if you are interested in contributing, you’re at the right place.
who is this site for?

people generally interested in the mycroft project.
developers interested in contributing to the mycroft project (or related projects).
those interested in a.i., the internet of things (iot), and home automation.
anyone interested in open source, linux, and the raspberry pi - in relation to voice control.

please take the time to make a post and share who you are and what you are interested in doing with mycroft! use the introductions category for your post!
miss out on our kickstarter? check out our extended campaign on indiegogo!
we released our secret sauce, the source code, dubbed: mycroft core - check it out!
join the mycroft slack channel to chat about the project!
","
we look forward to hearing from everyone! 

nice to see that we bet on the right project 

a very lovely introduction!  i am new to the community and hope to make relevant contributions.!

nice work guys…
keep it up…
regards.

i bought my mark a few months ago. i didn’t have time to play with it, but i would like to use more of it.
is there a release schedule for updates?
is there how to page?
can i add skills to it that i wrote?

you can find docs on the mycroft right here. release schedule for update is biweekly, usually on thursdays. you should expect a new update this thursday. to allow other people to install your skills either using msm (mycroft skills manager) or the install skills skill, you can do a pull request on this repo, to add your skill on there.
hope this helps!




 system:

mycroft


really love the incredible project!

hi everyone,
i found out about mycroft through the plexpod community in kc. i work for a global health organization called partners in health that provides healthcare, engages in health system strengthening activities, educates and trains health workers, and co-develops and implements open source electronic medical record systems (emr) in low and middle income countries.
the platform we use for emr is called openmrs. users include country governments, ngos, and private hospital systems. the platform is used in over 60 countries, with millions of patient records. one of the biggest challenges in moving from a “retrospective” to a “point-of-care” data collection model is the burden and interruption of workflow to clinicians who are already burdened with paperwork. in many countries, the government requires paperwork and the emr data is supplemental.
i hope that somewhere out in the openmrs ecosystem, there is someone who would want to test mycroft ai running on the same laptop or tablet that is running their emr application. my organization is not positioned to do this yet, because the locations where we work are too rural and internet bandwidth is a constraint. but i hope that this idea might catch someone’s idea, and might take hold somewhere else in the world.
please feel free to reach out for more information or you can join the openmrs community directly if you are a developer who is looking for an opportunity to begin the discussion about integrating voice with health data.

hello to the group. i just purchased a donation for the mark-ii unit, and i also downloaded and installed the core to a linux workstation to satisfy my impatience and learn more while i’m waiting for the mark-ii unit.
i know only a little bit about coding in general, but i’m looking forward to learning how to build my first skill.
the skill i’m hoping to build is the ability for the ai to go to a monitoring web site and report back the values in a given frame.
i have solar panels on my home, they are grid-tied through a solar edge inverter which communicates data to their web site. that web site has a “public” view web page with the current statistics on my solar energy system.
https://monitoringpublic.solaredge.com/solaredge-web/p/kiosk?guid=3d71e75a-b8cb-48ef-8d00-6f36fd077737
i’m looking forward to seeing how far i can get.  wish me luck!
~eric

great job guys…
well done

hi, i ma josé (jrd10) from france.
great project, congratulations 
not (still) a user. i will see if i can contribute in french.
use mycroft with ubuntu and soon with rpi 3+ :).
best regards, jrd10
tricassinux.org
"
1,nextcloud calendar skill,skill feedback,"
hey everyone, wrote this skill for managing a nextcloud calendar. i’ve been using it for managing my family calendars. it’s doing pretty well, though i wouldn’t say it’s really been put through it’s paces. other’s input/experience would be great to help it apply more generally for other people! one biggest issue is that the current installation process requires a few sections of code to be modified and run before it will work with your nextcloud account. i haven’t found a good way to make use of the home.mycroft.ai/skills configuration parameters for these specific settings.
to be more specific, your nextcloud server url, username, and password are all able to be setup from your homepage. but there are two dictionaries defined in __init__.py that map colloquial names to calendar names and vice versa. this allows you to say things like “add an event to my calendar”, and the event will be added to the personal calendar in nextcloud. similarly, when mycroft if confirming an addition to the personal calendar, it knows to say “adding ____ to your calendar.”
additionally, because of the issue with .optionally() and regex, i had to write my own parser to extract calendar names and starting/ending datetimes, so mycroft knows which calendar you want to manage, and can convert “how busy am i this weekend?” into upcoming saturday morning, through sunday night. but users will need to change a few lines in peg/calendargrammar.ebnf and will need to run ./generatemodel.sh so that calendargrammar.py is regenerated according to their desired grammar. any tips on how to make this configuration much easier for users would be super appriciated!
how to install skill name


install nextcloud calendar by …

fork my repo: https://github.com/markditsworth/mycroft-nextcloud-calendar

clone your fork locally.
edit the two dictionaries in __init__() function in __init__.py as needed.
edit the ownership rule in peg/calendargrammar.ebnf as needed.
install tatsu locally (venv recommeded): pip install tatsu

while in peg/ run ./generatemodel.sh

in your mycroft environment ( i run a picroft), install the skill with mycroft-msm install https://github.com/your_account/your_forked_repo.git

installation of caldav has been known to fail/timeout, but was has always been successful manually: mycroft-pip install caldav; mycroft-pip install tatsu and then rerun the skill install.
go to your skills in home.mycroft.ai, and enter your nextcloud server url, username, and password.



nextcloud calendar connects to the nextcloud server you provide each time you ask mycroft to add/list events.


how to test skill name

install the skill per the above instructions.
configure the skill settings in home.mycroft.ai
speak add an event to my calendar tomorrow at 2pm

mycroft should ask for the duration of the event, the name of the event, confirmation of event details, and assert that your calendar has been updated.
check your calendar for the created event.
speak tell me my events tomorrow.

mycroft should respond with “<event name> <tomorrow’s weekday> <month> <day> from <start time> to <end time>” for every event on you calendar tomorrow.

skill passes if it correctly adds an event to your specified calendar at the specified time. passes if it correctly lists all events on your calendar that occur within your specified time range (out to 2 weeks — if it is sunday and you say “how busy am i next week”, mycroft will list your events starting on next sunday through next saturday).
feedback is welcome here, on mycroft chat, or through the issues on my github repo (github is preferred).
",
2,audio working but no voice,support,"
hi
i have mycroft linux version on my rpi4 .
with the ps3 eye as mic and jack speaker
i have audio trouble when i say  “hey mycroft”
i heard the song when he start listenning i see the answer on the cli but i heard nothing
i ask him to put the volume to max no  change.
i made the mycroft audio test everything is ok
i try to put   “play_wav_cmdline”: “aplay %1”,
“play_mp3_cmdline”: “mpg123 %1”,
but nothing change
the following is my voice log
2021-01-10 17:38:40.711 | info     |  2596 | mycroft.messagebus.load_config:load_message_bus_config:33 | loading message bus configs
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition ‘cards.bcm2835_headpho.pcm.front.0:card=0’
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm front
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.rear
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.center_lfe
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.side
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition ‘cards.bcm2835_headpho.pcm.surround51.0:card=0’
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround21
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition ‘cards.bcm2835_headpho.pcm.surround51.0:card=0’
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround21
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition ‘cards.bcm2835_headpho.pcm.surround40.0:card=0’
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround40
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition ‘cards.bcm2835_headpho.pcm.surround51.0:card=0’
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround41
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition ‘cards.bcm2835_headpho.pcm.surround51.0:card=0’
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround50
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition ‘cards.bcm2835_headpho.pcm.surround51.0:card=0’
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround51
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition ‘cards.bcm2835_headpho.pcm.surround71.0:card=0’
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround71
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition ‘cards.bcm2835_headpho.pcm.iec958.0:card=0,aes0=4,aes1=130,aes2=0,aes3=2’
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm iec958
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition ‘cards.bcm2835_headpho.pcm.iec958.0:card=0,aes0=4,aes1=130,aes2=0,aes3=2’
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm spdif
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition ‘cards.bcm2835_headpho.pcm.iec958.0:card=0,aes0=4,aes1=130,aes2=0,aes3=2’
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm spdif
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.hdmi
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.hdmi
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.modem
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.modem
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.phoneline
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.phoneline
alsa lib pcm_oss.c:377:(_snd_pcm_oss_open) unknown field port
alsa lib pcm_oss.c:377:(_snd_pcm_oss_open) unknown field port
alsa lib pcm_a52.c:823:(_snd_pcm_a52_open) a52 is only for playback
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition ‘cards.bcm2835_headpho.pcm.iec958.0:card=0,aes0=6,aes1=130,aes2=0,aes3=2’
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm iec958:{aes0 0x6 aes1 0x82 aes2 0x0 aes3 0x2  card 0}
alsa lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) invalid type for card
alsa lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) invalid type for card
2021-01-10 17:38:41.848 | info     |  2596 | mycroft.client.speech.listener:create_wake_word_recognizer:328 | creating wake word engine
2021-01-10 17:38:41.852 | info     |  2596 | mycroft.client.speech.listener:create_wake_word_recognizer:351 | using hotword entry for hey mycroft
2021-01-10 17:38:41.857 | info     |  2596 | mycroft.client.speech.hotword_factory:load_module:403 | loading “hey mycroft” wake word via precise
2021-01-10 17:38:43.872 | info     |  2596 | mycroft.client.speech.listener:create_wakeup_recognizer:365 | creating stand up word engine
2021-01-10 17:38:43.878 | info     |  2596 | mycroft.client.speech.hotword_factory:load_module:403 | loading “wake up” wake word via pocketsphinx
2021-01-10 17:38:44.060 | info     |  2596 | main:on_ready:175 | speech client is ready.
2021-01-10 17:38:44.077 | info     |  2596 | mycroft.messagebus.client.client:on_open:114 | connected
2021-01-10 17:39:10.115 | info     |  2596 | mycroft.session:get:74 | new session start: a581b8e8-5dc3-480a-8ae2-6ad210b35d6f
2021-01-10 17:39:10.121 | info     |  2596 | main:handle_wakeword:67 | wakeword detected: hey mycroft
playing wave ‘/home/pi/mycroft-core/mycroft/res/snd/start_listening.wav’ : signed 16 bit little endian, rate 48000 hz, stereo
2021-01-10 17:39:10.945 | info     |  2596 | main:handle_record_begin:37 | begin recording…
2021-01-10 17:39:13.980 | info     |  2596 | main:handle_record_end:45 | end recording…
2021-01-10 17:39:14.695 | info     |  2596 | main:handle_utterance:72 | utterance: [‘what time is it’]
i don t really know what can i do , i try several tutoriel about  amixer  pulse audio and
mycroft audio troubleshooting
any help will be very appreciated
thank in advance
",
3,mycroft on respeaker core v2,mycroft project,"
mycroft on respeaker core v2
respeaker core v2 (short: rc2) is a single board computer with a technical specification similar to a raspberry pi3 - but with some more goodies:

builtin microphone array (6 mics) which supports

beamforming
noise suppression
echo cancellation


led pixel ring
user button
4gb emmc memory (so you don’t need a sd-card)
jst2-connector to connect a small loudspeaker

overall the rc2 is a nice package to run mycroft on…
preparation
rc2 runs with debian 9 (stretch). seeedstudio (the vendor of rc2) provides prebuilt images where drivers for the onboard sound card are already installed and configured.
first you must decide if you want to install the os to the onboard emmc memory or run it directly from sd-card.
go to fangcloud and navigate to respeakerv2 > debian > yyyymmdd.
for the emmc option download the
respeaker-debian-9-iot-flasher-********-4gb.img.xz.
in case you prefer to run the os from the sd-card you should download the respeaker-debian-9-iot-sd-********-4gb.img.xz.
burn the downloaded image to an sd-card (e.g. using etcher), then insert the sd-card in the rc2 card slot and power it on. when you have choosen the emmc option it will take approximately 10 minutes to copy the data from sd-card to the onboard memory.
configure wifi
after the os install you must connect the rc2 to a pc via usb cable using the micro usb port labeled “otg”. then run a serial console program using 115200 baud rate, 8bits, parity none, stop bits 1, flow control none.
login with user “respeaker” and password “respeaker”. then run the setup with sudo nmtui and enter your wifi credentials.
more detailed instructions for preparation and wifi setup can be found here
mycroft setup
os update
first you should update the operating system:
sudo apt update
sudo apt upgrade

respeaker daemon
next step is installing respeakerd which provides the software stack for beamforming, noise reduction and echo cancellation:
curl https://raw.githubusercontent.com/respeaker/respeakerd/master/scripts/install_all.sh|bash

next edit /etc/respeaker/respeakerd.conf and change the line starting with mode to mode = pulse
then edit /etc/pulse/default.pa and add following lines
load-module module-pipe-source source_name=""respeakerd_output"" format=s16le rate=16000 channels=1
set-default-source respeakerd_output

finally start the respeaker daemon with sudo systemctl start respeakerd
you may want to perform an audio test by:
parecord --channels=8 --rate=16000 --format=s16le hello2.wav
paplay hello2.wav

mycroft install
now your rc2 is ready to install mycroft. this is done by a git-install:
cd ~/
git clone https://github.com/mycroftai/mycroft-core.git
cd mycroft-core
bash dev_setup.sh

during the setup you will be asked some questions, here is a rundown of my recommended answers:

y)es, run on the stable ‘master’ branch
y)es, automatically check for updates
n - skip mimic build
y - adding mycroft commands to your path

note: when you have chosen to install the os on the emmc you have only 4gb available! whenever running dev_setup.sh you should consider adding the option -sm to skip the install of mimic as this will several hundreds mb additional memory…
after setup is finished you are ready to run mycroft for the first time:
cd ~/mycroft-core
./start-mycroft.sh debug

respeaker-io-skill
i have written a mycroft-skill that supports the led pixel ring and the user button. you can install the skill by
mycroft-msm install https://github.com/domcross/respeaker-io-skill.git
advanced options
here are some advanced options, inspired by the blog of mycroft community user @j1nx
use ram-disk for ipc
mycroft uses a message bus over which the different mycroft services communicate - the so called “inter process communication” (ipc). this will result in a lot of read and write operation to the file system which can “wear out” your sd-card. to avoid that you can redirect the ipc to a ram-disk:
sudo mkdir /ramdisk
the edit /etc/fstab and add following line to the end:
tmpfs /ramdisk tmpfs rw,nodev,nosuid,size=20m 0 0
sudo mkdir /etc/mycroft
edit (new) file `/etc/mycroft/mycroft.conf:
{
  ""ipc_path"": ""/ramdisk/mycroft/ipc/""
}

automatically start mycroft while booting
until now you have to manually start mycroft each time you have rebooted your rc2. here is how to automatically run by using systemd:
edit (new) file /etc/systemd/system/mycroft.service:
[unit]
description=mycroft personal ai
after=respeakerd.service

[service]
user=respeaker
workingdirectory=/home/respeaker/mycroft-core
execstart=/home/respeaker/mycroft-core/start-mycroft.sh all
execstop=/home/respeaker/mycroft-core/stop-mycroft.sh
type=forking
restart=always
restartsec=3

[install]
wantedby=multi-user.target

user respeaker must be member of groups pulse and pulse-access, otherwise sound output will no work:
sudo usermod -a -g pulse respeaker
sudo usermod -a -g pulse-access respeaker

then enable the “mycroft service” by
sudo systemctl enable mycroft.service
","
that looks like a nice piece of hardware!
thanks for sharing your learnings and writing that skill to make it even easier too

great write-up and thanks for the “ping”!
forked your skill, because somewhere in the near future i want to do the same for their 4-mic pi hat.
anyhow, will put that device on my todo/to buy list. looks very interesting. how are the;

beamforming
noise suppression
echo cancellation

performing?




 j1nx:

how are the;

beamforming
noise suppression
echo cancellation

performing?


i did only tests in a “laboratory stage” so far where the rc2 is sitting “naked” on my workdesk:
when running the respeaker-daemon in debug mode i can see that different directional information is shown when i change my position while speaking, so the beamforming/direction-of-arrival algorithm does work.
in another test i triggered a skill that plays mp3-stream and mycroft still recognized the wake-word and triggered the desired skill/intent - the echo cancellation does work as well. my mark-1 and my picroft with google-aiy-hat in the same scenario (playing music stream in with similar loudness) stayed silent as they couldn’t hear me any more…
i am now looking for a case where the rc2 fits into, then i will perform real word tests…
by the way: the respeaker pro case  is for the core v1 only - the rc2 has larger dimensions and does not fit into it (found it out the hard way…)

this sounds like an important development, because i’m finding the picroft distribution to be essentially useless for music without echo cancellation. have you found any drawbacks compared to the raspberry pi?

i did not perform actual benchmark testing but my impression is that overall performance of the rc2 is similar to the raspberry pi 3b. drawbacks: as stated above there is no case for the rc2 and it does not have extensive gpio support as the raspberry has.
if you want echo-cancellation with mycroft on a raspberry pi you might want to look at respeaker 6mic circular array or the  4mic linear array - both have hardware loopback channel which enables the echo-cancellation (all other respeaker mic hats for raspberry do not have hw-loopback and therefore no echo-cancellation).

has anyone actually had success with a raspberry pi hearing them through the respeaker while music is playing? that would be reason to buy the respeaker, but frankly i don’t care much about the lights.

for that you need echo-cancellation (ec). looking at respeaker devices only the 4-mic linear array and the 6-mic array can do ec (the 2-mic and 4-mic circular can’t do ec as they have no hardware loopback).
using one of these, follow the instructions for installation of the seeedvoicecard-driver from the wiki, then install respeakerd (you probably need to change some configuration settings depending on your mic-array model).

@dominik
hey can you tell me what is about this 4-mic speaker:
http://wiki.seeedstudio.com/respeaker_4_mic_array_for_raspberry_pi/
does ist support echo-cancellation, beamforming or noise suppression?

hi @suisat,
the “circular” 4-mic array does have doa (direction of arrival - beamforming), vad (voice activity detection) and kws (keyword spotting or keyword search) but does not have echo-cancellation due to hardware limitations (no loopback channel).
the “linear” 4-mic array has other hardware including loopback channel which enables echo-cancellation, but lacks the led pixel ring.
in case you want echo-cancellation and led pixel ring you should look at the “circular” 6-mic array.

@dominik
hi, i just want t follow up on this post from april. i have a rasp pi 4, and a seeed 6 mic array. i have installed the respeakerd. is this all i need to do? do i need to configure something on the mycroft side to enable the echo suppression?
thanks and regards,
stephen

hi @stephen_o_sullivan
no configuration on mycroft side necessary. the setup script for  respeakerd installs all required components, including librespeaker (which does the actual echo cancellation) and configures pulseaudio.
http://wiki.seeedstudio.com/respeaker_6-mic_circular_array_kit_for_raspberry_pi/#librespeaker-aduio-process

just ordered one of these. glad i saw this post as i was going to purchase the respeaker mic array v2.0 and use it on top of an rpi. knowing the core works as an integrated unit is great news.
seeed provide drawings and (i think) the files to 3d print a stand. which is comprised of multiple layers connected by standoffs, which shapes what looks like a chamber  for a down firing speaker.
http://wiki.seeedstudio.com/respeaker_core_v2.0/#resources

hi all,
@dominik, many thanks for the reply. i installed it, but my mic does not seem to be able to record since i installed the respeakerd library. even using arecord. i have no doubt i have made a mistake somewhere, i just need to go through everything with a fine tooth comb and find it.
i’ll check back here if i find something.

i found the source of my issue. it was the `https://github.com/domcross/respeaker-io-skill.git skill. uninstall and everything works fine.

looks like i have to add another topic on my to-do list 
i can not rule out that that the respeaker-io skill causes the respeakerd service to malfunction as i simply mixed some code examples from the respeaker wiki pages for that…

hi @dominik,
no big deal. your code was for the respeaker core v2, i have a respeaker 6-mic array. i found this: respeaker 4-mic array hat mycroft a.i. skill (respeaker 4-mic array hat mycroft a.i. skill) and the developer says he built it on your work. so maybe her can point you in the direction of the problem? this works on the rasp pi 4.
regards,
stephen

hi @dominik, i’ve received my v2 unit and followed these instructions. the microphone works brilliantly, very clear recordings.
i have no audio output of any kind though. not from mycroft, which uses pulse, or from raspotify, which uses alsa.
aplay -l shows a single output device, the seeed card. but a pacmd list-sinks shows only a null device. or rather, it complains that no daemon is running. after starting pulse manually and using the pacmd list-sinks command again, it shows the null.
the seeed wiki seems to indicate that the seeed voicecard audio output alsa device actually redirects the audio to pulse. this would explain why a service that doesn’t use pulse and rather relies on alsa directly, would still fail to produce any sound.
what pulse sink devices do you see on yours? did you have to take any extra steps to get audio output working?
i am using the iot image from seeed, which is cli only. i’m considering reflashing the lxqt image and starting again. i had hoped to build for rapid start times and low memory demand though.

don’t know if this helps but here are some outputs
    $ aplay -l
    **** list of playback hardware devices ****
    card 0: seeed8micvoicec [seeed-8mic-voicecard], device 1: 100b0000.i2s1-rk3228-hifi rk3228-hifi-1 []
      subdevices: 1/1
      subdevice #0: subdevice #0

$ pactl list sinks
sink #0
	state: suspended
	name: auto_null
	description: dummy output
	driver: module-null-sink.c
	sample specification: float32le 2ch 48000hz
	channel map: front-left,front-right
	owner module: 13
	mute: no
	volume: front-left: 23593 /  36% / -26.62 db,   front-right: 23593 /  36% / -26.62 db
	        balance 0.00
	base volume: 65536 / 100% / 0.00 db
	monitor source: auto_null.monitor
	latency: 0 usec, configured 0 usec
	flags: decibel_volume latency 
	properties:
		device.description = ""dummy output""
		device.class = ""abstract""
		device.icon_name = ""audio-card""
	formats:
		pcm

$ pactl list source
source #0
	state: suspended
	name: auto_null.monitor
	description: monitor of dummy output
	driver: module-null-sink.c
	sample specification: float32le 2ch 48000hz
	channel map: front-left,front-right
	owner module: 13
	mute: no
	volume: front-left: 65536 / 100% / 0.00 db,   front-right: 65536 / 100% / 0.00 db
	        balance 0.00
	base volume: 65536 / 100% / 0.00 db
	monitor of sink: auto_null
	latency: 0 usec, configured 0 usec
	flags: decibel_volume latency 
	properties:
		device.description = ""monitor of dummy output""
		device.class = ""monitor""
		device.icon_name = ""audio-input-microphone""
	formats:
		pcm

source #1
	state: suspended
	name: respeakerd_output
	description: unix fifo source /tmp/music.input
	driver: module-pipe-source.c
	sample specification: s16le 1ch 16000hz
	channel map: mono
	owner module: 22
	mute: no
	volume: mono: 65536 / 100% / 0.00 db
	        balance 0.00
	base volume: 65536 / 100% / 0.00 db
	monitor of sink: n/a
	latency: 0 usec, configured 0 usec
	flags: decibel_volume latency 
	properties:
		device.string = ""/tmp/music.input""
		device.description = ""unix fifo source /tmp/music.input""
		device.icon_name = ""audio-input-microphone""
	formats:
		pcm

when i check status of respeakerd by sudo systemctl status respeakerd i get a strange result:
   loaded: loaded (/lib/systemd/system/respeakerd.service; enabled; vendor preset: enabled)
   active: failed (result: exit-code) since thu 2020-03-05 20:23:11 cet; 4min 53s ago
  process: 1902 execstart=/usr/bin/respeakerd_safe (code=exited, status=1/failure)
 main pid: 1902 (code=exited, status=1/failure)

nevertheless mycroft works (wakeword and intents).



 fellhahn:

i am using the iot image from seeed, which is cli only. i’m considering reflashing the lxqt image and starting again. i had hoped to build for rapid start times and low memory demand though.


i am using the “iot” image as well. the reference to the “lxqt” in my entrance posting is actually a typo (unfortunately i am not able to edit it anymore).

thanks for taking the time to reply. unfortunately that output is exactly what i was seeing, so no clues there.
i did get pulse audio running in system mode, which provided some better looking output from pactl list sinks, i could see what looked like a proper audio device. alas, still no sound output!
i’ve actually reflashed the lxqt image, but haven’t had much time for the project since then.
"
4,precise runner failure on pinephone aarch64,mycroft project,"
i hope this saves someone some time, sometime   when using precise_runner on the pinephone (aarch64) the process will crash when reading from the microphone due to the use of a hard-coded sampling rate when initializing the stream.
2020-12-22 17:02:49 :: kalliope-0.7.0 :: say something!
exception in thread thread-3:
traceback (most recent call last):
file “/usr/lib/python3.9/threading.py”, line 954, in _bootstrap_inner
self.run()
file “/usr/lib/python3.9/threading.py”, line 892, in run
self._target(*self._args, **self._kwargs)
file “/usr/lib/python3.8/site-packages/precise_runner/runner.py”, line 231, in _handle_predictions
chunk = self.stream.read(self.chunk_size)
file “/usr/lib/python3.8/site-packages/precise_runner/runner.py”, line 186, in 
stream.read = lambda x: pyaudio.stream.read(stream, x // 2, false)
file “/usr/lib/python3.9/site-packages/pyaudio.py”, line 608, in read
return pa.read_stream(self._stream, num_frames, exception_on_overflow)
oserror: [errno -9999] unanticipated host error
another kalliope process returns “invalid number of frames” from the exact same call. the problem also occurs when using the speech_recognition module directly and even when not using python. the alsa utility arecord with default values has the same issue.
in precise_runner/runner.py:
class preciserunner
def start(self):
“”“start listening from stream”""""
if self.stream is none:
from pyaudio import pyaudio, paint16
self.pa = pyaudio()
self.stream = self.pa.open(
16000, 1, paint16, true, frames_per_buffer=self.chunk_size
)
changing the 16000 to (12000 and below) or (24000 and above) prevents the problem from occurring.
hth
lf
","
you should file this as an issue on github.

i think the hardcoded 16000 is because the training with it. isn’t it better to resample the input to that bit rate within pulseaudio?

thank you, i will file on github.
having tested further, while changing the sample rate prevents crashes, it also prevents recognition of the wakeword! is this because the model was trained at 16k and so words with different sample rates are not digitally equal? are there any models built with a different sample rate?
i am using the “sheila” wakeword with > 95% accuracy on my test machines.
resample within pulse earlier in the process. thank you j1nx. you wouldn’t happen to have a one liner available, would you?
off to do the research on how to do it myself.
thank you both
lf

no, sorry not something readily at hand. can also do some googling for you if you get stuck though.

can you tell what format the clips are recorded at?

no sound clips. this is straight from the microphone. the systems reposrts the source as 48k

incidentally, if you get it working on the pinephone without eating all the resources, i’d be very interested in that. i’m working with pocketsphinx because i don’t have the time or the expertise to worry about precise, and i’m more focused on functionality.

so after os and python updates on january 8th and 9th, everything magically works. arecord, python speech_recognition, and precise_runner.
ymmv
lf

i have it all working now, and precise_runner performance is not an issue at this time. battery consumption requires it be suspended after use so i am currently using a timeout of 40 seconds. my biggest performance problem appears to be recognition of the order after waking up. now that this speech problem is fixed i can start looking at that

we might be duplicating work. see hivemind (though you might be better off hitting us up in mycroft’s chat server at ~hivemind)
it’s a wake word and a thin client, connected to any mycroft device. i’m using a mycroft instance on my homeserver.
"
5,testing and feedback find a movie series across your streaming services,skill feedback,"
i got tired of searching through all of my streaming services to figure out where i can watch a particular movie or tv show. i wrote a skill called “video-finder-skill” that lets you ask mycroft where you can watch it instead of manually searching. all feedback is appreciated. i hope this is helpful to others!
how to install video-finder-skill


install video-finder-skill by …

mycroft-msm install https://github.com/rickoooooo/video-finder-skill




video-finder-skill connects to imdb and utelly via rapidapi. it also connects to plex directly using your plex credentials (if applicable).


how to configure video-finder-skill
you will need to create an account at rapidapi and subscribe to imdb and utelly. this is free, but you’ll be required to provide a credit card in case you go over the free limit. in the skill settings, you’ll need to provide your api keys and host values. you also need to specify a country code and a list of streaming services to search. finally, if you use plex you’ll need to enter your plex credentials. more details about this and all configuration options can be found in the video-finder-skill github page.
how to test video-finder-skill

configure skill settings as described on the github readme
identify a movie in one of your streaming services for testing purposes. for example, the movie die hard starring bruce willis.
ask mycroft “where can i watch die hard?”
mycroft will present you with the top three imdb results to choose from.
respond with “(find|get|i want|looking for) the (first|second|third) one”
mycroft should search all of your streaming services and tell you where the film is available, or tell you that it wasn’t found. make sure this output matches what you expect.

you can also specify an actor in the film. in this case, mycroft will choose the top imdb result and automatically search for that title instead of presenting you with options.

“where can i awtch die hard starring bruce willis?”
“where can i watch the bruce willis movie die hard?”

indicate clearly what constitutes a pass criterion for the skill, and what constitutes a fail criterion.
where feedback on video-finder-skill should be directed
feel free to post feedback here in this thread or via a github issue.



github



rickoooooo/video-finder-skill
searches streaming servics for movies or tv shows. contribute to rickoooooo/video-finder-skill development by creating an account on github.





",
6,self managed stream player,skill feedback,"
as a “hello world” skill i created a stream player skill that is a different take on the internet radio skill and uses some ideas from the npr news skill threads and the dutch-radio skill.
the stream player streams selections from a list of stations maintained in the user’s “stream player” skill configuration settings (https://account.mycroft.ai/skills).
if it works inside the mycroft environment, this method could also be used to “open up” the list of choices for the latest news skill.
users can also give permission for their stream list to be searched by other users.  not sure how that could happen within mycroft, but it could be the basis for a larger curated list of streams at some point in the future.
git clone https://github.com/bulwagga/stream-player-skill
to use, say “stream wamc” or “stop streaming” or “cancel streaming”.  to add your own streams, add them in your skill configuration settings.
if all works correctly, after you fill the available stream fields with your preferred streams, the software will add another empty field for your use.
hopefully this is food for thought as all these stream players converge!
thanks
",
7,volume skill not working on picroft stretch,support,"
hi,
i have installed fresh the image picroft stretch on a raspberry pi 3b+ with usb mic and speakers through the jack.
all sound is working great except that it gives error to control the volume via speaking by using volume skill.
configuration:
picroft - stretch 2018-09-13
mycroft - 18.8.1
mic - usb mic
speakers via jack 3.5mm
content of audio_setup.sh works fine and possible to set different volume levels by :
sudo amixer cset numid=3 “1” > /dev/null
amixer set pcm 99%
it is also possible to set different output sound level via the command line with ‘amixer set pcm xx%’ command.
however, by speaking to mycroft and requesting to set a volume level, there is a crash/error in the volume skill. log output of mycroft-cli:
22:34:00.615 - main:handle_record_begin:35 - info - begin recording…
playing wave ‘/home/pi/mycroft-core/mycroft/res/snd/start_listening.wav’ : signed 16 bit little endian, rate 44100 hz, stereo
22:34:03.423 - main:handle_record_end:40 - info - end recording…
22:34:03.432 - main:handle_wakeword:56 - info - wakeword detected: hey mycroft
22:34:05.373 - main:handle_utterance:61 - info - utterance: [‘set the volume to 5’]
22:34:05.581 - mycroft.skills.core:wrapper:716 - error - an error occurred while processing a request in volume skill
traceback (most recent call last):
file “/home/pi/mycroft-core/mycroft/skills/core.py”, line 707, in wrapper
handler(message)
file “/opt/mycroft/skills/mycroft-volume.mycroftai/init.py”, line 95, in handle_set_volume
level = self.__get_volume_level(message, self.mixer.getvolume()[0])
attributeerror: ‘volumeskill’ object has no attribute ‘mixer’
^— newest —^
thanks for the support.
excellent job for the project.
","
i had a issue with the volume skill after i configured “pycroft 2018-09-12 stretch lightning” to use a usb based speaker.  while the error message you have provided is not the same, it may have the same root cause for you to pursue.
what i found  when i used a usb based speaker was the mixer control name used for the usb speaker did not match the  default mixer control name used by the initialisation of the alsasound.mixer() object in the volume skill code.
on the pycroft, the volume skill initialisation is to be found at: /opt/mycroft/skills/mycroft-volume.mycroftai/__init__.py
if you can read python, you will see that it uses the mixer class from the alsaaudio module.  the documentation describes  this class.
class alsaaudio.mixer(control='master', id=0, cardindex=-1, device='default')

arguments are:
    control - specifies which control to manipulate using this mixer object. 
    the list of available controls can be found with the alsaaudio.mixers() function. 
    the default value is 'master' - other common controls may be 'master mono', 'pcm', 'line', etc.
        :
        :

you can check your mixer control name with the command.
more /etc/asound.conf
as an example this file is here.  you can see this usb harware mixer control name is ‘pcm’.
pcm.!default {
  type plug
  slave {
    pcm ""hw:0,0""
  }
}

ctl.!default {
    type hw           
    card 0
}

in the file /opt/mycroft/skills/mycroft-volume.mycroftai/__init__.py, here is the responsible code.   in lines 44 to 57:-
def __init__(self):
    super(volumeskill, self).__init__(""volumeskill"")
    self.default_level = self.config.get('default_level')
    self.min_volume = self.config.get('min_volume')
    self.max_volume = self.config.get('max_volume')
    self.volume_sound = join(dirname(__file__), ""blop-mark-diangelo.wav"")
    try:
        self.mixer = mixer()
    except exception:
        # retry instanciating the mixer
        try:
            self.mixer = mixer()
        except exception as e:
            self.log.error('couldn\'t allocate mixer, {}'.format(repr(e)))

i put a work around by over riding the control=master default and specifying the mixer control name to be pcm.  this can be done by changing line 51:
self.mixer = mixer()
to
self.mixer = mixer('pcm')
a proper fix may be to the rename the usb based hardware mixer name to ‘master’.
i did notice two side effects:
1. some error messages about various speaker configuration were presented when the skill volume was started.  so far it appears i was able to safely ignore these.
2. pycroft was aware of the change and would no longer automatically use git for upgrades to this skill.
overall, i plan to purse this (when time is available) to see if i can set the mixer control name for usb based hardware to master correctly and will post an answer if i can sort it out.
give it a go and see if it helps
good luck.

thanks, that seems to be shedding some light into this problem. i’ll try it later with changing the alsa to master, i don’t think we should be changing this within the skill if that is the problem.
also, in addition i found that the picroft code is also not consistent:
in auto_run.sh, the way to set volume uses pcm: amixer set pcm “${lvl}9%”
in audio_setup.sh (which is called from auto_run and is recommended to get things at startup the way you prefer), the way to set volume gives example using master: amixer set master 75%
clearly, in my setup pcm is working to control volume and master is not.
i’ll update if i’m successful on setting alsa to master but then the auto_run.sh script in the picroft image needs also to be changed accordingly.

hi again @djl
i have attempted to re-name the playback control name from pcm to master to fit the volume skill python code but i was not successful.
you have pointed to the correct issue, the call in volume skills calls simply self.mixer = mixer() leaving the default value of control to master. however, the configuration of alsa in picroft sets the name of the alsa controller as pcm.
not sure how others using the picroft image have the volume skill working or probably no-one had tested yet.
i’ll need to wait for someone with more alsa experience to take a look. i’m out of depth to fix.

i noticed the issue with the same symptoms with the guided setup for stretch lightning release.
amixer: cannot find the given element from control default.
amixer: mixer default load error: invalid argument.
running on rpi 3b+ with ps eye and usb speakers.
this topic was vaguely similar, so that’s how i landed here.
tell me if this is not related.

is there a tracking bug for this? the volume service is currently not working on picroft.

check github, and if not, file one?

must not have had any effect. i just installed and fully updated the latest of everything on a rpi 4 and requests to adjust the volume still don’t work on picroft.  so sad that we have to go to the command line to fix even simple things like this.
"
8,skill failing to load upon installation,support,"
hey all!
been beating my head against this wall almost all night, and i’m pretty sure it’s a really simple problem i’m just not seeing. i wrote a skill to interact with a nextcloud calendar caldav server. i got it “successfully” installed on my picroft with
(.venv) pi@picroft:~ $ mycroft-msm install https://github.com/markditsworth/mycroft-nextcloud-calendar.git
info - building skillentry objects for all skills
info - downloading skill: https://github.com/markditsworth/mycroft-nextcloud-calendar
info - installing system requirements...
info - installing requirements.txt for mycroft-nextcloud-calendar
info - successfully installed mycroft-nextcloud-calendar
info - invalidating skills cache

however, after a while, my skills on home.mycroft.ai/skills had not updated to include the skill and sure enough, in /var/log/mycroft/skills.log we have
2021-01-05 05:59:17.176 | info     |  1635 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-nextcloud-calendar.markditsworth
2021-01-05 05:59:17.188 | info     |  1635 | mycroft.skills.settings:get_local_settings:78 | /opt/mycroft/skills/mycroft-nextcloud-calendar.markditsworth/settings.json
2021-01-05 05:59:17.259 | error    |  1635 | mycroft.skills.skill_loader:_communicate_load_status:327 | skill mycroft-nextcloud-calendar.markditsworth failed to load

i’ve seen one other post here that seemed relevant, and that problem ended up being the create_skill() function accidentailly being part of the skill class, and that is not a problem in my code. my settingsmeta.json looks fine to me. in fact my entire skill repository was copied/modified from another skill repository that i already have working. so between that and the fact that the skill isn’t failing to install, it’s got to be some small mistake that i just can’t seem to find. if anyone can take a look to see if they see what’s wrong it’d be much appreciated!
thanks in advance!

skill repo: https://github.com/markditsworth/mycroft-nextcloud-calendar

device: picroft

","
figured it out. for some reason, uninstalling it and reinstalling it for a 5th or 6th time gave me this error output in the skill.log file:
2021-01-06 05:18:12.618 | error    |   691 | mycroft.skills.skill_loader:_create_skill_instance:289 | skill initialization failed with error('unknown extension ?<o at position 9')
traceback (most recent call last):
  file ""/home/pi/mycroft-core/mycroft/skills/skill_loader.py"", line 278, in _create_skill_instance
    self.instance.load_data_files()
  file ""/home/pi/mycroft-core/mycroft/skills/mycroft_skill/mycroft_skill.py"", line 1180, in load_data_files
    self.load_regex_files(root_directory)
  file ""/home/pi/mycroft-core/mycroft/skills/mycroft_skill/mycroft_skill.py"", line 1217, in load_regex_files
    regexes = load_regex(regex_dir, self.skill_id)
  file ""/home/pi/mycroft-core/mycroft/skills/skill_data.py"", line 107, in load_regex
    regexes += load_regex_from_file(join(path, f), skill_id)
  file ""/home/pi/mycroft-core/mycroft/skills/skill_data.py"", line 68, in load_regex_from_file
    re.compile(regex)
  file ""/usr/lib/python3.7/re.py"", line 234, in compile
    return _compile(pattern, flags)
  file ""/usr/lib/python3.7/re.py"", line 286, in _compile
    p = sre_compile.compile(pattern, flags)
  file ""/usr/lib/python3.7/sre_compile.py"", line 764, in compile
    p = sre_parse.parse(p, flags)
  file ""/usr/lib/python3.7/sre_parse.py"", line 930, in parse
    p = _parse_sub(source, pattern, flags & sre_flag_verbose, 0)
  file ""/usr/lib/python3.7/sre_parse.py"", line 426, in _parse_sub
    not nested and not items))
  file ""/usr/lib/python3.7/sre_parse.py"", line 731, in _parse
    len(char) + 2)

i had left out the p needed in the name capturing group. instead of (?p<owner>.*) i had (?<owner>.*) in a few lines. corrected that, and now it looks like the skill is installed just fine! (jury is still out on how flawless this skill works for the moment).

hey welcome to mycroft!
glad you worked it out and excited to see the skill when it’s ready to share 
"
9,better speaker than stock aiy v1,none,"
mycroft is as loud as i can get him, and sometimes he sounds a bit muffled. either the aiy hat is the root cause, or the speaker.
my main goal was to keep the mycroft assistant in a simple enclosure, containing the mic and speaker; that’s why i went for the google aiy route. the box i 3d printed works great.
i can’t get any info on the 3"" speaker that came with the kit, so i am curious if this is better or practically the same:
https://www.amazon.com/grs-3fr-4-range-speaker-driver/dp/b00k2esjz2/ref=sr_1_9?dchild=1&keywords=3""speaker&qid=1609810777&s=electronics&sr=1-9
edit: i don’t think a 3.5"" speaker will fit in my enclosure, unless i go to tinkercad and redesign the whole thing…
","
any 1w+ speaker should work.  the board doesn’t provide a huge amount of power.  they recommend using an external power supply and if you implement stereo, an additional amp module:



voice v1



voice v1
build your own natural language recognizer and connect it to the google assistant







@goldyfruit the only notation i found on the stock speaker was 3w then some other random numbers. the speaker above is 10w. i went ahead and ordered it.
i was actually just hoping for a better mono sound, and didn’t even notice the stereo instruction on that page. now, i’m really interested in that setup.
i’d like to pick your brain here. i want to see if i’m thinking of an idea correctly. for now, i’m going to stick with a single speaker mycroft. if i add that amp board, and connect my speaker to it, then i would have better amplification on my mono setup? that is, assuming i don’t cut the right channel jp4 pin just yet. and to complete that better sound setup, i would need to supply direct 5v to the hat?




 nerlins:

@goldyfruit the only notation i found on the stock speaker was 3w then some other random numbers. the speaker above is 10w. i went ahead and ordered it.


i didn’t say anything xd

what do you think of the second part of my response? would the adafruit amp board work as a better amplifier than the stock aiy board for a single speaker, or practically the same? i really can’t find anything online of someone installing that board on the voice hat.

honestly, the more i read, i don’t think it would work. seems to be a stereo only solution

i think that it’s a question for @gez-mycroft which is working on the mark ii.
have a look there:



github



mycroftai/hardware-mycroft-mark-ii
mycroft's mark ii rpi mechanical, electrical and industrial designs  - mycroftai/hardware-mycroft-mark-ii







@goldyfruit i just now realized i responded to you and not @baconator , earlier. oops… 
i appreciate you adding input though.

i have no direct experience with the stereo aspect of things, though it is tempting.

i found one guy with a youtube video, tracked the user over to twitter, then found him on linkedin. i shot him a direct message.
now i kind of feel like a stalker… 

yeah, the pi itself is pretty under powered for audio output so the sj201 (audio front-end board for the mark ii) includes an amp and everythings powered from a 12v barrel. we have swapped to an i2s audio amp (tas5806md) but the adafruit @goldyfruit linked to is probably the kind of thing you’re after.
the whole reason we ended up building the sj201 is because there isn’t really anything out there that gives you the whole package. once we get the mark ii’s pumping out, we may sell the boards themselves as i know we aren’t the only ones looking to solve this problem.
"
10,mycroftos a bare minimal production type of os based on buildroot,mycroft project,"
a small teaser type of annoucement was already made in my “journey” topic elsewhere on these forums.


j1nx – 19 sep 18



[dev] mycroftos - a bare minimal os based on buildroot - part 1 - j1nx
this is a very early post (probably way to soon anyway, but…) in the [dev] mycroftos project blog series. mycroftos is a bare minimal os based on buildroot which has only one purpose which is running mycroft without any further additional packages...






however as i am getting close to something that will be downloadable and to be flashed onto a sd card, i would like to open this second topic dedicated to the mycroftos project that i am running on my blog/website.
what is mycroftos
mycroftos is a bare minimal os based on buildroot to run the mycroft a.i. software stack on embedded devices such as the raspberry pi’s, chinese android boxes, asus tinker board, etc.
perhaps in the future an alternative for picroft and/or the mark-1 device. however only if you just want to use mycroft as end consumer type of device.
what mycroftos is not
mycroftos is not suitable as a development platform. development requires you to installl development headers/libraries/dependencies which all are not included within mycroftos nor easily be installed. mycroftos should be considered a production type of release for end consumers.
sourcecode
the sourcecode is released on github.



github



j1nx/mycroftos
mycroftos is a bare minimal linux os based on buildroot to run the mycroft a.i. software stack on embedded devices. the software stack of mycroft a.i. creates a hackable, privacy minded, open sourc...






latest development
the latest update on the development status has just been pushed to my website.


j1nx – 3 oct 18



[dev] mycroftos - a bare minimal os based on buildroot - part 2 - j1nx
in my previous first blog post about the mycroftos project, i informed you all about the start of the project. now i did mention in that blog post that it was most likely a bit premature to already blog about it. however in the last few weeks i have...






please let me know your thoughts, ideas, comments, etc. i am dutch so can handle “direct” responses, so any feedback, both positive as negative are more than welcome. i hope to learn a lot from this and know for sure, i can learn a ton from you guys.
","
can use some help from the experts / guru’s. running into some monkey business and just don’t see it anymore ?!


github.com/mycroftai/mycroft-core





issue: incidental findings from creating a buildroot package


	opened by j1nx
	on 2018-09-14




my findings while creating a buildroot package
i am in progress of creating a bare minimal embedded os for mycroft. still in...







running the services does not adhere the population of “~/” for the location of the configuration file within the users it home directory. it is most likely not a mycroft issues, more like a busybox init issue, but who knows you guys see it. (getting a bit blind for that stuff atm)

a first (very) early alpha release is ready. still a lot of things to fix/change/create/etc however it looks like i have all mycroft stuff working one way or the other. i could use some feedback, both positive as negative.
so if you have some free/spare time and a spare sd card, please give it a go and let me know your thoughts. however please have a look at the information section as this is like i said a very early release and still have some quircks.
mycroftos_0.1_rpi3_alpha2
you can download the image in 7z archive which is about 180 mb in size here;
https://j1nx.stackstorage.com/s/diut4deeb0ubr9l
in case you can’t handle the 7z archive, an uncrompressed img file which can be written to sd card straight away here (however this file takes a bit longer to download with it’s size of 1 gb);
https://j1nx.stackstorage.com/s/uaaigj4a0inh9we
if you are interested in the development, or want to check or build it yourself. all sources are available on github;



github



j1nx/mycroftos
mycroftos is a bare minimal linux os based on buildroot to run the mycroft a.i. software stack on embedded devices. the software stack of mycroft a.i. creates a hackable, privacy minded, open sourc...






information

linux kernel 4.14 (lt)
buildroot 2018.11.x
mycroft 18.08.11
raspberry pi 3b

i have a raspberry pi 3b, but i believe it should also boot and work on the normal rpi3 and rpi3b+. if you have one of those, please by all means give it a go and report back to me.
quircks
the system has the lan configured for dhcp. wifi has not yet been covered. so first boot with a lan cable connected.
find the ip of the device by any method you like. you can ssh into the box. the default credentials are;

user: root
passwd: mycroft

change the password by the normal linux command; “passwd” !!!
if you rather want to use the wifi, you need to manually edit the wpa_supplicant.conf file. in the next version, this will be handled by advertising the device as a hotspot;


github.com/j1nx/mycroftos






mycroftos: initial start of wifi-setup


  by j1nx
  on 10:05am - 01 nov 18 utc


3 commits
  changed 6 files
  with 60 additions
  and 0 deletions.







the filesystem is 1gb in size and the plan is to have this to be auto-expanded at first boot. however, that part is still one of the quircks. however you can run the script manually and then it works ?!? ssh into the device and run the command;

/etc/init.d/s00resize_sdcard start

this should expand the filesystem over the whole size of the sd card.
this image also has the drivers for the respeaker hat, so if you have one of those, again by all means give it a go and let me know the outcome. however;
i have no clue if standard usb microphones are working and that is where “you” come in. i will have a look to buy some of the mostly used mic’s but until then, you can see if you a) they work out of the box, or b) configure them manually over ssh and let me know the steps.
bare in mind that at the very first boot, mycroft does start but also is downloading all the default skills. so first boot takes a bit longer. the default 3.5 jack of the rpi is configured as default and mimic is available locally, so at some point you will hear the famous robotic words; i need to be activated, blah blah.
one other quirck that is still there is the issue with ncurses. the mycroft cli does not look good, however it does work. pure cosmetics…
the standard scripts to start/stop mycroft are available so you clould easily ssh into the box and run the ;

start-mycroft all
start-mycroft debug
start-mycroft cli
stop-mycroft all
etc

please feel free to report any issues here or even better on the issue tracking of github. puul requests are welcome as i am the only developer at the moment.
enjoy!

@gez-mycroft i wasn’t aware of the “modding” section on the forums. i believe that is a better section for this topic to be placed. (i am developing on the rpi atm, but has nothing todo with picroft other then that)
is it possible this topic get’s moved over there?

hi, i don’t have anything important to add other than i wanted to say great work and i’m glad you’re working on this. every last drop of performance we can get out of a pi will help the experience.
how is it progressing since the last post? also, how much work do you think would be required to get this working on different boards than pi?

thanks, much appreciated. to answer your questions;


progress, well not much or nothing at all. at the moment my daytime job is consuming most of my time. the small bits of time left are spent on the family and getting my home assisant installation up to specs.


adding other boards would be rather “simple”. as long as the linux kernel sources are available, they should be easy to add. you could consider mycroftos (buildroot) as the overlay filesystem / os. so if the kernel and drivers are available and the right confguration files are made, it should be easy to support more devices. as other projects using buildroot are also supporting them, here is an initial list of possible boards that could be integrated as soon as the rest of the os is in a workable state;




github.com


j1nx/mycroftos/blob/master/documentation/boards.md
# boards that are or can be supported.

## supported hardware:

### raspberry pi

| device | status | board |
|--------|-----------|-----------|
| raspberry pi a+/b/b+ | unknown, most likely under powered | n/a |
| raspberry pi zero | unknown, most likely under powered | n/a |
| raspberry pi zero w | unknown, most likely under powered | n/a |
| raspberry pi 2 b | unknown, could possibly work | n/a |
| raspberry pi 3 b/b+ | works | rpi3 |

## unsupported for now, but perhaps possible in the future

### hardkernel

| device | status | board |
|--------|-----------|-----------|


  this file has been truncated. show original






i need a bit of help, because i think my rpi development board has a broken wifi/bt of some kind. never ever used the wifi so far, other then some quick testing with raspbian when the pi arrived. quickly switched to a lan conection and did not look back.
however, for mycroftos i am now at the step to have a go at the wifi/network setup wizard system and could not get the wifi up. thought is has something todo with software as that is what mycroftos is all about. however after countless tries, i finally was smart enough to flash a fresh raspbian image and also no wifi there…
so i have pushed the current state of mycroftos online to download for someone that is willing to help me out a bit. you can download it here;
https://www.j1nx.nl/downloads/mycroftos-0-1-0-alpha3-rpi3/
if anyone has some time, a rpi3 and a sd card available, could you please download and flash the image to boot into mycroftos. wifi is not configured, so you need a lan cable as well.
so in short;

download and flash image to sd card
boot the rpi
wait a bit as the first boot prepares some stuff including expanding the image over the sd
ssh into the rpi over the lan and it’s ip it got/have
log in with; mycroft/mycroft

if someone could then give me the output of the command “dmseg”
secondly. what happens is you run the command “sudo ifconfig wlan0 up”
hope someone can help me out a bit here.

according to my experience in europe i would suggest you to look at the picroft installation and edit the wpa_supplicant.conf file manually, as i did.
you might change your wpa_supplicant.conf`
country=us <-- to you country code nl
network={
        ssid=""mynetworkssid""
        psk=""mypassword""
}

thanks, but that is not what this is about. this is about the kernel not seeing / initiating the wifi hardware.
even if i load the kernel driver itself manually, all it does is loading the firmware, but is does not see the chip, so you can’t bring it up nor configure it. at this point i am down to two possible causes;

missing/wrong dtb overlay
hardware failure

considering the fact that a fresh raspbian also doesn’t initiate the hardware i suspect it is caused by the 2nd. but to make sure, i need to test it on a second rpi. the problem is, i have only one myself.

i have uploaded a new alpha version for people to test. this is the fourth alpha version.
information

linux kernel 4.19.x
buildroot 2019.11.x
mycroft 19.08.4
raspberry pi 3/3b/3b+/(4)

i have a raspberry pi 3b, but i believe it should also boot and work on the normal rpi3 and rpi3b+. if you have one of those, please by all means give it a go and report back to me.
with buildroot 2019.11.x there is also raspberry pi 4 support. i compiled it blind for the rpi4, so untested. if you feel adventurous you could give it a go. again, please report back, both positive and negative.
at the very first boot, the os get’s expanded over the full size of the sd card and mycroft when started will first download and install all default skills including all the python packages via the requirements.txt within all those skills.
all sound input and output is forced to pulseaudio which has acoustic echo cancellation enabled.
respeaker rpi hat’s are supported, all tested with my 4mic hat, but the 2mic, 4mic lineair and 6mic should be supported as well.
quircks
the system has the lan configured for dhcp. wifi has still not yet been covered. so first boot with a lan cable connected. wifi setup manager is very high on the todo list and a requirement before i can call the next version “beta”
find the ip of the device by any method you like. you can ssh into the box. the default credentials are;

user: mycroft
passwd: mycroft

change the password by the normal linux command; “passwd” !!!
if you rather want to use the wifi, you need to manually edit the wpa_supplicant.conf file over ssh, so you need to boot with a lan cable first.
downloads
raspberry pi 3
https://www.j1nx.nl/downloads/mycroftos-0-1-0-alpha4-rpi3/
https://j1nx.stackstorage.com/s/dqxgcqp058t8vhi
raspberry pi4
https://www.j1nx.nl/downloads/mycroftos-0-1-0-alpha4-rpi4/
https://j1nx.stackstorage.com/s/nkmnt1ongmtzn2c

hmmm, strange. if you click the j1nx.nl redirect link, it doesn’t proper redirect, while if you copy&paste the link, it works ?!?! (perhaps an issue with the forum upgrade)
anyhow, included the redirect links in above post.
edit: images can also be downloaded from github



github



j1nx/mycroftos
mycroftos is a bare minimal linux os based on buildroot to run the mycroft a.i. software stack on embedded devices. the software stack of mycroft a.i. creates a hackable, privacy minded, open sourc...










 j1nx:

i have uploaded a new alpha version for people to test. this is the fourth alpha version.


gave it a quick try on my rpi3b with aiy-voicekit:
wifi does not come up (changed wpa_supplicant of course)
no sound (obviously as aiy-voicekit is not supported yet)
mycroft-cli-client fails with error message:
  file ""/bin/mycroft-cli-client"", line 11, in <module>
    load_entry_point('mycroft-core==19.8.4', 'console_scripts', 'mycroft-cli-client')()
  file ""/usr/lib/python3.8/site-packages/pkg_resources/__init__.py"", line 489, in load_entry_point
    return get_distribution(dist).load_entry_point(group, name)
  file ""/usr/lib/python3.8/site-packages/pkg_resources/__init__.py"", line 2852, in load_entry_point
    return ep.load()
  file ""/usr/lib/python3.8/site-packages/pkg_resources/__init__.py"", line 2443, in load
    return self.resolve()
  file ""/usr/lib/python3.8/site-packages/pkg_resources/__init__.py"", line 2449, in resolve
    module = __import__(self.module_name, fromlist=['__name__'], level=0)
  file ""/usr/lib/python3.8/site-packages/mycroft/client/text/__main__.py"", line 21, in <module>
    from .text_client import (
  file ""/usr/lib/python3.8/site-packages/mycroft/client/text/text_client.py"", line 39, in <module>
    locale.setlocale(locale.lc_all, """")  # set lc_all to user default
  file ""/usr/lib/python3.8/locale.py"", line 608, in setlocale
    return _setlocale(category, locale)
locale.error: unsupported locale setting

here is output of locale
lang=de_de.utf-8
lc_ctype=""de_de.utf-8""
lc_numeric=""de_de.utf-8""
lc_time=""de_de.utf-8""
lc_collate=""de_de.utf-8""
lc_monetary=""de_de.utf-8""
lc_messages=""de_de.utf-8""
lc_paper=""de_de.utf-8""
lc_name=""de_de.utf-8""
lc_address=""de_de.utf-8""
lc_telephone=""de_de.utf-8""
lc_measurement=""de_de.utf-8""
lc_identification=""de_de.utf-8""
lc_all=

how does it know that i am from germany ?
command export lc_all=""c"" helped, now cli is coming up and i could successfully pair the device
msm install picroft-google-aiy fails with
error: could not find a version that satisfies the requirement rpi.gpio (from versions: none)
error: no matching distribution found for rpi.gpio

quick test with what time is it and what is the weather via cli worked fine.

perfect feedback!
had a very busy day yesterday, but will dig into it today and report back.




 dominik:

gave it a quick try on my rpi3b with aiy-voicekit:


again, thanks. much appreciated that some “other eyes” are looking at it and different hardware at a different location. there is only so much that you can test yourself.



 dominik:

wifi does not come up (changed wpa_supplicant of course)


right! to be honest, because my rpi3 board is faulty if it comes to the wifi chip i did not really spent time on it. however didn’t realise it was this bad. will look at it with another wifi chip/dongle. although i am still not sure how to tackle the wifi setup, getting the wifi up manually should ofcourse still work.



 dominik:

no sound (obviously as aiy-voicekit is not supported yet)


google aiy voicekit is next on the list. still investigating how to use pulseaudio and udev together to automatically configure the device if aiy is found at boot.



 dominik:

mycroft-cli-client fails with error message:


at the very fist boot and therefor first start-up of mycroft, all default skill get downloaded and with that it will pip install a whole bunch of additional python packages via their requirements.txt
this take a few minutes to fully complete. runningh the mycroft-cli-client before this process is finished will give an error about some sort of python package not being found. this will resort itself as soon as everything is installed.
i need to figure out, how to communicate this to the user. however that part is on the nice to have list atm.
with the last buildroot upgrade, python also got upgraded to 3.8 however a lot of wheels are not present for 3.8 yet. and as we do not have any compilers and such within buildroot at run time, we can not install from sources (for the c related packages that it is)
i am compiling a new version at the moment, that reverts back to python 3.7 till all the wheels are there.



 dominik:

how does it know that i am from germany ?


to be honest. no clue 



 dominik:

command export lc_all=""c"" helped, now cli is coming up and i could successfully pair the device


thx, will include this simple command at boot time. this to make sure it works everywhere in the world.
anyhow, as soon as a new build with python 3.7.4 is ready i can have another look at the other things.

uploaded a new version, reverted back to puildroot 2019.08.3 which included python 3.7.4 instead of 3.8.
https://j1nx.stackstorage.com/s/r4g2dol4tdosg4m
@dominik if you have the change are willing to, could you give it a go? not touched the wifi stuff yet. just wanted to know if this is a better base to build upon.

download was a bit slooooow today, but here we are:

no more locale issues!
connected my respeaker 4mic usb v2… works perfect!
configured wpa_supplicant but doesn’t work


great, thx!
haven’t touched wifi as of yet. just reverted a lot of patches and buildroot.
onwards we go. wifi is next.

worked straight away for me with a ps eye. it’s throwing a “cannot get min/max values for control 2” for that device, but it doesn’t seem to affect its operation.
also i have it plugged into a monitor and it selected the hdmi rather than the 3.5mm for audio out, but that might be preferred for a lot of people.
nice work on this!

@gez-mycroft you tested the alpha5 version, right?

yep - that was alpha5
"
11,no registration code after snap install please wait a moment as i finish booting up,support,"
good evening,
i installed mycroft via snap package on my elementary os. unfortunately i’m not able to obtain any registration code, and the program seems to be stuck on “please wait a moment as i finish booting up”.
how can i move on?  thanks in advance for your help!
luca
","
hi luca,
i ran into the same issue trying to install the snap.  i mistakenly thought it would be harder than using github.  i was never able to get the snap to work beyond the point where you’re stuck now.
https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/get-mycroft/linux#start-mycroft-sh
it only takes a few minutes to set up and everything worked.
good luck!

the snap isn’t well supported, and the github method is much preferred to get working.

wouldn’t it be better to remove the package or, at least, warn people of this problem? 
thanks guys, i’ll try compiling the program 
luca

no idea who maintains the snap, so…?

i may also have this issue. i am not a native user. someone suggested simon however mycroft appeared to be the more current application and had an active community. the only other choice really that was fast and usable was google chrome. i am not looking for internet of things, just voice to text really, but prefer to use open source. i don’t like to support big tech companies too much and i believe in open source.
i am thinking to uninstall, how long should you give the install?
how hard is it to install manually? i just checked the posted link above and it’s all dutch to me. i dont even have a microphone set up yet i just wanted to have a look and have a go.

which platform are you trying to use?  for instance i am using an ubuntu pc.
for me the snap approach was a total fail.  at the same time the github install was a breeze.  not counting my time to read about git, mycroft install from git took perhaps 60 minutes but i wasn’t timing it.  that included me working through getting a mycroft account, etc.

yeah sadly the snap is not working so far.

hey, we adopted the snap from a community member with the intention to get it up and running. fixed a whole lot of bugs and it’s now tied into our ci process so it will receive any general updates but unfortunately there are still some major usability bugs. in the meantime i’ve added a warning in our docs and the main website to direct people to the git clone method for now.
"
12,mark ii update november 2020,mark ii,"
originally published at:			https://mycroft.ai/blog/mark-ii-update-november-2020/
this month most of the team braved the airports and came together in person for a two week sprint. the aim was to have a fully working device by the time we went home. spoiler alert – we did it!
obligatory covid note:
we took a lot of precautions to pull this off. with everyone taking tests before, during and after the sprint. remaining isolated until test results were confirmed. using the best ppe available. everyone is now safely back home with negative tests.
audio output
very quickly we discovered that the sj201 rev 3 had an issue reliably outputting audio. this hadn’t come up in our testing to date as the sound works when first booted. however after varying amounts of time the output cut out and would not return until rebooting the device.
as you can imagine, this was an unexpected turn of events, consuming a sizable portion of the sprint efforts.
the issue was tracked down to an incompatibility in the usb of the xmos chip and raspberry pi 4. it works fine on a pi 3 but that causes other problems, so we decided to work around it and use the i2s bus instead. as a temporary work around, we were able to add a jumper from one of the pi’s gpio pins to the xmos chip allowing us to reset the chip in software anytime the failure occurred. this meant we could continue prototyping with the current board, however it is not a final solution.
sj201 rev 4
as we mentioned in the last update we were already planning another revision of the board in order to switch the audio output to use a direct i2s connection. the change to i2s will also address this audio output issue as we will no longer be needing the usb driver.
our expectation is that this will be the final revision before we start shipping the mark ii dev kits.


mark ii dev kit with 3d printed audio chamber2560×1707

mark ii dev kit with 3d printed audio chamber

added in rev 4

i2s audio amp tas5806md
changed audio path from usb to i2s. better quality and gets around bug in
xmos/raspberry pi 4 over usb.

removed in rev 4

i2s to line out ic
max9744 amp

pantacor
an exciting new addition to the mark ii is pantacor, a comprehensive and robust software life-cycle management solution.
this is an open source solution that uses container technologies to securely and reliably maintain edge services on linux devices. that can be anything from home wifi routers through to industrial control systems.
cut the jargon! what the hell does all that mean for the mark ii?
most importantly this provides a very stable and resilient operating system and update service. if something goes wrong on your mark ii, the device will automatically roll back to a previously working state. this is the type of feature that you will never notice because it “just works”.
we are really excited to be working with the pantacor team. we have contracted them to set up the infrastructure and integrations needed to get us started, so we can stay focused on mycroft’s core capabilities and getting the hardware shipped.
future planning
while the team was together, we also took some time to map out some of the larger changes we want to make. we’ll be sharing these ideas and draft specifications with the community over the coming weeks and months to get your input and help prioritize these longer-term projects.
","
“by putting down a $1 deposit per speaker we’ll hold your place in line and ship them when mass production starts in late 2019 or early 2020.”
well maybe you should update the shop then…

hey there, thanks for flagging this because it looks like you found your way to our staging site which clearly hasn’t been updated in a while. it’s mostly the same as the regular https://mycroft.ai site and is just as secure but the store was meant to be deactivated which it clearly isn’t.
i’ll be fixing that up and will migrate your reservation over, but i was wondering if you remembered how you got through to that page? eg clicked a link in an email, a blog post, or just searched and clicked through? if there’s a public link sending people in that direction we’d like to get it fixed up.

hey, i just visited https://mycroft.ai and clicked on store in the top left. 
image1976×1270 212 kb


hey thanks for responding.
this navigation item (at least currently) links to the relevant store on each domain, so i’m thinking there must be a rogue link out there that’s getting people onto the test site in the first place.
will keep looking…

well it is still the same.

google “mycroft ai”, click the first result.

https://mycroft.ai loads.
click on store top left.

https://mycroft.ai/shop/ loads.
click on “reserve yours today!” under mycroft mark ii


and there you are. the page that told me the info above. now it says “by putting down a $1 deposit per speaker we’ll hold your place in line and ship them when mass production starts in 2021.”
so the description is fixed now and i do not get your story about rogue link or whatever. all i wanted to point out: update your content. i am/was an interested customer.  missinformation and old content on websites create trust issues. just saying. not sure if i really want to get a mark ii. looks nice and all, but no one finds information about the status, except in a forum here?
uff.

yep, to be clear, you ended up at: https://test.mycroft.ai/ not https://mycroft.ai
and reserved a mark ii via the staging site at this link: https://test.mycroft.ai/product/reserve-your-mark-ii-with-a-small-deposit/
that’s where the outdated text was which i’ve now updated and added the bright orange warning on the front page.
you aren’t the only person to have reserved a mark ii on the staging site either, which is why i believe we have a rogue link pointing there. the other (less likely) possibility is that google or another search engine decided to return our staging site as the primary result for some people or some period of time.
the updates for the mark ii are all on our blog, kickstarter and indiegogo. once we get the dev kits out in the world and get closer to shipping the production units we’ll be able to show more of the end-to-end product experience.
if you have any questions, happy to answer them 

well, i never saw the “test” in the url. i always went the path i described above. just btw: when i google “mycroft ai” the very first result is https://mycroft.ai and now i see the following page.

image2294×1364 164 kb

from my point of view it is broken now. as a user i always search for a product on google or other sites. the first result is the most clicked… and now shows the above. looks like: oh no we stopped the project, no one is taking care of it.
i guess the test.mycroft.ai site is somehow merged with the non test site. otherwise i can’t explain the issue.
"
13,integrating custom modules in a custom skill,support,"
hey all!
i’ve been working on expanding a custom skill i have for my picroft that allows me to issue commands to my computer. primarily for opening ides, cad tools, terminal, etc. i would like to have the option to be able to specify which working directory to use when launching an application. i had the idea of using a parser expression grammar to extract the intended directory from the utterance in the intent handler. this would be a pretty easy way to cover a lot of ground, lexiconically speaking, in the parsing. i used tatsu to write the grammar and generate a python source file, and initially just included it and it’s relevant files in a directory (called peg) the main skill directory like:
mycroft-remote-computer
|-dialog
|- vocab
|- test
|- peg
|  |- grammar.ebnf  # the formulated grammar
|  |- parser.py                   # the generated parser
|  |- test.py # unit testing the grammar.ebnf
|  |- generateparser.sh # for generating parser.py from grammar.ebnf
|- init.py
|- requirements.txt
|- settingsmeta.json
this did not work. even after undoing everything in init.py that referenced the peg module, the skill was broken just having the peg directory. i suppose i could just put the peg grammar in a separate repository and put it on the rpi, but that kind of defeats the point of having mycroft-msm take care of everything. so how do you integrate custom modules into your skill? surely there’s a way other than uploading it to pypi.
skill repo: https://github.com/markditsworth/mycroft-remote-computer
device: picroft
","
there’s an example here referencing other python modules.  .kodi_tools
add an init.py to reference your modules.
here…



github



pcwii/cpkodi-skill
updated mycroft.ai kodi skill using the common play framework - pcwii/cpkodi-skill







thanks! though i just discovered the regex files option, so i might be able to get away with just using those. if not, thanks for the example!
"
14,google aiy v1 with mycroft 20 08 no mic,mycroft project,"
i am getting really confused and frustrated with this.
first, i cannot find any way of downloading and testing prior images to 20.08. i’m not sure if anyone is aware, but if you go to a previous github release page and click on either the stable or unstable it always redirects to this current picroft_v20.08_2020-09-07.img.
if i install this image i can run picroft, but the mic does not work. it acts like it sees it, but the db level is always stock at 7 or something. i did not install the aiy skillset at this point, and wasn’t able to get very far audio troubleshooting.
i then installed the current google aiy voice image and the mic worked, but recorded in low audio. this proved to me that my mic is functional. when this raspbian version completed its first system update the mic stopped working.
after these failed attempts, i installed this found in the forums: mycroft with google aiy voice kit disk image . the mic worked just fine. mycroft could hear and respond to me. i didn’t like the idea though, since it was based on the mycroft 9.14 image and was 2 years old. anyway, after i rebooted, this final mycroft image decided to do a forced upgrade to 20.08. it was taking extremely long after the unpacking, so i just turned off the system and back on. at that point i had killed the installation.
so what gives?? i thought the aiy was confirmed as working? and i thought the aiy skill install for 20.08 would only be for the top button. why is the mic not working in the newest release? and, should i just use this hacked release from the forums and give it more time to update?
","
i’d try and let the update complete.  stopping it certainly won’t leave it in a happy place, and seems to be the best option you have at the moment.

i don’t understand what that means. are you saying that the newest release image doesn’t work with the aiy kit? i attempted to install it again and this time installed the skill. i used the command ‘msm install google-aiy-voicekit’. it acted like it installed but the light didn’t come on, and the mic still did not work.

i haven’t tried that particular combination recently, so i’m not entirely sure what to tell you about it.  if i get a chance this weekend 'll try and load it up.

what files can i extract from the working 9.14 and copy to the 20.08? obviously there is a .conf working in one and not the other. i figured i could extract the files from the old image, then edit them in the running new one.

i’ve opened an issue on the github page. this is my current update to that issue:
after hours of troubleshooting, this might be the fix: https://github.com/google/aiyprojects-raspbian/issues/724
the new pulseaudio breaks the aiy drivers. running the command in that post makes the mic work. i am still making sure everything else works. i wouldn’t call this fixed just yet.

thx for investigating - by coincidence i reactivsted my aiy this evening and after working my way through the troubleshooting guide i found your post. well i will reflash and try to run the fix
is this the exact command you run :” sudo rm /lib/udev/rules.d/91-pulseaudio-rpi.rules” and how can i check that it was applied?
cheers

that is the command. i just rebooted and it worked.
and, in response to baconator’s first suggestion to let the hacked image wait and update; it doesn’t work. i attempted it, and it is stuck at ‘this is a major branch update, cleaning default skills’ for two hours. the official 20.08 release seems to be the only way to go, but i am still not 100% everything is working after running the pulseaudio command.

well will give it a try after some hours of sleep 

the “sudo rm /lib/udev/rules.d/91-pulseaudio-rpi.rules” did not work but when i rerun the setup wizard “mycroft-setup-wizard” in ssh and chose option 4  “4) matrix voice hat.” and after several restarts it works with the latest image. geat moment to see the mic levels change 

can someone please post the content of that “91-pulseaudio-rpi.rules” file?
perhaps i can see what is wrong. the aiy v1 works perfectly with the latest linux kernel and pulseaudio on openvoiceos (former mycroftos).
all i have to do is add the “dtoverlay=googlevoicehat-soundcard” to config.txt and everything works… perhaps i can see what they do differently in the lastest raspberryos.

@j1nx please take a look at this post i found:



gitlab



microphone not recognized (hat googlevoice kit aiy rev1) after upgrade to...
hi everyone here's what i see before updating pulseaudio to rpi2 version:





the new rpi images break aiy voicehat with new pulseaudio settings. i might try downgrading like this person did. i am doing a fresh install using the recipe build method. if the mic works out of the box, then you’ll need to wait a few days before i break it with a sudo upgrade or something. i need to order another couple of microsd cards to copy my working install over to.

@j1nx where is the config file you are editing? can you share any other modifications you made to get the aiy soundcard to work?

@j1nx this is what is in the 91-pulseaudio-rpi.rules file:
subsystem!=“sound*”, goto=“end”
action!=“change”, goto=“end”
kernel!=“card*”, goto=“end”
env{sound_form_factor}!=“internal”, goto=“end”
attrs{id}==“b1”, env{pulse_profile_set}=“rpi-hdmi.conf”, goto=“end”
attrs{id}==“b2”, env{pulse_profile_set}=“rpi-hdmi.conf”, goto=“end”
env{pulse_profile_set}=“rpi-analog.conf”
label=“end”
and this is what is recommended to change:
subsystem!=“sound*”, goto=“end”
action!=“change”, goto=“end”
kernel!=“card*”, goto=“end”
env{sound_form_factor}!=“internal”, goto=“end”
drivers==“bcm2835_audio”, attr{id}==“b1”, env{pulse_profile_set}=“rpi-hdmi.conf”, goto=“end”
drivers==“bcm2835_audio”, attr{id}==“b2”, env{pulse_profile_set}=“rpi-hdmi.conf”, goto=“end”
drivers==“bcm2835_audio”, attr{id}==“headphones”, env{pulse_profile_set}=“rpi-analog.conf”, goto=""end
label=“end”

right, it looks there is more going on then i thought.
with the new vc4 drivers they also use the new sound from linux instead of the firmware blobs. hence the reason they configured the “rpi-hdmi.conf” profile.
as the aiy v1 looks like a copy of the old sound driver (sound_soc_simple) it looks like they push to much into that pulseaudio profile configuration.
isn’t that updated 91-pulseaudio-rpi.rules you posted not working?

changing the rules file has it working, but i can’t get my wake word to recognize.
edit: at first the wake word would not work, then i realized i needed to set it to precise model.

make sounds and check the mic monitor within the mycroft-cli-client
if it moves when making sounds, your mic is working.

yes it’s working, but i am having a lot of trouble with the wake word. it defaults to sphinx on boot. when i type to switch to precise mycroft hears my first command, then no other wake word commands afterward.
i am just trying to use the default ‘hey jarvis’.
is this what should be after the command mycroft-config edit user?
{
“max_allowed_core_version”: 20.8,
“listener”: {
“wake_word”: “hey jarvis”
},
“hotwords”: {
“hey jarvis”: {
“module”: “precise”,
“sensitivity”: 0.5,
“trigger_level”: 3
}
}
}

hi @nerlins
i’m not sure that there is an existing precise hey jarvis  wakeword trained, you will have to use pocketsphinx for that.

@goldyfruit obviously there is a model, because it is in the account.mycroft.ai choices; along with hey ezra, christopher, and hey mycroft.
edit: unless those models are not precise models. i’m just getting a handle on all of this.
i think i have figured it out. i was pronouncing the phoneme of jarvis in my office, with jarvis in my little shop room across the hall. he heard me. i’m slowly dialing the mic volume down in alsamixer, and i can walk closer and closer to him with wake word commands. essentially, i had to be 20 ft away with the default settings for him to hear me clearly. this is still using the above .conf text from my earlier post.
"
15,error switching wake word,support,"
i tried to switch to a new wake word like hey jarvis.
however when i switch to it via the cli i get this erro back:
2020-05-02 12:48:20.199 | info     |  3186 | mycroft.client.speech.listener:create_wake_word_recognizer:323 | creating wake word engine
2020-05-02 12:48:20.199 | info     |  3186 | mycroft.client.speech.listener:create_wake_word_recognizer:346 | using hotword entry for hey jarvis
2020-05-02 12:48:20.199 | warning  |  3186 | mycroft.client.speech.listener:create_wake_word_recognizer:348 | phonemes are missing falling back to listeners configuration
2020-05-02 12:48:20.200 | warning  |  3186 | mycroft.client.speech.listener:create_wake_word_recognizer:352 | threshold is missing falling back to listeners configuration
2020-05-02 12:48:20.200 | info     |  3186 | mycroft.client.speech.hotword_factory:load_module:386 | loading ""hey jarvis"" wake word via precise
2020-05-02 12:48:22.024 | warning  |  3186 | mycroft.client.speech.hotword_factory:initialize:401 | could not found find model for hey jarvis on precise.
2020-05-02 12:48:22.025 | info     |  3186 | mycroft.client.speech.hotword_factory:load_module:386 | loading ""hey jarvis"" wake word via pocketsphinx
2020-05-02 12:48:22.026 | error    |  3186 | mycroft.client.speech.hotword_factory:create_config:105 | pocketsphinx model not found at /opt/mycroft/mycroft/client/speech/recognizer/model/de-de/hmm
2020-05-02 12:48:22.030 | error    |  3186 | mycroft.client.speech.hotword_factory:initialize:406 | could not create hotword. falling back to default.
traceback (most recent call last):
  file ""/opt/mycroft/mycroft/client/speech/hotword_factory.py"", line 394, in initialize
    instance = clazz(hotword, config, lang=lang)
  file ""/opt/mycroft/mycroft/client/speech/hotword_factory.py"", line 91, in __init__
    self.decoder = decoder(config)
  file ""/opt/mycroft/.venv/lib/python3.6/site-packages/pocketsphinx/pocketsphinx.py"", line 277, in __init__
    this = _pocketsphinx.new_decoder(*args)
runtimeerror: new_decoder returned -1

here the entry in the config file:

""precise"": {
    ""dist_url"": ""https://github.com/mycroftai/precise-data/raw/dist/{arch}/latest"",
    ""model_url"": ""https://raw.githubusercontent.com/mycroftai/precise-data/models-dev/{wake_word}.tar.gz""
  },
  ""hotwords"": {
    ""hey jarvis"": {
      ""module"": ""precise"",
      ""sensitivity"": 0.5
    }
  }

","
hi, i’ve seen that happening if you’re running a different language than en-us (the default). is this the case for you?

jep thats the problem. so as a fix i did a copy of the en-us folder to the de-de folder and now it works… strange but seems to work wright now but this can’t be the solution?

we should probably add a fallback to the english version we ship if the language specific model doesn’t exist.

yes definitely, since like “hey jarvis” you can’t pronounce this in my case in german so i will still try to say it in “proper” english anyway

i created an issue for it in the mycroft-core repo
if you feel like tackling it you’re welcome to do so.

thx a lot for opening the issue

so is ‘hey jarvis’ in the precise community database? i was pocketsphinx had to be used, and no precise models have been created.

not at this time, but feel free to send some samples and we can create a precise model for it.

@baconator the reason i am asking is because  of @gekoch original question. look at his original question. he is using hey jarvis as his wake word in a precise model. he is listing github precise locations for his model training. am i missing something?  did he make his own wake word?




 gekoch:

https://raw.githubusercontent.com/mycroftai/precise-data/models-dev/


based on the initial url, if you have a look to the github repository on the models-dev branch[1] you can see the list which doesn’t contain hey jarvis model.
maybe it has been removed, the repository doesn’t seem to be up-to-date.
[1]https://github.com/mycroftai/precise-data/tree/models-dev

@goldyfruit , i appreciate the response. when i stumbled up on this thread i was confused that @gekoch was using hey jarvis in a precise model, but i couldn’t find it in the repository.
"
16,my audio is so quiet,mycroft project,"
hello!i ve just installed picroft to my rpi3.
the thing is that even the speaker volume is set to 9(the loudest) it is too quiet.i can barely  hear it.
another problem is that even if i reinstalled 3 times,the volume never comes out from the usb sound card,but always from the jack.
last,my mic is working through the same usb card,but i can’t say if it is loud or quiet because of my speaker status.
i tried to modify my settings via “alsamixer” but when i press f4 or f6 it simply takes me to the cli again…
any ideas??it seems quite promising…it’s a pity i cann’t use it.
","
can you share some more details on your hardware setup - soundcard, amplifier, speaker?

of course!so,my hardware is a rpi3,with a compatible usb sound card.(i use it to another rpi with no problems) and a speaker via the line in port.thing is that on the setup i choose usb card and i don’t get an output.when i choose jack,the voice is so low,and it makes no difference if i choose 1,2 or even 9.
i ve changed the settings via alsamixer but it won’t save it.

do you here the confirmation sound on the usb output? does the terminal command aplay -l show your usb device? did you press f6 in alsamixer and did you choose the usb device and set master and/or pcm and/or headphone and/or pcm to value of 88 (the beginning of the red zone)?

no i don’t hear it.yes,aplay-l and arecord-l both show my usb card.i did press f6 in alsamixer.i ve chosen my usb card,but when i go again to alsamixer it still shows the default device…every value is at 100,to be sure…thing is that outside picroft(linux command line) i can hear my recordings loud and clear…but when i m in the picroft i can’t…
and i m sure that my mic works correctly because it detects the wake up word and it answers.(in the command line)

dear all,
just started to try out mycroft.
i have run in the same issue here.
a fresh install on a linux mint: linux mint 20 ulyana base: ubuntu 20.04 focal
first i had issues with the mic; input was too high and i solved it with the forum.
but now the response is ultra silence even with the setting audio out to max. so outside of mycroft the audio is loud now and within the application i can barely here the response.
fyi i am a newbie to linux but can run simple commands.
any help is appreciated.
"
17,mycroft as sip client smart voice enabled ivr,skill suggestions,"
skill name: mycroft sip
i want to be able to call sip number and talke to my mycroft assistant.
possible additional usecases could be setting up mycroft as a call center menu and etc
as a (type of user) i want this skill to have (function or feature) so that i can achieve (objective)
eg. as a music lover, i want this skill to control my music player so i can have voice-enabled music on my mycroft device
what third party services, data sets or platforms will the skill interact with?
any sip infrastructure
are there similar mycroft skills already?




[telephony] sip skill suggestions


    skill name: sip-skill 
one of the features that is frequently requested for mycroft is intra-device communication - ie communication between two or more mycroft devices on the same network. one of the ways that we could envisage this happening is through session initiation protocol - sip - which is a videoconferencing and telephony protocol used by multiple vendors across the world. 
user stories: 


as a mycroft user with multiple devices in my home, i would like a way to do voice-based device …
  


being able to use mycroft device as a sip client is a nice idea, but  without a need for integration with mycroft itself is totally doable as a remote control of a sip client installed on the same device, unlike this idea.
other comments?
also mycroft could be integrated into asterisk/freeswitch/otherpbx as a full blown ivr menu system/conversational bot with administrative functions.
","



github



agprojects/python-sipsimple
sip simple implementation for python. contribute to agprojects/python-sipsimple development by creating an account on github.






could be used fo sip implementation
as i figured from mycroft source checking, this feature have to be implemented as a part of core project(at least with current architecture) maybe this topic should be moved. or maybe even as a git issue.

worth checking out jarbas’s hive mind project for this type of use case i think



github



jarbasal/hive_mind
join the mycroft collective, utils for mycroft-core mesh networking - jarbasal/hive_mind







i made a video using mycroft as a user within our ezuce vibe product some months ago ( https://www.linkedin.com/posts/mattkeys_ai-collaborationtools-activity-6540358177748205568-7vtr ). vibe has sip and h.323 capabilities built in. both our commercial uniteme or open source version sipxcom pbx software have call control capabilities through apis ( http://wiki.sipxcom.org/display/sipxcom/call+control ).

@jarbasal github for this project here: https://github.com/openjarbas/baresipy

our company has a working code of mycroft acting as a sip endpoint. it’s doable.

there is a skill for the average user



github



jarbasskills/skill-voip
voip for mycroft. contribute to jarbasskills/skill-voip development by creating an account on github.






"
18,google aiy voicekit skill breaks wake trigger,mycroft project,"
edit: please disregard. see the thread about the mic not working for the full resolution. the wake word was not working due to the mic level being too high. once adjusted, the wake word works along with the aiy-voicekit skill installed.
",
19,mycroft gui on rpi3b,support,"
hello, i’ve been trying to get mycroft-gui working on my raspberry pi 3b+. so far i’ve tried a few operating systems (raspbian, picroft, and ubuntu 18.04 server) but apt is unable to locate the qt5-* packages necessary for the installation. i also tried ubuntu 20.04 server but ran into issues while installing kde-plasma-desktop. any advice is appreciated, thanks!
","
keep an eye out for the next openvoiceos release
plasma bigscreen is using ubuntu and runs the gui, but requires a pi4

i see there’s an ‘alpha8’ version for rpi3 with os and gui. i tried loading it onto my pi but only got a black screen with no boot menu.
"
20,picroft respeaker 2 mics pi hat,mycroft project,"
hello there,
i am having problems setting up picroft (2018-09-11 stretch lightning) using the respeaker 2-mics pi hat. i flashed the image and installed the respeaker drivers as described in the wiki. now the audio test start-mycroft.sh audiotest works but mycroft doesn’t react to the wakeword. is there anything i missed to setup? anyone got that mic working with the latest picroft image?
thx
","
hi there @timbz - if the audiotest is working, but the wake word is not, it may indicate an error with the precise wake word engine - link.
if you have a look at;
/var/log/mycroft/voice.log
and
/var/log/mycroft/audio.log
are there any precise-related messages?
if you run the cli - mycroft-cli-client - does the microphone in the bottom right hand corner constantly move?

hi,
the mic in the cli does not move but i cannot find anything in the logs
/var/log/mycroft/voice.log


summary
12:09:03.170 - mycroft.configuration.config:load_local:109 - debug - configuration /home/pi/mycroft-core/mycroft/configuration/mycroft.conf loaded
12:09:03.182 - mycroft.configuration.config:load_local:109 - debug - configuration /home/pi/mycroft-core/mycroft/configuration/mycroft.conf loaded
12:09:03.189 - mycroft.configuration.config:load_local:109 - debug - configuration /etc/mycroft/mycroft.conf loaded
12:09:03.196 - mycroft.configuration.config:load_local:109 - debug - configuration /home/pi/.mycroft/mycroft.conf loaded
12:09:03.205 - mycroft.identity:_load:44 - debug - loading identity
12:09:03.228 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
12:09:03.862 - urllib3.connectionpool - debug - https://api.mycroft.ai:443 ""get /v1/device/... http/1.1"" 200 334
12:09:03.880 - mycroft.configuration.config:load_local:109 - debug - configuration /home/pi/mycroft-core/mycroft/configuration/mycroft.conf loaded
12:09:03.886 - mycroft.configuration.config:load_local:109 - debug - configuration /etc/mycroft/mycroft.conf loaded
12:09:03.892 - mycroft.configuration.config:load_local:109 - debug - configuration /home/pi/.mycroft/mycroft.conf loaded
12:09:03.900 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
12:09:04.578 - urllib3.connectionpool - debug - https://api.mycroft.ai:443 ""get /v1/device/.../setting http/1.1"" 200 3051
12:09:04.595 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
12:09:05.282 - urllib3.connectionpool - debug - https://api.mycroft.ai:443 ""get /v1/device/.../location http/1.1"" 200 4
12:09:05.309 - mycroft.configuration.config:load_local:109 - debug - configuration /etc/mycroft/mycroft.conf loaded
12:09:05.314 - mycroft.configuration.config:load_local:109 - debug - configuration /home/pi/.mycroft/mycroft.conf loaded
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.front
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.rear
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.center_lfe
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.side
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.surround21
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.surround21
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.surround40
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.surround41
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.surround50
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.surround51
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.surround71
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.iec958
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.iec958
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.iec958
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.hdmi
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.hdmi
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.modem
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.modem
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.phoneline
alsa lib pcm.c:2495:(snd_pcm_open_noupdate) unknown pcm cards.pcm.phoneline
alsa lib pcm_dmix.c:990:(snd_pcm_dmix_open) the dmix plugin supports only playback stream
alsa lib pcm_dsnoop.c:556:(snd_pcm_dsnoop_open) the dsnoop plugin supports only capture stream
alsa lib pcm_dmix.c:990:(snd_pcm_dmix_open) the dmix plugin supports only playback stream
alsa lib pcm_dsnoop.c:556:(snd_pcm_dsnoop_open) the dsnoop plugin supports only capture stream
12:09:07.063 - mycroft.client.speech.listener:create_wake_word_recognizer:243 - info - creating wake word engine
12:09:07.071 - mycroft.client.speech.hotword_factory:load_module:261 - info - loading ""hey mycroft"" wake word via pocketsphinx
12:09:07.503 - mycroft.client.speech.listener:create_wakeup_recognizer:263 - info - creating stand up word engine
12:09:07.508 - mycroft.client.speech.hotword_factory:load_module:261 - info - loading ""wake up"" wake word via pocketsphinx
12:09:07.664 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
12:09:08.277 - urllib3.connectionpool - debug - https://api.mycroft.ai:443 ""get /v1/device/... http/1.1"" 304 0
12:09:08.307 - mycroft.messagebus.client.ws:on_open:62 - info - connected
12:09:10.683 - mycroft.client.speech.mic:listen:487 - debug - waiting for wake word...
12:10:49.557 - mycroft.configuration.config:load_local:109 - debug - configuration /home/pi/mycroft-core/mycroft/configuration/mycroft.conf loaded
12:10:49.565 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
12:10:50.191 - urllib3.connectionpool - debug - https://api.mycroft.ai:443 ""get /v1/device/.../setting http/1.1"" 304 0
12:10:50.209 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
12:10:50.844 - urllib3.connectionpool - debug - https://api.mycroft.ai:443 ""get /v1/device/.../location http/1.1"" 304 0
12:10:50.888 - mycroft.configuration.config:load_local:109 - debug - configuration /etc/mycroft/mycroft.conf loaded
12:10:50.893 - mycroft.configuration.config:load_local:109 - debug - configuration /home/pi/.mycroft/mycroft.conf loaded


/var/log/mycroft/audio.log


summary
12:09:03.312 - mycroft.configuration.config:load_local:109 - debug - configuration /home/pi/mycroft-core/mycroft/configuration/mycroft.conf loaded
12:09:03.514 - mycroft.configuration.config:load_local:109 - debug - configuration /home/pi/mycroft-core/mycroft/configuration/mycroft.conf loaded
12:09:03.711 - mycroft.configuration.config:load_local:109 - debug - configuration /etc/mycroft/mycroft.conf loaded
12:09:03.911 - mycroft.configuration.config:load_local:109 - debug - configuration /home/pi/.mycroft/mycroft.conf loaded
12:09:04.103 - mycroft.identity:_load:44 - debug - loading identity
12:09:04.118 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
12:09:04.766 - urllib3.connectionpool - debug - https://api.mycroft.ai:443 ""get /v1/device/... http/1.1"" 200 334
12:09:04.972 - mycroft.configuration.config:load_local:109 - debug - configuration /home/pi/mycroft-core/mycroft/configuration/mycroft.conf loaded
12:09:05.141 - mycroft.configuration.config:load_local:109 - debug - configuration /etc/mycroft/mycroft.conf loaded
12:09:05.314 - mycroft.configuration.config:load_local:109 - debug - configuration /home/pi/.mycroft/mycroft.conf loaded
12:09:05.322 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
12:09:05.982 - urllib3.connectionpool - debug - https://api.mycroft.ai:443 ""get /v1/device/.../setting http/1.1"" 200 3051
12:09:05.999 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
12:09:06.642 - urllib3.connectionpool - debug - https://api.mycroft.ai:443 ""get /v1/device/.../location http/1.1"" 200 4
12:09:06.887 - mycroft.configuration.config:load_local:109 - debug - configuration /etc/mycroft/mycroft.conf loaded
12:09:07.105 - mycroft.configuration.config:load_local:109 - debug - configuration /home/pi/.mycroft/mycroft.conf loaded
12:09:07.524 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
12:09:08.193 - urllib3.connectionpool - debug - https://api.mycroft.ai:443 ""get /v1/device/.../subscription http/1.1"" 200 114
carnegie mellon university, copyright (c) 1999-2011, all rights reserved
mimic developers, copyright (c) 2016, all rights reserved
version: mimic-1.2.0.2 ()
12:09:09.278 - __main__:main:38 - info - starting audio services
12:09:09.292 - mycroft.messagebus.client.ws:on_open:62 - info - connected
12:09:09.314 - mycroft.audio.audioservice:get_services:58 - info - loading services from /home/pi/mycroft-core/mycroft/audio/services/
12:09:09.343 - mycroft.audio.audioservice:load_services:102 - info - loading chromecast
12:09:26.121 - mycroft.audio.audioservice:load_services:102 - info - loading mopidy
12:09:26.144 - mycroft.audio.audioservice:load_services:102 - info - loading mplayer
12:09:26.267 - mplayer__init__:<module>:20 - error - install py_mplayer with pip install git+https://github.com/jarbasal/py_mplayer
12:09:26.271 - mycroft.audio.audioservice:load_services:108 - error - failed to import module mplayer
importerror(""no module named 'py_mplayer'"",)
12:09:26.276 - mycroft.audio.audioservice:load_services:102 - info - loading simple
12:09:26.304 - mycroft.audio.audioservice:load_services:102 - info - loading vlc
12:09:26.562 - mycroft.audio.audioservice:load_services:123 - error - failed to load service. nameerror(""no function 'libvlc_new'"",)
12:09:26.567 - mycroft.audio.audioservice:load_services_callback:170 - info - finding default backend...
12:09:26.571 - mycroft.audio.audioservice:load_services_callback:174 - info - found local
12:10:49.557 - mycroft.configuration.config:load_local:109 - debug - configuration /home/pi/mycroft-core/mycroft/configuration/mycroft.conf loaded
12:10:49.567 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
12:10:50.194 - urllib3.connectionpool - debug - https://api.mycroft.ai:443 ""get /v1/device/.../setting http/1.1"" 304 0
12:10:50.213 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
12:10:50.831 - urllib3.connectionpool - debug - https://api.mycroft.ai:443 ""get /v1/device/.../location http/1.1"" 304 0
12:10:50.875 - mycroft.configuration.config:load_local:109 - debug - configuration /etc/mycroft/mycroft.conf loaded
12:10:50.884 - mycroft.configuration.config:load_local:109 - debug - configuration /home/pi/.mycroft/mycroft.conf loaded
12:10:50.893 - mycroft.configuration.config:load_local:109 - debug - configuration /home/pi/mycroft-core/mycroft/configuration/mycroft.conf loaded
12:10:50.900 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
12:10:51.557 - urllib3.connectionpool - debug - https://api.mycroft.ai:443 ""get /v1/device/.../setting http/1.1"" 304 0
12:10:51.573 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
12:10:53.084 - urllib3.connectionpool - debug - https://api.mycroft.ai:443 ""get /v1/device/.../location http/1.1"" 304 0
12:10:53.127 - mycroft.configuration.config:load_local:109 - debug - configuration /etc/mycroft/mycroft.conf loaded
12:10:53.135 - mycroft.configuration.config:load_local:109 - debug - configuration /home/pi/.mycroft/mycroft.conf loaded


thx

can you alter which microphone is selected using alsamixer?

what do you mean? i can select the mic in alsamixer
$ arecord -l
**** list of capture hardware devices ****
card 1: seeed2micvoicec [seeed-2mic-voicecard], device 0: bcm2835-i2s-wm8960-hifi wm8960-hifi-0 []
  subdevices: 0/1
  subdevice #0: subdevice #0

wait - can you do this with native alsa? i am also using the respeaker 2 mic hat, and was under the impression that pulseaudio was required.

well, the picroft image does not use pulseaudio and the google aiy seems to work with alsa.

ah - i followed this guide to install it: https://www.j1nx.nl/diy/diy-home-personal-ai-assistant-installing-configuration-part-4/ - which mentioned pulseaudio, but i’ll admit, if there is a less complicated route to output with alsa i’d jump at it.
what is in your /etc/mycroft/mycroft.conf?
i’m going to try resetting mine to not use pulseaudio - simpler is better.

$ cat /etc/mycroft/mycroft.conf
{
""play_wav_cmdline"": ""aplay %1"",
""play_mp3_cmdline"": ""mpg123 %1"",
""enclosure"": {
    ""platform"": ""picroft""
},
""tts"": {
    ""mimic"": {
        ""path"": ""/home/pi/mycroft-core/mimic/bin/mimic""
    }
},
""ipc_path"": ""/ramdisk/mycroft/ipc/""
}

i am also trying to avoid pulse audio

i’m seeing if i can setup /etc/asound.conf - perhaps i can select defaults there that will help…

ok i have wakewords working, and the mpg123 config works - but it’s not speaking to me.
/etc/asound.cnf:
pcm.!default {
    type hw
    card 1
}

ctl.!default {
    type hw           
    card 1
}

it will sing me a song, but not speak to me yet. i did have hw:1 in the mycroft.conf settings - so i’m trying to reboot without those in there.

did you use the picroft 2018-09-11 stretch lightning image?

curently i built my own on stretch using https://github.com/mycroftai/enclosure-picroft/blob/stretch/image_recipe.md#install-picroft-scripts, however, that is worth a try.

yeah, i wrote that piece of information when the new lightning release was not yet available and based on the information at hand by that time, pulseaudio was a requirement in all the info snippets here and there.
i will investigate if pulseaudio is “really needed” or an alsa only setup could be done aswell.
however, for the moment i stepped away from raspbian and focussed on buildroot instead.
as soon as i am at the sound stuff within that project, i will investigate it more. keep this thread under my bookmarks for now.

ok - tried the lightening image. i did an update and ran the seeed guide.
i had to take out the hw:0,0 references from /etc/mycroft/mycroft.conf.
i added a small section to asound.conf:
ctl.!default {
  type hw
  card 1
}

this is how i set up alsamixer to consider it the default mixer.
the mpg123 output is working, but the main output does nothing (even if i type the say skill).
i also don’t see a mic level.
digging more.

digging has lead me to https://github.com/mycroftai/mycroft-core/blob/dev/mycroft/client/speech/listener.py, which creates an intance of a mutable mic, a superclass of the microphone from https://github.com/uberi/speech_recognition/blob/master/speech_recognition/init.py. my thinking is that it’s the device_index perhaps.
listener.py uses config to get a “device_index” config. so i’ll try to find what config has for defaults for that.

no defaults - but this might work (since hte card index is 1) in /etc/mycroft/mycroft.conf:
  ""listener"": {
     ""device_index"": 1
   },

which is also the place to mess with wake words too…
nope - still no mic levels or waking.

well thats annoying. i added debug into client/speech/mic.py to log the parameters to the mutablemic, restarted mycroft, and then it seemed to work.
the only other thing i did, was cat the /ramdisk/mycroft/ipc/mic_level file to see what was in it…  it was static until i stopped mycroft and started it again. i’m going to check what happens at a reboot. in case the problem is at start up.
hmm - no dice on startup…
try stopping and starting mycroft…
and it works - something about the way it is starting - perhaps a race condition in the parts that run?
so after a reboot, the processes seem to be there, but there is one write to the mic_level file during startup and no further writes. i used debug, and it seems to get a few reads, then stall in self.pyaudio_stream.read(size, exception_on_overflow=false) (called from mutablemicrophone, passed into the mic listen loop, found in the wait_until_wake_word method). guess the next question is why that stream stalls, and then seems to be okay on a restart of mycroft (but not a reboot).
it’s not timing - sleeps dont help (the auto_run.sh has them anyway). i was comparing the env of my putty terminal with the console that auto run first starts the mycroft services, and didn’t find anything specific there. i have checked if there was a difference between source mycroft-core/start-mycroft.sh all and ./mycroft-core/start-mycroft.sh all but not seen a change in this behavior - it still only starts the second time.
i may try to log in with a screen and keyboard attached (which should put me at the console it’s started in i think), and see if a second run works or fails there, is my headless start making a difference? nope - still loads with mic levels on the second time.

you can add /etc/modprobe.d/alsa_base.conf with:
# this sets the index value of the cards but doesn't reorder.
options snd_soc_simple_card index=0   
options snd_bcm2835 index=1

# does the reordering.
options snd slots=snd_soc_simple_card,snd_bcm2835 

to change the sound card order

hey @timbz
did i understand that right? did you got the respeaker 2-mics pi hat to run with picroft?
"
21,first time user looking for advice,none,"
hello everyone,
i am new to your community and have decided to give a shot in setting up picroft on a budget (the past year hasn’t been very profitable ) . i am not a computer engineer nor do i have a lot of linux experience other than what a basic user level. i have managed so successfully set up home assistant to some extent so this gives me some confidence 
i am ordering the parts from the build and was wondering if i should go with pi3b+ or pi4 2gb ram? they cost the same so i was wondering if there is an advantage of using one over the other for this? i am aiming for ease of build and maximum compatibility since i would have problems troubleshooting some advanced issues…
also i have ordered the ps3 camera microphone, but as understand the respeaker is a good budget friendly alternative (seems to be sold out everywhere at the moment)? anything else you could recommend for a beginner?
or should i attempt to run in on the same device as home assistant is running (pi4)? i have read through a blog by j1nx from 2018, but it seems unfinished. though the instructions up to that seem clear.
any advice would be greatly appreciated
thanks,
enstain
","
the pi4 is definitely the better option in general, especially if you intend to run both ha and mycroft on it.
"
22,mark ii update december 2020,mark ii,"
originally published at:			https://mycroft.ai/blog/mark-ii-update-december-2020/
seasons greetings to everyone! 2020 has been a big year, and we hope that 2021 will be different in so many ways. first and foremost we’re excited to get devices shipping very early in the new year!
dev kit production
tl;dr: we’re shipping in february!
the development kits are our first priority, and we’re getting ready to ship! (yes, lots of exclamations in this update- we’re excited!) we have received almost all of the parts necessary to make our first production run of 400 kits, and we’re placing the order for our custom circuit board (the sj201) today. we’ve added a custom speed-controlled 40mm cooling fan (the sj222) to ensure the exposed circuit boards in the dev kits stay cool to the touch.
since we’re using a new manufacturer for this run of sj201’s, we’re doing the production in two stages. the first will be 20 boards to ensure that everything goes smoothly. in the past we’ve had companies put chips on rotated 180 degrees in the past. we will then do both automated and in-person testing of these samples, and assuming those check out we’ll immediately authorize the remainder.
all the final parts are being shipped to our fulfillment partner, crate crew. they produce hackerboxes and have years of experience doing exactly what we need – carefully packing a detailed set of electronics and other components for end-users to assemble and experiment with.
sj201 rev 6 
since our last report, we had to make two revisions to the sj201, our audio front end daughterboard. we discovered an issue with the audio output jack, and then subsequently an error in choosing a clock buffer chip. both were readily resolved and we’re now ready to go to production. 
if you’re interested in the details:


added i2s to line-out to headphone-out circuit. we promised headphone output, so when the speaker amp chip didn’t work the way we thought it did, another chip was required to enable the high-quality headphone output. note: with certain earbuds it appears to be way too loud, however we don’t expect that to be the normal use case.

removed the usb bridge chip. we simply don’t need it anymore, as we don’t have more than one usb device on board. (in fact, we have none at present.)
removed the buffers from the pi to the sj201 i2s clock lines. they were originally copied from the xmos reference design but are unnecessary and caused routing problems. (rev 6)

mark ii production
now that the dev kit production is fully under way, we’re turning our attention to the mark ii production. in a way, this is nothing more than higher-volume production of the same product as the dev kit. however there’s a lot more to it than that. starting with the fact that most people don’t want to build their own smart speakers from parts.
the major hardware change from the dev kit to the mark ii is the use of an injection-moulded housing. this is not only what people expect for an off-the-shelf product, but it’s a lot cheaper to manufacture in bulk. the first plastic enclosure will cost about $50,000. the second through 10,000th ones will cost about $1 each. overall that saves about $10 per device in cost relative to the laser-cut enclosures. the second major change is having the whole thing assembled at the factory. there goes our $10 savings in plastics. 
but before we commit to the million-dollar outlay for production parts and labor we want to have the dev kits in the field for long enough to ensure we didn’t miss anything regarding long-term stability, as well as testing the firmware and software update processes. so while we can gear up for mark ii production by getting the injection moulds underway and qualifying a final product assembly partner, we won’t have a solid ship date until we are confident in the performance of the dev kits.
","
is there provision for backers to update shipping address/country details in anticipation of the mkii shipping date? cheers

who exactly are the “dev” kits being shipped to?.. clearly not backers.

the backers who signed up for the dev kits.
“### mark ii dev kit - build it yourself” (see
https://www.kickstarter.com/projects/aiforeveryone/mycroft-mark-ii-the-open-voice-assistant)

i must have missed that

you may need to scroll a tiny bit…

devkit1297×729 239 kb

"
23,music spotify,skill suggestions,"
skill name: mycroft-spotify
user story:

as a music lover, i want to be able to play my spotify songs, playlists artists using voice commands so i can enjoy music wherever my mycroft device is.
as a music lover, i want to be able to search for songs on spotify by various attributes so i can find music during other activities such as reading.
as a music lover, i want to be able to go to sleep using spotify by playing music for a length of time, and then that music fades out, so i get a good night’s sleep.

what third party services, data sets or platforms will the skill interact with?
are there similar mycroft skills already?

https://github.com/forslund/mopidy_skill

https://github.com/forslund/spotify-skill
https://github.com/normandmickey/skill-internet-radio

not sure yet how they could be merged

what will the user speak to trigger the skill?

play {{playlist_name}}
play {{genre}}
play {{artist}}
play for {{number_of_minutes}} then go to sleep
search for {{playlist_name}} | {{track_name}} | {{album_name}}
next track | next song
previous track | previous song
what’s playing?

what phrases will mycroft speak?

now playing {{playlist_name}}
now playing {{genre}}
now playing {{artist}}
now playing for {{number_of_minutes}}
i found {{search_term}} in {{num_artists}} | {{num_tracks}} | {{num_albums}}. should i list them?
i’m sorry, i couldn’t find any music matching {{search_term}}
playing next track | song
playing previous track | song
the current track | song is {{track}}

what skill settings will this skill need to store?
see https://mycroft.ai/documentation/skills/skill-settings/ for more information

oauth information for the user’s spotify account
last played tracks so the user can do things like ‘play that last song again’

other comments?
put any other comments you think are relevant in here
","
also similar: https://github.com/forslund/spotify-skill

i think it would be appreciated to have options for “next track” or “next song”, as well as “previous track” or “previous song”. there are songs that i love, but they have to match my state of mind, and my state of mind may not be the same when i’m playing a playlist as it was when i created the playlist. oh, and let’s not forget pause and/or stop.
it also might be cool to be able to ask for information about what’s playing. you probably already know the band name and the song title (scrolling this on the screen of the mark i/ii would be awesome btw), but you may not know other details, like album name and the year it was published. that kind of thing.

theres is the internet radio skill and youtube skill also
my youtube skill works cool, usually plays what i want it to, and supports next track and previous track

what song is currently playing is really good to have. for example when running discover weekly or other generated playlist (or playlist from friends).

@forslund @linuxrants @jarbas_ai thank you so much for your suggestions, i have updated the skill specification to reflect them 

@jarbas_ai, i thought i would give your youtube music skill a try (picroft 18.02) and i am having an issue where there is no music playing. the intent is properly triggered and there does not appear to be any errors, a youtube url is found for the music i request but it just does not play. i have confirmed that vlc (cvlc) is installed and working. any troubleshooting tips to get it going?
thanks.

it stopped working at some point, im not sure why
either youtube changed something or there needs to be more steps for the vlc install…

yes, you are right said.
spotify stopped working at some point.
youtube changed any blob url or what else?

@forslund is spotify ok now?

spotify should be ok, it’s been reactivated so should be usable again.

i use mpd on the server with cantata on the client. would be fantastic if this skill could work with one/both of these.

sorry, but is a premium account necessary to use the spotify skill? i have a free one, is it possible to configure with it?

hi @patata unfortunately you must have a premium spotify skill in order to use the mycroft spotify skill. this is a limitation of the spotify api - we can only access it if the user has a premium account.

ok, i undertand. is there any other streaming music service that allows to develop a skill?

bandcamp and soundcloud are services i’d like to see getting a skill.




github



jarbasal/skill-bandcamp
skill-bandcamp - bandcamp audio skill






great work as usual @jarbas_ai, unfortunately this isn’t submitted to the mycroft-skills repo, so it won’t be available for voice installation.

none of my skills are…
im going to make a tool to auto handle skill submission, then ill get all my skills there
hopefully ill find the time for auto tests and py3 migration




 kathyreid:

this isn’t submitted to the mycroft-skills repo


excuse @kathyreid, but is there any reason for this?
could you validate and verify the skills developed by jarbas and include them in the mycroft-skills repo? or this is not possible?
"
24,installation script trying to remove packages,support,"
using the dev_setup script on linux mint 19 (based on ubuntu 18.04) tries to remove packages:
the following packages will be removed:
  blender gstreamer0.10-plugins-good ia32-libs libasound2-plugins:i386
  libavdevice-ffmpeg56 libfluidsynth1:i386 libjack-jackd2-0
  libjack-jackd2-0:i386 libsdl-mixer1.2:i386
the following new packages will be installed:
  flac gir1.2-harfbuzz-0.0 icu-devtools jq libasound2-dev libexpat1-dev
  libfann-dev libfann2 libffi-dev libglib2.0-dev libglib2.0-dev-bin
  libgraphite2-dev libharfbuzz-dev libharfbuzz-gobject0 libicu-dev
  libicu-le-hb-dev libicu-le-hb0 libiculx60 libjack-dev libjack0 libjpeg-dev
  libjpeg-turbo8-dev libjpeg8-dev libjq1 libonig4 libout123-0 libpcre3-dev
  libpcre32-3 libpcrecpp0v5 libportaudiocpp0 libpython-dev libpython2.7-dev
  libpython3-dev libpython3.6-dev libssl-dev libtool mpg123 portaudio19-dev
  python-dev python-gobject-2-dev python-setuptools python2.7-dev python3-dev
  python3.6-dev screen swig swig3.0 uuid-dev zlib1g-dev
0 upgraded, 49 newly installed, 9 to remove and 0 not upgraded.

why does it conflict with blender and ia32-libs? i need both. willing to investigate the cause as i would like to give mycroft a try but not by removing other software that i depend on. 
","
if you run “sudo apt install -f” does it tell you anything interesting?

apt-get install -f does not do anything because there are no proken packages to fix. after fiddling around with the packages and apt-get for a bit i can at least state that the problem seems to be with the dependency on libjack0. if you look at what packages the install script for mycroft wants to install, and you try to install them manually, you can actually make the problem with blender go away by explicitly stating it in the install request. the same goes for ia32-libs, but:
apt-get install python3.6-dev screen swig swig3.0 uuid-dev zlib1g-dev blender ia32-libs libjack0
results in
the following packages have unmet dependencies:
 libjack-jackd2-0:i386 : conflicts: libjack-0.116
                         conflicts: libjack-0.125
                         conflicts: libjack0 but 1:0.125.0-3 is to be installed
 libjack0 : conflicts: libjack-0.116:i386
            conflicts: libjack-0.125:i386

so obviously the installation of libjack0 conflicts with the i386 library for libjack0 from the ia32-libs package.

i can’t replicate this on ubuntu 18.04 unfortunately - ubuntu is our reference os.
totally understand the need to retain blender - it’s excellent software.
this may be a related thread;
https://forums.linuxmint.com/viewtopic.php?t=269466
tbh i feel this is more a linux mint 32-bit libraries issue than a mycroft issue to resolve.

hi,
i am getting the same issue with ubuntu 20.10 (64 bit, gnome)
libjack-jackd2-0 required by loads of things conflicts with libjack0 required by mycroft.
i got around this on my other pc by creating its own partition with xubuntu, but i can’t do that here at the moment.
i tried the snap, hoping to maybe get around this, but it failed to setup on first run.

@goldyfruit helped me hack my libjack-jackd2-0 package so it could co-exist with libjack0.




snap install failed support


    hi all, 
i tried installing the snap beta (ubuntu 20.10 64 bit) and it seems to have failed. 
when run the  first time, it looks like it was updating python (pip?) and then stopped somewhere. 
when i try to run mycroft from the desktop, nothing. 
from the terminal i get: 
/snap/mycroft/1078/bin/mycroft-launch: line 92: mycroft-cli-client: command not found
stopping all processes...
mycroft-skills: no process found
mycroft-enclosure-client: no process found
mycroft-speech-client: no process found…
  

"
25,supporting mycroft,general discussion,"
hey,
i must say i am really surised with the skills and the usability with mycroft so far and i would really like to support mycroft with a membership. but here comes the catch. i don’t have a credit card. so my question is, if there is a different option to support the mycroft team?
best regards
vadi
","
hey vadi, glad you’re liking it.
currently a credit card is the only way to contribute financially. we’ve looked into other options but the cost and time to get setup hasn’t been worth it yet.
there are many non-financial ways you can support us though:



mycroft



contribute - mycroft
contribute - mycroft







if you want to pay annon or have cash you can do the pay ahead for credit cards they don’t list your name if that is why or you dont need a credit card. you could maybe use it to make a one off donation if they allow that.
"
26,thoughts and advice,general discussion,"
hi, new here.
i work in devops, bash, python, and swift, both back-end tool sets, and pretty pretty gui based tools, to support teammates. current setup smart home was done for convenience of others, and is driving me batty… consists of wifi switches, (tuya) and some bulbs for a fan that “had to have a dimmer on the light” and some tuya based it seems wifi fan controllers (without dimmers), and roughly 10 amazon echo dots throughout the house… now everytime the wifi hiccups, or the oh so reliable spectrum internet dies, everything obviously is manual control… minor inconvenience, but the major problem is things seem to go wonky and switches dont reconnect in smart life right, and nothing wants to work right… so looking at using a raspberry pi to setup an ap for the switches with tuya-convert and ota reflashing, and decided to look down the rabbit hole of possibly getting my own voice assistant, to replace alexa, that i could build my own skill for to control the switches, etc, and have it to where in the internet does indeed go down, since my wifi is on battery, and the pi would be on battery (big battery), possibly have it setup to where i could still voice control my crap… because its still an inconvenience, no matter how minor… to have things in the house not work right, due to an internet outage… so i realize id be replacing the eco dots with a different device, possibly even pis, just wondering if this is going to be worth my time to attempt… or is there a better route or just live with the frustrations
","


hi @matts,
i’m not sure to understand what your question is (that is a very long sentence that you wrote here ).
is your question does mycroft worth your time? if yes then i would say it’s always good to learn new things and setup and using mycroft will be a great source of new knowledge.
does mycroft could replace alexa and other, i would answer yes (this is my opinion) if you know what you want and if you have some technical skills (which seems to be your case).




 matts:

replacing the eco dots with a different device


it’s def worth your time 

well mainly the questions is this: would i be able to setup using a zigbee gateway/hub, home assistant, and mycroft; to host everything on my own, and code my own skill to control all the devices, almost all are tuya devices… with voice, without internet
i would use tuya-convert to ota reflash all of the switches/devices, set them to connect to the zigbee gateway.
setup and host own mycroft server, and home assistant, and build raspberry pi devices to replace echo dots.
code skill to facilitate voice control of the devices through mycroft.
goal is to be able to voice control my devices if my internet happens to go down, by hosting everything here at the house, so the ultimate question is
can i do this, with mycroft… when considering that id be hosting my own server for it, running my devices over zigbee gateway(s), & running something like home assistant if needed.
or am i trying to do the impossible

i would try  with homeassistant or another home automation system that supports tuya and where a mycroft skill already exists, e.g. openhab, fhem. you will be more flexible with automation or integration of new iotdevices from other vendors.

im not as concerned with adding new devices, as all the light switches in the house are already the tuya wifi switches or dimmers. also all the fans are on wifi remotes, that look to be tuya based as well. there’s really not much in the way of additions ill be making here either, more in the automation. the biggest concern i have is the ability to voice control the devices while the internet is down, as i have some people here that walking is a major thing for, so its way easier for them to talk to the lights, fans, thermostat, & etc. the plan is to either use zigbee, or a pi 3 b+ as the access point for the smart devices. i can host mycroft on an actual server, ive got 192gb of ram, dual 10-core 2660 v2 xeons, and a 1.4 tb raid 6+0 array across 8 ssd’s off of an lsi 9266-8i.
im also looking at doing a cluster of pi 4 8gb’s, anywhere from 14-28 with poe hats, with 2-4 of the pi’s just being heads. looking at the replacements for the echo dots being the pi 4 8gb with pmod hats that allow connecting 3 digilent pmod modules.
"
27,picroft using google tts,none,"
i have today set up picroft. after som time i got everything working. i can ask mycroft what time it is, it responds. now i want some better tts. i have activated google voice in my profile. i get this in the error log:
tts execution failed (valueerror(‘unable to find token seed! did https://translate.google.com change?’))
does mycroft try to translate the response??? i am perfectly fine with english. in my profile i have stated that i live in sweden.
","
hi @markus_kruse
a late answer is better than no answer 
i guess you had this bug: https://github.com/mycroftai/mycroft-core/pull/2763
it has been fixed in dev branch but not yet merged into master .
"
28,initial thoughts and a few questions,general discussion,"
hi everyone, i just thought i’d share my initial experiences with mycroft and hopefully get some up-to-date answers while i’m at it.
i bought a pi 4b, a blue snowball mic and some creative pebble 2 speakers. installing/imaging picroft was easy with etcher. booted up, hdmi output didn’t work so checked my router for the dhcp-allocated ip address and ssh’d in. no problems so far other than having to wait a while until initial boot had finished (before i could connect) with no way of knowing (i’ve since installed the finished booting skill).
the first thing i did was change the robotic british male voice to the much better american male voice. i’m british so i would have preferred a brit but the difference in quality made this a no-brainer (and i understand the local vs cloud processing reasons). but here’s my first question. i read that my subscribing to mycroft you get better voices using mimic2, but my american voice says it’s already using that. another post says enable american male beta while it’s still free. what’s the state of play with this as of right now? are there actually more/better paid-for voices?
then i followed the instructions to set up a wireless connection and rebooted. all fine, but i feel like maybe the wireless credentials could be added on mycroft home for people less comfortable with the linux command line.
i added/configured my home assistant server and said “hey mycroft, turn on the hot water”. boom. hot water turned on. what a great start. then i realised that was about all i could do with home assistant (unless i’m missing something), i couldn’t use its existing connections to my sonos speakers, heating/climate controls, alarm system, cameras etc.
i wasn’t expecting to be able to say “hey mycroft, play me some elvis on sonos in the sitting room” right away, but that was certainly a goal. “hey mycroft, boost the heating” was another.
so next step, hook up a music service. i generally use youtube music (forced “upgrade” from google play music), but that didn’t seem to be available, which is ok - i have spotify premium as well. that skill seems to have oauth problems and can’t authorise to spotify. i saw the thread about a workaround which i will try later, but right now i just wanted some music playing. i have amazon prime too so i tried the amzn music skill but again, that doesn’t work anymore. i was running out of options. i managed to link it to my local emby server (i had to force using a password on the local network or it wouldn’t authorise but at least it worked) and finally got some music playing. that’s fine if i want to listen to any one of thousands of my mp3s from the 1990s but the availability of streaming means that my local song collection is hideously out-of-date.
i feel a basic step like linking a music service should be easier than this!
i’d be happy to contribute to skills but my python is practically non-existent. if they were written in php i’d have churned out a bunch of pull-requests by now. guess i’d better start learning python…
one final question - the whole process of answering questions or commands seems quite slow. i see the text output of the response and then a few seconds later it’s synthesised into speech. is that a normal delay? would something stronger than a pi 4b make a difference or is that because i’m using a voice that’s coming from the mycroft servers? can i replicate that system locally for a faster spoken response?
thanks for building a great tool though, i don’t mean to be critical i was just quite surprised that to get things to work you have to be pretty technical and have time to fiddle (i am/have both) but of course that’s what happens with free/open-source/community-driven software. keep up the good work and i will start swotting up on python!
","
hi misha,
welcome  to the community, i joined few weeks ago and i hear you on the issues you got and for sure things could be improve to ease the mycroft journey. this is with people feedback that it’s begin.
i was really annoyed by the spotify skill issue, which i know is not mycroft community issue but a decision from spotify to shut the api for this voice assistant. i tried the work around, not very user friendly but at the end it worked.
about sonos, there are couple skills available:


https://github.com/lnguyenh/spotify-sonos-bot-skill which only work for one speaker and requires to install node js and spotify

https://github.com/boxledev/sonos-controller which only work for one speaker and with local library.

i’m currently building a sonos skill[1] (yeah another one…) which will work on any sonos speakers you have and with different services (spotify, amazon music, local library, etc…).
this skill will be based on   soco  python library which will directly connect to sonos to retrieve the token sor the different registered services, they are trying to fix[2] the issue with the music services.
there are multiples way to improve the response time. for me it was the following improvements that made me stay with mycroft.

add  tsched=0  to  module-udev-detect in  /etc/pulse/default.pa which improved a lot the wake word (reboot required, the process reload/restart didn’t work).
reduce the   ""recording_timeout_with_silence""   timeout from  3.0 to  1.0  second increase the speed of the request.
use google voice as tts has been a speed gain too.
use cloudflare dns   1.1.1.1  which is has a pretty good response time.
make sure your raspberry pi has a good wi-fi connection.

[1]https://github.com/smartgic/mycroft-sonos-controller-skill
[2]https://github.com/soco/soco/pull/763
i hope it helps.




 misha:

“hey mycroft, play me some elvis on sonos in the sitting room”


you seem to have the right sense of where mycroft is - intermediate-phase small-company-plus-foss project - but i think the more relevant problem is illustrated right here.
that is, there are a lot more moving parts here than it might seem at first glance. first, a framework that responds to “play” - because lots of skills might be able to “play” - will go find the skill that’s most confident it can play “me some elvis.” but, wait, what about those extraneous words? what if there’s a better-matching skill, but it would only respond to “elvis”? what if a skill wants to disambiguate between elvises presley and costello?
but say it comes across clean, and the best-choice skill says “i can play you some elvis.” now it needs to do a whole separate thing with sonos.
and there are implications here, as well. worth mentioning that if it tried to find a skill to play “me some elvis on sonos in the sitting room” that’s just going to fail, because no catalog will find that.
but, if the correct granularity is accomplished - and i don’t think this exists right now, but i could be entirely wrong because i’ve never used any of the relevant skills - the “play” framework needs to feed the remainder of the input back into the intent parsers. that is, it needs to retain, “spotify is ready to play elvis,” and, having correctly sliced off the rest, feed “on sonos in the sitting room” back into the works.
then the sonos skill has to say, “i can do that!” except its intent wasn’t fed any “play ,” that information is being retained elsewhere. what needs to happen now is:

the sonos skill needs to open a new audio source for playback, or the spotify skill needs to do it and then communicate back
the sonos skill needs to set that source’s output to whatever it uses to route audio, and route that source to your sitting room
the spotify skill has to switch to that output
the spotify skill starts playback
mycroft needs to know that two possible meanings of “stop” or “stop playback” are  “[pause/terminate] the running spotify connection [then close the corresponding sonos connection and end process]”

no small feat. it’ll get there, but wow that’s a lot of moving parts and complex evaluations.

it actually just occurred to me that there’s even more to it! let’s whittle the utterance down.
“play x in y” - is that a location, or an application, or is “in” part of the name of the thing to play? heck, you might say, “play star trek picture in picture.” now we’re off to the races. what’s “star trek picture” and what application can play it “in picture?” oh, “picture in picture” is a thing that your <desktop skill/bigscreen/whatever> can do. okay. which star trek? whoa there are over 100 episodes of that! which one do you want to watch, or should we just pick one? or maybe you expect a particular streaming service to pick up where you left off. you didn’t specify. the skill may or may not ask you to clarify, depending whether it thinks that’s what you want it to do.
“play x on y” - same basic problem. is y a service or a device?
“play x at y” - location or volume?
“start x with y” - this doesn’t even have to be playback.
indeed, “play” could mean “open this video game!”
common frameworks for skills like these will interrogate the compatible skills, which will estimate their confidence that you meant to invoke that skill. each skill needs to account for whichever of those possibilities apply. this stuff is hard!

thank you goldyfruit - i’ve already installed sonos controller and it works well although it could do with a few tweaks. i’ll look out for your skill in the future. thanks also for the speed suggestions, i will try them out tomorrow!

thanks for your reply chancencounter, i wasn’t really expecting it to know that i prefer presley to costello 
i agree there are a lot of moving parts. but it doesn’t have to be quite so complicated when you break it down. play [song/album/artist] on sonos in [location].
the command starts with play so we know it’s music or a video (or possibly a game?). then scan the whole query for a few keywords. once we’ve found the phrase “on sonos” then that can be used as a delimiter, the part between “play” and “on sonos” must be something to do with a music request (since that’s what sonos is used for) - and the only thing allowed after the phrase “on sonos” should be a location/speaker name.
the tricky part then is only which music service to send the music query part to, if you could define a default or a search order that would help.
it’s obviously possible because amazon/google have pretty much mastered it (admittedly with far more money/time/people/testing).
i feel a bit sad though because i always tell people not to use alexa etc. but at this point i couldn’t possibly recommend anyone to use mycroft - well, any “normal”, non-technical people - the norms. i’d love it to get to a more user-friendly stage and will definitely be contributing to skills to get there in the future…

did anyone know about the subscription voices by the way?

i’m a subscribed user (yearly membership) and i didn’t see any differences, maybe i’m missing something. i subscribed to help the project, i didn’t know we had any advantages 

i have built a usb music skill that will play music from a usb thumb drive, a network share or local path if you have your own collection. there is a link in the skills area of the forum. cheers.

@misha what about a location without a service?
“play elvis in the sitting room” is pretty straightforward. a skill knows how to “play  in the sitting room.”
“play alice in chains.” what do?

the challenge here is identifying which slices to operate on, and then which slices get priority, and the chaining of intents that need to register themselves in various ways with a framework that needs to reconcile them.
this might be an argument in favor of some kind of knowledge graph like wikidata. find things in the utterance that are things, find the kind of thing, go from there.
""‘play {elvis’}, 99% confidence: ‘elvis presley’, has attribute: ‘musician’, ‘play {musician: elvis}’
“‘play {elvis in’}, nothing, split utterance”
of course, this still only gets you partway there. it would straightforwardly chunk the utterance (most of the time) to facilitate confidence on the next pass, and ease the burden on skills to parse a whole utterance. you’d still have a bunch to do.
pass #1, resulting normalized utterance: “play {musician: elvis presley} | in sitting room”
pass #2, resulting sequence of possible intents: “{padatious -> spotify: play elvis presley} | in sitting room”
pass #3: “{padatious -> spotify: play elvis presley} | {padatious: ‘play * in sitting room’ -> sonos}”
sonos to intent parser: wait for ready
sonos: open audio out to “sitting room”
sonos to intent parser: ready: audio device
intent parser to spotify: ""play elvis presley using audio device ""
and then you’ve gotta register it as a thing that’s running, so mycroft can pause it, and stop it, and disambiguate with other things that are playing stuff…
parsers are hard, mang. i’d much rather work on lingua franca, where somebody already wrote the spec hundreds of years ago =p
"
29,mic level near at max but no audio input detected,none,"
i have a screenshot attached, anyone know why this may be?
","
hi @calvin.frakes1 and welcome!
what is you microphone?
"
30,move it reminder,skill suggestions,"
move it
user story:
_as a working from home user i want this skill to have timing and reminder features so that i can achieve standing up reminders every 30 min’s.
eg. i would like to voice activate the skill perhaps with a prompt for the number of min’s. the skill will run a background timer and voice announce me to “stand up” or “sit down”.
**no other services or data sets or platforms should be needed.
**there are similar but not the same skills with the needed continuing run of the timer. **
**“start moving” or something similar. **
**mycroft would speak: “move it is started”; “stand up”; “sit down”; “move it is now stopped” **
what skill settings will this skill need to store? (error- the skills settings page could not be accessed)
other comments?
sitting for extended periods of time has been identified as a health issue. many folks are acquiring desks that allow them to stand or sit while working on their computers. this skill would be of benefit to ones health with a regular reminder to stand up at your desk. or just to get up for a moment to stretch and move your position.
","
hi @allan_may
that is a great skill suggestion 

i’m not sure, but isn’t this better realized within the framework of reminder skill by adding a reoccuring reminder?
intent: remind me of standing up every 10 minutes

well i wasn’t sure that a recurring reminder could be setup as needed. i thought a recurring reminder would repeat at a specific day and time. for the move it we need a reminder, say, every 30 min’s or whatever the user wants. but we also need to control when to start the reminders and when to end them, for example at the beginning and the end of the work day, however many hours that might be.

my bad, i mixed up  mycroft-timer with mycroft-reminder (indeed the skill just recognizes timedate). yet the same would apply to mycroft-timer (which extracts duration and has named timers) i’ll look into it.

have rewritten the skill passages to also host recurring timer. i have to look at the intents/vocs/regex to get it right but it works in german language. if you don’t mind (or anybody else  ) i would post the test git here.

sorry i wish i could help but i’m not a programmer. but i do appreciate your efforts on my move it suggestion!

unluckily i just nearly broke my finger and now i carry a wrist-finger splint. so this might take a little longer

yikes! so sorry to hear about that. maybe programmers should have special insurance for their fingers 
"
31,unable to register device,site feedback,"
just got started with mycroft on a pi today.
setup was easy, but i’m not able to register my device on the web page.
the web page loads ok, the side bar on the left is working, but add device or profile just comes up with an empty page.
i have tried different computers and my phone and logged in with email and github, the result is always the same.
anybody else has the same problem or is there a known fix for it?
","
it seems the site has a problem at the moment. you’re not the only one with this isse.
i think the mycroft team will take a look for it when they’re back in the office on monday.
the team should have a pleasant free time on sunday .

hi spencersgithub,
i have the same issue. seems to be that one of their severs is down. maybe for maintenance or any other reason.
i think we can just wait until they recognize that their users are facing that issue! or if you want to, just write a mail via the contact-form as i did. but i think in the meanwhile they already got a lot of them, so maybe just waiting as thorsten wrote might be enough for now. 
kind regards,
lumpy

grafik1857×289 36.8 kb


thanks for the quick replies.
i will check again tomorrow.

nooooo 
i setup my new rpi4 with seeed v2 mic array and now i can not register them…
so sad.

hi all, thanks for reporting the outage, the team are looking into this now, and will let you know when it’s been resolved.

now! wow, that’s cool.
please keep us informed here and thx.

the issue should be resolved now, see the incident report here

å


hi there,
on sunday the 10th of november, one of the mycroft service api endpoints became unavailable,  preventing certain pages from loading correctly. this included the profile, my devices and add device pages of home.mycroft.ai. as per our standard incident response process, a report for this incident was created and is available for the community to review.
our engineering team investigated and restored this service in a matter of hours, and all services are now fully functional.
a standard post-incident review is being conducted, and the team has taken immediate actions to reduce the chance of this type of incident reoccurring in the future, as well as to improve automatic recovery if a service does become unresponsive.
we apologize for those who could not access all of their account information during this time, and appreciate our communities responsiveness in reporting the issue. the communities support in identifying and debugging issues assists us in continually improving mycroft’s services.
thanks
the team at mycroft ai

i have this same issue now

so do i. i’ve tried chrome, chromium and firefox for linux with no success.

there is an ongoing issue, have look here: pairing is currently offline

it seems to be back.

i’m having this problem too. 22/12/2020

i’ve been having this issue as well. i get a “internal server error” from https://account.mycroft.ai/api/devices

there is an ongoing issue, the team is working on it.
(come to the chat to get more updates)
"
32,fresh install of picroft on rpi4 hangs,support,"
so just got a new rpi4, and seeed respeaker 6 array mic board, 64gb msd. put on picroft, went through the setup. had to do some manual install for the mic board, as the default install doesn’t support installing/setting it up. took me a little bit, but via the audio troubleshooting figured out how to set the pulse & mycroft mic/speaker settings. got it up and running, managed to get it paired with home.mycroft.ai, so i start talking to it!
i’ve run into several times where i had to restart it or reboot the rpi because mycroft hangs and goes unresponsive after it starts trying to perform a task. i have some small heatsinks on the cpu and other chips. with it running but hung, the vcgencmd measure_temp gave me ~72-73c. running idle after a mycroft-stop all gives me ~60c after waiting several minutes. so with it running it’s elevated temps, but not excessively high.  after doing mycroft-start all and idling at the mycroft-cli-client, it’s running about 62-63c.
the hanging always seems to occur after i ask mycroft something, i see it get the utterance, try and figure it out, and then i see a response but no voice plays and then it becomes unresponsive to new voice commands, and the mic level on the lower right doesn’t move.
it’s as if the audio output/tts hangs, and causes the rest of mycroft to hang and not accept new input.
","
can you play regular audio out of this with out issues?  also have you checked in the logs (/var/logs/mycroft/) to see if there’s any clues?

i do sometimes get audio response from mycroft, and the mic test i do get the recording back out (headphones via the seeed jack).
in the audio.log, apparently i’m getting an alsa error. that must be the issue, time to troubleshoot that.
[quote]alsa lib pcm_hw.c:1822:(_snd_pcm_hw_open) invalid value for card
aplay: main:828: audio open error: no such file or directory[/quote]
what’s the mycroft config options for the audio output? is it just the play_wav_cmdline and play_mp3_cmdline settings via mycroft-config edit system?   it seems that those got reset after one of the restarts i did. they’re back to plughw:arrayuac10,0 instead of plughw:2,0 which was working before.
hm. looks like something is going on with the audio stuff, i’ll have to dig into that more and troubleshoot. i might just rebuild from a bare raspian, do manual install of mycroft.

did you ever figure this out, i’m having similar issues?

just to give more context (from the chat) just in case from @fsa317 possible issue:

image1282×396 43.2 kb


picroft is still 32bit. wasn’t there someting about 32bit and that big(ger) amount of ram?
issue sounds like running out of memory




 j1nx:

picroft is still 32bit. wasn’t there someting about 32bit and that big(ger) amount of ram?


the memory limit for 32-bit system is 4gb, picroft usage is pretty far from this limit (except if a custom skill is leaking).
"
33,testing and feedback for linux rhythmbox skill,skill feedback,"
how to install rhythmbox skill
linux software required


ubuntu: apt install rhythmbox

linux mint: apt install rhythmbox

arch: pacman -s rhythmbox

install skill via git:

cd ~/mycroft-core/skills
git clone https://github.com/dwfalk/rhythmbox-skill.git

mycroft-pip install fuzzywuzzy

how to test rhythmbox skill

setup rhythmbox with music, playlists, etc.
utterances…
◦ “play **”
◦ “play ** playlist”
◦ “play something by **”
◦ “shuffle ** playlist on rhythmbox”
◦ “pause”
◦ “resume”
◦ “next song”
◦ “previous song”  [limited function within rhythmbox]
◦ “stop rhythmbox”

skill pass criteria

skill searches the rhythmbox song and playlist databases for the best match and queues it up.

skill failure criteria

skill does not install
skill fails to interface with rhythmbox
skill does not play music when requested

feedback

i am new to the mycroft forums and to github
on github at https://github.com/dwfalk/rhythmbox-skill/issues/new

in response to this topic

","
hi dwfalk,
i’ve dropped a new issue in your repo with some trouble i had accessing the file system, but wanted to say thanks for creating rhythmbox skill!

great feedback.  thank you.

gez-mycroft’s suggestion over on github simplifies installing the skill.  i added a requirements.txt file with the fuzzywuzzy dependency.  so now you only need to issue the following command to install the skill…
mycroft-msm install https://github.com/dwfalk/rhythmbox-skill

thanks for your work  , installation worked like a charm.
cps_match_query on the other hand, has me pretty puzzled it never finds what i’m asking for.
sometimes it doesn’t start music because it thinks stuff is not in my library  …
rhythmbox-skill_dwfalk - info - artist utterance: eminem
playback control skill - info - no matches
rhythmbox-skill_dwfalk - info - artist probabilities: (‘eminem’, 100)
and it doesn’t do anything else… i am sure it should work  ,
do you have any insights about the common play skill skill class?
i am trying to use it in my own music skill and i am also having problems adapting it to  common play   is it me using it wrong ? thank you again  have a nice day

colla69…if you wish, you can open an issue over on the github site i reference for skill feedback and post the pertinent contents of your rhythmbox database (~/.local/share/rhythmbox/rhythmdb.xml) and i’ll take a peek.
you can also look at my code on the github site to get an idea on how to work with common play skill.  i was a little intimidated with it myself, at first.  but took on the challenge of converting my original code to it and am rather happy i did.

hey @dwfalk…
i found out what the problem is  … it has notihg to do with your skill …
the common play framework has a timeout, you should be getting an answer from the search queries in max 1 sec time…
thats why i got

“playback control skill - info - no matches”
before
“rhythmbox-skill_dwfalk - info - artist probabilities: (‘eminem’, 100)”

my computer isn’t fast enough  (it’s getting old ^^ and my rhythmdb.xml is almost 20 mb and increasing )…
that meand your skill passed all my tests on a smaller library… all the problems i had seem to arise because of the performance of my pc…
at the moment i am trying to write a filecrawler that can load my data faster hoping i can structure the data to speed up the search… i’ll be developing a test version for my own player skill. if your intrested i’ll let you know when i have something working…
wishing you a nice day  ,
colla

nice ability. i have found that the complete request is always sent to playlist, artist and title. “hells bells by ac dc” instead of “hells bells” and artist “ac dc”. normally, the mycroft player module can separate this. i use german. how is that with you? maybe my translation in the mycroft player module is not correct either.

i had in mind to add “song by artist” to the skill.  its been a while since i tinkered around.  i appreciate the feedback.  it’ll stir me to get back to doing some coding. guten tag!

i published a new version of the skill that includes “title by artist” support.

supper works for me. i have only the problem with my 35mb database. would it help to change the xmlparser to iterparse?

thanks for the suggestion.  this was a hobby project of mine.  i’m not the most literate on the ins and outs of python and xml, having just a basic programming background.  a google search is leading me in the direction of lxml, interparse, and using element.clear.  but i am open to any recommendations.

i opened an issue over on the skill’s github site to track benchmarking alternative xml logic.  i’ll use it to comment on any progress or potential avenues in supporting larger rhythmbox databases.  please feel free to visit and leave comments as well on any suggestions you might have regarding the efficient processing of the rhythmbox xml databases.  regards.

the reading at the start of the skill could accelerate the whole thing
https://mycroft-core.readthedocs.io/en/stable/source/mycroft.util.html#get-cache-directory

i’m now caching the rhythmbox xml database in skill class list variables.  the first time accessing the skill, it will cause the cache to be built.  subsequent calls to the skill will use the cache.  hopefully that will help individuals with large rhythmbox databases.

the skill goes right for songs / artists.
might i suggest to add a command like “play album **” maybe with possibility to specify artist (“play album xx by artst xx”)?

thanks for the suggestion “docjuhnk”.  i’ve added album and album by logic.  i want to be careful not to weigh down the speed of the skill.  but i kinda anticipated this request and can see where it would be handy.
i really appreciate everyone that uses the skill.  its nice to be able to contribute something to the open source community and to the potential that mycroft exhibits.

works like a charm!
i can’t really say, that it has slowed my installation. (i have a little over 5k tracks / 460 albums / 270 interprets in my database.)
thanks for your quick reply and updating the skill.

@unwisebard contributed code to support genres and also some tweaks to shorten search times.  have a look.

@dwfalk this is just what i was looking for, thanks! i just got it installed and will give it a whirl. 
works like a charm. mycroft now pulls from my local music dir. next, i’ll get a map to my nas.  thanks again.
"
34,running setup py install for pyaudio error,none,"
hi, every one!
one year ago i tied start using mycroft but nothing. now i have suse 15.2, core i5 and same problem.
maybe some progress appeared through one year? can you help me?
building wheels for collected packages: pyaudio, psutil, pocketsphinx, fann2, gtts-token
building wheel for pyaudio (setup.py) … error
collect2: error: ld returned 1 exit status
error: command ‘gcc’ failed with exit status 1
error: failed building wheel for pyaudio
running setup.py clean for pyaudio
building wheel for psutil (setup.py) … error
error: command errored out with exit status 1:
error: failed building wheel for psutil
running setup.py clean for psutil
building wheel for pocketsphinx (setup.py) … error
error: failed building wheel for pocketsphinx
running setup.py clean for pocketsphinx
building wheel for fann2 (setup.py) … error
running setup.py install for pyaudio … error
","
hi @stslit,
sorry to hear that.
you are not suppose to run the  setup.py   script, a lot of dependencies are required before doing this, e.g:
   $ sudo zypper install -y git python3 python3-devel libtool libffi-devel libopenssl-devel autoconf automake bison swig portaudio-devel mpg123 flac curl libicu-devel pkg-config libjpeg-devel libfann-devel python3-curses pulseaudio
   $ sudo zypper install -y -t pattern devel_c_c++


how did you run the installation?
which procedure did you use?

try to follow this procedure: https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/get-mycroft/linux#installing-via-git-clone

for pyaudio specifically, it’s usually available as an os package.

both set command:

the last version is already installed.
nothing to  do

and i used your link https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/get-mycroft/linux

so you did run  dev_run.sh ?

do you mean dev_setup.sh?
yes, but same result. “warning: failed to install required dependencies. continue? y/n”

yes, sorry about the typo.
have you been able to install these packages in two steps?
sudo zypper install -y git python3 python3-devel libtool libffi-devel libopenssl-devel autoconf automake bison swig portaudio-devel mpg123 flac curl libicu-devel pkg-config libjpeg-devel libfann-devel python3-curses pulseaudio
sudo zypper install -y -t pattern devel_c_c++

so, i just deploy a fresh opensuse 15.2 leap, mode server install and i had no issue with the mycroft setup.
goldyfruit@localhost:~/mycroft-core> cat /etc/os-release 
name=""opensuse leap""
version=""15.2""
id=""opensuse-leap""
id_like=""suse opensuse""
version_id=""15.2""
pretty_name=""opensuse leap 15.2""
ansi_color=""0;32""
cpe_name=""cpe:/o:opensuse:leap:15.2""
bug_report_url=""https://bugs.opensuse.org""
home_url=""https://www.opensuse.org/""

services are all started.
goldyfruit@localhost:~> mycroft-start all
starting all mycroft-core services
initializing...
starting background service bus
caution: the mycroft bus is an open websocket with no built-in security
         measures.  you are responsible for protecting the local port
         8181 with a firewall as appropriate.
starting background service skills
starting background service audio
starting background service voice
starting background service enclosure

status from the logs, no errors, all the services are connected.
goldyfruit@localhost:/var/log/mycroft> grep -ri connected 
enclosure.log:2020-12-20 19:27:10.581 | info     | 15701 | mycroft.messagebus.client.client:on_open:114 | connected
enclosure.log:2020-12-20 19:27:20.839 | info     | 15701 | mycroft.messagebus.client.client:on_open:114 | connected
skills.log:2020-12-20 19:27:15.589 | info     | 15692 | mycroft.messagebus.client.client:on_open:114 | connected
skills.log:2020-12-20 19:27:15.594 | info     | 15692 | __main__:_start_message_bus_client:231 | connected to messagebus
voice.log:2020-12-20 19:27:20.780 | info     | 15698 | mycroft.messagebus.client.client:on_open:114 | connected
audio.log:2020-12-20 19:27:21.381 | info     | 15695 | mycroft.messagebus.client.client:on_open:114 | connected
audio.log:2020-12-20 19:27:37.361 | info     | 15695 | mycroft.audio.speech:mute_and_speak:127 | speak: i'm connected to the internet and need to be activated.

thank you, captain obvious!
mycroft is working thing (on fresh system)!
but some linux users have old distros and some of them survived after several dub.
for example, my way from 15.0 to 15.1 and 15.2.
if you can help my, please do it.


thank you, captain obvious!

you are welcome. 

for example, my way from 15.0 to 15.1 and 15.2.

how are we suppose to know if it’s not part of your initial message?

if you can help my, please do it.

i’ll have to pass and let someone less obvious and smarter to take the next part.




 goldyfruit:

i’ll have to pass and let someone less obvious and smarter to take the next part.


sorry goldyfruit didn’t mean to offend you. thank you for support.
i thought in start post any one can seen one year link old. next year i trying start one more post. 




 stslit:

sorry goldyfruit didn’t mean to offend you. thank you for support.


all good 



 stslit:

i thought in start post any one can seen one year link old. next year i trying start one more post. 


yeah but in one year many things could happened but now we have more context.
try this (within the mycroft  virtualenv ) and paste us the output please (we are going with baby steps):
``$ pip install --force-reinstall --no-cache-dir pyaudio==0.2.11
"
35,no sound after changing voice,mycroft project,"
hi everyone,
i have an issue when changing the voice or wakeword with the website acoount page (https://account.mycroft.ai/) on my rpi3+. i have a sound output when using the british or the american voice. but when i change to the ggogle voice, there is no sound output. did i miss something when changing to the google voice or is this an known issue?
br
eric
","
hey there, is this all sound or just the text-to-speech output?
do you still get a wake word acknowledgement tone?

hi, there is no tts nor a wake word acknowledgement. when i change back to british, than the there is also wake word sound, but tts works. when i write to the cli to tell me the weather, then i get a tts sound output. when i write to the cli “hey mycroft”, then i get “i dont know what that means”. there is also another issue:
when i run the command ""
(.venv) pi@picroft:~ $ mycroft-start wakewordtest
already up to date.
initializing…
starting wakewordtest
warning: no wav files found in /home/pi/mycroft-core/test/wake_word/data/with_wake_word
warning: no wav files found in/home/pi/mycroftcore/test/wake_word/data/without_wake_word
complete!
there seems something broken…
could someone please give an advise what i could do?

got same problem right after installing picroft, even without switching voices - any suggestion how to repair missing files? (was paired with british voice from start)
(.venv) pi@picroft:~ $ mycroft-start wakewordtest
already up to date.
initializing…
starting wakewordtest
warning: no wav files found in /home/pi/mycroft-core/test/wake_word/data/with_wake_word
warning: no wav files found in /home/pi/mycroft-core/test/wake_word/data/without_wake_word
complete!

does you pi is properly connected to internet? during the first start of the  voice service, some files are downloaded for mimic2.

yes, was connected before first boot (using picroft image). as i use respeaker2, i stopped first questions about config and installed sound before i tried to get mycroft to work

these are the steps from  auto_run.sh  about the seeed mic array 2.0: https://github.com/mycroftai/enclosure-picroft/blob/buster/home/pi/auto_run.sh#l377-l398
i’m using the same microphone on my raspberry pi 4, do you have any else connected (jack 3.5mm or hdmi) ?
"
36,spotify connect authorize does not work,none,"
in mycroft advanced skill settings, when i go on the spotify skill, click “connect”, log into my spotify account then i get the following error message:
invalid_client: failed to get client
can anyone help? this happens with different devices and different browsers, i suppose something with the base_uri is off, but no idea how to fix it 
many thanks!
your spotify struggler 
edit: i am living in europe and therefore have an european spotify account, does that matter?
","
i believe spotify continues to be an issue. see here.




attention: are you a spotify user? spotify disabled on mycroft general discussion


    for no reason we can desern, spotify has disabled mycroft’s api access.  this was used to search their library so that mycroft users who have paid spotify accounts can access the music they are paying for. 
we’ve been trying to get in touch with someone at spotify who can clarify their policy, but have not been able to get a reply. 
if you have a paid account, please open a support ticket with spotify, then get up on twitter and let @spotifycares know that you pay for an account and want to acce…
  



front line support is fairly useless on this. i’d love to be able to help figure this out. can someone on the mycroft dev team check to see if their app is still authorized? invalid_client seems to point to the request being rejected by spotify because it doesn’t trust the client id that we’re returning to them for the auth request.

i too see the exact same error.  looks like the the client id (client_id=5509ac5c42cc45d7b4625d0b851a0f1e)  on the redirect from the connect button in the spotify skill is no longer valid?  would love to get an answer to tatorzot and others question on this from the devs. tx!

i followed these steps: https://github.com/forslund/spotify-skill/issues/152#issuecomment-719563633
and added a comment with the missing ones: https://github.com/forslund/spotify-skill/issues/152#issuecomment-734514422
at the end all is working fine for me.

i’m running this on mac with virtual box so i don’t think the github fix will work for me. please fix spotify connect 

the github fix should work for you.
"
37,pairing is currently offline,mycroft project,"
we are aware of a current issue preventing users from pairing new devices.
we’re investigating now.
","
the error with the add device page should be fixed now.
an obscure angular 10 upgrade issue with viewchild that was not caught.  part of this update to selene was upgrading from angular 7 to angular 10.  this was probably the biggest change to selene since it was originally deployed.
you can find the incident report here:


docs.google.com



incident report - 2020-12-18 10:53 gmt
incident report: 2020-12-18 10:53 gmt status:  temporary workaround   reporter:	deathrealms via chat  symptom: unable to pair new devices  status: 10:54 gmt - investigating 12:14 gmt	- continuing to investigate 13:26 gmt	- continuing to investigate...






thanks to everyone for bringing the issues you encounter to our attention.  big apologies for the sloppy deployment.
"
38,regex experts wanted,none,"
i am looking for some help with regex to help with some of my skills.
if the user says play the song blue christmas the regex should return the song name as blue christmas now if the user says play the song blue christmas by elvis presley the regex should return the song name as blue christmas and the artist as elvis presley. i am able to get these working individually but not as a combined regex.
examples.
“play the song blue christmas”
(the |)(song|single) (?p<title>.+)

“play the artist elvis presley”
(the |)(artist|group|band|(something|anything|stuff|music|songs) (by|from)|(some|by)) (?p<artist>.+)

","
i don’t think there’s a good regex for this, sadly. the problem is identifying the break between capture groups.
even if you try to hinge on important words, they could all be part of a song title.
this is a brain wracking problem. i think the only good solution is to feed it into some kind of search algorithm that returns a tuple or json or something.

indeed a difficult one if you don’t want to risk sticking with a match that cuts the song title apart. since regex by default is greedy, trying to match very generic regex first against the music library, then, if nothing was found, trying the more specific regex with song and artist separation, probably is safest.
otherwise it makes sense to be strict:
((the song (?p<title>.+)|something)( from the artist (?p<artist>.+))?|the artist (?p<artist>.+))

matches:

the song abc
the song abc from the artist xyz
something

if playing a random song from an artist is allowed, playing any random song probably as well?


something from the artist xyz
the artist xyz

short from of 4., so it could be removed, but matches the example you gave.



the shorter/easier the signal words and the more alternatives you allow for those, like song/single, from/by, something/anything/stuff..., artist/group/band, the more false matches occur, so either multiple regex need to be looped through to not match wrong or miss a match in the music library, or one has to live with either missing results (when full matches are allowed only) or a large number of results (when a title/artist only needs to contain the string matched by the regex), when a song title or artist name contains one or more of those signal words.
"
39,picroft no sound with 3 5mm jack output,mycroft project,"
hello,
i’m trying to set up a picroft on a raspberry pi 3b with a ps3 eye and a speaker connected with a 3.5mm jack. the microphone is working fine but i have no sound… can someone help me?
rémi
","
what have you tried so far? what was the result of that effort? what’s in the logs (/var/log/mycroft/*)?

i tried to change the default sink (because it offers me two) as shown in the mycroft ai audio troubleshooting guide and i also tried to configure the sound output of the rapsberry (with the raspi-config menu) so that it uses headphones. but nothing worked, no sound comes out…
i have tried what is shown on this page (https://windowsreport.com/raspberry-pi-audio-not-working/) and i have some sound when i run “./hello_audio.bin”. but when i try something like “speaker-test -c2 -twav” (as recommended here https://www.tinkerboy.xyz/raspberry-pi-test-sound-output/), i have no sound…
do you think this is a problem with playing .wav?

is pulseaudio installed/used or plain alsa? i.e. by default, mycroft uses paplay to play wav files, so that would be a test that is coming closer. you say microphone is working fine, so you could try:
cd /tmp
arecord -d 3 test.wav # then make some noise for 3 seconds
paplay test.wav
aplay test.wav # just to test plain alsa as well

raspi-config btw produces the following asound.conf override for the current user (~/.asoundrc):
pcm.!default {
  type asym
  playback.pcm {
    type plug
    slave.pcm ""output""
  }
  capture.pcm {
    type plug
    slave.pcm ""input""
  }
}
pcm.output {
  type hw
  card $audio_out
}
ctl.!default {
  type hw
  card $audio_out
}

audio_out is set to the sound card id selected. as you said there were two, most likely those were hdmi sound on id 0 and “headphones” (=3.5mm jack) on id 1.
i played around with alsa configs and mycroft in the past (and do just now again) and i’m not sure whether the “asym” plugin is such a good idea here. you could test instead (cat for copy&paste to console):
cat << '_eof_' > ~/.asoundrc
pcm.!default {
  type plug
  slave.pcm {
    type hw
    card 1
  }
}

ctl.!default {
  type hw
  card 1
}
_eof_

try card 0 as well, if the above does not work, or verify which id is which card via aplay -l.
but as you see this is all about alsa, while i have not really an idea how pulseaudio acts on top of that.

hi remi, have you managed to fix this issue?
i’m having the exact same issue with a raspberry pi 4, 3.5mm jack out and ps3 eye in. microphone appears to be working fine but cannot get any output from 3.5mm jack.

hi,
no, i haven’t solved my problem yet and i must admit that i haven’t yet taken the time to try michaing’s advice.
can you try and then tell me if it worked?

i have the same problem - no any sound on 3,5mm jack audio. i tried what @michaing said above (btw i didn’t had .asoundrc by default) but it’s still not working.
temporary solution for now is to reboot device only with plugged 3,5mm jack without any hdmi screen and control device using ssh.

did you also run the paplay and aplay tests?
what i am wondering is that picroft by default comes with pulseaudio setup, afaik, but raspi-config audio config configures plain alsa. i have no experience with pulseaudio, but would assume that setting up both somehow might conflict and break sound .
edit: ah, based on the default config file, it uses plain alsa to play wav sounds but forces hdmi output in most cases on new rpi kernel (see below): https://github.com/mycroftai/enclosure-picroft/blob/buster/etc/mycroft/mycroft.conf
another thing is that raspberry pi with the newest kernel changed the audio interface. prior to the change, hdmi and 3.5mm jack output was all on asound card 0, now 3.5mm jack is on card 1, if a hdmi monitor is attached. probably picroft didn’t adapt to this change yet.

yes, i used both commands and neither gave me a sound

i just found: sound always go through hdmi
that symptom and workaround, which matches yours, would pretty much fit to the assumption that picroft ships with a audio config that has not been updated yet to support the new rpi kernel audio interface changes.
… checking source code: https://github.com/mycroftai/enclosure-picroft/blob/buster/etc/mycroft/mycroft.conf#l2
okay pretty simple: the default mycroft config forces audio output to card 0, which is hdmi only, if a monitor attached. these settings override the alsa config.
please edit /etc/mycroft/mycroft.conf and either change in the first two settings ""hw:0,0"" to ""hw:1,0"" or remove the card+device options completely so that the values are ""aplay %1"" and “mpg123 %1” and hence respect the alsa settings (~/.asoundrc).
edit: ah, since that config file will be overwritten on updates, afaik, better create/edit the local user override, which should be located at: $home/.mycroft/mycroft.conf
edit2: that should fix the issue so that raspi-config selection, respectively manual alsa configuration is respected: https://github.com/mycroftai/enclosure-picroft/pull/150

btw, to review all sound devices and their card and device ids, run: aplay -l
"
40,sound always go through hdmi,support,"
hi, i have problem. i can’t get sound on 3.5mm jack output even if i change use audio output with “pack set-default-sink”
i have only 2 output choices :
-alsa_output.platform-bcm2835_audio.analog-mono

alsa_output.platform-bcm2835_audio.analog-mono2
even with the wizard setup when i choose 3.5mm jack sound go through hdmi…
do you know how to get sound on 3.5mm jack?

","
hi @ewaca:
same happens to me recently i guest you’re using a rasberry pi with picroft.
i solved following the steps here:
https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/troubleshooting/audio-troubleshooting
and disconneting the monitor (hdmi output) and getting access throught ssh, previously i configured the wifi.
later i changed a few things  because was trying with different microphones, end up buying the “playstation 3 eye ps3” for input.
regards,

hey guys, please try to edit your mycroft.conf like mentioned here: picroft - no sound with 3.5mm jack output
"
41,skill not working probably the intent,support,"
so first of all: this is my first try in skill development.
i am unsure what’s wrong… the only thing i try is to let the skill tell me a random number. but it isn’t even finding my skill? could someone look into this and tell me why: https://github.com/vsgit/random-number-skill
i’m not sure if the intent and dialog are correct.
","
check the logs (/var/log/mycroft/skills.log) and see why it’s not loading?

i’m not sure if it’s the issue but you didn’t register you entity file.
https://mycroft-ai.gitbook.io/docs/mycroft-technologies/padatious

its still not working… i updated init.py and now the entitys are registerd… i think.
do you have other ideas why its not working?
"
42,google aiy voicekit 2 with raspberry pi 4 b,general discussion,"
hello everyone,
i was trying to install picroft on raspberry pi 4b + google aiy voicekit 2. but i can’t get the audio input & output working. i’ve tried https://github.com/chiisaa/picroft-google-aiy2-voicebonnet-skill and it’s not working. can anybody help me fix it?
","
hi toms!
the skill you tried to installed requires a working google aiy voicekit 2. if it’s not the case then this skill will be useless until then.
during the wizard (at the first boot), did you choose the following?
  4) google aiy voice hat and microphone board (voice kit v1)
this should should install the required packages from google and configure your raspberry pi properly. at the end a reboot is required.

thanks a lot for the quick response. i didn’t choose that option as i have voice kit v2.

did you run this shell script: https://github.com/chiisaa/picroft-google-aiy2-voicebonnet-skill/blob/master/install_aiy2.sh

yeah. i ran it too. no luck

fixed the issue.
installed chiisaa skill from marketplace and installed it. the led was workinf fine after that, but not the audio input & output. finally went to /boot/config.txt and commented out the below lines.
#dtoverlay=i2s-mmap
#dtoverlay=googlevoicehat-soundcard
#dtoverlay=googlevoicebonnet-soundcard
works like a charm now.
if you face any issues setting up raspberry pi4b + aiy voice kit v2, feel free to post in this thread so i can help.

thanks 
"
43,konnex domotica integrator,none,"
hi to all, this is my fist time…
i have a konnex home automation system that i would like to control with mark 2 that speaks in italian. it’s possible?
thanks for your reply. i am  a beginner i recently discovered mycroft and i love it.
","
hi @muttley & welcome! 
do you mean knx? (if not sorry for the rest of this message)
if yes, the solution could be to install home assistant (which has  knx integration) somewhere and then install the mycroft home assistant skill to command home assistant.
mycroft home assistant skill as a translation for italian language.

home assistant knx integration: https://www.home-assistant.io/integrations/knx/

mycroft skill: https://market.mycroft.ai/skills/af6c0e65-0ec4-4481-a2c8-4991d08fca9b

italian translation: https://github.com/mycroftai/skill-homeassistant/tree/20.08/vocab/it-it



hi goldyfruit neace to meet you.
thanks very much for your support. great but it’s the same read like japanese for me.
my son call me “nabbo” because i haven’t skill for this.
yes konnex is knx. if i understand what you tolto me, i will speak in italian with mark 2, that without home assistant make operation in my knx system. if it is how home assistant speak with knx system?
thank for your help. if you want i make a cake for you 
"
44,testing and feedback for magic mirror voice control skill,skill feedback,"
how to install magic-mirror-voice-control-skill



install magic-mirror-voice-control-skill by …


git clone https://github.com/dmwilsonkc/magic-mirror-voice-control-skill.git


when you first activate the skill, mycroft will attempt to find the ip.json which will hold the ip address of your magicmirror. if that file does not exist mycroft will say
""to activate the magic-mirror-voice-control-skill i need to know the i p address of the magic mirror. what is the i p address of the magic mirror you would like to control with your voice?""


be ready, because mycroft will expect an answer, and you will hear the listening chirp as soon as mycroft finishes speaking. be ready with the response “set ip address 192 dot 168 dot x dot xxx”. replace the x’s with your magicmirror’s ip adress. it can be finicky. so if it doesn’t work, just say ‘hey mycroft’… 'set ip adress 192 dot 168 dot x dot xxx again. be careful not to say ‘set ip adress to’ because mycroft tries to add the word ‘to’ into the ip adress.




magic-mirror-voice-control-skill connects to a working install of magicmirror from …


https://magicmirror.builders/


which can be installed by 'git clone https://github.com/michmich/magicmirror.git'


warning!!! if you plan to have both magic mirror and mycroft operating on the same raspberry pi, you must use magic mirror v2.3.1 and not the latest version 2.4.1 because the v2.4.1 has a newer version of electron which will cause extremely high cpu usage and will overheat your rpi.


you can use git checkout 60b9a5b


in the config.js you will need to modify your ip whitelist like so:
address: ""0.0.0.0"", port: 8080, ipwhitelist: [""127.0.0.1"", ""192.168.x.1/24""],
where 192.168.x.1/24 -----------x is your local network address


in addition to the default modules the mmm-remote-control module and my fork of the mmm-kalliope module must be installed for the skill to function as it does in the video. to add the modules to the config.js follow the instructions for each module.


if you would like to see the steps i took to have both a working copy of magicmirror and mycroft operating on the same rpi you can check out these two links creating my first skill… and trying to install both…




how to test magic-mirror-voice-control-skill
specify the steps the user should take to test the skill, such as;


speak —hey mycroft… hide clock


mycroft should —remove the clock from the magicmirror display


mycroft has a number of different “system action keywords”
refresh
restart
reboot
shutdown
show
hide
turn on
turn off
update
conceal
display
wake up
go to sleep
save
when these system action keywords are combined with “system keywords” like
article details
mirror
monitor
raspberry pi
pi
modules
screen
you can reboot pi for example or turn off monitor or save pi or update mirror
you should see the action taken, and get an audible response from mycroft.


there are also actions that can be directed at modules:
hide
show
display
conceal
install
add
turn on
turn off
update
that need to be coupled with “module keywords” like:
alert
update notification
clock
calendar
compliments
wunder ground
traffic
google traffic map
email
remote control
news feed
page indicator
remote control repository
button
buttons
carousel
carousel navigation
connection status
hide all
glance
module scheduler
on screen menu
tabulator
bitcoin
ethereum
lice
stock
stocks
in all, mycroft can interact with 340 different modules. but the only commands that mycroft will take action are on modules that are actually installed on the magicmirror. like hide clock, or show email, or turn off weather for example.


mycroft will respond to every intent that is recognized with an action which you will see on the mirror and a verbal response from the success.dialog:
done
complete
success
as you wish
by your command
if the command doesn’t make sense, like install raspberry pi, mycroft should respond with incrorrect.dialog.
‘that command is not valid, please restate it’ or
‘i cannot follow that command, please say it a different way’
‘that command does not make sense, please try again’
if a module is not installed, mycroft will respond:
“that module does not appear to be installed.” or
“i cannot find that module installed on the magic mirror.”
where feedback on magic-mirror-voice-control-skill should be directed to:
feedback is preferred here on this post, or via [mycroft chat] -> @dmwilsonkc  (https://chat.mycroft.ai 
","
@dmwilsonkc,
very interested in this project but currently don’t have the hardware to make it happen. it is on my wish list though. few questions (unrelated to the skill).

how big is the mirror and display?
did you use glass or acrylic?
could you have a larger mirror with smaller display in one region of the mirror?
can the mm be a separate installation (hardware) to mycroft?

looks great by the way.

@pcwii i have not built the physical mirror yet. my plan is to use an old 32” lcd monitor and acrylic mirror. as far as using a smaller display with a larger mirror, it is all up to you and your personal preference. you can use separate installations of mycroft and magic mirror. you can even use this skill with a mark 1 and a magicmirror as long as it is on the same network, and the mirror has the proper modules installed and the ip whitelist is configured in the mirror’s config.js

i’m love your skill iv got to say and after finding out about picroft i would like to us this ai with my magicmirror2, i’m running the latest magicmirror2, you mentioned about over heating issues is this with no pi cooling or not as i have a heat sink and fan on the cpu and ram and a heat sink on the smsc chip?? or is it better to just downgrade the mirror version, thanks for the great work, just one more thing is there any more progress with the skill??

@jmh474 you can use any version of magicmirror you like as long as the mmm-remote control module and my branch of the mmm-kalliope module  are installed. my concern about the overheating is because i had both the magicmirror and picroft installed on the same rpi. if you experience overheating issues, you can always use a seperate rpi for picroft, so long as they are on the same network and have access to the mm’s ip address. you could also use the skill (so long as the modules are installed on the mirror) with a mark 1 and quite probably a mark ii when they ship.
the the skill works by sending http requests to the magicmirror, so as long as the modules are installed and you’ve properly whitelisted the picroft’s ip, it should work perfectly. 

@jmh474 oh… i haven’t been able to do anymore work on the skill for quite some time. i’m not sure what other functionality you’re looking for, but i’m super busy with work and i’m not sure when i could get to it. if you’ve got cooling with your mirror overheating will probably not be a problem for you, i am using a basic rpi 3b with a 16gb sd card. nothing fancy, but overheating is a consideration for me.

thanks for the fast reply not much else i wanted really just asking if your still tinkering with the skill, but again grate work

as soon as the magic mirror server is not active, you will get a second error error message every second
“bei der verarbeitung der anfrage ist ein fehler im skill magic mirror voice
control skill aufgetreten.”  pretty nerf.

@gras64 i will work on error handling for that event. sorry.

how can we add more modules into the “availablemodules.json” file?

@anthony_36 you can create a pull request for the modules you want and i can add them. you could add them yourself but they will be replaced on an update, so you could create a backup of that file after you’ve added the modules you’d like added in case the skill gets updated. if you look at it, it is a simple json file and the parts for each module you want added are pretty self explanatory. if you do a pr please include a link to the module’s url. i will update it, but not if i have to hunt for the module.
submit the pr here

i thought that i must do much more for it to work. so if i add in availablemodules.json a small array containing the module’s url, mycroftname etc like all the others it will work just like that? if so i will do ti my self with a back up folder as you mentioned. thank you for the tip.

@gras64 so i had the opportunity to identify instances in the code that throw the “there was an error processing a request in the magicmirrorvoicecontrol skill” error. i think i have made the necessary code changes to account for possible errors and address them so the annoying “error processing request” errors no longer happen. i also made changes in the code to check for connection and whether my fork of the mmm-kalliope is installed and configured. if you get a chance, go to the magic-mirror-voice-control-skill directory in the skills directory /mycroft-core/skills/magic-mirror-voice-control-skill/ and type
git pull

to update the skill.
let me know if you find any other errors i need to address.
thx,
cheers!

hi @jmh474, @gras64, and @anthony_36
this skill is currently being reviewed to be included in the public skills marketplace. it sounds like you are all using the skill currently so your experience is the best indicator of whether it’s functioning as expected.
based on the skills current readme.md, if a user installed this from the marketplace, would they be able to get setup and perform all the actions described in the example intents?
is there anything that you think needs to be modified before it is merged into the marketplace?

hello, i have a problem with magic-mirror-voice-control-skill.
all works before yesterday.
yesterday i update mycroft to the latest version 19.8.0 and now at the start of mycroft i have the following error:

2019-09-27 22:47:28.701 | error    |  9316 | mycroft.skills.skill_loader:_load_skill_source:209 | failed to load skill: magic-mirror-voice-control-skill (importerror(""no module named 'mycroft.messagebus.client.ws'"",))
traceback (most recent call last):
  file ""/home/pi/mycroft-core/mycroft/skills/skill_loader.py"", line 202, in _load_skill_source
('.py', 'rb', imp.py_source)
  file ""/usr/lib/python3.5/imp.py"", line 234, in load_module
return load_source(name, filename, file)
  file ""/usr/lib/python3.5/imp.py"", line 172, in load_source
module = _load(spec)
  file ""<frozen importlib._bootstrap>"", line 693, in _load
  file ""<frozen importlib._bootstrap>"", line 673, in _load_unlocked
  file ""<frozen importlib._bootstrap_external>"", line 673, in exec_module
  file ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
  file ""/opt/mycroft/skills/magic-mirror-voice-control-skill/__init__.py"", line 31, in <module>
from mycroft.messagebus.client.ws import websocketclient
importerror: no module named 'mycroft.messagebus.client.ws'
2019-09-27 22:47:28.730 | error    |  9316 | mycroft.skills.skill_loader:_communicate_load_status:280 | skill magic-mirror-voice-control-skill failed to load


any suggestion? how fix it?
thanks a lot

i do not know why this error is occurring. obviously if it happened after the update, something changed with mycroft’s code. we can ask @gez-mycroft to look into what may have changed with mycroft-core’s web socket.




 dmwilsonkc:

obviously if it happened after the update, something changed


solved…
i made that change al the issue disappear:

image736×562 139 kb


image924×253 108 kb


i have the problem “no module named” also with many other skills. it seems to me that the dependent rent for updates are not installed properly. no idea.

hey, checkout this change to the reminder skill to see how you can switch it over to the new format.


github.com/mycroftai/skill-reminder






switch to messagebusclient

websocketclient was deprecated and replaced with messagebusclient

  by forslund
  on 12:03pm - 02 sep 19 utc


  changed 1 files
  with 2 additions
  and 2 deletions.








@gez-mycroft thanks! i’ve been swamped at work so i haven’t really had a chance to look at this. i’ll update the skill to the messagebusclient, but i probably won’t be able to get to it for a couple of weeks. thanks, dave.
"
45,question please help adapt 6 phrases embedded can it do it,adapt intent parser,"
hello -
i am trying to evaluate the potential for the adapt module for our purposes.  i would like to listen for a few specific phrases and parse the speech to text without a cloud connection.  those phrases are stop, wait, go, hello, yes, no.  that’s it.
i will have separate code that will evaluate what to do with the semantic result from those utterances (i won’t build skills with mycroft).   is it necessary to use the cloud to evaluate this speech or can adapt handle this on an embedded system?
","
adapt is an intent parser, and farther along the stack than you want to be. it works after speech-to-text. the stt results are fed back to the intent parsers for evaluation against their expected input. by the time you’ve invoked adapt, you’re halfway to writing a skill, even if you aren’t descending from mycroft.skill.
if you just want to actively listen for these phrases, and nothing else, you’re looking for a wake word listener, using several models. i’m no good with the listeners, so i won’t try to guide you, but you can start with the relevant section in the mycroft docs, and people who do understand the listeners are active on mycroft’s chat server.
this would not require a cloud connection. you’d just have the listener invoke the corresponding code on the corresponding wake word.
if you want to activate it with a button or a separate wake word, and then have it respond to one of those words, you’ll need to run the audio through some kind of stt. some can be run locally, or lan-hosted. others use the cloud. also not my area of expertise, but same deal: the docs have pointers, and people who understand the various stt engines are active in chat.




 chancencounter:

if you just want to actively listen for these phrases, and nothing else, you’re looking for a wake word listener,


kaldi-spotter might be worth a look for this use-case
"
46,skill not updated after web configuration,support,"
hi,
i’m writing a skill which interacts with  home.mycroft.ai   for configuration purpose.
from the web interface i’m able to see the configurations fields after the  new settings meta to upload  from the console.
however the  settings.json   file is not updated when i hit the save button or very randomly.
my skill has the callback:
def on_settings_changed(self):
    self._setup()
    self._run()

def initialize(self):
    self._setup()
    self.settings_change_callback = self.on_settings_changed

what am i missing?
thanks for your help 
","
it seems that there is some cache or lock.
if i do a lot of code changes and then run the msm update  command then the configuration update never happens but when i restart the  skills  service then the configuration is updated.

so, i confirm the workaround.
at every commit on my skill, i have to  msm remove my_skill  ,  msm install my_skill  and  mycroft-start skills restart .
i also tried to use the  msm update my_skill command but the result is the same.
do you have any idea about this behavior (i’m using the  dev version), is it something expected?

that is a strange one. you certainly shouldn’t need to restart the skill service.
can i clarify, is it slow to update settings and hence this is quicker, or no matter how long you wait, the settings will not update?

@gez-mycroft thanks for answering.
it worked maybe 3 or 4 times only with a very random timing (during the 10 hours i tried to debug).
after the  skills   service restart it’s working every time that i’m changing the configuration. from what i saw it refreshing around every ~60 seconds when a changed is detected.

i just saw this commit https://github.com/mycroftai/mycroft-core/commit/b0884301a3c63280890ac78232251b51e78d546b, is it related to this?
edit: no it’s not ^

@gez-mycroft it’s the same thing if there is an intent update, the   skills  service will have to be restarted.
i guess this is because the intents are read from the files and then loaded into memory.
"
47,play from youtube skill,mycroft project,"
hi, im new in this forum and decided to create an account and share some of the skills ive done for my mycroft, ive started cleaning them and uploading to github (1st time working with github!) i have some c++ coding experience and just started coding in python, honestly the simplicity leaves me scratching my head sometimes, i like to declare the types of my variables… enough ranting ets talk about this simple skill
play from youtube skill
this skill illustrates a very simple general music player.  it works by downloading
music files from youtube and playing with cvlc, at same time building an mp3 library on disk
its intended to be a cheap way to play anything without any account (eg. spotify) or having a music library on disk
requires pafy and youtube-dl for youtube searching and cvlc for playing
all my skills were for personal use and may need some adjustment, maybe some hardcoded path or not very good python practices (first time working with python like i said), suggestions are welcome 
just noticed someone added a youtube skill to mycroft-skills repo, it may be better suited than this one, i intentionally made it a little more complex than needed hoping i could extend it later
known issues:

random errors downloading, i think when 1st search result is a playlist?
random errors loading file, probably forbidden chars in youtube result name, unsure if proposed fix worked because im unsure of error source but didnt get this error anymore
-stop isnt working, but i think its my mycroft instance that is broken (doesnt work in any skill)

todo:
-maybe converting to mp3 and use playmp3 util function so cvlc is not needed (probably would need lame then)
or -play youtube video url directly without download, some other dependency would be introduced instead of cvlc and no library is created but removes pafy and youtube-dl dependences
-handle queuing up multiple songs
-refine video search
-fix know issues
link:



github



jarbasal/skill-youtube-play
skill-youtube-play - play music form youtube in mycroft





","
man , this skill is great but how to add it to mycroft-core . i am also new here 

just create a musicskill folder in your skills folder and download/git clone the repo to it 

hello jarbas!
i’ve installed your skill on my picroft (raspberry pi) and when i say: hey mycroft - play youtube i get this missage and nothing happens:



could you help me? thanks!

skill is currently not working, code was a mess, i started over but for some reason now get 0second files after download
do not use until fixed

oh ok! i didn’t know that. i’ll wait until going to be fixed 

a great skill…
can’t wait for you to fix it, please let us know when you do 
nathan

so i’m guessing this never got fixed?
as far as i can tell there isn’t currently a functional youtube skill.

there are a few out in the wild, but none that have been officially reviewed so not in the marketplace at the moment.
i haven’t tried any of these myself nor looked at the code, so can’t vouch for them, but if you are happy to install from untested skills, the best bet is probably:



github



augustnmonteiro/mycroft-youtube
a skill to play youtube. contribute to augustnmonteiro/mycroft-youtube development by creating an account on github.






and for desktop users wanting to actually watch the video:



github



shadowsith/mycroft-youtube-mpv
search and play for youtube videos with mpv media player - shadowsith/mycroft-youtube-mpv






we also have a brand new one coming that uses our new gui framework. so that will work for the mark ii and plasma users:



github



aiix/youtube-skill
fork of youtube mycroft skill. contribute to aiix/youtube-skill development by creating an account on github.







i’m trying to get youtube working on a headless picroft and as far as i can tell not one of those works.
i imagine it should be fairly straightforward to output a video stream to ffmpeg and have it transcode or otherwise strip out just the audio. admittedly that does mean i need to learn how python classes work!

i put together a skill to play audio from youtube:




youtube audio skill - testing and feedback skill feedback


    mycroft skill to play audio from youtube, using the common play framework. 
this is heavily based on the excellent i heart radio and tunein skills by johnbartkiw. 
it uses mycroft’s vlcservice for playback, and pafy / youtube-dl to fetch media details. 
it’s still a bit of a mess, but the basics work. 
how to install youtube audio skill
msm install https://gitlab.com/mcdruid/mycroft-youtube-audio

youtube audio skill connects to … youtube, without any credentials. 
how to test youtube audio skil…
  

feedback welcome in that thread or on gitlab.

just looking for a skill doing exactly this.
it works great so far.

hey guys,
do you have some issue with youtube skills to recently ? can’t load music since 2 days, it’s telling me that no result have been found , maybe youtube update something ?

i am having a similar issue with my kodi-skill, it is no longer returning youtube links either so something has changed.

acutally trying to debug it (lol first time i’m seeing python script). it seem that if you change skill to load only one music it’s working. so i think that the issue is with youtube-dl who can scrap youtube video because maybe youtube has made some changes

looks like the playlist id is formatted differently. the regex is failing in my kodi-skill.
these no longer work for me…
all_video_links = re.findall(r'href=\""\/watch\?v=(.{11})', html.decode())
all_playlist_results = re.findall(r'href=\""\/playlist\?list\=(.{34})', html.decode())

these do work…
all_video_links = re.findall(r'/watch\?v=(.{11})', html.decode())
all_playlist_results = re.findall(r'list\=(.{18})', html.decode())

not sure this is any help to anyone but me but thought i would put it out there. i am going to dig in when i get some time.

i tried to download jarbais plugin but the github seem no longer maintained, do you have a fork ?

anyone having any luck with youtube…  having same issue with youtube skill stopped working few days ago … saying “no results found”

same issue here. was using mcdruid plugin and since few days it’s ‘no result found’ 

there are several youtube skills scattered throughout the community. what skill(s) are having issues. from my own testing it looks like the regex is failing on most of these due to a change in youtube. i have seen a similar issue with my kodi-skill that i am correcting.

@mcdruid may be able to assist with their skill.
"
48,stock skill issues,mycroft project,"
hi
i’m running mycroft (picroft) on a pi 4.
from the cli-console: skills returns
mycroft-stock.mycroftai in red all other skills appear orange. naturally i don’t think the stock skill is working because whenever i use the recommended example:

“stock price of google” returns the following:  i’m sorry i don’t understand.

does anyone have a solution to this?
many thanks
","
the skill has been disabled on purpose:  skill has been intentionally disabled by mycroft
this is what i got from  skills.log :
2020-12-03 15:52:52.647 | info     |  3150 | mycroft.skills.skill_loader:load:185 | attempting to load skill: mycroft-stock.mycroftai
2020-12-03 15:52:52.666 | info     |  3150 | mycroft.skills.settings:get_local_settings:80 | /opt/mycroft/skills/mycroft-stock.mycroftai/settings.json
2020-12-03 15:52:52.667 | info     |  3150 | stockskill | the stock skill has been disabled due to a
2020-12-03 15:52:52.667 | info     |  3150 | stockskill | breaking change made to the 3rd party api
2020-12-03 15:52:52.668 | info     |  3150 | stockskill | for further information, see:
2020-12-03 15:52:52.668 | info     |  3150 | stockskill | https://github.com/mycroftai/skill-stock/issues/31
2020-12-03 15:52:52.669 | error    |  3150 | mycroft.skills.skill_loader:_create_skill_instance:295 | skill __init__ failed with exception('skill has been intentionally disabled by mycroft')
traceback (most recent call last):
  file ""/home/pi/mycroft-core/mycroft/skills/skill_loader.py"", line 292, in _create_skill_instance
    self.instance = skill_module.create_skill()
  file ""/opt/mycroft/skills/mycroft-stock.mycroftai/__init__.py"", line 131, in create_skill
    return stockskill()
  file ""/opt/mycroft/skills/mycroft-stock.mycroftai/__init__.py"", line 91, in __init__
    raise exception('skill has been intentionally disabled by mycroft')
exception: skill has been intentionally disabled by mycroft
2020-12-03 15:52:52.673 | error    |  3150 | mycroft.skills.skill_loader:_communicate_load_status:351 | skill mycroft-stock.mycroftai failed to load

ok thanks goldy
i may look for/develop an alternative at some point.

by the way you could also remove the skill to avoid log pollution.
$ . mycroft-core/.venv/bin/activate
$ msm remove mycroft-stock

and if required, restart the  skills service.
$ mycroft-start skills restart

thanks goldy i will do that
"
49,cant hear mycroft after starting pandora,none,"
still running picroft in a rpi3 lol. i can get mycroft to successfully start pandora, but once i do i can no longer hear mycroft until i reboot. havent been able to narrow down the problem. any suggestions?
","
update: autovolume was clearly bugging, and the problem seems to have resolved now that i have removed it.
"
50,master slave relation between devices,general discussion,"
hi,
i discovered mycroft recently and i have one device with mycroft installed on.
now, i want to installed it on other devices but i want to know if it’s possible to create a master-slave relation between multiple devices where slaves will only execute wake word, stt, and tts while the master will execute the rest of the remaining services.
my goal is to have only one ai for the entire house, and having the possibility to interact with, regardless of where i’m.
thanks in advance,
sorry, my english is not perfect, i’m french.
","
thats what the hivemind project is all about



github



jarbashivemind
jarbashivemind has 14 repositories available. follow their code on github.






thanks for your answer,
your project look interesting, is it ready to use ?
if no, you need help to develop it ?

it is technically functional, but very early. for instance, right now, it can handle all basic queries and operations, but it can’t reactivate speech to text on a slave. there is lots to do.
all worth doing! this is, among other things, how i’m getting mycroft going on my pinephone. all sorts of implications.
hivemind has a channel at the mycroft chat server, and would love help.

i’m currently working on using mycroft in docker on an x86 server (ie plenty of cpu), that sends/receives audio over the lan using pulse’s tcp module. individual rpis or similar can be placed around the house acting as pulse servers. the rpi only needs to be powerful enough to run pulseaudio, none of the mycroft services are hosted locally on it. so this greatly increases the range of hardware that can be used as an end point.
thanks to the magic of docker, creating additional mycrofts is a matter of copy pasting the docker-compose.yaml file, editing the container name/pulseserver address, and launching a new instance. the same mycroft.conf file is mapped across all instances, ensuring they are “in sync”.
i just had my first success with this last night, after changing track from the previous thought-train which was to use usbip. i want to do a post about it but need to tinker a bit longer.
"
51,selene backend device api question,general discussion,"
i am attempting to setup and use mycroft entirely offline.  i started with setting up the selene-backend and have all software installed.  i am at the step for creating the api service files, specifically the device api service file which shows an environment variable is required for google_stt api key.  i will not be using google_stt as i have a running deepspeech server.  so my question is do i need to define that google_stt api key or can i leave it blank, if i leave blank how do i tell selene to use deepspeech instead of google_stt?
thank you for you time,
david
","
are you setting up multiple mycroft’s and want them all to pull configuration from the selene backend?
if not, you can just define the speech to text settings in the mycroft.conf file on the local device. you’ll likely have to edit this file anyway to point the enclosure to your custom selene backend.
here’s a working excerpt for using deep speech stt:
  ""stt"": {
    ""module"": ""deepspeech_server"",
    ""deepspeech_server"": {
      ""uri"": ""http://deepspeech.domain.com:8080/stt""
      }
    },

if you just want to stop mycroft calling out to the default mycroft.ai servers, add the following to your mycroft.conf:
""skills"": {
    ""blacklisted_skills"": [
      ""mycroft-configuration.mycroftai"",
      ""mycroft-pairing.mycroftai"",
      ],
    ""upload_skill_manifest"": false
    },
""listener"": {
  ""opt_in"": false
  },

you’ll need to merge those fragments with your existing settings of course.
"
52,led matrix on picroft when mycroft speaks,none,"
greetings everyone! new user here with a question. i am successfully running picroft on a rpi3 and i have a led matrix wired up to it. what i would like to do is run a simple animation on it everytime mycroft says something. can anyone point me in the right direction?
","
you will have to catch the messages from the bus for the  play and  stop  events, check this link
once you get the message you will have to interact with your raspberry pi gpio, have a look here (it’s for a led but the concept stay the same)
here is an example of how to interact with a led matrix using the  rpi.gpio python library.

thank you! i’ll give that info a try and report back.

maybe check out these links too:

https://github.com/mycroftai/enclosure-picroft/issues/147
respeaker 4-mic array hat mycroft a.i. skill

"
53,free german tts voice for mycroft sneak preview,general discussion,"
hello.
we (some nice guys from mycroft community and me) are currently working on a free to use german tts voice based on my personal voice dataset contribution.
the model is based on a tacotron 2 combined with a pwgan vocoder. this model can be run locally and without cloud connection. we’re trying hard (try’n-error) to provide a free model with an acceptable quality in future for a daily usage, but we still have some work to do.
nevertheless we wanted to show you some sample audio as „sneak preview“ what is currently possible.

es ist im moment klarer himmel bei 18 grad.
ich verstehe das nicht, aber ich lerne jeden tag neue dinge.
ich bin jetzt bereit.
bitte warte einen moment, bis ich fertig mit dem booten bin.
mein name ist mycroft und ich bin funky.

also available on soundcloud
thanks @dominik, @baconator, repodiac, @olaf 
for more information on the dataset feel free to look at my github page:



github



thorstenmueller/deep-learning-german-tts
training files (csv and wav audio) for deep learning of tts/stt in german language. - thorstenmueller/deep-learning-german-tts






","
followed your efforts over the last weeks, i’m curious what hardware (cpu/gpu) should be used to facilitate a flawless experience.

currently you would need a gpu to produce speech in real time. so it would take 2 seconds to produce 2 seconds of audio, maybe 8-10 on a regular pi. so still not what we want, but up until now we didn’t have a free model at all. so a small step for us and a small step for mycroft  @dominik can give more info, because he has already tried it.




 sgee:

i’m curious what hardware (cpu/gpu) should be used to facilitate a flawless experience.


i am running my tests on a xavier agx. (a direct comparison with graphics card is difficult but a gtx 10x0 with 8gb should give similar or even better results).
in best case i see a real-time factor of 0.3 (1 sec audio requires 0.3 seconds of processing). as the model still has some problems with “stop attention” this goes up to 5.0. interestingly this happes with shorter phrases.
but with some tricks like caching of the synthesized audio files you will get a better experience.




 olaf:

currently you would need a gpu to produce speech in real time. so it would take 2 seconds to produce 2 seconds of audio, maybe 8-10 on a regular pi.


still uncharted terrority for me: you can convert the pytorch-model to tensorflowlite. this may result in better performance on a rpi…

just to say; cool guys! keep it up.

the main take away for me was that the data can be used to produce a reasonably good model. in the beginning it didn’t work and we didn’t know why. now we know that we can use thorsten’s data and can try different configs or combinations. @dominik thanks for the numbers.

little concerned that a top of the class board with 32 tops peaks at 5 rtf, but that’s maybe a configurational problem.
i wonder how the coral dev board resp. the broken out coprocessor (usb accelerator) would perform. don’t like the idea to let my winpc do the heavy lifting, since this would deem the pc to be powered 24/7.
is this benchmarked using mycroft?

in addition to what @olaf already said you might wanna take a look on this thread.



mozilla discourse – 16 may 20



running tts on constrained hardware (+ no gpu)
hey @_ca_a, have you tried squeezewave? they promisse a lot of speed up. they also have something to try on github. however, i couldn’t install the requirements. i’ve tried with python 3.6, pyhton 3.7 and python 3.8.  i think that they might be the...









 sgee:

little concerned that a top of the class board with 32 tops peaks at 5 rtf, but that’s maybe a configurational problem.


the current model needs some fine-tuning for shorter phrases (up to 6-7 words), longer sentences work better already.



 sgee:

don’t like the idea to let my winpc do the heavy lifting, since this would deem the pc to be powered 24/7.


xavier-agx has a very good “tops per watt” value. even in “max power mode” it idles around 5w and peaks at 30-40w.

for tacotron, a gpu would be ideal.  i use nvidia 1030’s, they don’t draw much when idle and fanless models are available.  yes, this necessitates running a host with them in it 24/7, but for quality and speed you’re going to have to make some trade-offs.
we’re quickly approaching a place were cpu can be used instead of a gpu, so this answer may change in the next year.

very recently an article popped up on how to setup a win (easily reproduceable in linux) deepspeech server for mycroft
yet, mozilla trained models seem a little bit different with using pbmm and some separate scorer.



 thorsten:

some nice guys from mycroft community and me


like the exclusion 



 baconator:

i use nvidia 1030’s, they don’t draw much when idle and fanless models are available.


got a stripped naked 1080 (only 2gb dedicated though)




 sgee:

like the exclusion


i already thought that’d catch someone’s eye  .
i’m trying to be “nice” though, but if i’m successful in it should “the other guys” say. 




 thorsten:

i’m trying to be “nice” though, but if i’m successful in it should “the other guys” say.


i can gladly confirm that thorsten is a nice guy, too. 




 sgee:

mozilla trained models seem a little bit different with using pbmm and some separate scorer


as it turns out this is a mmap-able format for inferencing. the pretty easy process to convert pb to pbmm is described here




 sgee:

yet, mozilla trained models seem a little bit different with using pbmm and some separate scorer.


this would be for deepspeech, not tts.

the deepspeech server  is serving stt/tts. i just don’t think it will run another model type.

deepspeech just does stt.  tacotron is tts.



github



mozilla/deepspeech
deepspeech is an open source speech-to-text engine which can run in real time on devices ranging from a raspberry pi 4 to high power gpu servers. - mozilla/deepspeech









github



mozilla/tts
:robot: :speech_balloon: deep learning for text to speech  (discussion forum: https://discourse.mozilla.org/c/tts) - mozilla/tts






this is about the later.

@baconator
oh ok, now i’ve dug a little deeper i saw that the articles talking about 2 servers with the second model already packaged, so i haven’t recognized it as such.
so, stt aside. is the tts serveing (described in the how-to) still viable? or what would you suggest?
is stt modeling sourced from one speaker beneficial?
@thorsten @dominik have you planned to upload the model?




 sgee:

is the tts serving (described in the how-to) still viable? or what would you suggest?


if you’re referring to https://github.com/mozilla/tts/tree/master/tts/server then yes, this is still viable and i just sent off a package earlier today using that.


is stt modeling sourced from one speaker beneficial?


possibly for that one person.  a wider set of submitted data would almost certainly help, even if the bulk of the data was from one person.
"
54,mycroft fails to start on manjaro linux,support,"
hi,
i’ve been struggling with installing mycroft this afternoon, at first it wouldn’t install the python dependencies into the virtual environment so i had to mnually enter the env and use pip. but now it complains that it is unable to load the configuration, also when running kept throwing errors about not being able to connect to the websocket.
output of command here
[tasty@tasty-pc mycroft-core]$ ./start-mycroft.sh debug
already up to date.
updating dependencies...
installing packages...
 installing packages for arch...
warning: git-2.26.2-1 is up to date -- skipping
warning: python-3.8.2-2 is up to date -- skipping
warning: python-pip-20.0.2-1 is up to date -- skipping
warning: python-setuptools-1:46.1.3-1 is up to date -- skipping
warning: python-gobject-3.36.1-1 is up to date -- skipping
warning: libffi-3.3-3 is up to date -- skipping
warning: swig-4.0.1-2 is up to date -- skipping
warning: portaudio-1:19.6.0-6 is up to date -- skipping
warning: mpg123-1.25.13-1 is up to date -- skipping
warning: screen-4.8.0-1 is up to date -- skipping
warning: flac-1.3.3-1 is up to date -- skipping
warning: curl-7.70.0-1 is up to date -- skipping
warning: icu-67.1-1 is up to date -- skipping
warning: libjpeg-turbo-2.0.4-1 is up to date -- skipping
warning: autoconf-2.69-7 is up to date -- skipping
warning: automake-1.16.2-1 is up to date -- skipping
warning: binutils-2.34-2.1 is up to date -- skipping
warning: bison-3.5.4-1 is up to date -- skipping
warning: fakeroot-1.24-2 is up to date -- skipping
warning: file-5.38-3 is up to date -- skipping
warning: findutils-4.7.0-2 is up to date -- skipping
warning: flex-2.6.4-3 is up to date -- skipping
warning: gawk-5.1.0-1 is up to date -- skipping
warning: gcc-9.3.0-1 is up to date -- skipping
warning: gettext-0.20.2-1 is up to date -- skipping
warning: grep-3.4-1 is up to date -- skipping
warning: groff-1.22.4-3 is up to date -- skipping
warning: gzip-1.10-3 is up to date -- skipping
warning: libtool-2.4.6+42+gb88cebd5-12 is up to date -- skipping
warning: m4-1.4.18-3 is up to date -- skipping
warning: make-4.3-3 is up to date -- skipping
warning: pacman-5.2.1-4 is up to date -- skipping
warning: patch-2.7.6-8 is up to date -- skipping
warning: pkgconf-1.6.3-4 is up to date -- skipping
warning: sed-4.8-1 is up to date -- skipping
warning: sudo-1.8.31.p1-1 is up to date -- skipping
warning: texinfo-6.7-3 is up to date -- skipping
warning: which-2.21-5 is up to date -- skipping
warning: jq-1.6-2 is up to date -- skipping
warning: pulseaudio-13.0-3 is up to date -- skipping
warning: pulseaudio-alsa-2-5 is up to date -- skipping
resolving dependencies...
looking for conflicting packages...

packages (2) python-filelock-3.0.12-3  python-virtualenv-20.0.18-1

total installed size:  4.89 mib

:: proceed with installation? [y/n] 
(2/2) checking keys in keyring                                                                                   [####################################################################] 100%
(2/2) checking package integrity                                                                                 [####################################################################] 100%
(2/2) loading package files                                                                                      [####################################################################] 100%
(2/2) checking for file conflicts                                                                                [####################################################################] 100%
error: failed to commit transaction (conflicting files)
python-virtualenv: /usr/bin/virtualenv exists in filesystem
errors occurred, no packages were upgraded.
starting all mycroft-core services
initializing...
starting background service bus
caution: the mycroft bus is an open websocket with no built-in security
         measures.  you are responsible for protecting the local port
         8181 with a firewall as appropriate.
./start-mycroft.sh: line 141: /var/log/mycroft/bus.log: permission denied
starting background service skills
./start-mycroft.sh: line 141: /var/log/mycroft/skills.log: permission denied
starting background service audio
./start-mycroft.sh: line 141: /var/log/mycroft/audio.log: permission denied
starting background service voice
./start-mycroft.sh: line 141: /var/log/mycroft/voice.log: permission denied
starting background service enclosure
./start-mycroft.sh: line 141: /var/log/mycroft/enclosure.log: permission denied
starting cli
2020-05-28 18:58:43.191 | error    | 22165 | mycroft.configuration.config:load_local:112 | error loading configuration '/var/tmp/mycroft_web_cache.json'
2020-05-28 18:58:43.239 | error    | 22165 | mycroft.configuration.config:load_local:113 | jsondecodeerror('expecting value: line 1 column 1 (char 0)')
2020-05-28 18:58:43.336 | warning  | 22165 | mycroft.util.file_utils:ensure_directory_exists:252 | failed to create: /tmp/mycroft/ipc
2020-05-28 18:58:43.337 | info     | 22165 | mycroft.messagebus.load_config:load_message_bus_config:33 | loading message bus configs
ere:

thanks in advance
","
i’m on manjaro too and i’ve been fighting with this for two days now . i can get it to run if i sudo it, but that caused other issues around audio devices. i was able to get my usb razor mic working but audio out is not. so i’m just going to start over … again. for those logs i chmod the permissions on those log file.
btw i had to run that setup 3 times before i was successful, some components would install and others would fail; each time more would install until i had all of them.

i just used the developer branch instead of stable and everything worked out of the box. give that a go @tasty213

tried installing of the dev branch had some trouble getting it to work but eventually found out i had to chmod the log directories and then turn of and on my laptop

i had the very same problem, so i opened up the dev_setup.sh script and copied the the arch installer section into a blank script and ran that first and then ran the setup script again. mycroft is running now.

i got the very same errors, no matter whether i use manjaro-repos, aur or the git installation method. how to troublehsoot this ?

for future manjaro or arch users who might be having trouble:
this pr should have fixed it, in october. however, as of this date, it hasn’t hit stable (should be soon, as the devs have started talking about the next stable release.)
so, in the meantime, switch to the dev branch, and it should be fixed. @maxnet is the only post here who tried it after that pr.
i didn’t write the fix, but it worked on my machine, as well as the author’s.
"
55,how to change mycroft language to german,languages,"
hey together,
i’m currently using mycroft on a debian vm for trial purposes. i am interested in changing mycrofts language to german. unfortunately, i do not understand the existing documentation correctly. also some links to urls is how to’s seems to be outdated or currently not working because of some bugs. i have basic administration skills in linux, but nothing related to tts, stt, mycroft skills, google-projects, etc.
maybe there is someone who is already using mycroft in german and wants to share a working config file.
many thanks.
","
in your mycroft configuration (either /etc/mycroft/mycroft.conf or ~/.mycroft/mycroft.conf
insert at the beginning right after the opening curly brace like following:
{
""lang"": ""de-de"",
  ""tts"": {
    ""module"": ""google"",
    ""google"" : {
      ""lang"": ""de""
    },
... more configuration ...
}

feel free to join mycroft-chat channel ~language-de

thanks dominik for your help. yes, it seems to work without bigger issues. 
what interests me now is how the language is related to the available skills. after the config customization you suggested, mycroft successful translated my spoken german into text and also wrote/spoke a german answer. however, it seemed like there were no skills anymore. neither the question “what is your name”, nor “how is the weather in berlin” could answered by mycroft.
also, the sound of the voice seems to be different than previously set-up on home.mycroft.ai. is the voice-sound independently changeable?

a lot - but not all - skills are already translated to german. try ""wie ist das wetter""or “wikipedia angela merkel”. unfortunately mycroft does not load a skill when on of the referenced vocab/intent files is not availlable. this happens from time to time when some new functionality is added to a skill and the translated vocab/intent file is not available (yet). but you can contribute missing translation at translate.mycroft.ai
the configuration changes the text-to-speech engine to google-tts as this is the only (“free”) tts-service that has a german voice.

hi there,
it seems, that there is a curly brace missing… 3 are opening, but unfortunately only 2 are closing…
where do i have to add it?
``
{
“lang”: “de-de”,
“tts”: {
“module”: “google”,
“google” : {
“lang”: “de”
}    here? ,
… more configuration …
}
cheers 
caruso

i got it on my own…
the place was the right on:
“lang”: “de”
} },
…more…

hi everyone, i want to change mycroft-language (linux) to german. in order to do that, i’ve changed the language of mycroft via configuration manager (with a command “mycroft-config edit user” in terminal “mycroft-core/mycroft/configuration”). after that, i pasted following command:
{
“max_allowed_core_version”: 20.8,
“lang”: “de-de”,
“stt”: {
“module”: “mycroft”,
“mycroft”: {
“lang”: “de-de”
}
},
“tts”: {
“module”: “google”,
“google”: {
“lang”: “de”
}
}
}
it seems that, mycroft is able to understand german spoken words  but responds with german phrases in an english accent. i would be verry appreciated, if you can help me to solve this problem.

check my language support guide, using catalan as reference language but is generic enough



github



jarbaslingua/mycroft-catalan.conf
mycroft in catalan tutorial. contribute to jarbaslingua/mycroft-catalan.conf development by creating an account on github.






it seems to be due to the google tts version i have included it in the instructions. https://github.com/gras64/docs-rewrite/blob/master/docs/using-mycroft-ai/customizations/languages/german.md . mycroft-pip install --upgrade gtts and mycroft-pip install --upgrade gtts-token help you.

upgrading gtts has worked for me 
"
56,using the bing tts engine,support,"
hi,
i am trying to use the bing tts engine (and the new azure speech service) but i was not able to figure out how to set the parameters in the mycroft.conf file. microsoft documentation says that i need to set the following parameters (in addtion to the api key “ocp-apim-subscription-key” which i allready have):

post /synthesize
http/1.1
host: speech.platform.bing.com

x-microsoft-outputformat: riff-8khz-8bit-mono-mulaw
content-type: application/ssml+xml
host: speech.platform.bing.com
content-length: 197
authorization: bearer [base64 access_token]

<speak version='1.0' xml:lang='en-us'><voice xml:lang='en-us' xml:gender='female' name='microsoft server speech text to speech voice (en-us, zirarus)'>microsoft bing voice output api</voice></speak>


i am not sure how to add these settings in mycroft.conf using the following format:

{
  “tts”: {
  “module”: “bing”,    <- this should be set to the name of your tts provider (ie ""google_cloud"", ""wit"" etc)
  “bing”: {            <- this should be set to the name of your tts provider (ie ""google_cloud"", ""wit"" etc)
    “lang”: “en-us”,           <- the ietf bcp-47 language code for your language (shown is hindi as spoken in india)
    “credential”: {            <- some stt engines require credentials - check the documentation for the stt engine
      “json”: {
      }
    }
  }
}


thanks in advance.
abdulrahman
","
hi there @abdulrahman2,
to try and find an answer for you here, i had a look at the mycroft-core source code for the bingtts class. you can see it here. based on the source code, the bingtts class is expecting something like;
{
  “tts”: {
  “module”: “bing”,
  “bing”: {           
    “lang”: “en-us”,
    ""api_key"": ""yourapikeyhere"", 
   }
 }
}


none of the other parameters are found in the bingtts class.
can you try the above configuration and let me know how you go?

thanks @kathyreid.
i tried that and unfortunately it did not work.
however, i think you are correct and the problem is actually caused by the proxy since i am correcting to the net through a proxy server.
even though the proxy setting is already set using the export commands and mycroft is working fine through the proxy, it seems that bing tts requests are not able to pass though the proxy.
thanks,
abdulrahman

hmmm. that doesn’t make sense though, because it’s mycroft that is handling the proxy requests, and the requests/responses to/from bingtts are handled thorugh mycroft.
is there anything in your voice.log, audio.log or skills.log with either;

confirms that this is a proxy issue
or provides an error relating to bingtts?

best, kathy

i don’t have access to the log files right now to check the errors. i will check them later.
however, i thought mycroft depends on this python library (python-bing-tts) (as mentioned in bing_tts.py) to do the bingtts requests.
thanks, abdulrahman

yes, that’s correct, mycroft uses python-bing-tts for abstraction
how did you go finding the logs?

if found the following errors in the audio.log file:

if the “python-bing-tts” package is not installed, i get the following error in the audio.log file:

modulenotfounderror: no module named 'bingtts'

if i use the following configuration:

“bing”: {
“lang”: “en-us”,
“api_key”: “[my api key]”
},
i get the following error in the audio.log file:
mycroft.audio.speech:mute_and_speak:126 - error - tts execution failed (languageexception('requested language en-us does not have voice male!',)).
so, i tried to use the gender field to pass the voice name.

if i use the following configuration:

“bing”: {
“lang”: “en-us”,
“gender”:“jessarus”,
“api_key”: “[my api key]”
},
i get the following error in the audio.log file:
mycroft.audio.speech:mute_and_speak:126 - error - tts execution failed (timeouterror(110, 'connection timed out')).
so now i am not sure where is the source of the error: is it the proxy or in using the python-bing-tts backage.
thanks, abdulrahman.

great information and troubleshooting @abdulrahman2
it sounds like bingtts is expecting a voice key-value pair. by default, mycroft sets the voice to “male” so i think that’s what’s happening.
do you know what voices are available with bingtts?
we could try adding a voice value like this:
“bing”: {
“lang”: “en-us”,
“voice”:“jessarus”,
“api_key”: “[my api key]”
},


it’s an old thread but i can’t access microsoft tts too. i got two keys form microsoft. both don’t work. are there any new hints?

do you get any error message (spoken, in cli or in log files)?
according to bing_tts implementation in mycroft-core you need to install a dependency - did you do that already?
last time the bing_tts code was updated is now 11 month ago - maybe there was a change on ms/bing api side?

i’v seen the error message:
                'bingtts dependencies not installed, please run pip install '                 'git+https://github.com/westparkcom/python-bing-tts.git ')
so i did it. now this message appears not longer but there is no answering from bing. what i’ve seen too is some differences between the mycroft tts documentation and a help text from @kathyreid some lines above. and i’m missing the option to give the endpoint of my microsoft azure into the configuration. i guess the endpoint is the connection to my account in combination with the key.
edit: here is a link to the the documentation from microsoft. maybe it’s useful:
https://docs.microsoft.com/de-de/azure/cognitive-services/speech-service/get-started-text-to-speech?tabs=script%2cwindowsinstall&pivots=programming-language-python. it’s in german, i guess you can understand it.
edit 2: now i tried again to connect to microsoft with another voice definition. these are the last audio.log lines:
2020-12-02 09:06:23.391 | info     |   712 | mycroft.audio.__main__:main:50 | starting audio services 2020-12-02 09:06:23.403 | info     |   712 | mycroft.messagebus.client.client:on_open:114 | connected 2020-12-02 09:06:23.410 | info     |   712 | mycroft.audio.audioservice:get_services:61 | loading services from /usr/lib/python3.8/site-packages/mycroft/audio/services/ 2020-12-02 09:06:23.416 | info     |   712 | mycroft.audio.audioservice:load_services:105 | loading chromecast 2020-12-02 09:06:31.656 | info     |   712 | mycroft.audio.audioservice:load_services:105 | loading mopidy 2020-12-02 09:06:31.664 | info     |   712 | mycroft.audio.audioservice:load_services:105 | loading mplayer 2020-12-02 09:06:31.673 | info     |   712 | mycroft.audio.audioservice:load_services:105 | loading simple 2020-12-02 09:06:31.687 | info     |   712 | mycroft.audio.audioservice:load_services:105 | loading vlc 2020-12-02 09:06:32.197 | info     |   712 | mycroft.audio.audioservice:load_services_callback:177 | finding default backend... 2020-12-02 09:06:32.199 | info     |   712 | mycroft.audio.audioservice:load_services_callback:181 | found local 2020-12-02 09:06:32.214 | info     |   712 | mycroft.audio.__main__:on_ready:30 | audio service is ready. 2020-12-02 09:06:43.433 | info     |   712 | mycroft.audio.speech:mute_and_speak:127 | speak: you can get started with mycroft. 2020-12-02 09:06:43.437 | debug    |   712 | mycroft.tts.mimic2_tts:get_tts:232 | generating mimic2 tss for: you can get started with mycroft. 2020-12-02 09:06:43.442 | debug    |   712 | urllib3.connectionpool | starting new https connection (1): mimic-api.mycroft.ai:443 09:06:43.442 - urllib3.connectionpool - debug - starting new https connection (1): mimic-api.mycroft.ai:443 2020-12-02 09:06:44.183 | debug    |   712 | urllib3.connectionpool | https://mimic-api.mycroft.ai:443 ""get /synthesize?text=you%20can%20get%20started%20with%20mycroft.&visimes=true http/1.1"" 200 105985 09:06:44.183 - urllib3.connectionpool - debug - https://mimic-api.mycroft.ai:443 ""get /synthesize?text=you%20can%20get%20started%20with%20mycroft.&visimes=true http/1.1"" 200 105985 2020-12-02 09:06:45.391 | debug    |   712 | urllib3.connectionpool | starting new https connection (1): api.mycroft.ai:443   
the last voice.log lines:
``
2020-02-07 16:51:03.220 | info     |   338 | mycroft.messagebus.client.client:on_open:114 | connected 2020-12-02 09:06:23.441 | warning  |   338 | mycroft.client.speech.listener:run:88 | audio contains no data. 2020-12-02 09:06:23.899 | info     |   338 | mycroft.util.audio_utils:find_input_device:196 | searching for input device: pulse 2020-12-02 09:06:23.912 | info     |   338 | mycroft.client.speech.listener:create_wake_word_recognizer:328 | creating wake word engine 2020-12-02 09:06:23.914 | info     |   338 | mycroft.client.speech.listener:create_wake_word_recognizer:351 | using hotword entry for hey mycroft 2020-12-02 09:06:23.917 | info     |   338 | mycroft.client.speech.hotword_factory:load_module:403 | loading ""hey mycroft"" wake word via precise 2020-12-02 09:06:27.665 | info     |   338 | mycroft.client.speech.listener:create_wakeup_recognizer:365 | creating stand up word engine 2020-12-02 09:06:27.667 | info     |   338 | mycroft.client.speech.hotword_factory:load_module:403 | loading ""wake up"" wake word via pocketsphinx 2020-12-02 09:09:09.527 | info     |   338 | mycroft.session:get:72 | new session start: bd330824-60eb-4595-bce8-5431e71effe7 2020-12-02 09:09:09.534 | info     |   338 | mycroft.client.speech.__main__:handle_wakeword:67 | wakeword detected: hey mycroft 2020-12-02 09:09:10.593 | info     |   338 | mycroft.client.speech.__main__:handle_record_begin:37 | begin recording... 2020-12-02 09:09:13.553 | info     |   338 | mycroft.client.speech.__main__:handle_record_end:45 | end recording... 2020-12-02 09:09:17.375 | info     |   338 | mycroft.client.speech.__main__:handle_utterance:72 | utterance: ['the institute on phone charger'] 

audio-log looks like you do not have bing-tts configured for tts. can you share the tts-section of your mycroft.conf?
but looking at the microsoft-documentation link you have provided it looks to me that there was a major change in den bing-tts api and mycroft’s bing_tts implementation needs an overhaul…

the tts section:
`
“tts”: {
“module”: “bing”,
“bing”: {
“lang”: “de-de”,
“voice”: “de-de-katjaneural”,
“api_key”: “123456789abcdef1234567”
},
`
i tried to find some hints, what parameters must given to the bing service, but i don’t understand enough of programing. maybe you can help.

current bing_tts implementation in mycroft-core accepts parameters „api_key“, „gender“ (default is „male“) and „format“ (for the audio format).
again, i think this implementation will no longer work as ms has changed the api. i don‘t have a bing api-key and i don‘t want to register for bing-services, otherwise i would look into it and try to apply the necessary changes - looking at ms/bing documentation it does not look too hard to do…

yes, you need an azure and microsoft account. that’s not good, but i’m looking for an alternative to google because it’s not reliable. i got the next step. there is a quickstart python code which i tested successfully. the address is https://github.com/azure-samples/cognitive-services-speech-sdk/tree/master/quickstart/python/text-to-speech. and you have to import a library called azure.cognitiveservices.speech (installed by pip3). with this code i heard the speech of the text i put into the input() var. it was german words with an us sound but it works generally. i the next days i will have a look at the library to get a voice with german sound. it would be very fine if you could find a way to integrate the necessary code in mycroft/picroft.

@dominik
now i got the right code to get a german answer from azure text-to-speech:
`
import azure.cognitiveservices.speech as speechsdk
speech_key, service_region = ""your_azure_key"", ""westeurope""
def speech_synthesis_with_language():
    """"""performs speech synthesis to the default speaker with specified spoken language""""""
    # creates an instance of a speech config with specified subscription key and service region.
    speech_config = speechsdk.speechconfig(subscription=speech_key, region=service_region)
    # sets the synthesis language.
    # the full list of supported languages can be found here:
    # https://docs.microsoft.com/azure/cognitive-services/speech-service/language-support#text-to-speech
    language = ""de-de"";
    speech_config.speech_synthesis_language = language
    # creates a speech synthesizer for the specified language,
    # using the default speaker as audio output.
    speech_synthesizer = speechsdk.speechsynthesizer(speech_config=speech_config)
    # receives a text from console input and synthesizes it to speaker.
    while true:
        print(""enter some text that you want to speak, ctrl-z to exit"")
        try:
            text = input()
        except eoferror:
            break
        result = speech_synthesizer.speak_text_async(text).get()
        # check result
        if result.reason == speechsdk.resultreason.synthesizingaudiocompleted:
            print(""speech synthesized to speaker for text [{}] with language [{}]"".format(text, language))
        elif result.reason == speechsdk.resultreason.canceled:
            cancellation_details = result.cancellation_details
            print(""speech synthesis canceled: {}"".format(cancellation_details.reason))
            if cancellation_details.reason == speechsdk.cancellationreason.error:
                print(""error details: {}"".format(cancellation_details.error_details))
speech_synthesis_with_language()
`

the modul azure.cognitiveservices.speech must be installed. the only vars to define are speech_key, service_region an language. the input must be set by messagebus i guess, by the text of an answer from the skill. what do you think: is it possible to integrate this into mycroft for working with azure speech services?

yes, that should work and necessary changes to bing_tts module shouldn‘t be too complex.

changed mycroft bing-tts module to use new azure rest-api - see pr #2775
"
57,mycroft repeats multiple times,support,"
hello,
depending the skill i’m using mycroft is repeating multiple time the things.
from previous posts, i saw people with multiple mycroft instances which is not my case.
for example, i asked mycroft:  who is steve job , it answered this 4 times. but if i ask to mycroft:  who is jojo it answers only 1 time.

here is the full process:

image1067×258 46.9 kb

output from the  audio.log
2020-12-02 12:48:34.482 | info     |  8743 | mycroft.audio.speech:mute_and_speak:127 | speak: steven paul jobs was an american business magnate, industrial designer, investor, and media proprietor
high performance mpeg 1.0/2.0/2.5 audio player for layers 1, 2 and 3
version 1.25.10; written and copyright by michael hipp and others
free software (lgpl) without any warranty but with best wishes

directory: /tmp/mycroft/cache/tts/googletts/
playing mpeg stream 1 of 1: 04daa2c0c044d766a4635d2111243486.mp3 ...

mpeg 2.0 l iii cbr32 24000 mono

output from  skills.log  :
2020-12-02 12:38:57.964 | info     |  8740 | questionsanswersskill | searching for who is steve job
removing event fallback-query.mycroftai:questionquerytimeout
removing event fallback-query.mycroftai:questionquerytimeout
['steven paul jobs was an american business magnate, industrial designer, investor, and media proprietor', ""he was the chairman, chief executive officer, and co-founder of apple inc., the chairman and majority shareholder of pixar, a member of the walt disney company's board of directors following its acquisition of pixar, and the founder, chairman, and ceo of next"", 'jobs is widely recognized as a pioneer of the personal computer revolution of the 1970s and 1980s, along with apple co-founder steve wozniak', 'jobs was born in san francisco, california, and put up for adoption', 'he was raised in the san francisco bay area', 'he attended reed college in 1972 before dropping out that same year, and traveled through india in 1974 seeking enlightenment and studying zen buddhism', ""jobs and wozniak co-founded apple in 1976 to sell wozniak's apple i personal computer"", 'together the duo gained fame and wealth a year later with the apple ii, one of the first highly successful mass-produced microcomputers']
2020-12-02 12:38:58.319 | info     |  8740 | questionsanswersskill | answer from mycroft-fallback-duck-duck-go.mycroftai
steven paul jobs (february 24, 1955 – october 5, 2011) was an american business magnate, industrial designer, investor, and media proprietor.
2020-12-02 12:38:58.917 | info     |  8740 | questionsanswersskill | answer from fallback-wolfram-alpha.mycroftai

i’m using the  dev version with the latest commits.
thanks for your help.
","
the issue seems to be from google voice, using mimic2 as tts make this issue disappeared.

same with polly (amazon) and watson (ibm) tts, no issues.
"
58,testing and feeback wake word led gpio,skill feedback,"
this skill is nothing really fancy, just a simple led connected to a raspberry pi’s gpio turning on when a wake word is detected and only during the record period (~3 seconds).
how to install wake word led gpio
follow the  readme  on the github repository to install and configure the skill.



github



smartgic/mycroft-wakeword-led-gpio-skill
control a led when a wake word is detected. contribute to smartgic/mycroft-wakeword-led-gpio-skill development by creating an account on github.





how to test wake word led gpio
trigger your mycroft device with your wake word.
where feedback on skill name should be directed a
any kind fo feedback will be appreciated on github repository issues.
ps: this is my first skill, be kind 
","
nice i will try it  ,i start to make my own script too

awesome, let me now if something goes wrong (here or on github).
"
59,no audio picroft 2 mic respeaker,support,"
i have an rpi4 and i am using a 2 mic respeaker and a mono speaker.  the audio seems to go to the hdmi as default and not to the speaker.  i use the respeaker / seeedvoice drivers from github, not the drivers that get installed via the setup-wizard.
when i first stood up my picroft, i had a monitor with built in speakers.  audio went out the monitor speakers by default.  not being able to re-direct them to the mono speaker plugged into the respeaker, i changed the monitor to something without speakers and did a fresh install.  however, i am still not getting sound out the speaker.
i’ve read all of the troubleshooting tips, but i’m not exactly sure what to do to force output to the speaker.
aplay -l output:

**** list of playback hardware devices ****
card 0: b1 [bcm2835 hdmi 1], device 0: bcm2835 hdmi 1 [bcm2835 hdmi 1]
subdevices: 4/4
subdevice #0: subdevice #0
subdevice #1: subdevice #1
subdevice #2: subdevice #2
subdevice #3: subdevice #3
card 1: headphones [bcm2835 headphones], device 0: bcm2835 headphones [bcm2835 headphones]
subdevices: 4/4
subdevice #0: subdevice #0
subdevice #1: subdevice #1
subdevice #2: subdevice #2
subdevice #3: subdevice #3
card 2: seeed2micvoicec [seeed-2mic-voicecard], device 0: bcm2835-i2s-wm8960-hifi wm8960-hifi-0 [bcm2835-i2s-wm8960-hifi wm8960-hifi-0]
subdevices: 1/1
subdevice #0: subdevice #0

mycroft.conf output:

cat mycroft.conf
{
“play_wav_cmdline”: “aplay -dhw:0,0 %1”,
“play_mp3_cmdline”: “mpg123 -a hw:0,0 %1”,
“enclosure”: {
“platform”: “picroft”
},
“tts”: {
“mimic”: {
“path”: “/home/pi/mycroft-core/mimic/bin/mimic”
}
},
“ipc_path”: “/ramdisk/mycroft/ipc/”
}

i’m assuming that aplay is pointing to the soundcard, but if it is, the output doesn’t go that direction.
any suggestions?
thank you.
daryl
","
after i posted this question, i was doing some testing with arecord and aplay,  as root, and my testing worked.  i read somewhere about my user needs to be in the audio group, which it is, so i’m wondering if there are further permissions, or another group the user should be in?
what group(s) should my user be in?  or what should permissions be for sound to be played?
thank you.
daryl
"
60,seedstudio 4 mic array not recognised by picroft,support,"
hello guys,
i have my picroft quite a while now, however, never functioning.
so here is the story:
i ve came up with a setup of a rpi 4 2gb ram with an headphone jack speaker, ethernet  connection, seedstudios 4 mic array and kingston high endurance 32gb microsd card.
so far so good…
i wrote the latest image to the card with the help of etcher, set everything up, followed the driver install guide from seedstudio (guide from seedstudio) and even installed the skill from j1nx “h.ttps://community.mycroft.ai/t/respeaker-4-mic-array-hat-mycroft-a-i-skill/5773/41” wich makes the leds blink.
sadly, whatever i am trying to do, i dont get the mic being recognised by mycroft, however the speaker output and blinking leds just turn out to be fine.
so here comes the part where i am searching for your advise:

whenever i make a call the mic level stays zero
when i make the call to get the name for the soundcard (as stated in the guide: ~/seeed-voicecard $ arecord -l) it just returns its a directory
when using the command from this site "" pactl list sources short""
mycroft troubleshooting page, it just returns :

0       alsa_output.platform-bcm2835_audio.analog-mono.monitor  module-alsa-card.c      s16le 1ch 48000hz       suspended

when trying to make a record with “arecord -dac108 -f s32_le -r 16000 -c 4 hello.wav”, it returns

alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm ac108
arecord: main:828: audio open error: no such file or directory

when using the mic array on a blank raspbian, it works just fine until the moment i install mycroft (followed guide “h.ttps://www.j1nx.nl/project-1/diy-home-personal-ai-assistant-installing-configuration-part-4/”)

i am assuming mycroft is messing up the mic input does anyone know how to fix this ?
any help would be great, thanks (:
","
mycroft isn’t doing anything here.  your system doesn’t recognize the device as a pulse input:
when i make the call to get the name for the soundcard (as stated in the guide: ~/seeed-voicecard $ arecord -l) it just returns its a directory
if it’s working at some point, and not after, then something about what happens in that step is going to be suspect.  someone who has that device might be able to offer more direct help, but reviewing what goes on in the guide you followed might be useful in the mean time.




 sples:

followed guide


this one is rather old (@j1nx maybe have some update on that one) , and i hope you followed the other 3 parts. from a short look i can see that pulse is set up systemwide, so mycroft has to set up to embrace this fact.
the arrays are relatively easy to set up with the install script (seeed-voicecard)  doing the heavy lifting. i wouldn’t make the mistake to troubleshoot from the perspective of a tut which main focus was to setup an embedded device: ovos (formerly known as mycroftos). why don’t you simply use his alpha image (and help develop it further)?  i would guess that at least the voicecard problem is gone using his image

not much time but if “arecord -l” doesn’t return the respeaker mic then the kernel modules are not loaded.
either they are not properly build for your kernel, not loaded at all or already crashed (see link below)
(keep in mind the respeaker drivers are very wonky with the new 5.x kernel. 32bit and 64bit)


github.com/respeaker/seeed-voicecard








spinlock issue / ""bug: scheduling while atomic""



        opened 08:09am - 14 oct 20 utc




          hintak
        





first reported in #246 (comment) , haven't been tackled in #249 .










thanks for all your replies !

baconator:
if it’s working at some point, and not after, then something about what happens in that step is going to be suspect. someone who has that device might be able to offer more direct help, but reviewing what goes on in the guide you followed might be useful in the mean time.

yea i know, because of that i have redone the seedstudio tutorial for the installation, since this is the only step i can repeat (the rest is the ready made setup from mycroft).

sgee:
this one is rather old (@j1nx maybe have some update on that one) , and i hope you followed the other 3 parts… easy to set up with the install script (seeed-voicecard)…why don’t you simply use his alpha image

as far as i could, i followed the guide, but i stumbled over additional problems. but its not the current installation, it was just a try to see if it solves my problem, it didnt… i have used an install script from seedstudio, its from seedstudios guide cd seeed-voicecard; sudo ./install.sh  (step 2), is this the one you stated ? … i am using a mycroft image right now, its this one “picroft stable disk image” (link), is that one correct, did you mean that with alpha image ?

j1nx:
not much time but if “arecord -l” doesn’t return…either they are not properly build for your kernel, not loaded at all or already crashed (see link below)
(keep in mind the respeaker drivers are very wonky with the new 5.x kernel. 32bit and 64bit)

i really hope my mic array isnt having any of the problems you stated. however when i read your quote of the arecord command, i realised that my syntax was wrong. from the tutorial it seemed to be smth. like this ~ $ arecord -l but it seems to be more like arecord -l
so i fed it in the console and actually got something: (left side is what i got, right side is how it should look)

console out comp1424×732 56.8 kb

it looks like there is something missing, do you know how to add that ?

furthermore i also made a picture of the console output, when i force mycroft to interact with me.
it then actually speaks out the question, starts the recording, i answer, but he cant recognise
(propably the sound file is empty)

mycroft output 11897×1001 57 kb


i’m talking about openvoiceos (mycroft based gui distro)
the tutorial you are talking about is work that lead to ovos. yet it’s set up differently and isn’t compareable (unless you have a deeper understanding of the core elements).
for debugging your problem: it is always good practice to look at /var/log/mycroft/*.log if you’re looking for causes

lot to unpack here.
if you are running picroft and not have tinkered with /etc/mycroft/mycroft.conf
i expect this line here “play_wav_cmdline”: “aplay -dhw:0,0 %1”
if you followed j1nx’ path first off you need to use paplay, then the specification -dhw:0,0 should be tossed  (in general) in both play_wav and play_mp3
since we don’t know where you are at with mixing jinx ovos setup/picroft we can’t give informed suggestions.
that said, there are some config files the voicecard installer sets softlinks to (if i’m not mistaken):  one of which is
seeedvoicecard/pulseaudio/pulse_config_4mic/default.pa
this default.pa values/modules are not exactly set up to what mycroft needs
the following lines are derived from ovos for a respeakerv2 usb, you just have to specify the base directory where you put the driver/install package
sudo sed -i ""s/^load-module module-stream-restore/#load-module module-stream-restore/"" /directory-where-you-put-the-driver/seeedvoicecard/pulseaudio/pulse_config_4mic/default.pa
sudo sed -i ""s/^load-module module-role-cork/#load-module module-role-cork/"" /directory-where-you-put-the-driver/seeedvoicecard/pulseaudio/pulse_config_4mic/default.pa
sudo sed -i ""s/^load-module module-suspend-on-idle/#load-module module-suspend-on-idle/"" /directory-where-you-put-the-driver/seeedvoicecard/pulseaudio/pulse_config_4mic/default.pa
sudo sed -i ""s/^load-module module-udev-detect/load-module module-udev-detect tsched=0/"" /directory-where-you-put-the-driver/seeedvoicecard/pulseaudio/pulse_config_4mic/default.pa
sudo sed -i 's/^#load-module module-native-protocol-tcp/load-module module-native-protocol-tcp auth-ip-acl=127.0.0.1;192.168.0.0\/16;172.16.0.0\/12;10.0.0.0\/8 auth-anonymous=1/' /directory-where-you-put-the-driver/seeedvoicecard/pulseaudio/pulse_config_4mic/default.pa
sudo sed -i ""s/^load-module module-native-protocol-unix/load-module module-native-protocol-unix auth-anonymous=1/"" /directory-where-you-put-the-driver/seeedvoicecard/pulseaudio/pulse_config_4mic/default.pa
sudo sed -i ""s/^#load-module module-zeroconf-publish/load-module module-zeroconf-publish/"" /directory-where-you-put-the-driver/seeedvoicecard/pulseaudio/pulse_config_4mic/default.pa

the last line needs module-zeroconf-publish installed: sudo apt-get install module-zeroconf-publish

to circumvent such problems for future users a setup script should be made where the (voicecard) installer got sourced - to work with their variables - (if possible) and sent to me to incorporate it in the setup refactor.

i had a very similar issue and just finally got my respeaker 2 mic to work.  after going around in circles for several days chasing down all sorts of errors and issues, i finally did a fresh install and this time i skipped the setup wizard and just installed the drivers from github.
respeaker / seeed-voicecard  my picroft is now working.  i have a minor issue to figure out, but once i get that worked out, then i can start having fun with it.
daryl

did you check if these two libs gpiozero spidev are installed ?
pip freeze
install python -m pip install gpiozero
install python -m pip install spidev

hey, thanks again for all these replies, didnt expected that much activity, im really impressed by the mycroft community now. sadly i havent had much time to apply all your suggestions, but this weekend i tried a new installation on a new micro sd card, after a few flashing problems, and skipping the automatic setup, like @rosede suggested. i just installed the seedstudio library, etc. and now everything works fine. im really happy with the end result. @maxim.nomens, yes they luckily already have been installed.
im also very thankful for the time @sgee must have spent to give so detailed information for debugging, i will keep that in mind if i step in additional problems
"
61,alternate uses of the mark 1,none,"
background: i have a mark 1 sitting in a box. i got it last year in the hopes of finding some way to run music locally, without amazon or google products.  (i ended up bowing to amazon music and using musicolet on an unused phone when the internet is out.)
so right now i’m looking at possibly setting up a web server that will run node.js as part of omertu’s googlehomekodi. (i had been using glitch as the web host piece but they dropped it.)  could i use the mark 1 to do that?  i don’t really want to run a full size pc and it’s just sitting there…
if i can’t, then is there anything else interesting to do with it?  i have echo dots in every room and a google mini that  is part of the kodi voice control, so i don’t really need the voice assistant part of it.  i understand that a pi3 is the heart of it, so it seems like there might be?
editing to add:  after reading more of the documentation, maybe my question is more simple.   does a raspberry pi include node.js and npm, so that i could just add the code from the project cited above? i’ve been poking around github and the mycroft docs and it doesn’t really say what a baseline pi includes.
","
i’d suggest looking to sell it. playing with a pi is fun, but there’s stuff in the mark 1 that’s hard to get if you’re a fan of mycroft. a lot of people i know end up making picrofts to fill the gap between the mark 1 and the mark 2. while the mark 2 is making exciting progress, i know several people (including myself) that would love to get their hands on a mark 1.

you can definitely use it for that. just connect via ssh and install npm.
i currently use my mark 1 as a web server and a noise-activated light switch (with a manual override when the button is clicked).

that’s a good point. i definitely overpaid looking at it as just a pi3. but i didn’t think that someone else might be able to put it to better use.  i guess i’m not opposed to re-homing it… gives me something to think about.
i just looked at ebay and there weren’t any for sale - even completed.  that hardly ever happens with tech stuff.

if you do decide to put it up for sale, please post the link here. i’m sure i’m not the only one with some interest.
"
62,mycroft is slow to respond details inside,general discussion,"
i’ve just installed mycroft on my raspberry pi.
i’m 100% new ai assistants so i have no experience, but i said to commands to mycroft, at first i thought he wasn’t listening or picking up my voice but then after a couple of minutes i got a reply.
should mycroft take this long to respond? is there anything i can do to improve this?
thanks for your help
","
have seen the same problem … at least 30 seconds to respond.
q: is there any particular log file (other than view_log) we shoudl be attaching to the there posts for others to view?
currently mycroft configuration
( ?? is there a print_mycroft_config ) command ?
v0.8
pi3 with official pi 3 power supply
ethernet connection to internet
3.5 speaker
no mic ( yet )

having the same problem.

also having the same problem with a pi3 and v0.8b of picroft

the response time is longer when the text to speak is long. if i ask it “how are you?”, the response time is quite acceptable. i believe it takes time for the tts engine to process it. the longer the text, the longer it takes to respond.
however, i believe it also takes time for the query to send to the online stt engine for processing.
i believe both the stt and tts engines are located in the internet. if both can be hosted locally, i believe it will solve the response delay problem.

getting the same problem on a brand new pi3b.  any solutions? thanks.

actually after a couple of reboots it’s a lot faster. don’t know why, but anyway, it’s just about acceptable speed.

rebooting makes no difference for me.
it responds to maybe the first ‘hey mycroft’ if i’m asking for the time or weather.
if i try again in a minute or so it might recognise the wake word, but it could be 30 seconds to not at all before it responds.
it’s quite odd the differences we are experiencing given that we are mostly using the same hardware.
perhaps the next release will address these issues.

i was absolutely in love with the idea of an open source ai assistant for the raspberry pi, but after installing and booting up mycroft on my pi 3 i was utterly disappointed.
after asking “hey microft, what time is it?” it gave me no sign that it had heard me and had no response to my question until minutes after. although i am relatively new to ai assistants, my realization is that mycroft is late to the party and incomparable to the alexa and google ai assistants which respond within seconds(my alexapi responds within 2-5 seconds upon asking a question and gives me an indicating ‘beep’ when i say the wake word ‘alexa’ and a responsive ‘beep’ after i ask a question).
i love the idea of an open source ai assistant, but the technology is just too slow and far behind right now to have much success.

i came looking for the same topic but my latency wasn’t more than 4/5 sec - considering what many here are experiencing - 30+ sec even minutes and on the same pi3 as i am, that imho has to be related to network issues -
i’ve set a static ip address for starters and use a managed opendns service along with dnscrypt pointing to the same cisco 53 service -
my router allows for prioritization of apps/traffic/hosts - i haven’t yet set it that way though, so maybe i could do that - flushing the caches instead of reboot could also be done by;
$ free -hb
mem:           937m        327m         31m         46m        579m        507m
swap:          1.0g        7.5m        1.0g
then as root
$ sudo su -
sync; echo 1 > /proc/sys/vm/drop_caches
sync; echo 2 > /proc/sys/vm/drop_caches
go easy on echo 3 – it will flush deep mem ;
sync; echo 3 > /proc/sys/vm/drop_caches
#!/bin/bash
note, we are using “echo 3”, but it is not recommended in production instead use “echo 1”
echo “echo 3  > /proc/sys/vm/drop_caches”
chmod 755 clearcache.sh
crontab -e
“append the below line, save and exit to run it at 2am daily.”
0  2  *  *  *  /path/to/clearcache.sh
swapoff -a && swapon -a
maybe an enlarged swap file , on 2gb usb3 flash drive would help some too (?on pi3) on pi4 shouldn’t be necessary not with 4gb ram anyways  - a google search on how to create it can easily be found
quoting quora;
"" so, in general, usb 3 should be nearly twice as fast as the fastest sd cards . however, a usb 3.0 bus often runs at speeds of 1/3 to 1/2 of the design maximum. … for a very fast 300 mb/s sd card , you can expect to pay about 4 times as much per gb as you would a 100 mb/s card ""
"
63,background audio library samples,feature requests,"
one of the less commonly talked about things that would be potentially useful for improving things would be collecting the background noise of people’s environments.  there’s tools to build denoising models out and about, and while we can rely on the stock models, building up enough samples to make a more representative mycroft model could be useful if/when denoising gets added to the audio pipeline.   a couple of 3-5 second clips of your location without anyone speaking (or even with non-distinct speaking in the background) would be it. if there’s significant variation from times of day, perhaps a sample from each relevant time.  since it’s not holding voice data, should be anonymous collection.  not a high priority by any means, maybe even add it as an option for a precise/wakeword collector tool.
",
64,plexmusic skill test and feedback,skill feedback,"
mycroft is learning how to read plex media servers. (plex homepage)
i would really appreciate any help to get the skill into the marketplace, feel free to give any kind of feedback to the sourcecode on github
nobody should ever be missing features 
install via msm by typing

msm install https://github.com/colla69/plexmusic-skill.git

i am interested in testing the installation and the synchronization with the plexmediaserver, as well as the requirements scripts. i work on kde and have no means to test the functionalities on different devices.
after the installation the skill will try to download metadata from the server. that will only be possible by adding configuration. after you configured the skill you can refresh your library using an intent.

hey mycroft, refresh|reload libray

feel free to reply in the comments or to contact me privately on a.colarietitosti@googlemail.com if you need any help.
please take a look at the readme on github as it contains further information about the configuration and usage of the skill.
i plan to extend functionalities, once i am happy with the vlc-backend. i would like to have it play radio channels as well as being able to add the song i am currently listening to i like to a playlist.
i wish you all happy testing 
colla
","
very exciting, thanks colla!
will try to find some time to give it a test run over the weekend 

thanks gez 
i read about your afk time…
congrats 

super excited about this skill as i don’t have/want to use spotify or any other commercial music service and i have all my music on my plex instance!
i’ll test it this very evening!

it doesn’t work on my settings.
connected to messagebus!
17:59:24.769 - playback control skill - info - resolving player for: helevorn
17:59:25.121 - playback control skill - info -    no matches
  ^--- newest ---^ 

i configured the skill through the https://account.mycroft.ai/skills site, like this

image683×681 28.2 kb

as you can see, i used the internal ip address with the port, instead of the published ip, as i connect plex through https://app.plex.tv

i’ve also tried to edit mycroft.conf and adding the settings, it doesn’t work either.
is like the skill isn’t properly installed, when i try to refresh library, skill.log says:
18:57:08.969 - questionsanswersskill - info - searching for refresh library
18:57:10.166 - questionsanswersskill - info - timeout occured check responses


hey malevolent  thanks for testing.
try configuring it with the ip ( http://192.168.1.100 ).
it should load your library on startup, it will say something as it is done loading, after that the search should work.
the “refresh library” intent should also work then, in case it doesn’t try changing the skill priorities in your mycroft.conf .
and let me know if you get it running 

sorry, i forgot to tell i also tried the url without the port. no luck.
i don’t know how to change priorities. and that means to set higher priority for plexmusic over pandora or spotify skills?

hey 
forget the priorities … we’ll fix that problem when and if it arises at all  /
my best guess at the moment is, the skill throws an exception on init and doesn’t load at all 
would you please try it this way:
-msm uninstall plexmusic-skill > wait for it to uninstall correctly.
-stop mycroft through “stop-mycroft.sh”
-clear the content of /var/log/mycroft/skills.log
-write the configuration in your mycroft.conf
“plexmusic-skill”: {
“musicsource”: “http://192.168,1,100”,
“plextoken”: “----8s6up----abc4srye”,
“plexlib”: “music”,
“ducking”: true
}
-start mycroft through command “mycroft-start.sh debug”
-let the boot process finish > send to me the content of /var/log/mycroft/skills.log
hopefully i can find out how the problem arises.
thank you for your help 

sorry for the late response… i think we’re are on a opposite timezone 
i had to do it a bit different as you suggested, as i have a mark i, but i think it will be valid for you:
msm remove plexmusic-skill
systemctl stop mycroft-skills.service
truncate --size=0 /var/log/mycroft/skills.log

edited mycroft.conf and added the plexmusic-skill section, then:
systemctl start mycroft-skills.service

here you have the skill.log after everything is up.
https://privatebin.net/?069749a8b24d0554#dpcnvyuvuewyoz2xmudrdrrsxhlyajkrfeb9st1m4ygj
what i didn’t undestood is why you didn’t made me install back plexmusic-skill, so, to save 24h, i repeated the steps but now adding the skill, just in case 
systemctl stop mycroft-skills.service
truncate --size=0 /var/log/mycroft/skills.log
msm install https://github.com/colla69/plexmusic-skill.git

the skill isn’t shown on home.mycroft.ai/skills
systemctl start mycroft-skills.service

and the skills.log:
https://privatebin.net/?4560a23ff2baa59b#dbwkqwtnmv6sappx12dggenzbhoswuky2yro1s5n7scw
here we can clearly view there is a problem loading the skill
12:34:42.328 - mycroft.skills.core:load_skill:122 - info - attempting to load skill: plexmusic-skill.colla69 with id plexmusic-skill.colla69
12:34:43.553 - mycroft.skills.core:load_skill:169 - error - failed to load skill: plexmusic-skill.colla69
traceback (most recent call last):
  file ""/opt/venvs/mycroft-core/lib/python3.4/site-packages/mycroft/skills/core.py"", line 135, in load_skill
    skill = skill_module.create_skill()
  file ""/opt/mycroft/skills/plexmusic-skill.colla69/__init__.py"", line 370, in create_skill
    return plexmusicskill()
  file ""/opt/mycroft/skills/plexmusic-skill.colla69/__init__.py"", line 168, in __init__
    self.vlc_player.get_media_player().audio_set_volume(100)
  file ""/opt/venvs/mycroft-core/lib/python3.4/site-packages/vlc.py"", line 2654, in get_media_player
    return libvlc_media_list_player_get_media_player(self)
  file ""/opt/venvs/mycroft-core/lib/python3.4/site-packages/vlc.py"", line 4926, in libvlc_media_list_player_get_media_player
    ctypes.c_void_p, medialistplayer)
  file ""/opt/venvs/mycroft-core/lib/python3.4/site-packages/vlc.py"", line 246, in _cfunction
    raise nameerror('no function %r' % (name,))
nameerror: no function 'libvlc_media_list_player_get_media_player'

by the way, vlc is installed.
pi@mark_1:~ $ dpkg -l| grep vlc
ii  libvlc5                            2.2.7-1~deb8u1                              armhf        multimedia player and streamer library
ii  libvlccore8                        2.2.7-1~deb8u1                              armhf        base library for vlc and its modules
ii  vlc                                2.2.7-1~deb8u1                              armhf        multimedia player and streamer
ii  vlc-data                           2.2.7-1~deb8u1                              all          common data for vlc
ii  vlc-nox                            2.2.7-1~deb8u1                              armhf        multimedia player and streamer (without x support)
ii  vlc-plugin-notify                  2.2.7-1~deb8u1                              armhf        libnotify plugin for vlc
ii  vlc-plugin-samba                   2.2.7-1~deb8u1                              armhf        samba plugin for vlc



i think you need to upload your vlc. i am currently running 3.0.7.1
╭─cola@cola-desktop ‹system› 
╰─$  dpkg -l| grep vlc

ii  libvlc-bin:amd64                                3.0.7.1-0ubuntu18.04.1                              amd64        tools for vlc's base library
ii  libvlc5:amd64                                   3.0.7.1-0ubuntu18.04.1                              amd64        multimedia player and streamer library
ii  libvlccore9:amd64                               3.0.7.1-0ubuntu18.04.1                              amd64        base library for vlc and its modules
ii  phonon4qt5-backend-vlc:amd64                    0.10.1-2                                            amd64        phonon4qt5 vlc backend
ii  vlc                                             3.0.7.1-0ubuntu18.04.1                              amd64        multimedia player and streamer
ii  vlc-bin                                         3.0.7.1-0ubuntu18.04.1                              amd64        binaries from vlc
ii  vlc-data                                        3.0.7.1-0ubuntu18.04.1                              all          common data for vlc
ii  vlc-l10n                                        3.0.7.1-0ubuntu18.04.1                              all          translations for vlc
ii  vlc-plugin-base:amd64                           3.0.7.1-0ubuntu18.04.1                              amd64        multimedia player and streamer (base plugins)
ii  vlc-plugin-notify:amd64                         3.0.7.1-0ubuntu18.04.1                              amd64        libnotify plugin for vlc
ii  vlc-plugin-qt:amd64                             3.0.7.1-0ubuntu18.04.1                              amd64        multimedia player and streamer (qt plugin)
ii  vlc-plugin-samba:amd64                          3.0.7.1-0ubuntu18.04.1                              amd64        samba plugin for vlc
ii  vlc-plugin-skins2:amd64                         3.0.7.1-0ubuntu18.04.1                              amd64        multimedia player and streamer (skins2 plugin)
ii  vlc-plugin-video-output:amd64                   3.0.7.1-0ubuntu18.04.1                              amd64        multimedia player and streamer (video output plugins)
ii  vlc-plugin-video-splitter:amd64                 3.0.7.1-0ubuntu18.04.1                              amd64        multimedia player and streamer (video splitter plugins)
ii  vlc-plugin-visualization:amd64                  3.0.7.1-0ubuntu18.04.1                              amd64        multimedia player and streamer (visualization plugins)

i hope this fixes the problem  good luck

the i heart radio skill was facing a similar issue with vlc, details are here:




i heart radio internet radio streaming skill - testing and feedback skill feedback


    this is a follow-up to my previous skill which uses tunein to find internet radio streams. i heart radio has a slightly different set of stations and is skewed towards the us market and commercial radio stations. the combination of tunein and iheartradio should get 95% of worldwide radio stations. 
important notes: 

most of the iheartradio streams are aac or mp4 streams so the skill uses vlc for streaming instead of mpg123. this currently is problematic as vlc doesn’t seem to install nicely usi…
  


tldr:
from mycroft.audio.services.vlc import vlcservice
https://github.com/johnbartkiw/mycroft-skill-iheartradio/blob/master/init.py

that’s exactly what i was thinking about when i wrote:
“i plan to extend functionalities, once i am happy with the vlc-backend…”  
i read the vlcservice code and it has all the functionalities i originally had in mind 
thank you, a lot 

hey @malevolent , i just swapped my vlc service with the one suggested by @gez-mycroft.
could please reinstall the skill,  and let me know if it solves your problem?
( just to be sure…
you should be using commit  a9dc40ff77ef7125e8e81046c15e05e8dca2a99c  or later  )
thank you both, 
colla

unfortunately, mark i is still on debian 8. and its vlc its frozen at version 2.2.
so no luck until mark i will be upgraded to debian 10 (hopefully some day)…
i can try the skill on arch linux, but it would have been great to listen music directly on the mark i.

thanks for the note, malevolent. i too am a mark i user, and also very interesting in plex capability. that would be really fun functionality to add to mycroft, and one of those features that could be a real differentiator (with respect to the other “big” names in digital assistant/voice automaton/etc).

i tried this skill on an arch linux desktop. it doesn’t install via mycroft-msm, and git cloning the repo and pip installing the requirements, either work
traceback (most recent call last):
  file ""/home/malevolent/development/mycroft-core/mycroft/skills/skill_loader.py"", line 204, in _load_skill_source
    skill_module = imp.load_module(
  file ""/usr/lib/python3.8/imp.py"", line 234, in load_module
    return load_source(name, filename, file)
  file ""/usr/lib/python3.8/imp.py"", line 171, in load_source
    module = _load(spec)
  file ""<frozen importlib._bootstrap>"", line 702, in _load
  file ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  file ""<frozen importlib._bootstrap_external>"", line 779, in exec_module
  file ""<frozen importlib._bootstrap_external>"", line 916, in get_code
  file ""<frozen importlib._bootstrap_external>"", line 846, in source_to_code
  file ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  file ""/opt/mycroft/skills/plexmusic-skill/__init__.py"", line 22
    <<<<<<< updated upstream
    ^
syntaxerror: invalid syntax
 19:31:24.538 | error    | 217249 | mycroft.skills.skill_loader:_communicate_load_status:287 | skill plexmusic-skill failed to load

looks like an unresolved git merge conflict.

@malevolent @j1nx
hey guys … sorry for taking so long…
i corrected the errors … that is kind of embarassing i should have foreseen that happening.
i also had to install a new mycroft device and set it up from scratch, it had some problems loading the configuration.
it eventually loaded the values after a couple restarts.
you can see that the configuration hasn’t loaded right when it logs:
 connecting to:

without any content.
if you still have the motivation to do so you should be able to test it now.
i broke it as i was implementing a set of new features that are still missing documentation, they should be working now.


by search  ->   “hey mycroft, play {title} by {author}”
if you had the skill running before you should reload the config to get the new search keys built. call “hey mycroft, reload library” and wait for it to confirm completition.

random music -> “hey mycroft, play [some] random music”
it will shuffle all the titles from your library

add to playlist -> “hey mycroft, add to playlist {playlistname}”
will add the title you are listening to to the named playlist. the playlist needs to exist and be loaded in your local config. so if you add a playlist after downloading your track list. you should “hey mycroft, reload library”  before you can add anything to it.

let me know how it goes  i am happy to get any kind of contribuition.
have a nice weekend and happy testing!
edit: added explanation for reloading config to init by search on an existing installation

humm, i cannot see plexmusic skill on home.mycroft.ai to configure it now.
and mycroft says configuration is missing, configure plexskill
i’ve rm -rf the skill and msm installed again, but it still doesn’t appears on home.mycroft.ai… can i be able to configure it via configuration file?
"
65,crowdfunding update stickers coins and mark is,general discussion,"
originally published at:			http://mycroft.ai/blog/crowdfunding-update-stickers-coins-and-mark-is/
we’ve got an update for our kickstarter and early indiegogo backers!
if you’re a super backer or challenge coin backer and you have completed your survey from crowdox, your perks are on the way! if you backed one of those perks and have not filled out your survey, what are you waiting for?? check your inbox for mycroft-mark-ii-the-open-voice-assistant@support.crowdox.com and fill it out!
if you can’t find your survey, email hello@mycroft.ai from the email you made your pledge with, and we’ll help out. we show about 85% of surveys are completed and we won’t rest until everyone has their pledges confirmed!
now for what you’re really interested in – mark i + mark ii family packs. as you can see, we’re hard at work assembling your mark i units for delivery to our fulfillment partner, easypost, by the end of april.

slight delay
with the mix-up we mentioned in this blog post, we had to return a batch of pi3 b+ units for pi3 bs, which set us back a couple of days. that means you might not see your tracking number in your crowdox portal by april 30 but fear not! easypost will be working hard to ensure your units are properly packed and headed your way. we'll get your tracking numbers uploaded as soon as we have them.
while you wait
haven’t pledged yet for a mark ii? you still can! we’re still on indiegogo with immediate shipping perks like superbacker stickers, challenge coins, and mark i’s, which are shipped on a rolling basis. and, you can get in line for your mark ii, which will start shipping in december.
thanks for supporting our journey. we really can’t do this without you! while your mark i’s are getting shipped and your mark ii’s are being built, head over to home.mycroft.ai to help train precise, our wakeword listener, and while you’re there, you can opt-in to help make mycroft better for everyone. after that, you can read, use, and suggest improvements to docs, download picroft or install mycroft on your linux computer, or just join the forum and chat to introduce yourself to the community and get engaged!
",
66,help with picking the right components appreciated,none,"
hi,
i’m a complete newby to mycroft and digital assistants in general. one reason for not having paid attention to them before, is my reluctance to get into bed with companies like amazon, google, apple, etc. recently i ran into jasper, a project similar to mycroft, and also got my first two raspberry pi’s. because the jasper project hasn’t seen any activity for at least 4 years, i searched a bit and stumbled upon mycroft.
i am not a linux whizz-kid and don’t know anything about electronics. however, i have been a self employed programmer for more than 35 years and know my way around linux and its command line. enough to do my job at least. i also managed to install and get mycroft up and running from the enclosure-picroft on github. not that difficult, as the image creation recipe is very clear and the update.sh script very mature and stable apparently. my sincere compliments for that.
because it is still an experiment, i connected an extremely old logitech desktop microphone and a set of almost equally and incredibly cheap unpowered speakers to the pi. results are bad to say the least. i did manage to have mycroft respond to two or three commands, but no more than that. i have to glue the speakers to my ears to hear anything useful and have the distinct feeling the microphone isn’t very helpful in making mycroft listen to me. so i spend several hours reading forum posts and browsing the internet, trying to decide which components to get. i still have now idea,
the way i see it there are two directions:

get an all in one, wired usb speakerphone like the jabra 410/510 or a wired usb microphone + usb speakers.
get a microphone array like respeaker core v2 or matrix voice and connect a speaker directly to the pi somehow.

both have there advantages and disadvantages. the first options seems simple and straight forward, but i have read quite a few stories of people struggling to get it up and running and keeping it that way. the second option takes me into the world of more advanced pi usage, which i’m not quite sure i’m ready for yet. it feels like the proper pi thing to do, but i’m hesitant because i haven’t the foggiest idea what i need exactly, let alone how to connect it and get it all to work.
i really want something that’s fail safe and bullet proof. i’m not a wealthy guy (understatement) but i prefer to spend a few euro’s more for good quality components that work without issues out of the box, than spending a lot of time and getting frustrated, just because i made poor choices for the components. so some help with picking the right components is very much appreciated.
","
can’t speak for the jabra thingy (yet “skype certified” makes me cringe), but my respeakerv2 (not the “core” one) has proven pretty reliable. even an enclosed version is available. the slightly higher price you pay is eased by a next to none setup time. works essentially ootb. and i could stand by with finetuning pulse.
(could even send a script, because i’m tweaking auto_run.sh right now to flesh out the setup procedure of respv2)

thanx for the swift reply.

yet “skype certified” makes me cringe

i understand what you mean  but i assume you refer to microsoft and not so much to jabra.
your satisfaction with the respeaker v2 is encouraging. so i guess i will stick to that. fortunately it’s available in the netherlands as well.

my respeakerv2 (not the “core” one)

i’m not quite sure what the differences are. from what i can see, the “core” one is an “intelligent” version of the mic array, with its on board cpu, memory and os. i can’t grasp the practical differences though and whether it has any benefits to choose the one over the other.

even an enclosed version 1 is available.

looks good as well. it might be less flexible if i ever summon the courage to create a fancy housing, but i guess i can always take it out of the enclosure by then 
so how about speakers? any suggestions for that as well? should i opt for usb speakers or something connected to the dedicated pi audio jack? i read quite a bit of problems with bluetooth components. should i stay away from bluetooth speakers?

(could even send a script, because i’m tweaking auto_run.sh right now to flesh out the setup procedure of respv2)

if i need it, a script would be much appreciated, although at this moment, i don’t know what to do with it yet  in a similar fashion i really don’t know what you mean by “respv2”. it will take some time before i get the hang of it all.

respv2 is just the lazy version of respeakerv2.
see, i can only refer to respv2. i don’t really know much about the core version and its intelligence.
yet, there are several things that let me shy away from core. most of which is the limited environment (software/ram). that they use a tailored version os make me believe its a pretty involved process to brew it yourself to use the cards features if there’s any problem with mycroft. the limited ram pretty much denies gui. and the price tag of 99 bucks… thats a usb respv2(barebone)+pi4b (roughly).
but it all comes down to your use case. if your ok with these limitations, go for it. personally i wouldn’t take the chance. the risk to price doesn’t add up.
if you want the extra careful and “guaranteed to be supported” route wait the revisions revision of sj201 (just to add this option to the table)
speaker: every contemporary 3-way-speaker > 20w (rms) should do, if you’re used to above average volume or want a wider range you have to pre-amp them.

thanx again.
i understand what you mean and tend to agree. the enclosed respeaker usb mic array is on its way 
as for the speaker, i have a an old powered pc speaker set catching dust somewhere. i’ll use that one to get the proof of concept running stable first and then look for a more sophisticated solution.
lastly about that script you mentioned. you also mentioned that the respeak v2 essentially works ootb, which i interpreted roughly as “out of the box”  so how does that script fit in? do i need that script?

out of the box is a very relative term theses days, even more true in a highly customized/customizeable environment.
auto_run.sh  - the startup/update script - sets you up in a general way, adding some pip packages, flashes the firmware and alters the mycroft.conf. yet 48kfirmware (and other multi purpose flavors) were added after the code was written. (the one used (16k) will still work)
the script i’m talking about is derived from finding a way for mycroft to play nice in regard of the intersection pulse server-alsa-mycroft and its services and pulses beneficial features (pretty much sidelining alsa ) by adding several lines of config here and there, which is not implemented yet in the official git.

once again your swift response is highly appreciated. however, you completely lost me now with all the technical details 

take this as entry points to dig deeper

i’ll give it a try and get back to you if i get stuck.

as far as getting mycroft up and running, i didn’t get stuck yet. this is what i did:

got myself a respeaker usb mic array.
got myself a jbl go2, which i connected to the respeaker.
followed the seeed getting started instructions.
followed the mycroft audio troubleshooting instructions.

this gave me a working installation. sort of, because it’s definitely not stable yet. after a few commands, sometimes multiple but sometimes just one, the audio output stops. using the cli, i can see that mycroft “understands” the wake up phrase and commands and responds with an appropriate reply, but no longer with audio, unfortunately. only after reboot doe i have the audio back for a few commands.
if hope there is someone out there who can share information about how to troubleshoot and/or solve this issue. otherwise i’ll try to dig into this a later and post my findings in a new topic.

most likely the “suspend on idle” pulseaudio module is loaded.

thanx peter. even if it hasn’t been idle, the problem occurs. i’ll definitely check it out though.
another, seemingly related thing concerns the jbl go2 : it turns itself of after some idle time. the two problems appear not to be related though, because the first problem also occurs when the jbl is powered on when rebooting the system.

that’s the script portion you want. put the file anywhere on picrofts home, chmod a+x fix.sh and fire it up ./fix.sh
maybe check /etc/asound.conf that the format is not all over the place

oh and
sudo sed -i -e ""s/aplay %1/paplay %1/"" /etc/mycroft/mycroft.conf
and i recommend changing tts in the mycroft.conf (either mentioned in /etc/mycroft/mycroft.conf or ~/.mycroft/mycroft.conf)

“tts”: {
“module”: “google”,
“google”: {
“lang”: “de”
},
""pulse_duck"": true
},

the tts provider is arbitrary

nice script! looks familair😜
@gez-mycroft maybe it is worth pinning it somewhere. here or at the docs.

thanx for the script. it has a few issues. i’ll try to fix them and send you the new version.

i’ll add them to the auto_run.sh

if [! -f /etc/asound.conf]
into
if [ ! -f /etc/asound.conf ]
sorry for that

missing a space on line 16:
echo""load-module module-role-ducking
into
echo ""load-module module-role-ducking
there also a typo on line 20
#config /etc/pulse.daemon.conf
into
#config /etc/pulse/daemon.conf
finally there is something weird on my side. i get a “permission denied” on the last line. the /etc/asound.conf is created nicely but the printf gives an error.

printf '# use pulseaudio by default\npcm.!default {\n   type pulse\n   fallback ""sysdefault""\n   hint {\n      show on\n      description ""default alsa output (currently pulseaudio sound server)""\n   }\n}\n\nctl.!default {\n   type pulse\n   fallback ""sysdefault""\n}\n' | sudo tee /etc/asound.conf

that’s not your system, that’s because you are the first tester of this script part 
"
67,radio skill based on pyradios,skill feedback,"
how to install radio

install radio by

mycroft-msm install https://github.com/andlo/radio-skill.git

the uses pyradios from pypi and that is instaled from the manifest.yml
the skill uses default audio backend just like the npr-skill and only uses the common play framework
https://mycroft-ai.gitbook.io/docs/skill-development/skill-types/common-play-framework
how to test radio
play your favorite radiostation by or use one of these examples
hey mycroft - play radio bbc radio 1""
hey mycroft - play radio kexp

if mycroft starts playing everything works as expeted - if not - it could be that you dind say a name of a radio that he could parse or find. it is diffucult to speak many radio names as they are not written funny like “radio4” and mycroft will hear that as “radio 4”. i have problem getting danish radio “dr p1” as he mostly hear something like “dear are pee one”.
but you can use mycroft-cli and write the name… ideas for how to “translate” better between radio station names and spoken words are most welcome.
i think enhancing with user configured programs is soem way to work around this problem.
adding posibility to
hey mycroft - play radio program 1

and have a setting on home for program 1 to 5 could make the skill more usefull.
feedback and ideas for making skill better please through issues on github, or via mycroft chat or here as reply on this thread.
you are also welcome to fork change and make pull requessts.
the idea for this skills comes from @stuartiannaylor in this topic




radio skill based on pyradios and http://www.radio-browser.info/gui/#!/ skill suggestions


    hey mycroft play “radio station name” radio… 
http://www.radio-browser.info/gui/#!/ 
is excellent as it seems a single person is maintining a whole tune-in alternative. 
you can either parse the web-site or use the api that is already exposed in
  


enjoy and happy mycrofting 
","
brilliant @andlo 

a bit of a warning, on mark-1’s this skill currently leads to a version conflict hindering core from starting up (leaving the device in yellow eyes mode) after installing this skill. we’ll need to bump core’s version requirement of requests to ensure compatibility.

ohhh - ill could maybe use another way  it wassnt in use anyway. i wanted to add a check if the url for the stream actual were working, but i couldnt get it to work.
i could just chnge using requests to httplib
changing from
    def exists_url(url):
        r = requests.head(url)
        if r.status_code < 400:
            return true
        else:
            return false

to
    def exists_url(url):
        c = httplib.httpconnection(url)
        c.request(""head"", '')
        if c.getresponse().status < 400:
            return true
        else:
            return false
``

the problem is that pyradios depends on requests >= 2.22.0 which conflicts with core’s requests == 2.20.0 so we should upgrade in core anyways or maybe even making core’s requirement more slack.

ohh i see. then until that part is solved the skill should have supported devices and not ahve platform_mark1  there.

i have installed the skill on picroft and get the “i’m not configured to play music yet …” message.
i have tried mycroft-wipe, reinstalling skill and updating.

i’m also experiencing this issue on a fresh install.

thanks for trying the skill. strange that it dosnt work for you…ill get back to the skill as soon as posible.

somehow managed to miss this while searching, ended up implementing my own https://github.com/hughwilliams94/radio-browser-skill. will take a look at yours for improvements etc.

i know this is an old thread, but i am also experiencing issues with picroft saying: “it sounds like you are trying to listen to some music, but you have not yet set up any music skills. please go to market.mycroft.ai to install and configure a music service skill”. just wanted to post in case anybody has figured anything out.
"
68,google tts very slow or no speech,support,"
since some month google tts is responding very slow. the quality of voice is good (german, female). but very often the speech translation comes back up to one minute later since the text was sent.
if we use google tts by mycroft/picroft this is more a form of web-scraping, isn’t it?  is it possible to use google-tts as a registered user and what settings do i have to make? i want to try out as a registrated user. what files do i have to edit for example to fill api key or user name etc.
",
69,solved increase recording interval,support,"
hi,
i would like to know if there is a way to increase the recording window when the wake word is detected. right is 3 seconds and i would like to increase it to 5 seconds.
i checked within the mycroft.conf  configuration[1] but i found nothing.
[1] https://github.com/mycroftai/mycroft-core/blob/master/mycroft/configuration/mycroft.conf
thanks for your help.
","
ok, i checked the configuration sample twice and them found what i was looking for…  recording_timeout_with_silence
sorry for the noise…


github.com


mycroftai/mycroft-core/blob/master/mycroft/configuration/mycroft.conf#l196



  // in milliseconds
  ""phoneme_duration"": 120,
  ""multiplier"": 1.0,
  ""energy_ratio"": 1.5,
  ""wake_word"": ""hey mycroft"",
  ""stand_up_word"": ""wake up"",


  // settings used by microphone to set recording timeout
  ""recording_timeout"": 10.0,
  ""recording_timeout_with_silence"": 3.0
},


// settings used for any precise wake words
""precise"": {
  ""dist_url"": ""https://github.com/mycroftai/precise-data/raw/dist/{arch}/latest"",
  ""model_url"": ""https://raw.githubusercontent.com/mycroftai/precise-data/models/{wake_word}.tar.gz""
},


// hotword configurations
""hotwords"": {






"
70,testing and feedback sonos spotify mycroft skill,skill feedback,"
this skill allows to use voice commands to play (specifically) spotify music on a sonos speaker



github



lnguyenh/spotify-sonos-bot-skill
mycroft voice assistant skill that allows spotify music playback on sonos speakers - lnguyenh/spotify-sonos-bot-skill





how to install spotify sonos bot

check the readme in the github repo
warning: the setup is on the rough side because it requires also installing another open source project. but i will gladly help personally, just hit me here, on gitter, or on the mycroft chat server https://chat.mycroft.ai/community/channels/skills


","
i will definitely try this, the only downside is to have node.js running.
good job @sinopsychoviet

great, let me know how it goes! i use it everyday and it works like a charm, but it is obviously tailored to my needs  . having sonos & spotify was a deal breaker for me when deciding whether or not adopting mycroft.

after the skill has been installed, mycroft doesn’t start anymore.
during handling of the above exception, another exception occurred:
traceback (most recent call last):
  file ""/home/pi/mycroft-core/mycroft/skills/mycroft_skill/mycroft_skill.py"", line 1296, in default_shutdown
    self.stop()
  file ""/opt/mycroft/skills/spotify-sonos-bot-skill.lnguyenh/__init__.py"", line 62, in stop
    stop(self.speaker)
  file ""/opt/mycroft/skills/spotify-sonos-bot-skill.lnguyenh/sonos_api.py"", line 25, in stop
    return get(api_url + speaker + '/' + 'pauseall')
typeerror: can only concatenate str (not ""nonetype"") to str
 18:27:50.190 | error    | 11904 | mycroft.skills.skill_loader:_create_skill_instance:313 | skill initialization failed with type~~~~
traceback (most recent call last):
  file ""/home/pi/mycroft-core/mycroft/skills/skill_loader.py"", line 307, in _create_skill_instance
    self.instance.initialize()
  file ""/opt/mycroft/skills/spotify-sonos-bot-skill.lnguyenh/__init__.py"", line 23, in initialize
    self.on_settings_changed()
  file ""/opt/mycroft/skills/spotify-sonos-bot-skill.lnguyenh/__init__.py"", line 37, in on_settings_changed
    set_volume(self.speaker, volume)
  file ""/opt/mycroft/skills/spotify-sonos-bot-skill.lnguyenh/sonos_api.py"", line 40, in set_volume
    return get(api_url + speaker + '/' + 'volume/' + str(wanted_volume))
typeerror: can only concatenate str (not ""nonetype"") to str
 18:27:50.195 | error    | 11904 | mycroft.skills.skill_loader:_communicate_load_status:351 | skill spotify-sonos-bot-skill.lnguy~~~~

this is due to this commit: https://github.com/lnguyenh/spotify-sonos-bot-skill/commit/444a1a1e03bb0c8ae69e42753e528c7cf0e0cff9
i just removed the   20   from and now the service starts and the skill is available from the web interface and now i’m able to play spotify on my speaker.
one tricky point to understand was why i was not able to play playlists… the answer is that my spotify credentials was required by  node-sonos-http-api   application.
any plan to add the multi-speaker support?
thanks 

good feedback! i will update the readme with a bit more info about the node-sonos-http-api  setup, and get rid of the superstitious commit :). now that there is more than one user, i might get motivated to add multi room. great to hear that you can play spotify. let me know if you feel you miss some voice commands.
just wondering, how is the wake-word recognition working for you? that was arguably the second deal breaker for mycroft adoption, and at the start it didnt work well for my wife and kids. but there is a beta “hey mycroft” wake-word model that you may be interested in if you experience some wake-word issues.

@sinopsychoviet you are very welcome!
i got some questions:

why not try to implement the sonos integration to the existing spotify module?
as alexa and google, why not to just have the sonos skill available which could open the range of integration with mycroft and sonos?
having the node.js to interact with sonos deployed automatically with the skill could be a great addition.

i know that all of theses questions have pros and cons, i just would like to hear your feedback from it. 
about my wake word, i had few issues mostly because of a very bad microphone[1]… then i switched to a better microphone[2] which solved so many issues!
then i changed the wake word and used mine (mirror mirror) by using the precise engine[3], i reduced a little bit the threshold and now it works 95% of the time (still think the microphone is not perfect).
[1] https://www.amazon.ca/tanbin-computer-microphone-smallest-recording/dp/b06xvmp7ff/ref=sr_1_5?dchild=1&keywords=mini+usb+microphone+for+computer&qid=1606576409&s=electronics&sr=1-5
[2] https://www.amazon.ca/gp/product/b0823n6xl5/ref=ppx_yo_dt_b_asin_title_o01_s00?ie=utf8&psc=1
[3] https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/customizations/wake-word

when i started to play with mycroft, i was more thinking about using a sonos skill and found this one: sonos controller skill - testing and feedback , which i tried but it did not support spotify. so i created my own skill, for a number of reasons like: 1) it was fun to try, 2) i could add the commands that felt most natural to me.
i did discard using the existing spotify skills, but maybe for wrong reasons, i realize now. 1) i thought the skill was limited to playing spotify on the actual mycroft device. 2) i thought that there was an issue with credentials that made the skill not “working” anymore. i believe there is still this issue, but that you can work around it by using your own credentials, like i do myself. i got a bit “scared” by the warnings in the readme of the skill and some forum/chat discussions.
so no real good reasons for your 2 first questions @goldyfruit :), except that i knew what i wanted to achieve (the exact commands i need when cooking dinner  ) and just pushed forward until i had the device i wanted, having fun in the process building a skill from scratch and discovering how the mycroft, spotify and sonos ecosystems work.
you made me re-read about the existing spotify skill, and adding support for sonos there may be a good/better way forward for mycroft in general, especially if we manage to solve the dev credentials issue which is not “general public” friendly.
having a a general dedicated sonos app would also be a great thing to have, i agree.
"
71,attention are you a spotify user spotify disabled on mycroft,general discussion,"
for no reason we can desern, spotify has disabled mycroft’s api access.  this was used to search their library so that mycroft users who have paid spotify accounts can access the music they are paying for.
we’ve been trying to get in touch with someone at spotify who can clarify their policy, but have not been able to get a reply.
if you have a paid account, please open a support ticket with spotify, then get up on twitter and let @spotifycares know that you pay for an account and want to access it on your smart speaker.
we’re investigating changing our default music player to deezer going forward and this may accelerate that change.
if you know anyone at spotify, please reach out and see if we can get in touch.  this might be a misunderstanding, but it should be noted that they had to actively reach in and disable our service, so whatever is going on, it is probably intentional.
","
if you use spotify, or want to help those who do, you can consider liking this post:



community.spotify.com – 26 aug 20



spotify on mycroft smart speaker
spotify just disabled api access for the mycroft smart speaker.  i can no longer use it to listen to the spotify account i pay for.   this is really frustrating.  the skill was community developed and maintained and worked fine. ...





after contacting spotify directly, i was told that the only way to have a change made to spotify is to have it garner enough attention in the community section. i do not like this option, and as this is not a feature, it seems like it should not need requesting in the same way, but it looks like this is at least a way that the average user can help.

this sucks… spotify is one of my main applications for a language assistant.
i don’t understand why they block access for paid users. these are just additional reasons to cancel the subscription. 

as a premium paid user i sent in a request for us

i made it to tier 3 tech support and i am awaiting an email. i suggested to them that, if this was purposefully disabled, they will likely lose a lot of premium customers, and that someone from their management team should be brought into the discussion.

let me know if you need any information to help the case (i’m the maintainer of the spotify skill)

sorry to be simple here but is it not the case that someone owns the api access via the https://developer.spotify.com/ and has either deleted it or someone else needs to re-create it?

i owned it. i got notice from spotify that they would disable it. then they disabled it and now it’s not usable anymore. i can create a new one but as it’s technically breaking the terms of service it’ll just get disabled again (at some point in time, when they realize it’s a violation).
a new api key can easily be created. right now i’m sort of waiting for response from spotify’s official channels about getting permission (have applied previously a couple of times and been rejected so i’m not hopeful) just in case.
after that maybe just doing it…

thank you very much for the update. this gives a great bit of background.
other apps i have used have given users details on how they create their own app in the developers dashboard and add the client id and client secret to their local application.  would that work in this case?
thank you for your hard work on this

i’m actually working on adding support for just that. 

can you briefly explain how/why there is a tos violation?

spotify doesn’t allow using their api for voice integrations without written authorization. either i missed this when i signed up or it wasn’t in the tos back in 2017 when i registered the app.

any news on the issue?
spotify connection was an important part for me while building my mycroft.

if voice control is not a must have, install spotifyd then your device just pops up as spotify-connect device to which you can still throw you music to with the spotify app.

i’ve always been upset with spotify.  i paid for a premium account early on and used open source software.  pandora is much more friendly to open source i find.  perhaps because they need the money.
love

yeah, i can cast music to it, but i really would like voice control, i guess i have to send a mail to spotify 

so, i had a chat with someone on spotify support, and although i don’t think he works with the technical stuff himself, he promised to send the question further.
i also invited him (or the spotify person looking into this) to join this forum to follow our discussion.
i got the feeling that he really was going to take this further, so hopefully something will happen in the future

thanks for doing that. i’m in the process of re-applying for becoming a “hardware partner” to get official permission to use the api for voice, let’s see if that application goes through this time.

i hope it will work out for you, i actually know  a person working for spotify, don’t know whith what though, but if needed, i might ask who should be contacted to get things happening.
i asked for a swedish speaking tech in the support chat, and that person actually sounded like he could walk down  the hall and talk to the people who knew stuff about this.
if there ever will be a commersial mycroft to buy, it’s pretty important that stuff like this works out of the box, so you are doing important work here.

hey forslund, any news from spotify yet?
a friend of mine just dropped some music on spotify, so i came to think of this again.
"
72,why does mycroft keep thinking i said what is my region,none,"
a couple of uses ago, i asked mycroft “what is my region?” now, each time it starts up, after it greets me (e.g. by saying “you can get started with mycroft”), the next thing that happens is that it invariably shows on the mycroft-start debug  screen (which is my main way of interfacing with mycroft) “what is my region?” as if i said it to which mycroft then audibly replies “i’m not sure how to answer that.”
it is just kind of annoying. can i edit some startup file so that it no longer thinks i am saying that?
","
odd…what’s in the voice.log?  does it work as expected when you ask it anything else?  have you made any changes?

it stopped. not sure why.
"
73,change voice at command line,support,"
the internet interface is not working to allow me to change the voice.  is there a way i can do this at the command line over ssh?
","
hey there, sorry for the slow response.
was it the american female voice you were trying to use?
we’ve taken that down while we get a new model prepared. so it would have defaulted back to british male. i’ve taken the button off the web interface now.
if it was another voice, it would be good to know so we can look into it further.

hi
i’ve just tried the same voice almost a year later and it doesn’t seem to be working now.
cheers

what voice are you using and which one are you trying to change it to?
"
74,the support skill isnt sending me an email,none,"
is anyone successfully using the support skill?  mycroft tells me it’s being kicked off but no email ever comes back.
","
can you check what’s in the logs to see if there’s any useful info there?

i made several attempts and this is the typical output.
audio.log:2020-11-24 15:48:34.841 | info     |  1712 | mycroft.audio.speech:mute_and_speak:127 | speak: mycroft support helper has already been installed
audio.log:2020-11-24 16:40:45.836 | info     |  1712 | mycroft.audio.speech:mute_and_speak:127 | speak: i’m not sure how to play a support ticket
audio.log:2020-11-24 15:48:34.841 | info     |  1712 | mycroft.audio.speech:mute_and_speak:127 | speak: mycroft support helper has already been installed
audio.log:2020-11-24 16:40:45.836 | info     |  1712 | mycroft.audio.speech:mute_and_speak:127 | speak: i’m not sure how to play a support ticket
audio.log:2020-11-24 16:41:12.775 | info     |  1712 | mycroft.audio.speech:mute_and_speak:127 | speak: would you like to create a troubleshooting package?
audio.log:2020-11-24 16:41:12.780 | info     |  1712 | mycroft.audio.speech:mute_and_speak:127 | speak: this will upload logs to the zero ex zero dot s t service and email a link to you with more information.
voice.log:2020-11-24 15:48:34.826 | info     |  1715 | main:handle_utterance:72 | utterance: [‘install the support helper’]
voice.log:2020-11-24 16:40:44.206 | info     |  1715 | main:handle_utterance:72 | utterance: [‘start a support ticket’]
voice.log:2020-11-24 16:41:12.558 | info     |  1715 | main:handle_utterance:72 | utterance: [‘create a support ticket’]
here is the syslog from the mycroft startup.
syslog:nov 24 12:39:23 harry /usr/libexec/gdm-x-session[1685]: starting all mycroft-core services
syslog:nov 24 12:39:23 harry /usr/libexec/gdm-x-session[1685]: caution: the mycroft bus is an open websocket with no built-in security
syslog:nov 24 12:39:24 harry /usr/libexec/gdm-x-session[1781]: dbus-update-activation-environment: setting path=/path pruned/snap/bin:/home/dad/mycroft-core/bin
i don’t see anything here that is unexpected.  one of my attempts to open a ticket was to “start a ticket”, but i had previous failures to receive an email before i had said that.

that seems to be a snap install, it causes all sorts of issues and there really isnt anyone around to give support, i recommend you use a git install

i had originally installed a snap, but hit the problem referenced in: "" no registration code after snap install … “please wait a moment as i finish booting up”"".
so i thought i had fully removed the snap installation before installing from git.  apparently not.
what makes is look like a snap installation?  i will remove the remnants of the snap and reinstall.
also, how long does it usually take to get an email response from the support skill?
thanks!
"
75,severe weather information skill,skill feedback,"

 severe weather information
checks your national weather service and notifies you when there are alerts for your region
the severe weather information skill can connect to dozens of weather alerting services, among them many from following lists:
severe weather information center
alerting world weather
meteoalarm
after installation go to home.mycroft.ai and select your preferred weather service from the list on the skills configuration page.
you can also set a “watchdog” that automatically checks for new alerts and notifies you…
how to install severe weather information skill


install severe weather information skill by …

msm install https://github.com/domcross/severe-weather-information-skill
there are some dependencies in requirements.txt which should be automatically installed by msm




severe weather information skill connects to public weather information services …

a number of services is included and can be selected in the skill configuration (home.mycroft.ai)
you can use custom services as long as they provide a atompub or rss feed with entries in cap-xml format.



how to test severe weather information skill

configure the skill settings in home.mycroft.ai
speak are there weather alerts

mycroft should reply with “there are the following alerts …” (continuing with alert information) or with “currently there are no alerts for your region”

where feedback on severe weather information skill should be directed at

as comment here in this topic
issues on github

mycroft chat.

","
on a mark i, core release 19.08.2b:
installation failed via copy-and-pasted installation line: “msm install https://github.com/domcross/severe-weather-information-skill”
tail end excerpt of the  stdout where the errors began:

ign http://mirrordirector.raspbian.org jessie/contrib translation-en_gb
ign http://mirrordirector.raspbian.org jessie/contrib translation-en
ign http://mirrordirector.raspbian.org jessie/main translation-en_gb
ign http://mirrordirector.raspbian.org jessie/main translation-en
ign http://mirrordirector.raspbian.org jessie/non-free translation-en_gb
ign http://mirrordirector.raspbian.org jessie/non-free translation-en
ign http://mirrordirector.raspbian.org jessie/rpi translation-en_gb
ign http://mirrordirector.raspbian.org jessie/rpi translation-en
error: timeout was reached
reading package lists… done
reading package lists…
building dependency tree…
reading state information…
the following extra packages will be installed:
libgeos-3.4.2 libgeos-c1
suggested packages:
libgdal-doc
the following new packages will be installed:
libgeos-3.4.2 libgeos-c1 libgeos-dev
0 upgraded, 3 newly installed, 0 to remove and 65 not upgraded.
need to get 815 kb of archives.
after this operation, 2,099 kb of additional disk space will be used.
get:1 http://mirrordirector.raspbian.org/raspbian/ jessie/main libgeos-3.4.2 armhf 3.4.2-6 [469 kb]
get:2 http://mirrordirector.raspbian.org/raspbian/ jessie/main libgeos-c1 armhf 3.4.2-6 [164 kb]
get:3 http://mirrordirector.raspbian.org/raspbian/ jessie/main libgeos-dev armhf 3.4.2-6 [182 kb]
fetched 815 kb in 1s (615 kb/s)
dpkg-deb: error: subprocess tar was killed by signal (segmentation fault)
dpkg: error processing archive /var/cache/apt/archives/libgeos-3.4.2_3.4.2-6_armhf.deb (–unpack):
subprocess dpkg-deb --control returned error exit status 2
dpkg-deb: error: subprocess tar was killed by signal (segmentation fault)
dpkg: error processing archive /var/cache/apt/archives/libgeos-c1_3.4.2-6_armhf.deb (–unpack):
subprocess dpkg-deb --control returned error exit status 2
dpkg-deb: error: subprocess tar was killed by signal (segmentation fault)
dpkg: error processing archive /var/cache/apt/archives/libgeos-dev_3.4.2-6_armhf.deb (–unpack):
subprocess dpkg-deb --control returned error exit status 2
errors were encountered while processing:
/var/cache/apt/archives/libgeos-3.4.2_3.4.2-6_armhf.deb
/var/cache/apt/archives/libgeos-c1_3.4.2-6_armhf.deb
/var/cache/apt/archives/libgeos-dev_3.4.2-6_armhf.deb
error: timeout was reached
e: sub-process /usr/bin/dpkg returned an error code (1)
info - successfully ran requirements.sh for severe-weather-information-skill
info - installing system requirements…
info - installing requirements.txt for severe-weather-information-skill
info - problem performing action. restoring skill to previous state…
error - failed to install skill severe-weather-information-skill
traceback (most recent call last):
file “/opt/venvs/mycroft-core/lib/python3.4/site-packages/msm/mycroft_skills_manager.py”, line 372, in install
skill.install(constraints)
file “/opt/venvs/mycroft-core/lib/python3.4/site-packages/msm/skill_entry.py”, line 82, in wrapper
func(self, *args, **kwargs)
file “/opt/venvs/mycroft-core/lib/python3.4/site-packages/msm/skill_entry.py”, line 473, in install
self.run_pip(constraints)
file “/opt/venvs/mycroft-core/lib/python3.4/site-packages/msm/skill_entry.py”, line 313, in run_pip
pip_code, proc.stdout.read().decode(), stderr
msm.exceptions.piprequirementsexception:
pip returned code 1:
requirement already satisfied: feedparser in /opt/venvs/mycroft-core/lib/python3.4/site-packages (5.2.1)
requirement already satisfied: xmltodict in /opt/venvs/mycroft-core/lib/python3.4/site-packages (0.11.0)
requirement already satisfied: requests in /opt/venvs/mycroft-core/lib/python3.4/site-packages (2.20.0)
collecting shapely
downloading https://files.pythonhosted.org/packages/a2/fb/7a7af9ef7a35d16fa23b127abee272cfc483ca89029b73e92e93cdf36e6b/shapely-1.6.4.post2.tar.gz (225kb)
complete output from command python setup.py egg_info:
failed cdll(libgeos_c.so.1)
failed cdll(libgeos_c.so)
traceback (most recent call last):
file “”, line 1, in 
file “/tmp/pip-install-tq7z_0xt/shapely/setup.py”, line 80, in 
from shapely._buildcfg import geos_version_string, geos_version, 
file “/tmp/pip-install-tq7z_0xt/shapely/shapely/_buildcfg.py”, line 167, in 
fallbacks=[‘libgeos_c.so.1’, ‘libgeos_c.so’])
file “/tmp/pip-install-tq7z_0xt/shapely/shapely/_buildcfg.py”, line 161, in load_dll
libname, fallbacks or []))
oserror: could not find library geos_c or load any of its variants [‘libgeos_c.so.1’, ‘libgeos_c.so’]
----------------------------------------

/opt/venvs/mycroft-core/lib/python3.4/site-packages/cryptography/hazmat/bindings/openssl/binding.py:163: cryptographydeprecationwarning: openssl version 1.0.1 is no longer supported by the openssl project, please upgrade. a future version of cryptography will drop support for it.
utils.cryptographydeprecationwarning
deprecation: python 3.4 support has been deprecated. pip 19.1 will be the last one supporting it. please upgrade your python as python 3.4 won’t be maintained after march 2019 (cf pep 429).
command “python setup.py egg_info” failed with error code 1 in /tmp/pip-install-tq7z_0xt/shapely/
you are using pip version 19.0.3, however version 19.1.1 is available.
you should consider upgrading via the ‘pip install --upgrade pip’ command.


thanks for trying  my skill and reporting this.
in the stdout you have provided i see some reoccuring error messages:

error: timeout was reached
dpkg-deb: error: subprocess tar was killed by signal (segmentation fault)

maybe there was a network error while you tried to install?
did you try to install some other apt-packages before and that failed somehow and now your apt-database is somehow broken?
how old is the sd-card in your mark-1 - maybe that card is too old (assuming a 24/7 usage of the mark-1 one year is old) and is simply dying - which results in erratic behaviour when trying to write data to the card…?
the error messages at the end are kind of exspected errors as the libgeos-packages were not succesfully installed before but are required by the pip-install of the shapely pypi-package.

i tried again, this time logging in with pi (and not su-ing to mycroft this time) and running the command with the “mycroft-” prefix. i got esstenially the same errors.
it is possible that some other apt stuff has mucked things up, perhaps through the requirements.sh of another skill, but i don’t recall doing anything like that on purpose.
do you think a “factory reset” as described in https://mycroft.ai/wp-content/uploads/2017/06/mark_1_user_guide.pdf would suffice to cleanup/repair the apt stuff (if it is indeed a problem)? i don’t know if that does any kind of os image work, or just purges skills and configuration files.

hi @jrwarwick
sorry, i somehow missed your reply… as far as i know the reset function from the mark-i menu does only wipe your settings. after that you will have to go through the wifi setup and register the device again at home.mycroft.ai
honestly, i don’t think this will help with your problem as either your apt database is corrupted  or your sd-card is dying. is flashing the mark-i image to a new sd card a feasible option for you?

yes, i think i can handle a fresh flashing on a fresh card. it will take me a little while to do this though. i’ll report back with results. thanks, dominik.

good news: a completely fresh install on fresh sd card, updated, then installed severe weather information skill as the second skill installed; completely successful, as you predicted. msm seemed very happy. then i configured the skill on home.mycroft.ai, and that seemed to work fine. i decided to move some of the threshold settings down a bit since the area i live in actually has very little severe weather.

image379×786 5.81 kb

after saving configuration, i attempted to speak the test phrase “are there weather alerts”, but alas, this failed. “an error occurred while processing a request in severe weather information”. i can attach a larger excerpt of the log, but it looks like this is the problem:
21:34:07.185 | error    |   857 | mycroft.skills.mycroft_skill.mycroft_skill:on_error:798 | an error occurred while processing a request in severe weather information     traceback (most recent call last):   file ""/opt/venvs/mycroft-core/lib/python3.7/site- packages/mycroft/skills/mycroft_skill/event_container.py"", line 66, in wrapper     handler(message)   file ""/opt/mycroft/skills/severe-weather-information-skill.domcross/__init__.py"", line 106, in handle_information_weather_severe     self._check_for_alerts()   file ""/opt/mycroft/skills/severe-weather-information-skill.domcross/__init__.py"", line 102, in _check_for_alerts     self.alerts = self.get_filtered_alerts(feed.entries, max_entries=5)   file ""/opt/mycroft/skills/severe-weather-information-skill.domcross/__init__.py"", line 306, in get_filtered_alerts     if not self.in_geo_location(info, longitude=lon, latitude=lat):   file ""/opt/mycroft/skills/severe-weather-information-skill.domcross/__init__.py"", line 345, in in_geo_location     for lola in area[""polygon""].split("" ""): attributeerror: 'nonetype' object has no attribute 'split'

more good news:  when i changed the location filter option to area description and gave my county name, the trigger phrase worked! so 97% successful test, now. just a small problem with getting lat/long from the mycroft device.
beyond that, i cannot say when or if i would actually experience severe weather to test the active/automatic notification of the condition.

you might be interested in the new geolocation api that we’ve added to the backend.
it’s not documented yet, but we’ve started experimenting with it in skills eg this branch of the date-time skill

that is exciting news, gez. i have long anticipated the day when i would be able to wake up groggy and mumble: “hey mycroft, where are we?” and he unconfuses me. the ever-alert adventure buddy.

@jrwarwick thanks for providing me your location data (through pm)
unfortunately nws/noaa does provide longitude/latitude polygon data for some, but not all areas. for the area you are living the polygon data is missing - which caused the exception you have reported. i have pushed a fix for that error.
in order to get weather warnings for your area you may want to look up your county/zone code at https://alerts.weather.gov, then in the skills settings change location-filter to “geo-code” and enter your county/zone code to “location description”

@dominik your suggestion seems to work just fine. using city, state styled “area description” seemed to work, too, interestingly.
i just noticed the faceplate display complete with general sky condition graphic: very nice touch!
one small suggestion:  add a little more utterance analysis for this case of habitual speech pattern:  i tried asking “are there any weather alerts in my  location?” and “are there any weather alerts in my area?” to which mycroft replied that he didn’t know about that location. maybe include a few more common location phrases like “around here”, “the area”, “near by”, too and just interpret them to mean “use the skill configured default location”.

@jrwarwick thanks for testing and your suggestions how to improve the phrases - this is very helpful for me as non-native english speaker. i will update the intent-files soon…

fyi, now that i’ve had an explicit geocode in there for a while, and seen a couple of reboots, i am still seeing this in the background activity periodcially:

18:53:34.219 | info     |  5180 | severeweatherinformation | auto_alert_handler
18:53:34.220 | info     |  5180 | severeweatherinformation | url: https://alerts.weather.gov/cap/us.php?x=0
18:53:34.220 | info     |  5180 | severeweatherinformation | header: {‘accept’: ‘application/atom+xml’}
18:53:37.600 | info     |  5180 | severeweatherinformation | add alert noaa-nws-alerts-ak125f35957a44.winterstormwarning.125f35a26bf0ak.ajkwswajk.dba7f84da7cd60e04206f0d04df0f80e
18:53:38.618 | info     |  5180 | severeweatherinformation | add alert noaa-nws-alerts-ak125f35957a44.winterstormwarning.125f35a2e120ak.ajkwswajk.d59c95a675c716a3d5bb1b229b0b2bf9
18:53:40.198 | info     |  5180 | severeweatherinformation | add alert noaa-nws-alerts-ak125f35955cf8.highwindwarning.125f3595e8d0ak.ajknpwajk.b2aafd2368054c9cb67aad86d8badf3d
18:53:40.756 | info     |  5180 | severeweatherinformation | add alert noaa-nws-alerts-ak125f35955cf8.highwindwarning.125f3595e8d0ak.ajknpwajk.5dd2ccccc5ac6b32cf028f8a4c85096a
18:53:41.321 | info     |  5180 | severeweatherinformation | add alert noaa-nws-alerts-ak125f35955cf8.highwindwarning.125f35a1f6c0ak.ajknpwajk.479dc599182b265e70fe8f1de1fffffa
18:53:41.329 | error    |  5180 | mycroft.skills.event_scheduler:on_error:314 | an error occured executing the scheduled event keyerror(‘messagetype’)
traceback (most recent call last):
file “/opt/venvs/mycroft-core/lib/python3.7/site-packages/mycroft/skills/mycroft_skill/event_container.py”, line 100, in wrapper
handler()
file “/opt/mycroft/skills/severe-weather-information-skill.domcross/init.py”, line 159, in auto_alert_handler
msgtype = alert[‘messagetype’]
keyerror: ‘messagetype’


did you enable “automatic alert” in the skill settings?

yes, with 15 minute interval:

image685×603 9.5 kb

also, perhaps unrelated, i just noticed mismatch of scheduled event names: “severeweatherinformation” vs “severeweather”
line 88:
            self.schedule_repeating_event(handler=self.auto_alert_handler, when=datetime.now(), frequency=self.update_interval, name='severeweather')
line 92:
            self.cancel_scheduled_event('severeweatherinformation')

good observation, looks like you found a little bug. will take care of it soon and report back here when it is fixed.

another minor, more “cosmetic” issue: when pronouncing the output from alerts.weather.gov, the summary provided included asterisk characters, which mycroft pronounces, i.e., by saying the work “asterisk”.  at a glance it looks like this particular weather information feed is using the asterisk as some kind of delimiter of sorts (rather than an indication of emphasis, which i would consider more typical).
considerably more tricky, so just fyi: mycroft also pronouned the timezone abbreviation like a word “acksst” instead of saying something like “alaska standard time”.
sample of speech transcript from cli:


there are the following alerts                                                                             debug output
high wind warning issued february 20 at 4:47am akst until february 20 at 3:00pm akst by nws juneau         skills.log, other
…high wind warning in effect from 9 am this morning to 3 pm akst this afternoon… * what…southeast    voice.log
winds 35 to 45 mph with gusts up to 60 mph expected. * where…cape fairweather to cape suckling coastal
area. * when…from 9 am this morning to 3 pm akst this afternoon. * impacts…damaging winds will blow
down trees and power lines. widespread power outages are expected. travel will be difficult, especially



eventually found time for updating this skill:

fixed some bugs (scheduler etc.)
removal of unwanted “*” and “…”
changes for mycroft 20.02 (some api functions were changed or deprecated)


if anyone wonders, on arch linux you need the system package python-shapely installed first.
"
76,cant pair on raspian pi ps3 eye setup,support,"
i’ve recetly installed mycroft following the guide on a raspberry pi running the lastest raspbian (or now raspberry pi os). i’ve connected a small speaker to the 3,5mm jack and have filtereed the audio do use it.
i’ve also connected a ps3 eye to the rpi via usb, is there anything else i have to install because i can’t get it to talk to me.
am i being an idiot and missing something obvious? whenever i say “hey mycroft, pair my device” as per the linux documentation nothing happens.
below you’ll find the log ssh output of everything putty would let me copy, in case something went wrong, i don’t know.
thanks in advance!
https://paste.ubuntu.com/p/wwxdnsxwsf/
","
a couple of updates from my troubleshooting.
the ps3 eye and it’s microphone are definitely being picked up by the rpi4. i tested it in audacity and successfully managed to record my voice.
when i try to use cli or debug this error comes up:
pi@raspberrypi:~/mycroft-core $ ./start-mycroft.sh debug
already up to date.
starting all mycroft-core services
initializing…
starting background service bus
caution: the mycroft bus is an open websocket with no built-in security
measures.  you are responsible for protecting the local port
8181 with a firewall as appropriate.
starting background service skills
starting background service audio
starting background service voice
starting background service enclosure
starting cli
traceback (most recent call last):
file “/usr/lib/python3.7/runpy.py”, line 183, in _run_module_as_main
mod_name, mod_spec, code = _get_module_details(mod_name, _error)
file “/usr/lib/python3.7/runpy.py”, line 109, in _get_module_details
import(pkg_name)
file “/home/pi/mycroft-core/mycroft/init.py”, line 17, in 
from mycroft.api import api
file “/home/pi/mycroft-core/mycroft/api/init.py”, line 22, in 
from mycroft.configuration import configuration
file “/home/pi/mycroft-core/mycroft/configuration/init.py”, line 15, in 
from .config import configuration, localconf, remoteconf
file “/home/pi/mycroft-core/mycroft/configuration/config.py”, line 23, in 
from mycroft.util.json_helper import load_commented_json, merge_dict
file “/home/pi/mycroft-core/mycroft/util/init.py”, line 24, in 
from mycroft.util.format import nice_number
file “/home/pi/mycroft-core/mycroft/util/format.py”, line 38, in 
from padatious.util import expand_parentheses
file “/home/pi/mycroft-core/.venv/lib/python3.7/site-packages/padatious/init.py”, line 15, in 
from .intent_container import intentcontainer
file “/home/pi/mycroft-core/.venv/lib/python3.7/site-packages/padatious/intent_container.py”, line 25, in 
from padatious.entity import entity
file “/home/pi/mycroft-core/.venv/lib/python3.7/site-packages/padatious/entity.py”, line 17, in 
from padatious.simple_intent import simpleintent
file “/home/pi/mycroft-core/.venv/lib/python3.7/site-packages/padatious/simple_intent.py”, line 17, in 
from padatious.id_manager import idmanager
file “/home/pi/mycroft-core/.venv/lib/python3.7/site-packages/padatious/id_manager.py”, line 17, in 
from padatious.util import strenum
file “/home/pi/mycroft-core/.venv/lib/python3.7/site-packages/padatious/util.py”, line 15, in 
from xxhash import xxh32
file “/home/pi/mycroft-core/.venv/lib/python3.7/site-packages/xxhash/init.py”, line 1, in 
from ._xxhash import (
modulenotfounderror: no module named ‘xxhash._xxhash’
pi@raspberrypi:~/mycroft-core $
i tried installing xxhash (pip install xxhash) but that hasn’t remedied the problem.
"
77,does ps3 eye work well when playing loud music,none,"
i’ve read different things one says it recognizes the activation word as well as google home the other says you have to shout at the microphone, what’s right?
","
i used a ps3 eye to start with, it was fine in the same room with precise.  the pocketsphinx listener isn’t as good and might have been the source of some issues, though.  try it out and see how it works for you.

the beamforming, tracking, aec sony software resides in the ps3 not the usb mic/cam.
its a old cam with 4 mics that are no different than having a single mic if you don’t have the software to use the array.
there is no working opensource beamforming, tracking and aec do exist and any mic without will not work well in the presence of loud music.
"
78,speech result very slow,support,"
mycroft is working correctly, however, she is very slow to ‘say’ the results.
example - i say “mycroft, what time is it?”.
the time is displayed instantly, but her saying the time happens about 5 minutes later.
this is with any response.  i asked for the running version.  again it was displayed almost instantly, but her ‘saying’ the installed version took a long time and then a long time afterwards came the next response of “you are on the current version”.
","
i have the same issue. is there any solution?

what tts are you using?

i’m trying to use google in german
i used the guide from mycroft: (https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/customizations/languages) and this: (https://github.com/gras64/docs-rewrite/blob/master/docs/using-mycroft-ai/customizations/languages/german.md)
but at the end i get the male english voice from google. stt in german works perfectly fine

you are providing links to tts documentation which gives a lot of options for tts engines  - which one did you configure (google, espeak, …)?
from what i read here in the forums google tts seems to have problems recently. in case gtts times out the local mimic1 voice might be used as a fallback. maybe that is the scenario that you are  observing right now?
btw: i am running a local tts engine with this free german tts voice for mycroft (sneak preview)

i am using google as well.  i have had it fallback on mimic before, but in this case, when she does respond, she is using google and has the right response (other than time is now wrong).

today, she is back to normal response times (i didn’t change anything on my side).

you can parse the audio.log file to see if there’s something taking a long time to process there.  perhaps even the voice.log file if that’s where it’s being slow.  but if you haven’t changed anything and the speed is varying independently of your setup, then it’s probably not your end that’s the problem.

i will do this wen i get home tonight.  fyi - she back to being slow this morning.

audio.log…
2020-11-16 18:22:50.142 | info     |   886 | mycroft.audio.speech:mute_and_speak:127 | speak: it’s six twenty two
2020-11-16 18:24:44.032 | warning  |   886 | gtts.tts | unable to get language list: ‘nonetype’ object is not subscriptable
high performance mpeg 1.0/2.0/2.5 audio player for layers 1, 2 and 3
version 1.20.1; written and copyright by michael hipp and others
free software (lgpl) without any warranty but with best wishes
directory: /tmp/mycroft/cache/tts/googletts/
playing mpeg stream 1 of 1: a6629d8c1f9f165e6347cc9c7db23af7.mp3 …
mpeg 2.0 layer iii, 32 kbit/s, 24000 hz mono
[0:01] decoding of a6629d8c1f9f165e6347cc9c7db23af7.mp3 finished.

you need about 20 lines before that as well. check the voice.log as well to see when that gets handed over to audio.

i can get those tonight.  tried to pare down the audio log to where the issue showed, but can definitely get more info.

cat voice.log
2020-11-17 06:44:09.910 | info     |   883 | mycroft.session:get:74 | new session start: c73604f8-54ce-4441-b4fb-d23a01a8d97c
2020-11-17 06:44:09.914 | info     |   883 | mycroft.client.speech.main:handle_wakeword:67 | wakeword detected: hey mycroft
2020-11-17 06:44:10.328 | info     |   883 | mycroft.client.speech.main:handle_record_begin:37 | begin recording…
2020-11-17 06:44:11.879 | info     |   883 | mycroft.client.speech.main:handle_record_end:45 | end recording…
2020-11-17 06:44:12.947 | info     |   883 | mycroft.client.speech.main:handle_utterance:72 | utterance: [‘what time is it’]
2020-11-17 06:48:51.889 | info     |   883 | mycroft.session:get:74 | new session start: 2536d902-fd9d-48c8-80c7-c6a52c51a38a
2020-11-17 06:48:51.893 | info     |   883 | mycroft.client.speech.main:handle_wakeword:67 | wakeword detected: hey mycroft
2020-11-17 06:48:52.309 | info     |   883 | mycroft.client.speech.main:handle_record_begin:37 | begin recording…
2020-11-17 06:48:53.786 | info     |   883 | mycroft.client.speech.main:handle_record_end:45 | end recording…
2020-11-17 06:48:54.789 | info     |   883 | mycroft.client.speech.main:handle_utterance:72 | utterance: [‘where are you’]
2020-11-17 21:59:53.318 | info     |   883 | mycroft.session:get:74 | new session start: 688835f1-60a4-45f8-8481-404b5f51836f
2020-11-17 21:59:53.323 | info     |   883 | mycroft.client.speech.main:handle_wakeword:67 | wakeword detected: hey mycroft
2020-11-17 21:59:53.741 | info     |   883 | mycroft.client.speech.main:handle_record_begin:37 | begin recording…
2020-11-17 22:00:03.742 | info     |   883 | mycroft.client.speech.main:handle_record_end:45 | end recording…
2020-11-17 22:00:06.134 | info     |   883 | mycroft.client.speech.main:handle_utterance:72 | utterance: [‘what time is it’]
cat audio.log
2020-11-17 06:44:13.095 | info     |   886 | mycroft.audio.speech:mute_and_speak:127 | speak: six forty four
2020-11-17 06:46:08.390 | warning  |   886 | gtts.tts | unable to get language list: ‘nonetype’ object is not subscriptable
high performance mpeg 1.0/2.0/2.5 audio player for layers 1, 2 and 3
version 1.20.1; written and copyright by michael hipp and others
free software (lgpl) without any warranty but with best wishes
directory: /tmp/mycroft/cache/tts/googletts/
playing mpeg stream 1 of 1: 1bd639a02fab7d3f908988161004bea7.mp3 …
mpeg 2.0 layer iii, 32 kbit/s, 24000 hz mono
[0:01] decoding of 1bd639a02fab7d3f908988161004bea7.mp3 finished.
2020-11-17 06:48:55.131 | info     |   886 | mycroft.audio.speech:mute_and_speak:127 | speak: my settings tell me i’m at fort smith arkansas united states
2020-11-17 06:50:49.139 | warning  |   886 | gtts.tts | unable to get language list: ‘nonetype’ object is not subscriptable
high performance mpeg 1.0/2.0/2.5 audio player for layers 1, 2 and 3
version 1.20.1; written and copyright by michael hipp and others
free software (lgpl) without any warranty but with best wishes
directory: /tmp/mycroft/cache/tts/googletts/
playing mpeg stream 1 of 1: 9329c3edccf7174a14a0a5ca78aad504.mp3 …
mpeg 2.0 layer iii, 32 kbit/s, 24000 hz mono
[0:04] decoding of 9329c3edccf7174a14a0a5ca78aad504.mp3 finished.
2020-11-17 22:00:06.286 | info     |   886 | mycroft.audio.speech:mute_and_speak:127 | speak: it is ten o’clock
2020-11-17 22:00:08.155 | error    |   886 | mycroft.audio.speech:mute_and_speak:134 | tts execution failed (valueerror(‘unable to find token seed! did https://translate.google.com change?’))

i’d start checking google for those errors and see what you can find.  possible config error maybe, also check for updates to that tts tool, i think there were a couple that made it into a recent pr.

i ran
sudo apt update
```and

sudo apt full-upgrade
and this is what showed up audio.log

2020-11-19 07:13:23.253 | info     |   849 | mycroft.messagebus.load_config:load_message_bus_config:33 | loading message bus configs
2020-11-19 07:15:19.370 | warning  |   849 | gtts.tts | unable to get language list: 'nonetype' object is not subscriptable
2020-11-19 07:15:20.101 | info     |   849 | mycroft.audio.__main__:main:50 | starting audio services
2020-11-19 07:15:20.123 | info     |   849 | mycroft.messagebus.client.client:on_open:114 | connected
2020-11-19 07:15:20.152 | info     |   849 | mycroft.audio.audioservice:get_services:61 | loading services from /opt/venvs/mycroft-core/lib/python3.7/site-packages/mycroft/audio/services/
2020-11-19 07:15:20.162 | info     |   849 | mycroft.audio.audioservice:load_services:105 | loading chromecast
2020-11-19 07:15:25.679 | info     |   849 | pychromecast | querying device status
2020-11-19 07:15:25.838 | info     |   849 | pychromecast | querying device status
2020-11-19 07:15:25.958 | info     |   849 | audioservice_chromecast:autodetect:165 | master bedroom speaker found.
165 | master bedroom speaker found.
2020-11-19 07:15:25.961 | info     |   849 | audioservice_chromecast:autodetect:165 | kitchen found.
2020-11-19 07:15:25.965 | info     |   849 | mycroft.audio.audioservice:load_services:105 | loading mopidy
2020-11-19 07:15:25.976 | info     |   849 | mycroft.audio.audioservice:load_services:105 | loading mplayer
2020-11-19 07:15:26.047 | error    |   849 | audioservice_mplayer:<module>:20 | install py_mplayer with pip install git+https://github.com/jarbasal/py_mplayer
2020-11-19 07:15:26.051 | error    |   849 | mycroft.audio.audioservice:load_services:114 | failed to import module mplayer
modulenotfounderror(""no module named 'py_mplayer'"")
2020-11-19 07:15:26.055 | info     |   849 | mycroft.audio.audioservice:load_services:105 | loading simple
2020-11-19 07:15:26.094 | info     |   849 | mycroft.audio.audioservice:load_services:105 | loading vlc
2020-11-19 07:15:26.382 | error    |   849 | mycroft.audio.audioservice:load_services:129 | failed to load service. nameerror(""no function 'libvlc_new'"")
2020-11-19 07:15:26.386 | info     |   849 | mycroft.audio.audioservice:load_services_callback:177 | finding default backend...
2020-11-19 07:15:26.389 | info     |   849 | mycroft.audio.audioservice:load_services_callback:181 | found local
2020-11-19 07:15:26.392 | info     |   849 | mycroft.audio.__main__:on_ready:30 | audio service is ready.
2020-11-19 07:24:29.561 | info     |   849 | mycroft.audio.speech:mute_and_speak:127 | speak: it is eleven twenty four
2020-11-19 07:26:22.510 | warning  |   849 | gtts.tts | unable to get language list: 'nonetype' object is not subscriptable
high performance mpeg 1.0/2.0/2.5 audio player for layers 1, 2 and 3
        version 1.20.1; written and copyright by michael hipp and others
        free software (lgpl) without any warranty but with best wishes

directory: /tmp/mycroft/cache/tts/googletts/
playing mpeg stream 1 of 1: aca1ec5357c72b13588844da21a91e2f.mp3 ...

mpeg 2.0 layer iii, 32 kbit/s, 24000 hz mono

[0:02] decoding of aca1ec5357c72b13588844da21a91e2f.mp3 finished.

yeah, looks like almost two minutes. google tts?

yes, i am using google tts.

other than trying a different tts choice to ensure it’s not your local system, not sure what else to tell you.

i have some testing to do, but i think it is going to either be changes to the gtts calls on google side or changes to the gtts library used by mycroft and doing a ‘sudo apt-get full-upgrade’ installed a newer one (losing mycroft changes).

i moved the install the to ‘unstable’ build, tested, back to the ‘stable’ build and tested and both have the same results (slow speech with google tts).  i did this in hopes of forcing the stock mycroft installs just in case they might have been updated by something else with the os ‘full’ update.
"
79,testing and feedback voice assisted package manager,skill feedback,"
hi,
this is a bit of a non orthodox skill, that will (for now) require some work to install. (github: https://github.com/whtecmp/vapm)
how to install vapm
for now, only works with apt. the following instructions assume you are on a linux machine with apt and sudo.
the installation process is a bit complicated, because the skill requires some root access (for installing and removing stuff).
if you want to just test the skill, without using it, you can skip many of the parts. also the only non functional parts will be installing and removing packages.
first clone the repo to /opt/mycroft/skills/:
cd /opt/mycroft/skills/
git clone https://github.com/whtecmp/vapm.git

as root, create root directory for mycroft and copy scripts there:
mkdir -p /opt/mycroft-root/skills/vapm/
cp -r /opt/mycroft/skills/vapm/scripts/ /opt/mycroft-root/skills/vapm/
chmod +x /opt/mycroft-root/skills/vapm/scripts/*.sh

(if you want to only test the skill, without actually installing and removing packages you can stop here)
so in order to keep it safe you’ll need to create a new user on your linux and allow it to execute some specific scripts as root (using sudo). the mycroft process will be ran as that user.
create new user. as root run:
useradd mycroft
mkdir -p /home/mycroft
chown mycroft:mycroft -r /home/mycroft
visudo

in the text editor that had been opened, add the following lines:
mycroft all=(all) nopasswd: /opt/mycroft-root/vapm/scripts/install.sh
mycroft all=(all) nopasswd: /opt/mycroft-root/vapm/scripts/remove.sh

lock the mycroft acount with passwd -l mycroft.
anytime you need to loging as mycroft run sudo -u mycroft bash.

now, you’ll need to move ownership of mycroft to the new mycroft user. chown the following files like so, as root run:
chown mycroft:mycroft -r /opt/mycroft/
chown mycroft:mycroft -r /tmp/mycroft/
chown mycroft:mycroft -r /var/log/mycroft/
chown mycroft:mycroft /var/tmp/mycroft_web_cache.json
chown mycroft:mycroft /tmp/mycroft-msm.lck

you’ll also have to reinstall mycroft. first login as mycroft with sudo -u mycroft bash, then run cd ~.
follow the installation instructions from the mycroft official site.
note that any other skill you have will also be ran as mycroft user, so if it creates files or otherwise interacts with your system, permission issues could happen. mostly, you’ll just have to make sure to chown stuff correctly though.
(important: the security of this skill is still in question so don’t use it in any sensitive enviornmet)
how to test vapm
you have to first initiate a search:

find package python.

then you can further query on the results:

filter by contains word python and ends with dev.
describe second package.
read five of them.

or you can just install/remove packages:

download package python.
remove package gcc.

(you can’t use “install” and instead have to use “download” because “install” is reserved for installing mycroft skills)
when vapm refers finds a full match, this means you can use the pronoune “it” instead of the package name.
where feedback on vapm should be directed
here or in github.
","



 ctm-8400:

for now, only works with apt. the following instructions assume you are on a linux machine with apt and sudo.


i’m guessing you’ve heard of pako

yep, it is in my todo list 




 ctm-8400:

remove package gcc.


i would use another example, like xterm or xeyes or something less useful than gcc 
it seems this skill will work nicely on mark1, as mycroft user is already created and has the proper permissions all directories, and mycroft is being running by this user; it just would need to have superpowers to execute install.sh and remove.sh.
i think it would be also cool to upgrade.sh  the system and control another package manager like pacman, dnf, zypper or emerge, to name a few, pako seems useful for that purpose
"
80,virtualenv setup fails when installing mycroft core on openmediavault,support,"
i’m trying to install mycroft-core on my openmediavault server and keep running into a “failed to set up virtualenv for mycroft, exiting setup.” error when i run dev_setup.sh
i am ssh’d into the omv server with a user that has a home directory enabled where i am trying follow the linux installation instructions. i had to edit the script to change all source commands to . since the source command doesn’t seem to work on omv. here is the output from dev_setup.sh whenever i try to run it:
$ bash dev_setup.sh
installing packages...
 installing packages for debian/ubuntu/mint...
reading package lists... done
building dependency tree
reading state information... done
autoconf is already the newest version (2.69-10).
automake is already the newest version (1:1.15-6).
bison is already the newest version (2:3.0.4.dfsg-1+b1).
build-essential is already the newest version (12.3).
flac is already the newest version (1.3.2-1).
git is already the newest version (1:2.11.0-3+deb9u7).
libglib2.0-dev is already the newest version (2.50.3-2+deb9u2).
libicu-dev is already the newest version (57.1-6+deb9u4).
jq is already the newest version (1.5+dfsg-1.3).
libfann-dev is already the newest version (2.2.0+ds-3).
libffi-dev is already the newest version (3.2.1-6).
libtool is already the newest version (2.4.6-2).
mpg123 is already the newest version (1.23.8-1+b1).
libssl-dev is already the newest version (1.1.0l-1~deb9u1).
pkg-config is already the newest version (0.29-4+b1).
portaudio19-dev is already the newest version (19.6.0-1).
pulseaudio is already the newest version (10.0-1+deb9u1).
pulseaudio-utils is already the newest version (10.0-1+deb9u1).
python3-setuptools is already the newest version (33.1.1-1).
python3 is already the newest version (3.5.3-1).
python3-dev is already the newest version (3.5.3-1).
screen is already the newest version (4.5.0-6).
swig is already the newest version (3.0.10-1.1).
curl is already the newest version (7.52.1-5+deb9u12).
libjpeg-dev is already the newest version (1:1.5.1-2+deb9u1).
0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.
  % total    % received % xferd  average speed   time    time     time  current
                                 dload  upload   total   spent    left  speed
100 1842k  100 1842k    0     0  1539k      0  0:00:01  0:00:01 --:--:-- 1538k
deprecation: python 3.5 reached the end of its life on september 13th, 2020. please upgrade your python as python 3.5 is no longer maintained. pip 21.0 will drop support for python 3.5 in january 2021. pip 21.0 will remove support for this functionality.
collecting pip==20.0.2
  using cached pip-20.0.2-py2.py3-none-any.whl (1.4 mb)
installing collected packages: pip
  attempting uninstall: pip
    found existing installation: pip 20.0.2
    uninstalling pip-20.0.2:
      successfully uninstalled pip-20.0.2
successfully installed pip-20.0.2
warning: you are using pip version 20.0.2; however, version 20.2.4 is available.
you should consider upgrading via the '${virtualenv_root}/bin/python -m pip install --upgrade pip' command.
failed to set up virtualenv for mycroft, exiting setup.

i checked the bash script and found the install_venv function seems to be what triggers that error if things don’t go right. however, manually running the python3 -m venv ""${virtualenv_root}/"" --without-pip command from terminal doesn’t throw any errors if i swap the env variable for the actual location (actually doesn’t seem to do anything, just returns). and pip does exist at the ${virtualenv_root}/bin location. so i’m not sure what is causing the error.
any ideas what the problem is or what to check next?
","
the first thing caught my eye that



 foxsly42:

you should consider upgrading via the ‘${virtualenv_root}/bin/python -m pip install --upgrade pip’ command.


$virtualenv_root isn’t resolved, which it should be. thus
$opt_python -m venv ""${virtualenv_root}/"" --without-pip
won’t install the venv in the first place. to confirm that make a test script with
#!/bin/bash
cd $(dirname $0)
top=$(pwd -l)
echo ""$top""
virtualenv_root=${virtualenv_root:-""${top}/.venv""}
echo ""$virtualenv_root""

ps:
in my refactor i use
top=$( cd ""$( dirname ""${bash_source[0]}"" )"" && pwd )
which runs more reliable, especially when it comes to sourcing stuff

ah, that’s my bad. sorry for the red herring there. $virtualenv_root does resolve and i used the resolved value when manually running the python3 venv command and checking the folder pip exists in. it was late and i replaced it in the output with the variable name not wanting the username posted but not realizing that what i wrote might be misleading.
i created and ran your test script. output is right here and i believe it is what is expected:
$ bash testscript.sh
/srv/dev-disk-by-label-toshibaext/home/foxsly42/mycroft-core
/srv/dev-disk-by-label-toshibaext/home/foxsly42/mycroft-core/.venv

there, redacted the user name in a more useful manner  and i get the same output from the script whether i use the first expression for $top or the expression in your refactor.

then [[ -x ${virtualenv_root}/bin/pip ]] failing because pip is not executable.

thanks, i hadn’t gotten to researching that line yet. i thought it might be some way of storing a bool for whether or not the file exists.  my shell skills are on the minimal side.
after checking on the server again, it seems you’re right but i don’t know why it’s not executable. my linux terminal skills are also only rudimentary but this is the output i got when trying to get it working. it seems that the user i’m logged in as owns the file and it is marked executable but i get a permission denied when attempting to execute it.
$ ls -l ./.venv/bin
total 52
-rw-r--r--+ 1 foxsly42 users 2183 nov  8 19:21 activate
-rw-r--r--+ 1 foxsly42 users 1299 nov  8 19:21 activate.csh
-rw-r--r--+ 1 foxsly42 users 2463 nov  8 19:21 activate.fish
drwxrws---+ 2 foxsly42 users 4096 nov  8 21:58 bin
-rwxrwxr-x+ 1 foxsly42 users  290 nov  8 17:44 easy_install
-rwxrwxr-x+ 1 foxsly42 users  290 nov  8 17:44 easy_install-3.5
drwxrws---+ 2 foxsly42 users 4096 nov  8 21:58 include
drwxrws---+ 3 foxsly42 users 4096 nov  8 21:58 lib
lrwxrwxrwx  1 foxsly42 users    3 nov  8 21:58 lib64 -> lib
-rwxrwxr-x+ 1 foxsly42 users  281 nov  8 18:23 pip
-rwxrwxr-x+ 1 foxsly42 users  281 nov  8 18:23 pip3
-rwxrwxr-x+ 1 foxsly42 users  281 nov  8 18:23 pip3.5
lrwxrwxrwx  1 foxsly42 users    7 nov  8 17:44 python -> python3
lrwxrwxrwx  1 foxsly42 users   16 nov  8 17:44 python3 -> /usr/bin/python3
-rw-rw----+ 1 foxsly42 users  130 nov  8 21:58 pyvenv.cfg
-rwxrwxr-x+ 1 foxsly42 users  268 nov  8 17:44 wheel
$ chmod +x ./.venv/bin/pip
$ ls -l ./.venv/bin
total 52
-rw-r--r--+ 1 foxsly42 users 2183 nov  8 19:21 activate
-rw-r--r--+ 1 foxsly42 users 1299 nov  8 19:21 activate.csh
-rw-r--r--+ 1 foxsly42 users 2463 nov  8 19:21 activate.fish
drwxrws---+ 2 foxsly42 users 4096 nov  8 21:58 bin
-rwxrwxr-x+ 1 foxsly42 users  290 nov  8 17:44 easy_install
-rwxrwxr-x+ 1 foxsly42 users  290 nov  8 17:44 easy_install-3.5
drwxrws---+ 2 foxsly42 users 4096 nov  8 21:58 include
drwxrws---+ 3 foxsly42 users 4096 nov  8 21:58 lib
lrwxrwxrwx  1 foxsly42 users    3 nov  8 21:58 lib64 -> lib
-rwxrwxr-x+ 1 foxsly42 users  281 nov  8 18:23 pip
-rwxrwxr-x+ 1 foxsly42 users  281 nov  8 18:23 pip3
-rwxrwxr-x+ 1 foxsly42 users  281 nov  8 18:23 pip3.5
lrwxrwxrwx  1 foxsly42 users    7 nov  8 17:44 python -> python3
lrwxrwxrwx  1 foxsly42 users   16 nov  8 17:44 python3 -> /usr/bin/python3
-rw-rw----+ 1 foxsly42 users  130 nov  8 21:58 pyvenv.cfg
-rwxrwxr-x+ 1 foxsly42 users  268 nov  8 17:44 wheel
$ ./.venv/bin/pip --version
-sh: 8: ./.venv/bin/pip: permission denied
$ whoami
foxsly42
$ /srv/dev-disk-by-label-toshibaext/home/foxsly42/mycroft-core/.venv/bin/pip
-sh: 11: /srv/dev-disk-by-label-toshibaext/home/foxsly42/mycroft-core/.venv/bin/pip: permission denied
$ ./.venv/bin/pip install
-sh: 13: ./.venv/bin/pip: permission denied


is anything below that directory not belonging to that owner:user?
srv -> bin

looks like the user owns all files and directories in the tree…
$ ls -lr ./.venv/bin
./.venv/bin:
total 52
-rw-r--r--+ 1 foxsly42 users 2183 nov  8 19:21 activate
-rw-r--r--+ 1 foxsly42 users 1299 nov  8 19:21 activate.csh
-rw-r--r--+ 1 foxsly42 users 2463 nov  8 19:21 activate.fish
drwxrws---+ 2 foxsly42 users 4096 nov  8 21:58 bin
-rwxrwxr-x+ 1 foxsly42 users  290 nov  8 17:44 easy_install
-rwxrwxr-x+ 1 foxsly42 users  290 nov  8 17:44 easy_install-3.5
drwxrws---+ 2 foxsly42 users 4096 nov  8 21:58 include
drwxrws---+ 3 foxsly42 users 4096 nov  8 21:58 lib
lrwxrwxrwx  1 foxsly42 users    3 nov  8 21:58 lib64 -> lib
-rwxrwxr-x+ 1 foxsly42 users  281 nov  8 18:23 pip
-rwxrwxr-x+ 1 foxsly42 users  281 nov  8 18:23 pip3
-rwxrwxr-x+ 1 foxsly42 users  281 nov  8 18:23 pip3.5
lrwxrwxrwx  1 foxsly42 users    7 nov  8 17:44 python -> python3
lrwxrwxrwx  1 foxsly42 users   16 nov  8 17:44 python3 -> /usr/bin/python3
-rw-rw----+ 1 foxsly42 users  130 nov  8 21:58 pyvenv.cfg
-rwxrwxr-x+ 1 foxsly42 users  268 nov  8 17:44 wheel

./.venv/bin/bin:
total 16
-rw-r--r--+ 1 foxsly42 users 2183 nov  8 21:58 activate
-rw-r--r--+ 1 foxsly42 users 1299 nov  8 21:58 activate.csh
-rw-r--r--+ 1 foxsly42 users 2463 nov  8 21:58 activate.fish
lrwxrwxrwx  1 foxsly42 users    7 nov  8 21:58 python -> python3
lrwxrwxrwx  1 foxsly42 users   77 nov  8 21:58 python3 -> /srv/dev-disk-by-label-toshibaext/home/foxsly42/mycroft-core/.venv/bin/python3

./.venv/bin/include:
total 0

./.venv/bin/lib:
total 4
drwxrws---+ 3 foxsly42 users 4096 nov  8 21:58 python3.5

./.venv/bin/lib/python3.5:
total 4
drwxrws---+ 2 foxsly42 users 4096 nov  8 21:58 site-packages

./.venv/bin/lib/python3.5/site-packages:
total 0

i just realized while typing this that it looks like python3 is a link to /usr/bin/python3 which is definitely owned by the root user and $opt_python in the script was evaluating to python3. is that possibly the problem?
i’m pretty sure that it would not be a good idea to chown /usr/bin/python3 to the current user. any ideas how i would go about fixing the link or getting a locally owned install of python for the virtual environment? i thought that was what the setup script already does.

that is not what i meant. an example: my venv/pip is sitting /home/gui/mycroft-core/.venv/bin/pip. if i chown gui to root:root and call /home/gui/mycroft-core/.venv/bin/pip i get the permission denied, too. in those nas cases it is not unlikely that ```“dev-disk-by-label-toshibaext” is not accessible to the user that want to use pip



 foxsly42:

any ideas how i would go about fixing the link or getting a locally owned install of python for the virtual environment? i thought that was what the setup script already does.


this is what i would do anyway, since 3.5 is a bad idea (and you should also thinking about raising the system python in the near future)
2 pathways

dev_setup.sh with the argument --python 3.7 (the most easy thing but never used it as such tbh)

pyenv -> (modify .bashrc) pyenv install 3.7, and then set pyenv local 3.7 in the base directory (this is before you create the venv, so you have to relink which should look like  python3 -> /home/###/.pyenv/versions/3.7.9/bin/python3)

yet this wouldn’t encompass the problem described above (the script is only checking pip, not what’s happening down the drain)

ah okay. yes, it appears that is the case. the nas disk is owned by root. thanks, i didn’t realize that execution and ownership up the tree worked that way.
$ ls -l /srv
drwxr-xr-x 11 root root    4096 nov  8 16:55 dev-disk-by-label-toshibaext

is it safe/okay to just chown the nas folder to the foxsly42 user? or would the “right” way to do it be like adding foxsly42 to the root group to give access?
and thanks for the python 3.7 recommendation. i noticed the deprecation warnings but hadn’t thought through dealing with upgrading. probably better to get that taken care of sooner rather than later.

alright, i think i got it solved. just got the dev_setup.sh script to run and it looks like it successfully installed and built everything.
so i tried chown-ing all the directories up the tree to the current user and i still was getting permission denied errors for pip. i didn’t really like changing ownership on those folders anyways so no complaints reverting all that.
it seems like another part of the problem is that omv by default mounts nas or data disks using the noexec option. i first tried changing the /etc/openmediavault/config.xml file to remove the noexec option from the mntent entry for that disk. after doing that i tried both running omv-mkconf fstab while logged in as root (i’m on omv 4, on omv 5 the command would be omv-salt deploy run fstab) and restarting the server. neither of those seemed to work and /proc/mounts still listed the noexec flag for the drive and sharedfolders.
so i took a different approach that seems to work. i ssh’d to the server as the user i’m trying to install mycroft with and created a folder on / (the root directory) and then created a symlink in my user’s home directory to that folder. after that i began the mycroft install directions inside the symlinked folder in my home directory and the dev_setup.sh script ran fine.
i’m thinking the symlink folder option is probably a better solution than making it run on the nas disk anyways.
thanks for all the help!
p.s. running the dev_setup.sh from the symlink folder also meant that i didn’t have to change all the source commands in it to . as mentioned in the original post. script ran just fine as it was.
"
81,unable to get the registeration code,none,"
in a few moments you will see the contents of the speech log.  hit
ctrl+c to stop showing the log and return to the linux command line.
mycroft will continue running in the background for voice interaction.
initializing…
starting cli
10:47:55.186 - mycroft.configuration.config:load_local:124 - debug - configuration /home/pi/mycroft-core/mycroft/configuration/mycroft.conf loaded
10:47:55.349 - mycroft.configuration.config:load_local:124 - debug - configuration /home/pi/mycroft-core/mycroft/configuration/mycroft.conf loaded
10:47:55.508 - mycroft.configuration.config:load_local:124 - debug - configuration /etc/mycroft/mycroft.conf loaded
10:47:55.668 - mycroft.configuration.config:load_local:129 - debug - configuration ‘/home/pi/.mycroft/mycroft.conf’ not found
10:47:55.682 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
10:47:55.857 - mycroft.api:is_paired:386 - warning - could not get device info: connectionerror(maxretryerror(“httpsconnectionpool(host=‘api.mycroft.ai’, port=443): max retries exceeded with url: /v1/device/ (caused by newconnectionerror(’<urllib3.connection.verifiedhttpsconnection object at 0x75e7d750>: failed to establish a new connection: [errno -3] temporary failure in name resolution’,))”,),)
10:47:56.017 - mycroft.configuration.config:load_local:124 - debug - configuration /var/tmp/mycroft_web_cache.json loaded
10:47:56.176 - mycroft.configuration.config:load_local:124 - debug - configuration /etc/mycroft/mycroft.conf loaded
10:47:56.334 - mycroft.configuration.config:load_local:129 - debug - configuration ‘/home/pi/.mycroft/mycroft.conf’ not found
i am getting this error on my first boot
this is my first time with mycroft please help i havent got the registeration code.
","
the conf file not found isn’t a big deal.  the failure in name resolution, though, will cause some problems.  is it connected to the internet?

hi learning_champ, welcome to the community!
sorry to hear there was a hiccup on the first run. did the cli interface for mycroft successfully launch or did it return you to the normal linux command line?
to get a full copy of your logs, you can run the following command that will read the contents of the file and upload it to termbin.com. it will then give you a url to access it. it would be good to see your enclosure and skills logs, so can you run:
cat /var/log/mycroft/enclosure.log | nc termbin.com 9999
cat /var/log/mycroft/skills.log | nc termbin.com 9999

then drop the links here for us to take a look?

2020-11-20 13:58:47.963 | warning  |   983 | mycroft.api:check_remote_pairing:531 | could not get device info: connectionerror(maxretryerror(“httpsconnectionpool(host=‘api.mycroft.ai’, port=443): max retries exceeded with url: /v1/device/c25d4eed-d513-4072-bdb2-e482d8bc3a6e (caused by newconnectionerror(’<urllib3.connection.verifiedhttpsconnection object at 0x7fe613daeca0>: failed to establish a new connection: [errno -3] temporary failure in name resolution’))”))
i’m having this same problem when i try to start mycroft during bootup as described in the linux installation instructions.
my termbin links are
https://termbin.com/rngob
https://termbin.com/8frc
when i start from the command “./start-mycroft.sh all” everything is fine.
my guess is that the boot process is prohibited from accessing the internet, but i don’t have an idea where to begin attacking the problem.
thank you for any ideas.

verify you can connect to https://api.mycroft.ai from the host you’re trying to pair (curl/wget/browser, should give you a token not found error).

mycroft connects just fine when i start it from the command line.  for the moment i am starting it from my .profile file so it starts when i log in.
i believe this means i am connecting to https://api.mycroft.ai.

i tried asking the mycroft service to wait until networkmanager starts, but it didn’t work.
[unit]
description=mycroft ai
after=pulseaudio.service
after=networkmanager.service

add a sleep command to the startup perhaps?

nice idea, but i’m not sure how to add a sleep command to systemd.  i’ve solve the problem by including the startup command in my .profile.  it works and i’m the only person using the computer.
thank you both!
"
82,wake word not detected,support,"
i just set up mycroft via dietpi on my raspberry pi 4. i’m using blue snowball as microphone and normal 3.5mm speakers. i can’t get mycroft to detect the default wake words (“hey mycroft” nor “wake up”). i tried changing the listener and having another person say it.
my speaking is understood if i get it to talk to me via cli (set time, for example). i tried different microphone input levels with no change.
","
/var/log/mycroft/ is a good place to gather information about errors/warnings

root@dietpi:/var/log/mycroft# cat audio.log
2020-11-09 22:36:50.117 | info     |   766 | __main__:on_stopping:38 | audio service is shutting down...
2020-11-09 22:36:50.615 | info     |   766 | mycroft.audio.audioservice:shutdown:467 | shutting down local
2020-11-09 22:36:50.628 | info     |   766 | audioservice_simple:stop:152 | simpleaudioservicestop
2020-11-09 22:38:15.129 | info     | 19890 | mycroft.messagebus.load_config:load_message_bus_config:33 | loading message bus configs
2020-11-09 22:38:15.212 | info     | 19890 | mycroft.tts.mimic2_tts:__init__:176 | getting pre-loaded cache
2020-11-09 22:38:15.490 | info     | 19890 | mycroft.tts.mimic2_tts:__init__:178 | successfully downloaded pre-loaded cache
2020-11-09 22:38:15.500 | info     | 19890 | __main__:main:50 | starting audio services
2020-11-09 22:38:15.523 | info     | 19890 | mycroft.messagebus.client.client:on_open:114 | connected
2020-11-09 22:38:15.550 | info     | 19890 | mycroft.audio.audioservice:get_services:61 | loading services from /mnt/extern/dietpi_userdata/mycroft-core/mycroft/audio/services/
2020-11-09 22:38:15.605 | info     | 19890 | mycroft.audio.audioservice:load_services:105 | loading chromecast
2020-11-09 22:38:22.812 | info     | 19890 | mycroft.audio.audioservice:load_services:105 | loading mopidy
2020-11-09 22:38:22.846 | info     | 19890 | mycroft.audio.audioservice:load_services:105 | loading mplayer
2020-11-09 22:38:23.355 | error    | 19890 | audioservice_mplayer:<module>:20 | install py_mplayer with pip install git+https://github.com/jarbasal/py_mplayer
2020-11-09 22:38:23.366 | error    | 19890 | mycroft.audio.audioservice:load_services:114 | failed to import module mplayer
modulenotfounderror(""no module named 'py_mplayer'"")
2020-11-09 22:38:23.380 | info     | 19890 | mycroft.audio.audioservice:load_services:105 | loading simple
2020-11-09 22:38:23.443 | info     | 19890 | mycroft.audio.audioservice:load_services:105 | loading vlc
2020-11-09 22:38:23.654 | error    | 19890 | mycroft.audio.audioservice:load_services:129 | failed to load service. nameerror(""no function 'libvlc_new'"")
2020-11-09 22:38:23.670 | info     | 19890 | mycroft.audio.audioservice:load_services_callback:177 | finding default backend...
2020-11-09 22:38:23.684 | info     | 19890 | mycroft.audio.audioservice:load_services_callback:181 | found local
2020-11-09 22:38:23.695 | info     | 19890 | __main__:on_ready:30 | audio service is ready.
2020-11-09 22:45:20.987 | info     | 19890 | mycroft.audio.speech:mute_and_speak:127 | speak: i'm starting a timer for twenty minutes
playing wave '/tmp/mycroft/cache/tts/mimic2/e3b2de400ff30d4bb8bcac841ad9ccf9.wav' : signed 16 bit little endian, rate 22000 hz, mono




root@dietpi:/var/log/mycroft# cat voice.log
2020-11-09 22:36:51.098 | info     |   769 | __main__:on_stopping:179 | speech service is shutting down...
2020-11-09 22:38:10.988 | info     | 19893 | mycroft.messagebus.load_config:load_message_bus_config:33 | loading message bus configs
expression 'alsa_snd_pcm_hw_params_set_period_size_near( pcm, hwparams, &alsaperiodframes, &dir )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 924
expression 'alsa_snd_pcm_hw_params_set_period_size_near( pcm, hwparams, &alsaperiodframes, &dir )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 924
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_headpho.pcm.front.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm front
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.rear
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.center_lfe
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.side
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_headpho.pcm.surround51.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround21
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_headpho.pcm.surround51.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround21
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_headpho.pcm.surround40.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround40
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_headpho.pcm.surround51.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround41
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_headpho.pcm.surround51.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround50
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_headpho.pcm.surround51.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround51
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_headpho.pcm.surround71.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround71
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_headpho.pcm.iec958.0:card=0,aes0=4,aes1=130,aes2=0,aes3=2'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm iec958
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_headpho.pcm.iec958.0:card=0,aes0=4,aes1=130,aes2=0,aes3=2'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm spdif
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_headpho.pcm.iec958.0:card=0,aes0=4,aes1=130,aes2=0,aes3=2'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm spdif
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.hdmi
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.hdmi
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.modem
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.modem
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.phoneline
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.phoneline
expression 'alsa_snd_pcm_hw_params_set_period_size_near( pcm, hwparams, &alsaperiodframes, &dir )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 934
expression 'alsa_snd_pcm_hw_params_set_period_size_near( pcm, hwparams, &alsaperiodframes, &dir )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 934
expression 'alsa_snd_pcm_hw_params_set_period_size_near( pcm, hwparams, &alsaperiodframes, &dir )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 934
alsa lib pcm_oss.c:377:(_snd_pcm_oss_open) unknown field port
alsa lib pcm_oss.c:377:(_snd_pcm_oss_open) unknown field port
alsa lib pulse.c:243:(pulse_connect) pulseaudio: unable to connect: connection refused

alsa lib pulse.c:243:(pulse_connect) pulseaudio: unable to connect: connection refused

alsa lib pcm_a52.c:823:(_snd_pcm_a52_open) a52 is only for playback
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_headpho.pcm.iec958.0:card=0,aes0=6,aes1=130,aes2=0,aes3=2'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm iec958:{aes0 0x6 aes1 0x82 aes2 0x0 aes3 0x2  card 0}
alsa lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) invalid type for card
alsa lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) invalid type for card
expression 'alsa_snd_pcm_hw_params_set_period_size_near( pcm, hwparams, &alsaperiodframes, &dir )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 924
2020-11-09 22:38:12.888 | info     | 19893 | mycroft.client.speech.listener:create_wake_word_recognizer:328 | creating wake word engine
2020-11-09 22:38:12.894 | info     | 19893 | mycroft.client.speech.listener:create_wake_word_recognizer:351 | using hotword entry for hey mycroft
2020-11-09 22:38:12.903 | info     | 19893 | mycroft.client.speech.hotword_factory:load_module:403 | loading ""hey mycroft"" wake word via precise
2020-11-09 22:38:15.494 | info     | 19893 | mycroft.client.speech.listener:create_wakeup_recognizer:365 | creating stand up word engine
2020-11-09 22:38:15.516 | info     | 19893 | mycroft.client.speech.hotword_factory:load_module:403 | loading ""wake up"" wake word via pocketsphinx
2020-11-09 22:38:15.711 | info     | 19893 | __main__:on_ready:175 | speech client is ready.
2020-11-09 22:38:15.847 | info     | 19893 | mycroft.messagebus.client.client:on_open:114 | connected




 green:

alsa lib pcm_a52.c:823:(_snd_pcm_a52_open) a52 is only for playback


check the audio troubleshooting docs, or  start googling those errors in the voice.log and see what you can find to resolve them.  seems like it can’t set up a listening device.

make sure that the user is present in audiospecific groups (permission problems)



 green:

alsa lib pulse.c:243:(pulse_connect) pulseaudio: unable to connect: connection refused



i tried googling already, but most threads are either very old or don’t have solutions that work.
audio troubleshooting has some results:
root@dietpi:/# mycroft-start audiotest
already up to date.
initializing...
starting audiotest
 ========================== info ===========================
 input device: default device @ sample rate: 48000 hz
 playback commandline: aplay wav_file

 ===========================================================
 ==         starting to record, make some noise!          ==
 ===========================================================
traceback (most recent call last):
  file ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  file ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  file ""/mnt/extern/dietpi_userdata/mycroft-core/mycroft/util/audio_test.py"", line 132, in <module>
    main()
  file ""/mnt/extern/dietpi_userdata/mycroft-core/mycroft/util/audio_test.py"", line 119, in main
    record(args.filename, args.duration)
  file ""/mnt/extern/dietpi_userdata/mycroft-core/mycroft/util/audio_test.py"", line 66, in record
    with mic as source:
  file ""/mnt/extern/dietpi_userdata/mycroft-core/mycroft/client/speech/mic.py"", line 140, in __enter__
    return self._start()
  file ""/mnt/extern/dietpi_userdata/mycroft-core/mycroft/client/speech/mic.py"", line 151, in _start
    input=true,  # stream is an input stream
  file ""/mnt/dietpi_userdata/mycroft-core/.venv/lib/python3.7/site-packages/pyaudio.py"", line 750, in open
    stream = stream(self, *args, **kwargs)
  file ""/mnt/dietpi_userdata/mycroft-core/.venv/lib/python3.7/site-packages/pyaudio.py"", line 441, in __init__
    self._stream = pa.open(**arguments)
oserror: [errno -9996] invalid input device (no default output device)

this is strange, because i can get output/input if i give commands via mycroft-cli-client (if i tell him “set timer”, he understands if i say “5 minutes” or so)
root@dietpi:/# sudo usermod -ag ""cat /etc/group | grep -e '^pulse:' -e '^audio:' -e '^pulse-access:' -e '^pulse-rt:' -e '^video:' | awk -f: '{print $1}' | tr '\n' ',' | sed 's:,$::g'"" ""whoami""
usermod: group 'cat /etc/group | grep -e '^pulse:' -e '^audio:' -e '^pulse-access:' -e '^pulse-rt:' -e '^video:' | awk -f: '{print }' | tr '\n' '' does not exist
usermod: group '' | sed 's:' does not exist
usermod: group '$::g'' does not exist

changed the quotes to fit in this format (and was too lazy to change it in reverse ‘""""’)
there you go:
https://app.box.com/s/ogfpui4uud8fhdthe2jemzz5b88j6tc5

root@dietpi:/# sudo usermod -ag `cat /etc/group | grep -e '^pulse:' -e '^audio:' -e '^pulse-access:' -e '^pulse-rt:' -e '^video:' | awk -f: '{print $1}' | tr '\n' ',' | sed 's:,$::g'` `whoami`
root@dietpi:/#

yeah, he sprincled some ``         `   in with my attempt

that was the result from permissions.sh.

the result is that now the user is in the groups (there is no output, unless there is a failure)
cat /etc/group

root@dietpi:/# cat /etc/group
root:x:0:
daemon:x:1:
bin:x:2:
sys:x:3:
adm:x:4:
tty:x:5:
disk:x:6:
lp:x:7:
mail:x:8:
news:x:9:
uucp:x:10:
man:x:12:
proxy:x:13:
kmem:x:15:
dialout:x:20:
fax:x:21:
voice:x:22:
cdrom:x:24:
floppy:x:25:
tape:x:26:
sudo:x:27:
audio:x:29:pulse,mycroft,root
dip:x:30:
www-data:x:33:
backup:x:34:
operator:x:37:
list:x:38:
irc:x:39:
src:x:40:
gnats:x:41:
shadow:x:42:
utmp:x:43:
video:x:44:root
sasl:x:45:
plugdev:x:46:
staff:x:50:
games:x:60:
users:x:100:
nogroup:x:65534:
systemd-journal:x:101:
systemd-timesync:x:102:
systemd-network:x:103:
systemd-resolve:x:104:
input:x:105:
kvm:x:106:
render:x:107:
crontab:x:108:
netdev:x:109:
messagebus:x:110:
bluetooth:x:112:
spi:x:999:
i2c:x:998:
gpio:x:997:
rdma:x:115:
systemd-coredump:x:996:
dietpi:x:1000:
mysql:x:111:
redis:x:113:www-data
pihole:x:995:www-data
pulse:x:114:root
pulse-access:x:116:root
mycroft:x:994:

not sure if “root” is the best to run with. in this case pulse has to be run systemwide, isn’t it?
@j1nx might have some idea
you have a user mycroft, this has to be run from there.
su mycroft

could it be that the user “mycroft” needs access to pulse?
i’m still not sure why only wake words are not working with this, though.
root@dietpi:~# mycroft-start audiotest -l
already up to date.
initializing...
starting audiotest
 initializing...
alsa lib confmisc.c:767:(parse_card) cannot find card '0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: no such file or directory
alsa lib confmisc.c:392:(snd_func_concat) error evaluating strings
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: no such file or directory
alsa lib confmisc.c:1246:(snd_func_refer) error evaluating name
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm sysdefault
alsa lib confmisc.c:767:(parse_card) cannot find card '0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: no such file or directory
alsa lib confmisc.c:392:(snd_func_concat) error evaluating strings
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: no such file or directory
alsa lib confmisc.c:1246:(snd_func_refer) error evaluating name
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm sysdefault
alsa lib confmisc.c:767:(parse_card) cannot find card '0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: no such file or directory
alsa lib confmisc.c:392:(snd_func_concat) error evaluating strings
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: no such file or directory
alsa lib confmisc.c:1246:(snd_func_refer) error evaluating name
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm front
alsa lib confmisc.c:767:(parse_card) cannot find card '0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: no such file or directory
alsa lib confmisc.c:392:(snd_func_concat) error evaluating strings
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: no such file or directory
alsa lib confmisc.c:1246:(snd_func_refer) error evaluating name
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm rear
alsa lib confmisc.c:767:(parse_card) cannot find card '0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: no such file or directory
alsa lib confmisc.c:392:(snd_func_concat) error evaluating strings
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: no such file or directory
alsa lib confmisc.c:1246:(snd_func_refer) error evaluating name
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm center_lfe
alsa lib confmisc.c:767:(parse_card) cannot find card '0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: no such file or directory
alsa lib confmisc.c:392:(snd_func_concat) error evaluating strings
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: no such file or directory
alsa lib confmisc.c:1246:(snd_func_refer) error evaluating name
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm side
alsa lib confmisc.c:767:(parse_card) cannot find card '0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: no such file or directory
alsa lib confmisc.c:392:(snd_func_concat) error evaluating strings
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: no such file or directory
alsa lib confmisc.c:1246:(snd_func_refer) error evaluating name
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround21
alsa lib confmisc.c:767:(parse_card) cannot find card '0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: no such file or directory
alsa lib confmisc.c:392:(snd_func_concat) error evaluating strings
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: no such file or directory
alsa lib confmisc.c:1246:(snd_func_refer) error evaluating name
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround21
alsa lib confmisc.c:767:(parse_card) cannot find card '0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: no such file or directory
alsa lib confmisc.c:392:(snd_func_concat) error evaluating strings
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: no such file or directory
alsa lib confmisc.c:1246:(snd_func_refer) error evaluating name
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround40
alsa lib confmisc.c:767:(parse_card) cannot find card '0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: no such file or directory
alsa lib confmisc.c:392:(snd_func_concat) error evaluating strings
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: no such file or directory
alsa lib confmisc.c:1246:(snd_func_refer) error evaluating name
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround41
alsa lib confmisc.c:767:(parse_card) cannot find card '0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: no such file or directory
alsa lib confmisc.c:392:(snd_func_concat) error evaluating strings
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: no such file or directory
alsa lib confmisc.c:1246:(snd_func_refer) error evaluating name
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround50
alsa lib confmisc.c:767:(parse_card) cannot find card '0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: no such file or directory
alsa lib confmisc.c:392:(snd_func_concat) error evaluating strings
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: no such file or directory
alsa lib confmisc.c:1246:(snd_func_refer) error evaluating name
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround51
alsa lib confmisc.c:767:(parse_card) cannot find card '0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: no such file or directory
alsa lib confmisc.c:392:(snd_func_concat) error evaluating strings
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: no such file or directory
alsa lib confmisc.c:1246:(snd_func_refer) error evaluating name
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround71
alsa lib confmisc.c:767:(parse_card) cannot find card '0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: no such file or directory
alsa lib confmisc.c:392:(snd_func_concat) error evaluating strings
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: no such file or directory
alsa lib confmisc.c:1246:(snd_func_refer) error evaluating name
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm iec958
alsa lib confmisc.c:767:(parse_card) cannot find card '0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: no such file or directory
alsa lib confmisc.c:392:(snd_func_concat) error evaluating strings
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: no such file or directory
alsa lib confmisc.c:1246:(snd_func_refer) error evaluating name
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm spdif
alsa lib confmisc.c:767:(parse_card) cannot find card '0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: no such file or directory
alsa lib confmisc.c:392:(snd_func_concat) error evaluating strings
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: no such file or directory
alsa lib confmisc.c:1246:(snd_func_refer) error evaluating name
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm spdif
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.hdmi
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.hdmi
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.modem
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.modem
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.phoneline
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.phoneline
alsa lib pcm_hw.c:1822:(_snd_pcm_hw_open) invalid value for card
alsa lib pcm_hw.c:1822:(_snd_pcm_hw_open) invalid value for card
alsa lib pcm_hw.c:1822:(_snd_pcm_hw_open) invalid value for card
alsa lib pcm_hw.c:1822:(_snd_pcm_hw_open) invalid value for card
alsa lib pcm_hw.c:1822:(_snd_pcm_hw_open) invalid value for card
alsa lib pcm_hw.c:1822:(_snd_pcm_hw_open) invalid value for card
alsa lib pcm_oss.c:377:(_snd_pcm_oss_open) unknown field port
alsa lib pcm_oss.c:377:(_snd_pcm_oss_open) unknown field port
alsa lib pulse.c:243:(pulse_connect) pulseaudio: unable to connect: connection refused

alsa lib pulse.c:243:(pulse_connect) pulseaudio: unable to connect: connection refused

alsa lib pcm_a52.c:823:(_snd_pcm_a52_open) a52 is only for playback
alsa lib confmisc.c:767:(parse_card) cannot find card '0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: no such file or directory
alsa lib confmisc.c:392:(snd_func_concat) error evaluating strings
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: no such file or directory
alsa lib confmisc.c:1246:(snd_func_refer) error evaluating name
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm iec958:{aes0 0x6 aes1 0x82 aes2 0x0 aes3 0x2  card 0}
alsa lib pcm_hw.c:1822:(_snd_pcm_hw_open) invalid value for card
alsa lib pcm_hw.c:1822:(_snd_pcm_hw_open) invalid value for card
alsa lib pcm_hw.c:1822:(_snd_pcm_hw_open) invalid value for card
alsa lib pcm_hw.c:1822:(_snd_pcm_hw_open) invalid value for card
alsa lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) invalid type for card
alsa lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) invalid type for card
alsa lib confmisc.c:767:(parse_card) cannot find card '0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: no such file or directory
alsa lib confmisc.c:392:(snd_func_concat) error evaluating strings
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: no such file or directory
alsa lib confmisc.c:1246:(snd_func_refer) error evaluating name
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm dmix
 ====================== audio devices ======================
  index    device name

 ========================== info ===========================
 input device: default device @ sample rate: 48000 hz
 playback commandline: aplay wav_file

 ===========================================================
 ==         starting to record, make some noise!          ==
 ===========================================================
traceback (most recent call last):
  file ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  file ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  file ""/mnt/extern/dietpi_userdata/mycroft-core/mycroft/util/audio_test.py"", line 132, in <module>
    main()
  file ""/mnt/extern/dietpi_userdata/mycroft-core/mycroft/util/audio_test.py"", line 119, in main
    record(args.filename, args.duration)
  file ""/mnt/extern/dietpi_userdata/mycroft-core/mycroft/util/audio_test.py"", line 66, in record
    with mic as source:
  file ""/mnt/extern/dietpi_userdata/mycroft-core/mycroft/client/speech/mic.py"", line 140, in __enter__
    return self._start()
  file ""/mnt/extern/dietpi_userdata/mycroft-core/mycroft/client/speech/mic.py"", line 151, in _start
    input=true,  # stream is an input stream
  file ""/mnt/dietpi_userdata/mycroft-core/.venv/lib/python3.7/site-packages/pyaudio.py"", line 750, in open
    stream = stream(self, *args, **kwargs)
  file ""/mnt/dietpi_userdata/mycroft-core/.venv/lib/python3.7/site-packages/pyaudio.py"", line 441, in __init__
    self._stream = pa.open(**arguments)
oserror: [errno -9996] invalid input device (no default output device)




 green:

could it be that the user “mycroft” needs access to pulse?


that’s for sure … see edit above

root@dietpi:~# su mycroft
this account is currently not available.

is there a necessity that mycroft is sitting on a mount?
btw i hope you have not exposed owncloud to the outside.




 sgee:

not sure if “root” is the best to run with. in this case pulse has to be run systemwide, isn’t it?
@j1nx might have some idea


happy to help, but could anyone formulate the exact question to me? not sure, how/what/when…

i guess there are many questions. at least on my side 
first of which, under what user is mycroft running? this setup is a bit confusing. > `
ps aux | grep python

i just installed mycroft via default dietpi installation.
what’s wrong with my owncloud setup?
root@dietpi:~#  ps aux | grep python
root       387  0.2  0.5 428700 19868 ?        ssl  12:08   0:02 /usr/bin/python3 /usr/bin/fail2ban-server -xf start
mycroft    941 17.8  1.5 310084 62192 ?        sl   12:09   1:50 python3 -m mycroft.messagebus.service
mycroft    945 13.9  2.5 2441356 101296 ?      sl   12:09   1:26 python3 -m mycroft.skills
mycroft    949  1.5  1.7 1845656 68968 ?       sl   12:09   0:09 python3 -m mycroft.audio
mycroft    953  7.1  2.2 2169876 90572 ?       sl   12:09   0:44 python3 -m mycroft.client.speech
mycroft    957 16.9  1.5 511528 59864 ?        sl   12:09   1:44 python3 -m mycroft.client.enclosure
root      2431  0.0  0.0   4768   636 pts/0    s+   12:19   0:00 grep python
"
83,upgraded ubuntu from 18 04 to 20 04 no more tts,support,"
hello, everybody. i hope you are well. i’m working on this now for several days and am not finding a solution. the problem is essentially the same as this one, but i am not achieving the fix with the same results.
summary: after i upgraded from ubuntu 18.04 to 20.04. i can wake mycroft, and i even hear the wake chime. the speaker mutes. i look at the log file and can see all the time signatures that it’s trying to hear me. but there is no audible response.
i have tried these, without success…
30 sec + delay between wake word and recording
picroft + rpi4 + respeaker mic array v2.0 - audio issues
mycroft not responding to “pair my device”
mycroft does not work in ubuntu 20.04
mycroft not responding ubuntu 20.04
i have also tried some tts and alsa troubleshooting guides. most of them tell me to do things that i don’t really know what they mean. (i honestly just learned what alsa is.)
i’m more than a little beyond my comfort zone now of attempting fixes. if anybody has any ideas, i’m all ears.
thank you!
dan.
","
hey, just to check, did you install mycroft via the git method, the alpha-release snap or some other method?
from your message in the other thread, what happened when you tried to run mycroft using python3.8? can you check what version of python is being used in the virtual environment?
cd ~/mycroft-core
source .venv/bin/activate
python --version
deactivate

mycroft works on 20.04 but i haven’t done an upgrade from 18.04 so there seems to be some issues there.

@gez-mycroft thank you for the reply! 
originally, with ubuntu 18.04, i installed using git. this installation carried over into ubuntu 20.04. i first tried solving the issue with downgrading python from 3.8 to 3.7.5.
i also tried uninstalling and reinstalling using snap. that presented the same issue. i uninstalled again and reinstalled using git, which is the current installation. (i’m trying my best to maintain a single variable per trial, other than maintaining python 3.7.5 throughout.)
i checked the virtual environment python version, and it is 3.7.5. (that never crossed my mind to check, which seems so obvious now!)
i thought about wiping everything with a fresh ubuntu 20.04 installation. but i don’t like not knowing what’s causing this problem. plus, i suppose, there’s no guarantee that a fresh install will actually fix it.
dan.

have you tried the audiotest?  can you check your default source in pulse audio?

hi, @baconator. thanks for the reply.

have you tried the audiotest?

i’m not sure what the audiotest is. that sounds like something i should familiarize myself with. i did some searches that gave me results about different methods for testing audio. which one in particular should i do?
i tried this one that had a bunch of different alsa+pulseaudio hardware and software diagnostics. it gave me information. and i’m not sure what to do with it.

can you check your default source in pulse audio?

i ran $ pactl info and it returned default source: alsa_input.pci-0000_00_1f.3.analog-stereo. is this useful?

in both cases, they’re only useful if you can tell.
pactl list sources short
should list the inputs, one should hopefully seem familiar.
if that’s not the one you have for default source you’d need to change it.  also check out the audio troubleshooting guide for more.

thank you again. i used mycroft audio troubleshooting for these actions.
i ran audiotest and i heard my voice playback. so according to the text output, mycroft can hear what i’m saying and should be able to act on it.
i ran pactl list sources short and got:

1	alsa_output.pci-0000_00_1f.3.analog-stereo.monitor	module-alsa-card.c	s16le 2ch 44100hz	suspended
2	alsa_input.pci-0000_00_1f.3.analog-stereo	module-alsa-card.c	s16le 2ch 44100hz	suspended

i tried changing it using pactl set-default-source 1. this made mycroft not listen. so i changed it back to 2, and it’s listening again. still no tts, though. i tried to find a mimic troubleshooting guide. but i didn’t see one.
one thing i noticed is when i stop mycroft, i get some errors with stopping audio.

~/mycroft-core$ ./stop-mycroft.sh all
stopping all mycroft-core services
stopping skills (5430)…stopped.
stopping audio (5729)…failed to stop.
killing audio (5729)…killed.
stopping speech (5774)…stopped.
stopping enclosure (5793)…stopped.
stopping messagebus.service (5379)…stopped.

could this possibly be anything useful?
does anybody know of a mimic troubleshooting guide?

never wrote one. :o
nothing interesting in the audio.log?

this seems relevent from audio.log

2020-10-16 19:48:26.221 | info     |  4004 | mycroft.messagebus.load_config:load_message_bus_config:33 | loading message bus configs
2020-10-16 19:48:26.337 | error    |  4004 | mycroft.tts.tts:create:529 | the tts could not be loaded.
traceback (most recent call last):
file “/home/dan/mycroft-core/mycroft/tts/mimic_tts.py”, line 187, in validate_connection
subprocess.call([bin, ‘–version’])
file “/usr/local/lib/python3.7/subprocess.py”, line 339, in call
with popen(*popenargs, **kwargs) as p:
file “/usr/local/lib/python3.7/subprocess.py”, line 800, in init
restore_signals, start_new_session)
file “/usr/local/lib/python3.7/subprocess.py”, line 1465, in _execute_child
executable = os.fsencode(executable)
file “/usr/local/lib/python3.7/os.py”, line 810, in fsencode
filename = fspath(filename)  # does type-checking of filename.
typeerror: expected str, bytes or os.pathlike object, not nonetype
during handling of the above exception, another exception occurred:
traceback (most recent call last):
file “/home/dan/mycroft-core/mycroft/tts/tts.py”, line 519, in create
tts.validator.validate()
file “/home/dan/mycroft-core/mycroft/tts/tts.py”, line 435, in validate
self.validate_connection()
file “/home/dan/mycroft-core/mycroft/tts/mimic_tts.py”, line 189, in validate_connection
log.info(""failed to find mimic at: "" + bin)
typeerror: can only concatenate str (not “nonetype”) to str
2020-10-16 19:48:26.340 | error    |  4004 | main:on_error:34 | audio service failed to launch (typeerror(‘can only concatenate str (not “nonetype”) to str’)).

these are the tts settings in mycroft-core/mycroft/configuration/mycroft.conf

// text to speech parameters
// override: remote
“tts”: {
// engine.  options: “mimic”, “google”, “marytts”, “fatts”, “espeak”,
// “spdsay”, “responsive_voice”, “yandex”, “polly”
“pulse_duck”: false,
“module”: “mimic”,
“polly”: {
“voice”: “matthew”,
“region”: “us-east-1”,
“access_key_id”: “”,
“secret_access_key”: “”
},
“mimic”: {
“voice”: “ap”
},
“mimic2”: {
“lang”: “en-us”,
“url”: “https://mimic-api.mycroft.ai/synthesize?text=”,
“preloaded_cache”: “/opt/mycroft/preloaded_cache/mimic2”
},
“espeak”: {
“lang”: “english-us”,
“voice”: “m1”
}
},

there does not appear to be a /etc/mycroft/mycroft.conf
.mycroft/mycroft.conf has…

{
“max_allowed_core_version”: 20.8
}


it’s not finding mimic2 or 1, it seems.  surprised there’s not another error message about mimic2.

i get exactly the same error (and no voice) as @dan, but only when using the british male voice, while the other two choices from https://account.mycroft.ai/ work fine.
mimic is not build locally, but that is optional, as well for the default voice, isn’t it?

i suppose i should close this one out. however, the verdict is not at all satisfying.
i opted to do a fresh linux installation, as mycroft was not the only issue i was encountering with the upgrade. i was getting tired of spending so much time troubleshooting, and it seemed like everybody is doing well with new installs.
sorry for taking the easy way out! 
dan.

yes. mimic(1) is the default fallback.

however, this seems to fail:
2020-11-17 21:25:26.828 | debug    |  1902 | urllib3.connectionpool | starting new https connection (1): api.mycroft.ai:443
2020-11-17 21:25:27.715 | debug    |  1902 | urllib3.connectionpool | https://api.mycroft.ai:443 ""get /v1/device/<uuid>/subscription http/1.1"" 200 17
2020-11-17 21:25:27.723 | error    |  1902 | mycroft.tts.tts:create:529 | the tts could not be loaded.
traceback (most recent call last):
  file ""/mnt/dietpi_userdata/mycroft-core/mycroft/tts/mimic_tts.py"", line 187, in validate_connection
    subprocess.call([bin, '--version'])
  file ""/usr/lib/python3.7/subprocess.py"", line 323, in call
    with popen(*popenargs, **kwargs) as p:
  file ""/usr/lib/python3.7/subprocess.py"", line 775, in __init__
    restore_signals, start_new_session)
  file ""/usr/lib/python3.7/subprocess.py"", line 1436, in _execute_child
    executable = os.fsencode(executable)
  file ""/usr/lib/python3.7/os.py"", line 809, in fsencode
    filename = fspath(filename)  # does type-checking of `filename`.
typeerror: expected str, bytes or os.pathlike object, not nonetype

during handling of the above exception, another exception occurred:

traceback (most recent call last):
  file ""/mnt/dietpi_userdata/mycroft-core/mycroft/tts/tts.py"", line 519, in create
    tts.validator.validate()
  file ""/mnt/dietpi_userdata/mycroft-core/mycroft/tts/tts.py"", line 435, in validate
    self.validate_connection()
  file ""/mnt/dietpi_userdata/mycroft-core/mycroft/tts/mimic_tts.py"", line 189, in validate_connection
    log.info(""failed to find mimic at: "" + bin)
typeerror: can only concatenate str (not ""nonetype"") to str
2020-11-17 21:25:27.725 | error    |  1902 | mycroft.audio.speech:handle_speak:99 | error in mute_and_speak
traceback (most recent call last):
  file ""/mnt/dietpi_userdata/mycroft-core/mycroft/tts/mimic_tts.py"", line 187, in validate_connection
    subprocess.call([bin, '--version'])
  file ""/usr/lib/python3.7/subprocess.py"", line 323, in call
    with popen(*popenargs, **kwargs) as p:
  file ""/usr/lib/python3.7/subprocess.py"", line 775, in __init__
    restore_signals, start_new_session)
  file ""/usr/lib/python3.7/subprocess.py"", line 1436, in _execute_child
    executable = os.fsencode(executable)
  file ""/usr/lib/python3.7/os.py"", line 809, in fsencode
    filename = fspath(filename)  # does type-checking of `filename`.
typeerror: expected str, bytes or os.pathlike object, not nonetype

during handling of the above exception, another exception occurred:

traceback (most recent call last):
  file ""/mnt/dietpi_userdata/mycroft-core/mycroft/audio/speech.py"", line 95, in handle_speak
    mute_and_speak(chunk, ident, listen)
  file ""/mnt/dietpi_userdata/mycroft-core/mycroft/audio/speech.py"", line 123, in mute_and_speak
    tts = ttsfactory.create()
  file ""/mnt/dietpi_userdata/mycroft-core/mycroft/tts/tts.py"", line 519, in create
    tts.validator.validate()
  file ""/mnt/dietpi_userdata/mycroft-core/mycroft/tts/tts.py"", line 435, in validate
    self.validate_connection()
  file ""/mnt/dietpi_userdata/mycroft-core/mycroft/tts/mimic_tts.py"", line 189, in validate_connection
    log.info(""failed to find mimic at: "" + bin)
typeerror: can only concatenate str (not ""nonetype"") to str

this is btw not a single instance but i tested it on three different vms (different debian versions) and a notebook, so it must be an issue with our implementation, however i cannot find an issue (tanking into account that with all other voices it works fine) and the error messages doesn’t tell me much as i am no good python programmer .

fixed  in dev branch


github.com/mycroftai/mycroft-core








handle mimic missing properly


mycroftai:dev ← forslund:bugfix/mimic-missing-exceptions



        opened 07:37am - 10 oct 20 utc




          forslund
        



+58
-36










edit:
you are using mimic tts without mimic installed, so you if you choose british male then you must install mimic, mimic is optional, in the sense that you can choose other voices, but if you choose mimic without having mimic… thats a problem on your side 
i recommend you always install mimic, because if the other voice fails, like google recently did, then you get mimic as fallback

many thanks for implementing proper handling of this error and clarifying that mimic == british voice. i thought for each voice there is an “online” version. since this voice is configured by default, this information could be added to the installers output, however finally found it in the documentation tts section as well.
i built/installed it, needed to learn that it needs to be called from the mycroft-core directory (to have binary and libs being found by mycroft core services), and now the british voice works perfectly fine. many thanks!
"
84,help with padatious intent parser,adapt intent parser,"
im using padatious for a project, how can we register entity file in a basic python script?
im doing this to load intent file. is there any similar way for entity?
container.load_file(‘tasks’, ‘vocab/en-us/tasks.intent’)
",
85,bluetooth hsp hfp,mycroft project,"
i am having a bad one as it should be possible to use
    index: 1
    name: <bluez_card.9b_fd_d5_6e_01_ca>
    driver: <module-bluez5-device.c>
    owner module: 23
    properties:
            device.description = ""a10""
            device.string = ""9b:fd:d5:6e:01:ca""
            device.api = ""bluez""
            device.class = ""sound""
            device.bus = ""bluetooth""
            device.form_factor = ""headset""
            bluez.path = ""/org/bluez/hci0/dev_9b_fd_d5_6e_01_ca""
            bluez.class = ""0x240404""
            bluez.alias = ""a10""
            device.icon_name = ""audio-headset-bluetooth""
            device.intended_roles = ""phone""
    profiles:
            a2dp_sink: high fidelity playback (a2dp sink) (priority 40, available: unknown)
            headset_head_unit: headset head unit (hsp/hfp) (priority 30, available: no)
            off: off (priority 0, available: yes)
    active profile: <a2dp_sink>
    sinks:
            bluez_sink.9b_fd_d5_6e_01_ca.a2dp_sink/#1: a10
    sources:
            bluez_sink.9b_fd_d5_6e_01_ca.a2dp_sink.monitor/#1: monitor of a10
    ports:
            headset-output: headset (priority 0, latency offset 0 usec, available: unknown)
                    properties:

            headset-input: headset (priority 0, latency offset 0 usec, available: no)
                    properties:

i have read about that hsp/hfp is only supported by ofono and i am trying valiantly but failing to set it up.
anyone got any guides on setting up hsp/hfp on a pi with a bluetooth dongle as am aware of the bluetooth problems.
its sort of nuts as ubuntu is the same but other distro’s, android and win all work perefect just the debian bases seem a complete mare!
","
hey stuart, i saw in chat that you might have had some success with this.
if there’s anything we can add to our docs for the future, please let us know. 

still at it, as the sco routing in the hfp/hsp profile isn’t working with bluealsa.
both bluealsa and the pulseaudio-bluetooth-module support ofono which supposedly works with hfp/hsp but the qt dependencies just seems so much bloat.
i want to try and get bluealsa working and if you have an a2dp mic/speaker rather than hfp then it works.
prob is they tend to be mainly high end price and the idea is low price means you can scatter these around.
my growing collection of bluetooth dongles and speaker mics is awaiting a budget bt5 arival which i am hoping will be a2p mic/speaker and maybe even dual mode where you can hang 2 speaker/mic off it but that might just be a sink and not with the hfp/hsp subset.
if not its just a matter of a another dongle for each as they don’t tend to interfere.
i don’t have a a2dp speaker/mic or headset to try with mycroft yet, but if alsa it can be used as mycroft depending on setup needs both (alsa/pulseaudio) depending on modules.
so fired a load of logs off at the dev and awaiting deliveries, so will have to see.
https://github.com/arkq/bluez-alsa the main guy is pretty active and guess will just have to see but things are quite fresh.
bt support in linux because of bluez dropping hsp/hfp in v5, that bluez made it a somebody elses problem and pulseausio did the same.
its messy and extreme bloat hence why i have my fingers crossed for the bluealsa dev, which is a raspbian package even though old and hsp/hfp definately doesn’t work with whats shipped in buster.

@stuartiannaylor scrolling a bit through the forum, reading some posts that i might have missed the last few weeks. this is one of them 
i have this exact same task on my (long) todo list as well. have not started it yet, but thought i might give you my saved bookmarks/pointers about hsp/hfp
if i remember correctly the onboard bt chip of the rpi needs some help for the sco packets. anyhow, here are my pointers i found from hassos
https://raw.githubusercontent.com/rpi-distro/pi-bluetooth/cbdbcb66bcc5b9af05f1a9fffe2254c872bb0ace/lib/udev/rules.d/90-pi-bluetooth.rules
https://raw.githubusercontent.com/home-assistant/operating-system/dev/buildroot-external/package/bluetooth-bcm43xx/bthelper%40.service
https://raw.githubusercontent.com/rpi-distro/pi-bluetooth/cbdbcb66bcc5b9af05f1a9fffe2254c872bb0ace/usr/bin/bthelper
they might help you or at least give you some additional clue’s about where to look next / google for.

even if you fire off the pi bt and use a dongle its far from simple.
because bluez dropped hsp/hfp pulse have said you can do it through ofono.
which also needs ofono phonesim as really what it does is a create a virtual phone so you can connect to that and the virtual phone uses pulseaudio.
its such a horrid fudge in implemenetation that i am not going to bother.
bluealsa is still a work in progress.
it actually installs qt4 and even after that it didn’t work for me but started to wonder if ofono was set up for a system-wide pulseaudio session and root credentials whilst we are running user sessions.
its just far easier to find a2dp devices aka bt 5.
i am a big fan of the pi and did try the sco fixes and there are still open issues on github with it.
the combo unit on the pi is relatively sucksville as also can produce wifi co-existance problems.
but there is a 3rd problem with the pi 4 as i have got one of those lovely ‘armour’ heatsink cases and my temps are nothing.
but i am expecting in this heatsink  the range will be pants also if i could get it working.
my opinion is just turn off the bt and get a $5 bt5 dongle that will stick out of a usb port and also gain range.
there is a problem as it wasn’t just linux or the pi that had problems with hsp/hfp and a lot of manufacturers have created many proprietory solutions.
hence in 5.2 they have released le audio that part of it is to pull back and adopt a common standard.
if you are buying its much easier just to get bt5 and a2dp and scrap the idea of hsp/hfp which i think is going to eventually happen.
even then with bt5 its dependent on device.
i think some 4.0 untits will work but you need edr+ and don’t think the pi supports currently.

linux and sound architecture is already a nightbare. add bluetooth to that mix and you loose even the last couple of hairs… 
i haven’t really dived into the whole bt things as of yet, other then just enabling it to make  mycroft act as a bluetooth speaker. (send music to it from any other mobile device) works great, using a2dp so stereo if you would connect a stereo speaker system of course.
anyhow, that brings me to the point why i have hsp/hfp on my list to do it the otherway around. connecting a headset to mycroft to use the mic and speaker as input/output device. if i remeber correctly a2dp is stereo but because of that also uni-directional. this does not make it the best setup for mycroft as you can not barge into mycroft when it speaks. so no ducking support and making it kind of useless if you want to tell mycroft to stop your music playback.
i don’t use alsa and would like to stay away from it. pulseaudio although having it’s own challenges is the path i follow for all sound for my project.
first of all, never start a service as root. just run it under a/the dedicated pulse user and make sure the system rights are setup correctly (for bluetooth this also means you need to add that user to the “lp” group)
i will see if i can move the task up on the list, so we could work on this stuff together. 4 eyes might get us both faster at the point of success.
for now, here is my implementation of bluetooth within my project. not one-on-one to be used with rasbian, but might give you some pointers again.


github.com/j1nx/mycroftos








mycroftos: add bluetooth speaker functionality.



        committed 06:16pm - 11 jan 20 utc




          j1nx
        



+82
-2












github.com/j1nx/mycroftos








mycroftos: fix bt speaker service for the rpi onboard bt chip.



        committed 08:06pm - 21 jan 20 utc




          j1nx
        



+18
-0






rpi bt chip needs some extra work that is and can be solved with
an extra service running before bluetooth.service and btspeaker.service...






but again, this is still for the other way around.

i will leave it you you  tell me how you get on.
if you run into problems check the ofono service and ofono d-bus profile as yeah running pulseaudio as usual as user session, but its ofono is set up as root and i think it rejects d-bus calls.
dunno i bagged it off at that stage as was having a tantrum how convoluted hsp/hfp profiles where.

right, i already have so little hair left… 

your ok don’t worry as with me its my sanity i worry about 
good luck and hopefully you will crack it as satelite bluetooth speakers/mics is a great addition.
ps @j1nx my other thought was just to have a pi zero w and a dongle in operation with the broadcom combo and just have two a2dp streams and 2 dongles.

@j1nx peter another thing i just remembered as well as the sco fix you also need to do the mtu fix.
the broadcom use a mtu of 64 whilst in pulse is set to 48.
https://www.freedesktop.org/wiki/software/pulseaudio/notes/11.0/
improved bluetooth mtu configuration
the packet size (a.k.a. mtu, “maximum transmission unit”) that pulseaudio uses with the bluetooth hsp profile was previously always configured to be 48 bytes. that worked with most hardware, but some adapters require a different packet size. now pulseaudio asks the kernel what packet size should be used, which fixes the problem.

however, a new problem appeared: some adapters that used to work with 48 byte packet size don’t any more work with the size that the kernel tells pulseaudio to use. if you find that hsp audio stopped working when upgrading to pulseaudio 11.0, you can revert to the old behaviour by passing option “autodetect_mtu=no” to module-bluetooth-discover in /etc/pulse/default.pa

so i presume autodetect_mtu=yes as hopefully the kernel will report 64 on the pi as presume its a config option.
but profile switching will work just playback will freeze and pop due to the mtu missmatch.
so it would be module-bluetooth-discover autodetect_mtu=yes headset=ofono.
so if you navigate to https://www.freedesktop.org/wiki/software/pulseaudio/documentation/user/bluetooth/#index1h3
install ofono and ofono-phonesim and remember its ofono-phonesim on the cli and not phonesim.
just check
sudo useradd -g bluetooth pulse

the d-bus access policy also doesn’t allow pulseaudio to communicate with ofonod by default when running pulseaudio in the system mode. to grant the permission, add this to /etc/dbus-1/system.d/ofono.conf:
  <policy user=""pulse"">
    <allow send_destination=""org.ofono""/>
  </policy>

to set up phonesim, first create or edit the file phonesim.conf in /etc/ofono. it should contain the following lines:
if have forgot the dir for the raspbian ofono-phonesim install but you will find it and edit with.
[phonesim]
driver=phonesim
address=127.0.0.1
port=12345

for now just run from the cli but ofono-phone should be a service.
ofono-phonesim -p 12345 /usr/share/phonesim/default.xml&
with also being part of that service
dbus-send --print-reply --system --dest=org.ofono /phonesim org.ofono.modem.setproperty string:""powered"" variant:boolean:""true""


once the modem is set up properly, you can connect your headset and the “headset head unit (hsp/hfp)” profile should be available in pulseaudio.

but i find not.
as to alsa vs pulseaudio isn’t really an argument as they are both unescapable and any interoperable by many tools and pulseaudio is just a server and abstration layer of alsa and for a headless system could be considered bloat.
i don’t mind either and if bluealsa did work then i would use it because of an aversion to the massive bloatware ofono.

great info! bookmarked next to the other stuff i have saved for it.
i will give it a go in a few days. at the moment i am in progress of updating buildroot, kernel and drivers, so have to push it forward a bit till i have a fresh bootable os again, but after that will have a look.
keep you posted…

please do and thanks for the input, fingers crossed.
i almost wrote that out verbatum from memory and with my memory is extremely indicative of its been tried too many times.
it might just need another pair of eyes and could be something simple or dumb by me, it often happens.
there is also the aur package for archlinux that i cloned just for reference that gives you an alternative datum.
the new master of ofono and phonesim has been updated to qt5 but raspbian still has qt4 in the repo aswell.

i’ve tried to bring up bluealsa from the following tutorial https://peppe8o.com/fixed-connect-bluetooth-headphones-with-your-raspberry-pi/ and successfully bought the interface up with a jbl bluetooth speaker with inbuilt mic (tested by recording and playing sound with arecord and aplay). but mycroft is not able to recognize this as a source or a sink when i run “pactl list cards”. i would think that if this stream were to be connected as hsp/hfp source and sink in pulseaudio, things would work. any ideas on how to do that?
"
86,pinephone support,feature requests,"
it would be epic if mycroft could be like siri for the pinephone. i used mycroft on my computer i thought it was cool but i had no use because i could just search it. but if i could have it on the pinephone then i would rather use it than google.
","
also this is the first time i made a topic and i didn’t know what category this would go in so i put it in features

chance did a video

source code: https://github.com/jarbashivemind/hivemind-ptt
note: not a full mycroft core, it connects to an existing one
note2: can be changed, but by default uses googlestt (free  ) not mycroft’s servers

indeed! being as i got credit for the video, somebody should give jarbas credit for writing everything you see there. up to that point i’d just been troubleshooting =p
work on this has barely begun, but you can start helping us test once we’ve got a bit of middleware to help it execute actual code at the phone end.
in that video, i’m running manjaro.
only a handful of people are involved so far, but discussion has been taking place in the ~mobile channel on mycroft’s chat server.

we also got ~hivemind now, for hivemind specific stuff
"
87,hy mycroft take a photo,mycroft project,"
hey, i’m new here but i’m wondering how i could go about creating a mycroft skill to ask to take a photo using my jetson nano 2gb? something like:
user: “hey mycroft, take a photo”
mycroft: “photo captured”
or
user:“hey mycroft, take a timed photo”
mycroft: “how many seconds delay?”
user: “three”
mycroft: 3…2…1…""
i have lots of ideas for mycroft, so thanks for suggestions.
","
did you already read through section “skill development” in mycroft documentation?
"
88,picroft host name entry,none,"
i noticed that when picroft connects to my network, the device becomes available as picroft.network_name.host
does anyone know how this is done? i scanned all of the scripts and i do not see how this is happening.   and is this reliable?
thank you
","
probably avahi-daemon is running, broadcasting the hostname to the local network`?
when connecting via dhcp, some routers also automatically provide their hostnames to the network.
in case compare with cat /etc/hostname .
"
89,is picroft working with a raspberry pi 2,none,"
because only the raspberry pi 3 and 4 are listed on the download page. but in an older manual i read that the raspberry pi 2 also works.
","
|model|level of support
|pi 2|functions very slowly, limited wifi support
https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/get-mycroft/picroft#hardware-recommendations
"
90,how much ram does mycroft need,mycroft project,"
i’m working on a project where i need a very small sbc. i’m looking at the nanopi neo air due to it’s 40x40mm form factor, camera, speaker, microphone, and wifi capabilities. my main concern with this board being able to run mycroft (as compared to the recommended pi 3b+) is memory.
i’ve read the requirements for picroft and have even tested mycroft on dietpi on a pi zero (same amount of ram as an air and likely the os i’ll be running on the air). with ~480mb usable ram on the zero, mycroft + the os leave about 20-30mb of ram free. i assume this is space used to move pages to and from swap. as the picroft installation instructions suggest, it runs pretty rough. i’m not sure if this is solely due to the cpu being single core or if the swapping of memory is also a part of it.
does anyone have experience running picroft/mycroft-core on something with a cpu like a pi 3 but half the ram?
","
the rpi 3a+ has 512mb ram as well and is not recommended for use with mycroft.
recently there was an updated list of recommended hardware here in this forum or the mycroft docs, but i cant find it right now…

the picroft page lists the pi a+ as not supported but doesn’t seem to mention the 3a+. it mentions the pi 2 as semi-workable which has 1gb ram and a 900mhz quad core cpu.

you might be lucky, let us know how it goes if you try it, im curious about nanopi (never played with one)




picroft is working on raspberry pi 3 a+ picroft


    so i read this page and saw that the raspberry pi compatibility chart looks as follows: 


pi3 b+ : supported <– you should get this device if you want to work on picroft
pi3 b : supported <– you should get this device if you want to work on picroft
pi2 : functions but very slow, limited wifi support. not recommended.
pi b/a+/zero/zero w/zero wh : not supported


today i got picroft working on my raspi 3 a+. all i had to do is put the newly written image raspbian-stretch_picroft_2018-09-12.zip i…
  


thanks @jarbasai for linking that other post. that clears up the dev’s standpoint on the pi 3a+ as working but not supported. i will assume the air will fall into the same category and report back my results with it.

yes, i can confirm that the nano pi and pi 3a+ are not supported.
you may get them to work, but we cannot offer support or investigate issues with them.
mycroft already supports several platforms - raspberry pi, linux, mark 1, so i hope you can understand that we can’t support every sbc 




 aiosdev05:

i will assume the air will fall into the same category and report back my results with it.


did you try this out? if so what were the results?

i have a testing instance with a relatively clean build, duckduckgo and “finished booting skill” apart from defaults, probably 1-2 more from tests a year ago, and the 5 mycroft processes (enclosure, speech, audio, skills, messagebus) take ~400 mib on idle.
now you need ~50 mib for basic system services + cron jobs and you’re on the physical memory limit of a 512 mib ram sbc. of course you can create a large swap file on a fast external drive, e.g. an ssd, but on most sbcs that means limited at least by usb speed, and you don’t want your system reading/writing forcefully to a swap space for every little task, in case of mycroft of course every skill use means an additional process (or several) with additional ram demand.
so without testing it, i think it’s clear that 512 mib will be not of pleasure .
"
91,no speech output,support,"
i’m brand new to mycroft and installed it using the github instructions on a ubuntu guest on an os x host using virtualbox. when using it with cli i can see that mycroft hears my speech and responds appropriately with text output, but there is no speech output. i know the speakers are working (i can hear the ding of mycroft responding to the wake word). tails for the voice and audio logs are at https://pastebin.com/z2sldb2x. any assistance would be greatly appreciated. thanks!
","
@raj1 do you have mimic built? to check go to the mycroft-core/mimic directory. there you can do ./mimic -t ""hello world"". you should then hear mycroft say “hello world”. if not then you most likely do not have mimic built. you can run ./dev-setup in the mycroft-core root directory again to build it.
hope that helps
-michael

that solved it, thank you!

i have similar problem (docker on linux). voice is recognized, ding is heard, yet i see answers only in “start-mycroft.sh debug” window, no speech output.
mimic works properly. this happened after i tried to switch to google synthesizer and then back.
“stt”: { “module”: “mycroft” },
“tts”: { “module”: “mimic”,
“mimic”: {
“voice”: “ap”
},
“mimic2”: {
“lang”: “en-us”,
“url”: “https://mimic-api.mycroft.ai/synthesize?text=”
},
“espeak”: {
“lang”: “english-us”,
“voice”: “m1”
}
},
i suspect remote is set wrongly and overrides correct settings in the conf file. how can i see value of remote config?

my mistake, sorry - mycroft home still had google tts specified. changed it to mimic, had to restart mycroft and now it speaks!

i seem to have the same problem – i hear the ding, i get cli responses, but no voice. i tried both changing tts in the config file to mimic2 and compiling mimic1 locally, but none of that helped :-(. i can’t see any log messages on the console regarding voice output. any other ideas?

hi there, i wanted to check, after compiling mimic 1 locally, have you set the voice to british male in your device settings?
it should automatically fall back if mimic2 fails, however explicitly telling mycroft to use british male should stop it from attempting to use other services first.
any tts logs should be written to: /var/log/mycroft/audio.log
so worth checking that to see if there are any clues as to what’s going wrong.

thanks! yes, i’ve set it to british male.
the audio.log  file was strangely only used the 1st time i ran mycroft (and contains an error message about failing to find mimic), but on the next runs, no other messages appeared there. all the other log files show a lot of newer messages, but not this one. the permissions are set exactly the same for all of them :-/.

sounds like the audio service is failing to launch at all.
can i confirm how you installed mycroft?
is this a linux version using git clone, a picroft image or something else?

sorry, i had some pressing issues and got distracted from trying it out more. i’ve only got back to it now. it must be an issue with pulseaudio and my soundcard. with a system & mycroft update + this fix, it seems to work (at least basic tts responses, i haven’t tested spotify or anything more complex yet).
my configuration is rpi4 + respeaker 2 mics pi hat + raspbian + mycroft installed from git.

hey guys, i got the same problem and tried everything i understood from this forum…
audio is correct, mic is working mycroft is working but the voice is not and mimic is installed.
it worked for like 3min. i use a dietpi.

did you try to switch the voice via https://home.mycroft.ai/?
in my base british male fails with an error stack in /var/log/mycroft/audio.log each time it tries to speak, but american mail and google voice work fine.
"
92,mycroft on dietpi with other software,support,"
currently, i’m running a raspi 4 with dietpi for owncloud and pi-hole. will i be able to run mycroft on it without any problems, while keeping the other two services active? my ram usage is at about 20%. i’d install it via dietpi.
my plan is to buy a usb-microphone (probably the ps3 eye) and speakers via the headphone jack.
will i be able to use mycroft in two languages?
","
not gonna speak to pi, because i don’t speak pi =p but i’ll drive by your question about languages: short-term, not at the same time. long-term, absolutely.
mycroft is only officially supported (by the company) in english, because, so far, only english is fully supported in code. however, the code that makes it multilingual is there, and you can absolutely use it in a number of non-english languages, with varying degrees of success.
in order to use mycroft in a particular language, you need a few things, only some of which come from mycroft or this community:

speech-to-text (not made here, but exists in plenty of languages)
text-to-speech (same deal)
lingua franca (parser/formatter library, made here) needs to support the language in question, contributions always appreciated
mycroft skills need the correct vocabulary to work in your language (contribute your socks off)

how to make mycroft truly multilingual, as in, speaks more than one language at the same time, is probably one of the next big challenges for the whole community, because all the moving parts have to fit together.

would guess the dietpi (even though its base is debian) portion of the setup is the most problematic, not knowing your bandwith of services/users served by both owncloud and pi hole.* this “optimization” comes at a price. yet, this is purely speculation.
*you may discover lower responiveness when those peak.

owncloud and pihole are only for a few devices (though owncloud has many files, and some huge ones).
dietpi installation should work, at least it’s supported.
if only peak times are problems, i’ll try it. just ordered a microphone!

on an raspberry pi 4, a home owncloud/nextcloud together with mycroft should be no problem. the chance is rare that high webserver/php usage peaks and a certain mycroft task are done concurrently, and even if, the delay should be minimal, priorities could be set via dietpi-config (i.e. service nice levels).
pi-hole (the dns requests) alone uses close to zero resources, when used as home or small office instance, it runs with hundreds of users on an rpi zero , so that can be ignored.
however, currently our implementation seems to not run ootb on most devices, but we discuss this in the separate topic you opened.
"
93,easy way to use respeaker 2 mic pi hat with mycroft,none,"
i am not an expert with linux, i once installed ubuntu and used the terminal a few times.
is there an easy way to use respeaker 2-mic pi hat?
",
94,short sound to indicate error,feature requests,"
as a skill developer if there is some error currently you generally speak some dialog, eg:

“something went wrong”
“i didn’t understand which movie you meant”
“i can’t connect to x please check your settings”
etc

sometimes though, you don’t need a long response, you just want a way to inform the user that it didn’t succeed. well, there’s an interesting pr to add an error sound. currently it’s only about an error sound, however this could very easily extend to other common sounds eg success, email sent, turned on, etc.
so i was wondering how skill authors would prefer to use this. the existing suggestions are:




speak_dialog(dialog, data, is_error)
report_error(dialog, data)
report_outcome(type=""error"", dialog, data)




0
voters




if you have deeper thoughts, please comment here or join the discussion on this pr.
","
more flexibility is always good in such case, to cover warnings or other types of meta information in a generic way. this probably requires some sort of additional logging backend, so that log messages are optionally not only printed to file but additionally spoken, with a sort of identifying sound or prefix for each log level type.
related additional config options could be:


acoustic_log_level=none (default) critical error warning …

acoustic_log_dialog=true|false
when false, play notification sound only which identifies the log level, but do not speak log text.

for logs below warning this probably doesn’t make sense as mycroft-wide logs appear faster then it is able to speak them. but having a generic backend on regular/existing logging basis is probably better after all then adding a new independent type/set of functions?
ah, i think when it’s disabled by default, it somehow breaks the idea to allow forcing the acoustic output by app developers? 
although something informative like “i didn’t understand which movie you meant” is probably even better to have as gracefully “handled” regular answer, while errors are best to reserve for unforeseen issues, especially internal code errors, due to kernel/hardware-level, access issues and what not, that is not only due to an invalid question or non-accessible remote resource.
another issue: those python error traces can be very long and impossible to reasonably “speak”, so that would need to be excluded anyway… okay not as simple on the second thought.
"
95,user registration requirement,general discussion,"
here is a dumb question.
why is device registration required through an online form?
i know you get to name the device and add or remove skills and change the voice.  but if we can do this through mycroft, can we make this step optional?
if i want to make 20 devices, i would want to use the same settings for each.
is there a way to bypass the registration requirement?
","
probably this helps: https://github.com/mycroftai/mycroft-core#using-mycroft-without-home
this requires manual speech-to-text setup then: https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/customizations/stt-engine
"
96,anyone using respeaker 6 mic hat in picroft,mycroft project,"
i’m mainly wondering if the instructions on their site (https://wiki.seeedstudio.com/respeaker_6-mic_circular_array_kit_for_raspberry_pi/#getting-started) are the ones to follow, and how to use librespeaker in conjunction with mycroft (if needed). thanks!
","
well, i may be returning this product depending on how this nets out. the driver repo (https://github.com/respeaker/seeed-voicecard) does not compile with the latest kernel. and the “solution” to the issue they propose is to use a fork that downgrades the kernel to one that is nearly a year old. i’m not sure this is a product i’d like to continue using considering the premium paid for a product that should be plug and play, but i’m waiting for an official response from support.

haven’t tried that one sorry, sounds like a rough suggested work around too… hope they come back with something more useful

hey @fmstrat,
i just setup mycroft on my pi4 with the respeaker 6 mic array and it is working. it took me a few hours of debugging the audio input and output setup, but it seems to working well now!
i found this repo rather helpful: https://github.com/adelhult/respeaker-with-picroft
my initial attempt to setup the respeaker failed because of some kernel incompatibility errors, but i rebooted and ran sudo ./install.sh 6mic and that worked while the first time i did not include the 6mic at the end as it does not say this in the seeed documentation.
i did not mess with the ~/.asound.rc as reccomended in this repo though. once i setup the hat, i just went into the mycroft audio setup and started debugging issues based on their audo troubleshooting section
i am planning to hook into some the mycroft message bus to trigger the light array that comes with the respeaker6mic. i hope you found some success with your 6mic, and i am happy to share any more info that might help!
"
97,controll phillips hue lights,mycroft project,"
hi everybody
im a 15 year old student, who wanted to make a picroft device as a school project. the installation of picroft worked totally fine on my raspberry pi 3 model b and picroft responds to me. my goal was to connect my phillips hue light bulbs with the picroft, so that i can turn the lights off just with my voice. so i installed the wink iot skill, downloaded the wink app for my smartphone and connected it with my phillips hue account. then i went to the mycroft skill settings, and added my wink account to the wink skill. from the wink app i cant turn my lights on and off with no problem, but when i ask picroft to do so i dont get any answers from it.
do you actually need a wink hub to make this work, or what am i doing wrong?
any help is appreciated!!!
","
hi @legolas_21, welcome!
you might want to look into the philips hue skill: https://github.com/christopherrogers1991/mycroft-hue
i’m not sure what the current status of it is/if it will work, but try installing it using the command  msm install https://github.com/christopherrogers1991/mycroft-hue

hey @legolas_21 curious if you ended up getting  the wink iot skill to work in the end? or if you successfully used the hue skill form christopher rogers?
i am setting up mycroft on my pi 4 now, and i will test the hue skill mentioned above and update here for anyone curious.

hi @kennette21
i wasn’t able to get the wink iot solution working, so i switched to an other project called jarvis, where i wrote two little scrips on my raspi to turn my lamp on and off. by default jarvis is very stupid and you have to train it with your own voice and youre own commands etc.
greetings legolas21
jarvis: https://github.com/alexylem/jarvis

hi @legolas_21, thanks for sharing your project, it looks really cool. feel like i see a new open source voice assistant solution every day! it is so encouraging to see so many clever hackers out there 
i also tried to build my own jarvis using pythons speechrecognition and building my own language parser… i soon realized there were many hurdles to overcome and thats when i moved to mycroft.
i got everything setup on my pi4 last night, and the https://github.com/christopherrogers1991/mycroft-hue skill worked for me! some functionality does not work perfectly, so i might be committing some improvements there later on.
thanks for the info! i am also curious if you are still using your voice assistant regularly?
"
98,different skill settings across devices,feature requests,"
hey all, following some discussions on github, i wanted to get broader input on how skill settings across multiple devices might work in the future.
skill settings are currently universal across all devices. so you’ve likely seen that in your skill settings all your registered devices pull the same settings down. this is beneficial for many use cases because you set your preferences once and the same thing is synced across all devices. eg if i have a premium music service i most likely want that available on all my devices without having to authorize each device one by one.
there is a small caveat to this. to be more accurate, all skills with the same settings definition are synced together. if you modify a skills settings definition you’ll find that it shows up as a separate tab under the settings block, but there is no clean way to do this at the moment. it looks like so:

screenshot from 2020-10-26 10.14.07718×307 10.3 kb

there are a number of cases where it may be beneficial or even necessary for skills to have different settings on different devices. there are two main cases i can see:


as a skill developer, i want each installed instance of my skill to have unique settings
eg an inter-device voip skill may require unique credentials for each device. the developer of such a skill should be able to set a flag to enforce that each device gets its own tab under that settings block


as a user, i want to provide different settings for specific skills on a specific device
eg by default use these netflix credentials, but in the kids room use these instead


do they seem to cover all the main use cases?
particularly as an end-user, would you prefer to have:

all your settings to be synced universally
a default settings with the ability to modify the settings for individual devices
settings to be grouped based on their “placement” (eg kitchen, bedreem etc - this is an existing optional field that is in your device settings)
all devices to have their own settings
something else??

be great to hear your thoughts on this, or if you have completely different ideas on how this could work.
","
having the ability for different skill settings across devices would indeed be a big plus. the kids might want to wake up to a different alarm tune than us, and an alarm in the kitchen does not need a fade in for example.
default settings with the ability to modify the settings for individual devices seems the best of both worlds to me.

i think i’d like a menu that lets me select which settings to sync to a particular device (c.f. your browser’s sync thing, or a cloud service’s integration with your os, where you have tickboxes for the various things it can sync.)
this could be cumbersome, with a long enough list of installed/configured skills, but if it lived in its own ui tab, a checklist is a checklist.
bonus if those settings could be saved as part of some kind of device profile, so that, if i set up a new mycroft device in my dining room, i can tell it to configure itself exactly like my last three “on a table in a room” devices.

i am not entirely sure that i understand the settings mechanism and its scope right.

in principle, each user should be able to change his/her own settings to personalize using mycroft. changes can be applied per device, for device groups, or all devices.
an “administrator” (parents, or in b2b also service providers) may need to change settings in the same way for the users. these changes may require overriding the settings of a user. it also requires a role-based access to settings.
a developer may need to change settings in the same way for the users and admins (e.g. if an endpoint changes?). these changes may require overriding the settings of a user or admin. this case may be solved by delivering a full skill update.

in my planned use cases, i have one mycroft that many users can interact with via the andoid app. those users would like to modify aspects of the skill. on the other hand, i (administrator) need to configure the skill for my users or user groups because they can’t do some things themselves. and as a developer i may need to continuously adjust some settings (in the background) to improve the user experience.
something that would be beneficial in this context is a change history for settings. that increases transparency if multiple people change settings.

nice ideas @chancencounter - i think we’ll need to do some ux design around this and test some options.
@scienceguy - some really interesting use cases there that we should definitely be considering.
a note on the developer dotpoint. currently any skill that updates their settings definition will update the settings for all users of that skill. however this actually causes some issues as we currently treat the whole settings as one block so even if you only add a new setting, the rest of the users settings will be blank with the update. this really needs to change so that we are updating individual settings, or pulling compatible settings forward with any changes.
i’ve also been chatting with jarbas and wanted to document a further use case. there may be some skills that have global settings and per device settings. eg a voip skill may be better having a global ring tone, but per device credentials. alternatively this could be achieved by being able to change a single setting value (ring tone) and having the option to apply this to all devices.
in this case, do we give the skill developer control of how that setting is used, or the user?
"
99,use custom speech to text,general discussion,"
we do have an own speech to text solution whcih we do want to integrate. is there any documentation on how to integrate such a custom module into mycroft?
","
https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/customizations/stt-engine
"
100,mimics audio files location,mimic,"
hi,
i would like to keep a record of mycroft’s tts answers and i’m wondering if mycroft has some tmp folder where it stores the tts output, or is it synthesized in real time?
is there a command that would allow me to save tts output as audio in a specific directory every time mycroft utters?
thanks.
","
look in /tmp/mycroft, there’s a cache of recent tts items usually.

if you want to automate that maybe the package “inotify” with the command inotifywait  is something for you

it will be part of a bigger python script so i’ve already implemented watchdog. didn’t know of inotify though, thanks!

ok i ran into a follow-up problem. is it somehow possible to know which file mycroft is “reading” from? since the cache is not modified unless mycroft says something new, i would need to somehow know when it’s reading from an already cached file, and which one.
thanks a ton!

check the audio.log.

simply delete the cache and monitor it from there.

that’s what i’d normally do but there are a few more things involved. basically if i clear the cache sometimes the file has not yet been copied, and sometimes there’s still a tts lined up to be spoken, so i want to know if the file has been both, backed up and spoken out before cleaning it.

audio log seems to show the text which is being spoken but not which file it is reading from.

if there’s a cache hit it gets logged in audio.log file.  i thought it logged the file names, but it’s an ident string instead.

i sorted it with a fresh head this morning haha  i just clean the oldest files in the cache and then ones created within a close range (supposedly of the same utterance) . this is definitely not the most accurate way but if i come up with something better i will post it here.

the file names of the audio cache files are hash-values of the actual phrase. look into mycroft-core code to see how it is done, from there you should be able to identify the files.
"
101,mycroft not say all the sentence,none,"
hi guys.
i’ve seen that my mycroft not say the first seconds of the sentence.
for example, if i ask information about a movie, it skip the first words and continue, but in the debug console, i can see that the answer is correct.
and if i ask “how are you”, i can see the answer in the console “i’m fine thanks” but i can’t hear anything…i think that it depends on speech delay setting.
do you have this problem?
","
i am experiencing the opposite, that last few words are not spoken.

i think that the problem is related to hdmi audio output ( i’m connetced to the tv )…do you use the jack with normal speakers??

has anyone else encountered this problem?
i’ve not solved yet 

not for several years.
check in /tmp/mycroft/ there’s a cache directory for tts output, try playing those wav files via command line and see if they output correctly.
"
102,testing and feedback dust sci fi short movies,skill feedback,"
dust skill

1152×213

about
dust short science fiction movies for mycroft

1071×885


1076×885

installation notes
this requires latest cps refactor, not yet merged in core

skill-playback-control@cps_refactor_testing
cps_extend_timeout
exact voc_match
cpsmatchtype
self.gui.play_video

github



github



jarbasskills/skill-dust
dust short science fiction movies for mycroft. contribute to jarbasskills/skill-dust development by creating an account on github.





examples

“open dust”
“play sci fi short movie”
“play dust”

credits

jarbasal

aiix  - gui
dust

category
entertainment
tags
#scifi
#shortfilms
#video
#entertainment
",
103,testing and feedback cult cinema classics,skill feedback,"
cult cinema classics skill

1019×188

about
cult movie classics for mycroft

1199×787


1210×786


1208×777

installation notes
this requires latest cps refactor, not yet merged in core

skill-playback-control@cps_refactor_testing
cps_extend_timeout
exact voc_match
cpsmatchtype
self.gui.play_video

examples

“open cult movie classics menu”
“play zontar the thing from venus movie”
“play cult movie classics”
“play invasion of the saucer men”

github



github



jarbasskills/skill-ccc
cult cinema classics, public domain movies for mycroft - jarbasskills/skill-ccc





credits

jarbasal

aiix  - gui
cult movie classics

category
entertainment
tags
#movies
#publicdomain
#classicmovies
#video
#entertainment
",
104,testing and feedback kings of horror,skill feedback,"
kings of horror skill

893×213

about
indie horror movies for mycroft

1204×787

installation notes
this requires latest cps refactor, not yet merged in core

skill-playback-control@cps_refactor_testing
cps_extend_timeout
exact voc_match
cpsmatchtype
self.gui.play_video

github



github



jarbasskills/skill-kings-of-horror
indie horror movies for mycroft. contribute to jarbasskills/skill-kings-of-horror development by creating an account on github.





examples

“open horror movies menu”
“play ninja zombies movie”
“play horror movie”

credits

jarbasal

aiix  - gui
kings of horror

category
entertainment
tags
#movies
#horror
#indie
#video
#entertainment
",
105,skill ideas based on amazon alexa newsletter,skill suggestions,"
hey guys.
don’t be mad on me, but i receive an alexa (sadly, this device marks current state of the art) newsletter on a regulary basis where amazon promotes new (or existing) features of their device.
i think on pasting content of these newsletters (translated to english) with new features to get something like a collection of skill ideas for mycroft features which could be implemented.
could be like this:

“wakeword, who sings this song?”
“wakeword, give me a kanye west quote”
“wakeword, are you athletic?”
“wakeword, set a sleep timer for 30 minutes”
“wakeword, give me bad poetry!”
""wakeword, read, blackout‘ by marc elsberg ""
“wakeword, set the bass to three”
„wakeword, set the middle to minus 4""
„wakeword, less treble"" etc.
“wakeword, start nature sounds”
“wakeword, give me the weather report for the next 7 days”
“wakeword, what’s my daily summary?”
“wakeword, make a call”

is this legally okay to copy’n paste these newsletter content here and if so, do you think it’s useful?
",
106,will the matrix voice microphone array be supported,mycroft project,"
matrix voice microphone range is an excellent piece of hardware. will the mycroft create an install script as they’ve done with other hardware?
","
even if not, might be worth trying to get running so you can see what issues might be run into.  who knows, it might even just work out of the box.

i can confirm it doesn’t work right out of the box.

you’re welcome to have a look at the existing picroft script and see how the install scripts are done for existing hardware, and have a go at creating one for matrix voice.



github



mycroftai/enclosure-picroft
mycroft interface for raspberry pi environment. contribute to mycroftai/enclosure-picroft development by creating an account on github.







the fine folks at the matrix community put this guide together now that mycroft is on raspbian stretch  https://community.matrix.one/t/matrix-voice-running-mycroft-ai/2033 or https://www.hackster.io/matrix-labs/matrix-devices-running-mycroft-ai-ee9d4a

we also have a pr pending to automatically support this on picroft https://github.com/mycroftai/enclosure-picroft/pull/84

thank you! i’m am very excited.

this pr is now live!! let us know how you go 

i want to give it a try. after burning current image from https://mycroft.ai/to/picroft-image and burning to sd card the initial setup is starting.
the choice for matrix voice is missing. as seen in the pr there should option number 4 for matrix voice

echo ""  4) matrix voice hat."")

but this option is missing.
the pr seems to be in the “stretch” branch, but not in the “master” branch. how can i be sure to use the right version?
do i have to install the matrix voice kernel modules manually after the wizard or will this be done by mycroft setup (picroft / mycroft.ai with matrix voice).
thorsten

i noticed this and i’m curious to hear as well. given that it’s supposed to reboot three times during the wizard one might assume that it is installing kernel modules but i’m not sure.


github.com


mycroftai/enclosure-picroft/blob/bb67fb66d545ca638ffcfa84a891ed7db867ef44/home/pi/auto_run.sh#l317

   ;;
3)
   echo ""$key - google aiy voice hat""
   break
   ;;
4)
   echo ""$key - matrix voice hat""
   echo ""the setup script for matrix voice hat will run at the end of""
   echo ""the setup wizard. press any key to continue...""
   read -n1 -s anykey
   touch setup_matrix  # setting flag to run setup_matrix_voice.sh
   skip_mic_test=true
   skip_last_prompt=true
   break
   ;;
5)
   echo ""$key - other""
   echo ""other microphone _might_ work, but there are no guarantees.""
   echo ""we'll run the tests, but you are on your own.  if you have""
   echo ""issues, the most likely cause is an incompatible microphone.""
   echo ""the ps eye is cheap -- save yourself hassle and just buy one!""






although i’m a bit curious where setup_matrix_voice.sh is downloaded from as i don’t see it in repository.




 botacious:

although i’m a bit curious where setup_matrix_voice.sh is downloaded from as i don’t see it in repository.


as far as i can tell that is correct - there isnt ant setup_matrix_voice.sh script, and it isnt beeing downloaded. but if looking for setup_matrix the installation starts arounf line 590 if the file setup_matrix exists.
then the repos are added and packages installed and kernelmodules and pulseaudio.




 andlo:

but if looking for setup_matrix the installation starts arounf line 590 if the file setup_matrix exists.


that’s a good hint  .
between lines 590 and 665 all required steps for a matrix voice installation seems to be done.
but i currently dont know how to choose option 4 for matrix voice during initial setup since this option is not provided. maybe i should create the file “setup_matrix” during the initial configuration process manually and then skip the mic configuration.

could you clarify?
thank you

happy new year 2019.
while waiting for @kathyreid to point us to the right direction i tried the following:

writing official raspberry pi mycroft image to sd card
canceling mycroft initial setup
creating empty file /home/pi/setup_matrix
replaced auto_run.sh with the one in “stretch” branch
reboot

the initial setup starts installing the matrix modules, makes some reboots and everything looks good until it comes to the mic test when “make some noise!” is prinited to the console. in this step the hole system stops working. no keyboard reaction, etc. powering off the raspberry pi is the only option.
this is really depressing.

yes - that sounds depressing.
i cant advice or help as i dont have a matrix.
what you did do sounds like a good idea, and replacing auto_run.sh was smart. as i understands auto_run.sh should download latest auto_run.sh.
splacing the setup_matrix file do make the auto_run.sh run the installation f the matrix stuf, so what you did were a good idea aswell.
what you could try further is rebooting and on when auto_run.sh is tsrated choose skip, as you have (maybe) alreddy installed what is nessesary to use the matrix. just a ques.
it isnt fun having a mycroft device that dsnt speak or listen 

unfortunately i don’t have a matrix microphone here to test with either, so this is more of an educated guess;


the picroft stretch image is a base image. there have been hundreds of commits over and above this base image. these commits (to mycroft-core and enclosure-picroft) come down when the device updates - there should be an update.sh bash script in the picroft root directory - if you run this script does it do an update?


if you run the command git log can you tell me which commit your picroft is on? that way i can check for sure that the matrix commit link to commit is pulled down.


is there anything in the voice.log or audio.log which provides a clue about what’s happening?
link to information on logs



hi.
after rebooting the raspberry pi yesterday it seems to work for the last day. that’s better than anything i archied in the past  .



 kathyreid:

there should be an update.sh bash script in the picroft root directory - if you run this script does it do an update?


–> would you like to install picroft on this machine
for testing i choose “yes”. it’s doing a lot of steps and finishes with ""mycroft-wipe saved [3000/3000]



 kathyreid:

if you run the command git log


i executed “git log” within “/home/pi/mycroft-core” which returns 8fe127bd5d583bb1294ade245929a61b3560c6fe from @steve.penrod on dec 31 07:05:09 2018.

whenever i reboot the raspberry sometimes the matrix voice works perfect and sometimes i get a system freeze while accessing the microphone.
because i was confused by this behavior i checked in detail that my hardware setup fits the minimum requirements and it might be that …
… i am an idiot 
matrix says the raspberry pi has to use a power supply with 2.5a and i currently use a 2.0a power supply.
https://matrix-io.github.io/matrix-documentation/matrix-voice/device-setup/
this might be an explanation of the system crashes on accessing the mic  .
i now order a 3a power supply and hope that the problems with the stability are gone.
maybe that is not solving my issue but i think it was my fault - sorry for that!

ah! i should have checked this with you too!

@thorsten  so what what your step by step your process to get this working?
"
107,update skill not existing in marketplace,mycroft project,"
how to keep updated my custom skill? can i keep automatically updated? or i have to update manually? how?
",
108,unsatisfying audio troubles,general discussion,"
hi there,
i’ve had a lot of different setups for raspberry pi, bananapi, genuine linux mint distributions and there is no setup, that works properly concerning the audio.
when using netbook with internal microphone and speakers, recording works fine, but replay sounds like mickey mouse on speed. the bits seem not to match from record to playback.
using raspberry is worse, because audiotest works fine with me, making some noise is correctly replayed, but when starting mycroft cli, there is no audio recording / recognition, even the level meter shows nothing. putting in commands by keyboard, mycroft will work and tells me the wheather or plays the news.
on bananapi no audio works, doesn’t matter, if it’s pulseaudio or alsa.
there’s a lot of trouble setting alsa or pulseaudio to new defaults. sometimes raspberry requires sudo, but when installing new defaults as sudo, these defaults won’t be recognized, when starting mycroft cli the normal way.
starting mycroft als sudo, lots of text is produced on the screen, at least it isn’t working.
all this stuff makes me sad, because i hoped to get an independant voice assistant, but seems to me, that i don’t have any luck with this.
my proposal is, to put more efforts in reliable audio support, adjustable audio setting in the cli and very important an easy way to adapt to other languages. all trials to put a german version on my raspberry ended in a bunch of chaos.
please please please do me the favor and put some work on these problems. the forum tells lots of stuff to similar problems with audio, working not properly.
it could be a great milestone to get more users by having easier setups with less problems.
thank you.
cheers
caruso
","
hi @caruso
yes audio is frustrating and sometimes requere big effort to get working. the different combination of hardware and software is challenging.
that said, the picroft image do work as long as one is using the hardware that the setup is made for. but if one has other hardware the “fun” begins.
i dont expect mycroft (the company) can have support and settings for to many different hardware. but the hardware in the mark i and ii one can expect will work out of the box. but other hardware support has to be community driven. and for that the community needs people like you having hardware that dosnt work out of the box with mycroft.core. and then when you or someone else figure out how to get it working, and which settings are requered then add info in the docs and if on raspberrypi enhance the setup script for picroft. dooing pr’s to the respective repos so it can be included into the releases for others to enjoy.
editing mycroft configs can be done by mycroft-confir edit (user|system) which seems to me a easy way to edit the configs. the docs has some documentation on audio and audiotroubleshooting.
https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/troubleshooting/audio-troubleshooting
and then if this isnt enough ask in forum and chat to get more help.
this hardware is repported working with picroft and if yu get some other hardware to work it would be great that that is added to the list.



mycroft-ai.gitbook.io



picroft
picroft is a ready-made way to run mycroft on a raspberry pi 3 or raspberry pi 3b+ and is provided as a disk image that you can burn to a micro sd card.





you can add to the docks by making a pr to the docs repo or open an isssue or tell @gez-mycroft in the chat or a post here on forum.
an easier way to change language is a good idea. i were thinking of making a skill that change the nessesary settings so shifting between languages could be done by voice. i hassnt yet come further thn thinking of such a skill.
i really hope you figure out how to get audio working with your hardware as having a independant voice assistant that is open and hackable in any way one could think of is really fun.

i also had audio problems especially with an usb soundcard. at least i solved the problems by deactivating all hardware resp. drivers which are unused. have a look at this theme:
https://community.mycroft.ai/t/newbie-problems-no-audio-output-on-usb-card-no-change-to-german-language/8203/9

hi there,
i’ve found a useful offer for a seeed-2mic-array at ebay’s, so i purchsed it. with installing mycroft / picroft, stopping the setup-wizard and following the short installation guide on github it works perfectly without further configuration.
only problem is, that some skills won’t work, so the date-time-skill is installed, but when trying to ask for time oder date, mycroft tells me, that “he didn’t understand, but is learning new things every day…”
uninstall and reinstall the skill had no effect, but that’s another topic.
cheers caruso

i’m curious, do you know what an intent is? there is so much confusion floating around in this thread i have to ask. the assessment “some skills won’t work” then becomes “it doesn’t understand the sequence of words” shortly. (without the audio setup being involved whatsoever)
i get it, mycroft has a steap learning curve, especially if you step outside the realm of “officially supported”. and the most things i’ve read here are outside that realm (devices, linux distributions, language, etc.) which require you to do a deep dive into the mechanics of mycroft.
a look into the vocab is always beneficial:

(das |) (uhrzeit | uhr) ((rechts |) jetzt |) (bitte |)
hast du die (aktuelle|) uhrzeit
aktuelle zeit

this is what you have to ask. there is another way by combining query.voc and time.voc. the upper one showing the state of german translation (of this skill) - lengendary translation i have to add. reminds me of “ich möchte diesen teppich nicht kaufen”. yet there are many ways to contribute

hi sgee,
yes, i browsed through all of these folders, contained in the skills and i also found the de-de-files with the correct activation words for the skill. the problem is obviously not the word itselt, but the activation of the skill itselt.
i checked the logs and found a lot of strange errors concerning the date-time.
a few minutes ago i updated the “requests”, the pip and vlc, now the error is gone and time/date works with the same words used before.
so there was no problem in using the vocab.
thanks anyway.
cheers
caruso

then it would be beneficial to take those strange errors seriously and give feedback about those, so that these errors can be narrowed down by the staff.

…unfortunately i cleared the logs to gather only the current messages, so older errors are no longer available.
if it occurs again, i will do so.
thank you.
cheers
caruso
"
109,tunein internet radio streaming skill testing and feedback,skill feedback,"
first time skill writer looking for feedback on this skill to play internet radio streams based on your search term.
how to install mycroft-skill-tunein

install mycroft-skill-tunein by

using msm to install from my repo:



msm install https://github.com/johnbartkiw/mycroft-skill-tunein

mycroft-skill-tunein connects to tunein internet radio directory

this skill uses the public tunein web api that doesn’t require an account or any authentication.
there are no settings required for this skill



how to test mycroft-skill-tunein
after install there are no further steps to configure as there are no settings.


speak `intent phrase “stream” followed by a station name, number, genre, etc. to search for.


valid intent forms:


“stream kexp”


“stream jack fm radio”


“stream jazz 24 internet radio”


“stream 80s metal (on|with|using) tune in”


“stream 70s country radio (on|with|using) tune in”


“stream 99.9 internet radio (on|with|using) tune in”


“stream 107.7 radio station”


“stream npr news internet radio station”


“stream kexp radio station (on|with|using) tune in”


“stream kexp internet radio station (on|with|using) tune in”


mycroft should say it’s starting to stream a station name and then the radio audio stream should start. if it can’t find any station that matches your search criteria then it will tell you that if couldn’t find any stations matching.


where feedback on mycroft-skill-tunein should be directed:
feel free to leave feedback here or as an issue on the github repo (https://github.com/johnbartkiw/mycroft-skill-tunein/issues)
","
really love this skill with the few stations i tried (haven’t tried and categories like 80s metal)! could you try to incorporate it with the common play framework - that way people just have to say “play”!

i looove it to. have send a pr with danish translation and some other minor stuff.
also opend to issues - one by some stations fails to play and another that i cant get it to stop. the last could be related to the fat i am using danish on my mycroft device, and the stop skill mabe isnt working:)

thanks for the quick feedback! i’ll take a peek at the pr and i appreciate the translations.
i can’t repro the problem with not being able to stop a stream but i have seen another random station that wouldn’t play. i think that it may be the format of the stream not being compatible with the player. i’ll see if something like vlc can play those streams successfully.
i had  originally tried to use the common play framework but wanted to get something working easily since this is my first shot at making a skill. i’ll see if i can get that incorporated now that i have basic intents working.

that is a really cool skill. just found out that my favorite german music stations are available on tunein as well!
pr with german translation and .gitignore is waiting for you…

the problem that some stations dosnt play seems to be that some stations uses .m3u file and not a plain mp3 file. so the play_mp3 command just passes the url to the player (which on picroft and mark1 are mpg123.
mpg123 can pay streams from a mp3 right from the url. to play a m3u file there is need for parameter -@ as is told in the log.
but there dosnt seem to be a way to pass parameter to the mpg123 via the play_mp3 function 
so i think there is thre waays to go….
adding stuf to play the stream your self (look at the play_mp3 funvtion and use that and add optional parameter)
or if the url is a m3u - get it parse it and get the stream url and pass that to play_mp3
or make the changes to the play_mp3 function in core so it can take optional parameterand.

or maybe just make a hack like this:
if self.stream_url[-3:] == 'm3u':
    self.process = play_mp3(self.stream_url[:-4])
else:
    self.process = play_mp3(self.stream_url)
return

on line 80
i notied that the stations i had problems whit used a url like this
http://live-icy.gss.dr.dk/a/a03h.mp3.m3u
and just by removing the last part it worked.
but maybe test with other stations and not only dr p1 like i have.

that all makes sense and i was able to repro. give me a day or so to check out some other options before i go for the hack.

i’ve updated the skill to use the common play framework. my own testing appeared to work find but i honestly haven’t tested it enough with the other skills that utilize the framework to understand where it may fail.
i also changed the extension stripping code a bit to make it easier to add onto later.

.pls files may also need to be handled differently
triple j uneartherd for example provides this url:
http://www.abc.net.au/res/streaming/audio-live/shout-mp3/unearthed.pls
which contains two usable streams, both play the same thing. so you would need to parse that file, and extract the stream url instead of using the playlist url.
love this skill though. i’m such a lazy music lover, so much easier if someone else decides what songs i’m going to listen to 

thanks for the test case. i’ll see if i can get parsing of results that serve up a .pls file handled correctly.

it was easier to get quick parsing together than i thought. i tested with normal mp3 streams, streams that return .m3u files and .pls files and it appears to work now. let me know if there are any other broken test examples that you can find.

totaly great skill
works super well on en-us, but i do have problems getting it to work on da-dk.
if saying
“stream dr p1” he will play dr p1, but
“spil dr p1” he answers that he cant find that station
looking at the code to figure out gave me headace, as i am not well in regex, and i think it is the paring of the utterance that are different when using common_play and or call the skill directly by the intent_file_handler

i’m assuming that spil is “play” in da-dk and that you’re testing the common play path?
do you have translated regex’s locally since i don’t have those in the depot yet?
i’ve noticed that sometimes that particular regex can return no stations. i haven’t had time to dive into logs on a repro. i’ve had no problem with some of the longer regex matches. so in en-us something like:
“play kexp on tune in” instead of just
""play kexp:

yes i have translated the regex localy, and pr them when i get it to work. and spil = play 
ti works for some stations but not the dr p1 and npr news. cant see any thing why…
and “spil dr p1 på tune in” (that is play dr p1 on tune in) dosnt work. but on my mark_1 who speaks en-us it does work by “play dr p1” or “play dr p1 on tune in”
so it has to be someting regarding to the regex which i am not god at understanding.

i’m not a regex wizard either but hopefully we can figure this out.  a couple of questions:

does that regex form work on other station names or do they all fail?
does the regex still fail if you input the string via the cli instead of by voice?
what shows up in the debug log for the search term in that situation? (log.debug(""could not find a station with the query term: "" + search_term))

-j

it seems that the data is “spil dr p1” and not as expected “dr p1” when this is called
 def cps_start(self, phrase, data):
        log.debug(""cps start: "" + data)
        self.find_station(data)


ok that may be a bug in the common play framework then as the “play” part of the request is supposed to be pulled off by that before it gets to this call. can you check the repo for that to see if maybe they don’t support da-dk yet?

ohhh yes - it donst support da-dk, as i am the one translating da-dk  and just did translate it fast to test your great skill.
but then i know where to look.

and yes  - i made a mistake in the trnslation in the common play skill…. your skill work great 
"
110,precise how it works,general discussion,"
hi!
i’m trying to use mycroft, but i still don’t understand how it works.
could someone give me some more information regarding the followings link:



github



mycroftai/mycroft-precise
a lightweight, simple-to-use, rnn wake word listener - mycroftai/mycroft-precise








github



mycroftai/mycroft-precise
a lightweight, simple-to-use, rnn wake word listener - mycroftai/mycroft-precise





???
thanks!
","
maybe start with what questions you have about those links?

well, for example what is a chunk?
furthermore, referring to this picture:
https://camo.githubusercontent.com/bb3ba66863d48deedb3c6d121149f5bae1adb13a/68747470733a2f2f696d61676573322e696d67626f782e636f6d2f66372f34342f364e3478465537445f6f2e706e67
are  mfcc provided, at the first gru available, one window at a time?
"
111,playing music from emby,none,"
i just got mycroft to play music from my emby server. awesome!  i have one problem though.  i asked for jake owen to be played and mycroft played alan jackson?  does anyone know how to fix this?
thanks,
david
","
hi campdog, welcome to the community!
based on your description so far, this could be either the speech recognition not hearing correctly, or the skill selecting the wrong music.
if you are using the cli, you can see what mycroft hears when you speak, but this is also saved in /var/log/mycroft/voice.log. what type of mycroft device are you using?
if you’re using a local linux machine, you can do a quick grep:
grep ""info - utterance"" /var/log/mycroft/voice.log
if you’re using a picroft or mark 1, this will fetch the contents of the log file and just return the list of utterances:
scp pi@192.168.*.*:/var/log/mycroft/voice.log /dev/stdout | sed -n -e 's/^.*info - utterance: //p'
just be sure to set the right ip address.
if it is hearing the utterance correctly, then you want to check skills.log to see what the emby skill is up to.

@campdog,
glad to hear you’re enjoying the emby skill! how did you ask to play jake owen? the skill searches emby based on utterance, but its could be smarter about what item is picked when multiple results are returned especially if an artist is featured on a song with another artist. add the logs as suggested and we can get more info.

gez-mycroft and or sampsonight,
i am running mycroft on a linux machine using the cli.  the interpretation is accurate, except for “emby”  is recognized as “mb”.  the artist or song is correct.  below is text from the skills log with the last attempt to play music.    i asked to play lita ford from emby, and it played wasp, which is what has been happening, it plays music, just not what i asked for.
i would appreciate a hint on what to due next as i don’t really understand what is happening by reading the skills log.
thanks for the response, as it helped me get to this point.
10:56:38.429 - mycroft.skills.intent_service:handle_utterance:329 - debug - utterances: [‘play lita ford from mb’]
10:56:38.538 - mycroft.skills.intent_service:handle_utterance:349 - debug - padatious intent: {‘name’: ‘emby.rickyphewitt:emby.intent’, ‘sent’: ‘play {media} from mb’, ‘matches’: {‘media’: ‘lita ford’}, ‘conf’: 1.0}
10:56:38.538 - mycroft.skills.intent_service:handle_utterance:350 - debug -     adapt intent: {‘intent_type’: ‘mycroft-playback-control.mycroftai:play’, ‘mycroft_playback_control_mycroftaiplay’: ‘play’, ‘mycroft_playback_control_mycroftaiphrase’: ‘lita ford from mb’, ‘target’: none, ‘confidence’: 0.375, ‘tags’: [{‘match’: ‘play’, ‘key’: ‘play’, ‘start_token’: 0, ‘entities’: [{‘key’: ‘play’, ‘match’: ‘play’, ‘data’: [(‘play’, ‘mycroft_playback_control_mycroftaiconverse_resume’), (‘play’, ‘mycroft_audio_record_mycroftaiplay’), (‘play’, ‘game_zork_forslundplay’), (‘play’, ‘mycroft_playback_control_mycroftaiplay’), (‘play’, ‘mycroft_pandora_mycroftaichange’)], ‘confidence’: 1.0}], ‘end_token’: 0, ‘from_context’: false}, {‘start_token’: 1, ‘entities’: [{‘key’: ‘lita ford from mb’, ‘match’: ‘lita ford from mb’, ‘data’: [(‘lita ford from mb’, ‘mycroft_playback_control_mycroftaiphrase’)], ‘confidence’: 0.5}], ‘confidence’: 0.5, ‘end_token’: 4, ‘match’: ‘lita ford from mb’, ‘key’: ‘lita ford from mb’, ‘from_context’: false}], ‘utterance’: ‘play lita ford from mb’}
10:56:38.541 - mycroft.skills.padatious_service:handle_fallback:148 - debug - padatious fallback attempt: play lita ford from mb
10:56:38.542 - emby - info - {‘media’: ‘lita ford’, ‘utterance’: ‘play lita ford from mb’}

@campdog,
yea mycroft always seems to recognize emby as mb but the skill handles it. there are a few ways you can ask to play media from emby. below are the utterances that the skill attempts to match:
""play {artist/song/album} from emby""
""play artist {artist} from emby""
""play album {album} from emby""

if you just say 'play {media} from emby the skill takes the value of ‘media’ and searches emby for that media item. if more than one entry is found (e.g. an artist and album have the same name) the 1st result returned is used. once a match is found mycroft plays an emby instant mix using that media item. if you want to be more specific just preface the media item with ‘artist’ or ‘album’ that will force the search to be limited to that type and will play just songs from found artist/album.
from the logs it shows that mycroft played the instant mix which is why songs similar to the artist/album that was requested was played.

thanks for the help…

no problem! let me know if you have any other questions 

yes, i have a question :),  is possible to play a custom playlist?
so i shoud say ehy mycroft play mylist from emby

@reikidude,
you should be able to say ""play playlist {playlistnamehere} from emby""
if you run into any issues feel free to open an issue on the skills github: https://github.com/rickyphewitt/emby-skill
"
112,openhab skill gone missing,general discussion,"
once again, my mark i got messed up, and my first attempt to reload it failed. i wound up zero-filling the whole sd card, then installed my emergency image. i set up the unit again, it upgraded to 20.08.30, if i have understood mycroft correctly. i find that the openhab skill is now missing from the skill store, which i checked after mycroft said he couldn’t find that skill to install.
i am still digging around for options, but i was wondering what is going on with the openhab skill. it was really handy to be able to voice-control my lights.
","
dunno why it’s not in the skill list. per https://www.openhab.org/docs/ecosystem/mycroft/
to manually install the skill:
clone the skill repository into the mycroft skills directory, then trigger installation of needed dependencies:

a number of skills “disappeared” from the marketplace. probably some changes in mycroft-core invalidated the skills?

i finally got the time to attempt this. i followed the instructions, cloned the relevant repository, triggered the dependencies, and added the configuration stanza to /etc/mycroft/mycroft.conf, and it is not working. i did reboot the unit. maybe dominik is right and i’m out of luck.
"
113,mycroft on invoke,none,"
hi there mycroft community,
i have only recently discovered mycroft/picroft and i think it is a great initiative. i have started to look for alternatives after microsoft discountinued cortana from the invoke speaker. the main reason i chose cortana was the very natural almost human like voice, and not a robotic one like google or amazon.
i am very interested in both mycroft mark ii but in picroft as well. i do not have any tech skills (i am a 3d artist) but will there be a way to somehow hack hk invoke and install picroft ? all i want is to control my philips hue lights, nest thermostat and play spotify via wi-fi connect.
also… is there  a way to use cortana’s voice with mycroft? that would be amazing.
thanks and if there’s anything i could help this project with, please let me know. i don’t know any programming but i can deliver good quality cg imagery.
best of luck to you all,
","
i don’t see any hacks to the invoke that would allow it to run other software, so that may be a dead end.  secondly, the hardware inside may require custom drivers if it’s not something that’s well-known.  there’s always the swap-the-guts method, but that seems to not be what you’re looking for.
there’s support for the azure tts voices, but the cortana voice doesn’t appear in the listing of available speakers.
the hue/nest bits are usable with the homeassistant skill, and there’s a spotify skill as well for music.

hi,
thanks for replying. i have a friend who is good with electronics and he might be able to put a raspberry pi and rewire the mics and speakers and so on but i really wanted to get cortana’s voice and intonation. i know people managed to do that in windows with some registry hacks to get tts for external software(they even got the voice in elite dangerous) but again i have absolutely no knowledge about this i don’t even know where to start learning about it.

this would be amazing. it’s unfortunate that they’re sunsetting the device to just bluetooth, rather than converting to some other voice assistant.
i have two of these speakers and have been trying to find resources for modding it. maybe i’ll pull one apart to see what kind of hardware is inside them, and see if anything can be customized about it.
"
114,hey mycroft laugh maniacally,general discussion,"
does someone in the community have time to whip up a quick skill that makes mycroft laugh ( randomly ) like a maniac?  maybe call it the “maniac laugh skill”.
example of laughs that would be great ( but without the doors soudtrack )

“install maniac laugh skill”
“hey, mycroft, laugh like alexa”
“hey, mycroft, random laughter”
it’s worth a copper mycroft challenge coin.  if you work as a team, each team member gets a challenge coin ( up to 10 team members ).  needs to be production ready with auto test.
","

needs to be production ready with auto test.

what does this mean exactly?



github



jarbasal/skill-laugh-skill
skill-laugh-skill - make mycroft do an evil laugh





features:

laugh once on request intent
laugh randomly, 1 to 30 minutes interval between laughs, intent
stop random laughs intent
11 different laugh sounds
en-us and pt-pt


i installed this late last night on the vm running on my 'puter, and forgot i told it to laugh randomly…
it scared the bejeesus out of me this morning!!!
@j_montgomery_mycroft -  give that man his challenge coin! 

i opened a few issues with improvements i intend to make shortly https://github.com/jarbasal/skill-laugh/issues
feel free to add any and all suggestions!

awesome!  let me know when you’ve connected with steve and included the autotest phrases and i’ll have @johnnyd-mycroft  send out a challenge coin.

still waiting on that coin 

give that man a coin!
"
115,mark ii update october 2020,mark ii,"
originally published at:			https://mycroft.ai/blog/mark-ii-update-october-2020/
tldr; we got a new revision of the sj201 & sj202 boards created and the first of these are currently on their way to our dev team.
what’s changed
the big change in rev3 is that xmos updated their firmware to enable i2s output. this allows us to use the xmos chip as a single source, being a sound card for both recording and outputting audio over i2s. 
as the current amplifier takes an analog input, we added an i2s to line out converter chip to bridge the gap until we switch the amplifier over to one with a direct i2s input. the change of amplifiers is planned for rev4 to the tas5806 ic. this reduces parts, complexity, and cost. it also has the added benefit of better sound quality.

front of sj201 v0.67c pcb


back of sj201 v0.67c pcb

added in rev3:

xmos i2s output to uda1334a (i2s to line out ic)

attiny404

controls leds
tested controls from i2c from xmos over usb – works
wrote firmware for attiny
in rev4, the attiny will remove the power on reset sequencing bits needed for the xmos


changed the position of the mounting holes
4 pin connector to sj202 instead of usb connector. this allows a much simpler and stronger sj202

removed in rev3:


usb sound card cm108b

removed complexity and didn’t work as expected


removed 2 physical indentations no longer needed.



system diagram of the sj201r31473×699

sj201r3 overview

attempts to speed up production
in an attempt to speed up the turnaround time for rev3 we selected a local pcb house to produce this run of boards. they were more expensive but were theoretically going to be quicker than getting them shipped from overseas. that unfortunately did not work out as planned.
thankfully we also ordered a small batch of single sided boards from our previous supplier in china as a backup. these ended up arriving first, however they are single sided and therefore required many parts to be manually added.
we’ll keep looking for ways to speed up the process however we can.
enclosure code
now that the hardware has firmed up, our software team has been able to get into the enclosure code. this connects mycroft with all of the low level hardware systems so that you can do all the things you’d expect from a smart speaker – like changing the volume, muting the microphone, or getting visual feedback from the leds on top of the device.
this code will continue to be iterated on over time, but the core foundations of that code has been written.
mechanical design
we have completed the first round of 3d printed housings for the enclosure. these need to be tested for acoustic and thermal properties before we can share with the community and distribute to our internal team. the intention is to keep the 3d printed design as close to the final injection molded design as possible. this will allow anyone who buys a board only dev kit in the future to print their own enclosure. we aren’t sharing images yet because we are exploring options to protect the mycroft brand from potential misuse in the future. however, it will still be open source hardware, and we will share cad data.
until we can further test the new 3d printed housings we are continuing to use our simple laser cut enclosures (sj230) to get an approximate experience to the final assembled product.
precise tagger
another piece of work our team has been focused on is bringing our precise data tagger back online. whilst this is not mark ii specific, it is a critical piece of our technology that is necessary for the mark ii to be usable by the majority of people. 
our current wake word detection is frankly terrible for anyone that isn’t an english speaking male, most likely from the us. for women and children in particular, the current model is not accurate enough. this primarily comes down to data. we have an enormous amount of wake word data from men in the us. which means that the wake word detection works great for that group of people. 
to address this, we have been looking at ways to better balance our training data. we’ve had some promising early results with our existing data, but the biggest limitation is that it has only been categorized as either a wake word, or not a wake word. we don’t know whether the speaker sounds young or old, masculine or feminine, what type of accent they have, or whether there is other background noise. balancing these characteristics should significantly improve our wake word detection for the broader population. 
which brings me back to the precise tagger: this is a web interface that allows anyone in our open source community to help make wake word detection more accurate. it presents a short audio sample and asks you to answer a few simple questions based on how the speaker sounds and other noise that may be present.
the first prototype of the new precise tagger is being code reviewed and we hope to release to the community for testing shortly.
",
116,off grid dc system use,none,"
has anyone ever set up mycroft on an off-grid dc system ?
what i’m thinking off specifically is using a smart plug to allow recharging a dc battery on which the entire house electricity system runs. this way, it can allow for recharging of the dc battery during off-peak hours (which is cheaper). the ac devices (because most domestic household items can only be bought in ac-version) are powered from the dc battery through an inverter. additional smart plugs can be placed in power sockets in which certain ac appliances sit (washing machine, dryer, dishwasher, …) thus allowing to run only if enough power is left in the battery (because you don’t want the battery to get too low either as it’s bad for the battery itself and you may want to reserve some extra power for other devices (lights, tv, refrigerator).
what’s tricky here is that many power companies no longer set specific times for off-peak and peak hours but rather use a digital meter in which any time during the day can be peak or off-peak, depending on the exact time and energy demand/supply ratio on the grid.
","
i might have not been entirely clear here: i mentioned it is a “offgrid” system but really it 's a sort of hybrid system as it is grid-connected but not directly (only the battery connects to the grid and the system can and would be detached whenever no recharge of the battery is needed). this is useful for economic and ecological reasons (it requires the use to be more energy conservative and obviously, energy costs are lower if only drawing power at off-peak hours)

i still fail to see how this is related to mycroft.

there may be a related question hidden in his posts (running mycroft offline) but it’s overshadowed by the extraneous bits.

well, its pretty obvious how it is related to mycroft: mycroft would handle the automation of the system. for instance, it would decide when to deactivate any of the smart plugs when it detects the battery capacity gets too low. also, it would start the recharge of the battery when battery is low and off-peak hours get into effect (after a certain hour and untill a certain hour of the day), or if the power company tariffs change depending on current power draw/network stress, then it needs someway to know it’s currently off-peak or peak hour (this is often done by power companies through a smart meter system, so needs to be able to connect to this somehow).
lastly, manual activation (speech) should also allow the user to recharge the battery manually, show present current power capacity of the battery, show power draw of each smart plug or activate/deactivate particular smart plugs.
mycroft could also automatically disconnect the battery with an additional switch for safety (in case a solar storm puts excess power on the grid, possibly also damaging appliances in connected homes). this is quite exceptional to occur, but it would give extra ease of mind to owners of the system, and an extra reason on why to install such a system.
another thing i think of is that, when connected to a smart meter, and if the owner has photovoltaic panels and has a net metering agreement with the power company, it could allow giving current power transmission (kwh) data to the user (but that’s perhaps more of a gimmick). more interesting is allowing mycoft of showing  monthly or weekly energy production data and the possible energy production (available watts/m2 for that location x 24 hours x one week or month). the idea here is to give the user information on how he best sets the panel tilt for that month or 4 months or orientation for optimal energy production, hereby increasing revenue for the user.




 brian:

well, its pretty obvious how it is related to mycroft: mycroft would handle the automation of the system


with the right skill programming mycroft “could” do this but “should” mycroft do this? mycroft is really developed as a smart speaker interface and already has skills to interface with openhab and home assistant which are better equipped to handle the automation side of things (imho). with that said mycroft works great to trigger said automations as well as an automation event notification client.

openhab has a smartmeter binding which “reads sml messages (push) and supports iec 62056-21 modes a,b,c (pull) and d (push).”
if such device is present openhab will most likely be able to interface it. and in this case the existing skill could be expanded to cover this function.
and if you are capeable of monitoring (or interact with) this device, node red (skill) is also a possibility to “do something with this data feed”.
have a dumbed down application in my basement to drive the ventilation dependent on humidity, co2, dust and outside conditions (fully node red controlled). yet this has nothing to do with “smart metering”
"
117,grocery list and food ordering,none,"
does mycroft allow to store a “grocery list” by simply dictating it to mycroft orally (for example after you look into the fridge and pantry and see you’re missing some items) ?
also, is there any way for mycroft to order food directly from the supermarket (food delivery) ?
i’m not sure which software solutions exist for this which can work with mycroft’s os.
i’m mainly thinking of say orders from ocado, farmdrop, tesco, gousdo, asda, instacart, amazonfresh, safeway, walmart, my cloud grocer, …
i know this might already be possible through “smart fridges”, but those things are expensive and i already own a fridge, i think this is a good alternative then (no need to change my fridge, and still have this functionality more or less).
","
this and this should be viable

i see that solves the grocery list creation issue, however what programs exist to export that as an “order” to any of the mentioned companies so that this list of items actually get delivered to me at my doorstep ?

there is no such thing
"
118,changing language and tts,languages,"
sorry, but i am new here and not that good at programming.
i want to set mycroft to german and use espeak as tts. i know that this should help me



mycroft-ai.gitbook.io



text-to-speech
text-to-speech (tts) is the process of synthesizing audio from text. mycroft uses our own tts engines by default, however we also support a range of third party services.





i really tried my best, but did not succeed. my config looks as follows
{
“max_allowed_core_version”: 20.2
}
what does the conf have to look like? please help me and sorry for being that helpless.
","
community member @gras64 did a nice write up on how to to configure mycroft for german language support: https://github.com/gras64/docs-rewrite/blob/master/_pages/german.md

that page is no longer available.

it was updated and in a new path https://github.com/gras64/docs-rewrite/blob/master/docs/using-mycroft-ai/customizations/languages/german.md

my main reason for experimenting with mycroft, is the reluctance to use the system of one if the big guys and their doubtful respect for our privacy. i guess this is th case for many mycroft users. but now i’m curious: how much do we give away when we start using google’s tts/stt?

in these instructions it is possible to set up a deepspeech sst offline with espeak. both can already be carried out on a pi 4.

out of the box, your stt and tts requests are proxied through mycroft-the-company, who are only caching requests long enough to avoid repeated, identical api queries within a short time. they don’t log, nor store the cache where employees can see it, and the way i know that is because the one time they thought they’d accidentally stored accessible logs, they freaked the f*** out.
and then determined that they hadn’t.
so, while it’s not perfect, and hosting your own deepspeech is obviously the most secure solution, you’re loads better off than if you were just using google yourself. all google sees is yet another request from the same place (“all” mycroft users.)

what chance said.  @pjdevries if you’re on (american) english, deepspeech can be really good.

@chancencounter thanx for the explanation. i didn’t realize all request are going through mycroft. from a privacy point of view, that’s definitely a plus. i would prefer to completely do without google though 
@baconator my native language is dutch. although my english is good enough to use mycroft effectively, dutch would be easier and more comfortable. for my girlfriend even more so.




 pjdevries:

from a privacy point of view, that’s definitely a plus.


that the stream is anonymized means nothing nowadays. there are so much datapoints out there, it should be pretty easy for a company this potent to match those to the feed.

i guess your right, in the sense that a digital assistant should not be the only thing you must be careful with. but what i hear you saying is: “don’t bother. they know who you are and what you do, no matter what”. so why bother using mycroft then? why not simply use an assistant from one of the big guys?

don’t forget that english speakers can easily circumvent the problem with little effort. and mycroft (by now) only supports english language.
yet, as stated in the devsyncs by josh, internationalization is pretty high on the priority list post mk2. and that mozilla don’t actively furthers commonvoice anymore can be seen as an opportunity to get some viable buisiness operation going and kicking back advancements the same time.
and, there are options… but not in the quality one would like to have.




 sgee:

don’t forget that english speakers can easily circumvent the problem with little effort. and mycroft (by now) only supports english language.


don’t get me wrong. i’m not trying to be critical about mycroft. not at all. it’s is a nice hobby project. if i can get mycroft to work reliable and stable in english, i’m a happy dude 
"
119,skill vetting process,mycroft project,"
hello guys!
does anybody have any insight into how the vetting process works?!
i have read some posts on the forum and also information from here: https://mycroft.ai/contribute/#skills-acceptance-process.
it says…
“all skills are reviewed, vetted and approved by a group of community volunteers as part of our formal acceptance process. this self-governance maintains the quality and security of the marketplace while keeping the community’s needs first.”
from what i understand, new skills are reviewed manually without any automated tool. good approach to ensuring security. 
however, i am curious about how these volunteers vet the skill. is there some kind of response queue, or any other structure in place?! if so, how much time does a skill need to be reviewed completely?
","
hi
you can read about the skills acceptance process here
https://mycroft-ai.gitbook.io/docs/skill-development/marketplace-submission/skills-acceptance-process
you can follow the submission on the pull requests on the skills reposotory on github.



github



mycroftai/mycroft-skills
a repository for sharing and collaboration for third-party mycroft skills development. - mycroftai/mycroft-skills





sometimes it goes quick to submit, and sometimes not. it depends on he skill and how much time testeres got to test and reviev.

but that said, a skill works just fine not beeing submittet to the market and can be installed and used anyway. they just hassnt yet been accepted for the market place.

alright! thank you for the prompt answer and the pointers. i will have a more in depth look at it.
all the best!

good to know as a person should thoroughly test the skill before submitting it.
"
120,how to use google search in mycroft so that it gives search results of google,mycroft project,"
hi, i have install mycroft in my linux, now i want if i say who is president of united state?
it should return with answer form google search , how can we do it , any changes and code is helpful
","
you can write a skill to do. start here https://docs.mycroft.ai/skill.creation

currently  by default mycroft uses its own search engine to perform search how can i change it to google. where i can get accurate result ,  can we integrate google search api , if yes how ???. or any other help .
i am also know to mycroft skill development so any help for developing skills which fulfill above requirements is helpful

there is currently no api available from google to perform searches of that type (there used to be a research api available to university researchers, but that was deprecated many years ago). your best bet would be to write a skill that operates against a structured database of facts such as wikidata, but it’s a very complicated task unless you restrict yourself to a narrow class of facts.

would the duck duck go api be any good?



duckduckgo



duckduckgo instant answer api
duckduckgo instant answer api, giving free access to instant answers.







unfortunately duckduckgo isn’t the best solution because you don’t get full search capabilities.
"
121,american male voice fallback to allan pope,support,"
on a newly installed picroft on a pi4 he voice falls back to allan pope mos of the times. if using femaile (google) voice it never does.
i cant figure out why…did look into network and power issues bu nothing to see and nothing seems to help.
anyone having same issue or have any ideas for wha to do ?
","
hi andlo, how does “tts” section of your mycroft.conf does look like?

it is a plain vanilla picroft…
/etc/mycroft/mycroft.conf
´´```
{
“play_wav_cmdline”: “aplay %1”,
“play_mp3_cmdline”: “mpg123 %1”,
“enclosure”: {
“platform”: “picroft”
},
“tts”: {
“mimic”: {
“path”: “/home/pi/mycroft-core/mimic/bin/mimic”
}
},
“ipc_path”: “/ramdisk/mycroft/ipc/”
}

he do uses american voice, but sometimes, most times he falls back to allan pope voice.
like
hey mycroft - who is abaraham linkoln ?
(american voice) "" abrahim linkoln is…""
hey mycroft - what time is it
(allan pope) “it is seven twenty three”
hey mycroft - where is the iss ?
(american voice) “the international spacestation is located at…”
hey mycroft  - how is the weather ?
(allan pope) “todays forecast is…”

more inportantly! welcome back😉

glad to be home again  but still not enouh time  but winter is comming and then hopefully i can have some fun again 

maybe you can try so set google-tts explicitly:
""tts"": {
    ""google"": {
      ""lang"": ""en""
    },
    ""module"": ""google""
}


well - he works when using google voice set from home.mycroft.ai. the problem is when using the american voice (mycroft mimic2). and i do like a male voice. but i dont like allan pope and not like that he uses some of the one and some of the other 

but maybe i should give openvoiceos  try 

try deleting the tts cache, if then you only get the mimic1 voice perhaps the issue is as simple as a dns problem not be able to reach te servers.
btw, by the time you are fully back we should be able to give you a fancy ovos build😜

where is the tts cache located ?
i did look at networkissues, but didnt find anything that cold be problem.
i will try my pi3 board and see if it could be the pi4 tht has some hardware issues.

cache is under /tmp/mycroft/
sounds like it’s timing out somehow and coming back at odd times.  but audio.log should have something about that somewhere if so?

ohhh yes - my audiolog tells me an error 500

image1092×416 23.6 kb

cleaing the tts temp didnt change anything.

but it isnt consistant - right after it didnt error 500…

image1096×472 31.1 kb


hence the back and forth.  odd.

could be our tts server is borking and giving out 500 errors.  anyone else having this trouble?

i just set up one more picroft on a pi3 board to make sure it wassnt he pi4 board that had some issues. but same behavior. so if it isnt my network which i have checked in and out it points to the mycroft tts server which also is the one giving the 500 error.

@gez-mycroft - can you take a peek at our tts server and see if it has fallen on it’s head or if someone has scripted a tts transcription of the holy bible or something?

and if so replace the text with something from lovecraft or herbert.

not sure if this helps or even related but i am using the american male voice as well and it loves to switch back to allan pope. `setup was working fine before i did the latest pull. was using code back in february before that pull.
2020-10-23 19:06:04.641 | debug    |  3890 | urllib3.connectionpool | https://mimic-api.mycroft.ai:443 ""get /synthesize?text=how%20long%20of%20a%20timer%3f&visimes=true http/1.1"" 200 70680
19:06:04.641 - urllib3.connectionpool - debug - https://mimic-api.mycroft.ai:443 ""get /synthesize?text=how%20long%20of%20a%20timer%3f&visimes=true http/1.1"" 200 70680
2020-10-23 19:06:08.141 | debug    |  3890 | mycroft.audio.audioservice:_restore_volume_after_record:342 | no audio service to restore volume of
2020-10-23 19:06:08.879 | info     |  3890 | mycroft.audio.speech:mute_and_speak:127 | speak: i'm starting a timer for twenty five minutes
2020-10-23 19:06:08.880 | debug    |  3890 | mycroft.tts.mimic2_tts:get_tts:232 | generating mimic2 tss for: i'm starting a timer for twenty five minutes.
2020-10-23 19:06:08.956 | debug    |  3890 | urllib3.connectionpool | https://mimic-api.mycroft.ai:443 ""get /synthesize?text=i%27m%20starting%20a%20timer%20for%20twenty%20five%20minutes.&visimes=true http/1.1"" 200 139201
19:06:08.956 - urllib3.connectionpool - debug - https://mimic-api.mycroft.ai:443 ""get /synthesize?text=i%27m%20starting%20a%20timer%20for%20twenty%20five%20minutes.&visimes=true http/1.1"" 200 139201
2020-10-23 19:06:16.163 | info     |  3890 | mycroft.audio.speech:mute_and_speak:127 | speak: an error occurred while processing a request in timer skill
2020-10-23 19:06:16.164 | debug    |  3890 | mycroft.tts.mimic2_tts:get_tts:232 | generating mimic2 tss for: an error occurred while processing a request in timer skill.
2020-10-23 19:06:16.257 | debug    |  3890 | urllib3.connectionpool | https://mimic-api.mycroft.ai:443 ""get /synthesize?text=an%20error%20occurred%20while%20processing%20a%20request%20in%20timer%20skill.&visimes=true http/1.1"" 500 290
19:06:16.257 - urllib3.connectionpool - debug - https://mimic-api.mycroft.ai:443 ""get /synthesize?text=an%20error%20occurred%20while%20processing%20a%20request%20in%20timer%20skill.&visimes=true http/1.1"" 500 290
2020-10-23 19:06:16.260 | error    |  3890 | mycroft.audio.speech:mute_and_speak:131 | backend returned http status 500
2020-10-23 19:06:16.261 | debug    |  3890 | mycroft.audio.speech:mimic_fallback_tts:146 | mimic fallback, utterance : an error occurred while processing a request in timer skill
2020-10-23 19:23:28.223 | debug    |  3890 | mycroft.audio.audioservice:_restore_volume_after_record:342 | no audio service to restore volume of
2020-10-23 19:23:29.250 | info     |  3890 | mycroft.audio.speech:mute_and_speak:127 | speak: the timer for twenty five minutes has seven minutes thirty nine seconds remaining
2020-10-23 19:23:29.251 | debug    |  3890 | mycroft.tts.mimic2_tts:get_tts:232 | generating mimic2 tss for: the timer for twenty five minutes has seven minutes thirty nine seconds remaining.
2020-10-23 19:23:29.254 | debug    |  3890 | urllib3.connectionpool | resetting dropped connection: mimic-api.mycroft.ai
19:23:29.254 - urllib3.connectionpool - debug - resetting dropped connection: mimic-api.mycroft.ai
2020-10-23 19:23:29.588 | debug    |  3890 | urllib3.connectionpool | https://mimic-api.mycroft.ai:443 ""get /synthesize?text=the%20timer%20for%20twenty%20five%20minutes%20has%20seven%20minutes%20thirty%20nine%20seconds%20remaining.&visimes=true http/1.1"" 500 290
19:23:29.588 - urllib3.connectionpool - debug - https://mimic-api.mycroft.ai:443 ""get /synthesize?text=the%20timer%20for%20twenty%20five%20minutes%20has%20seven%20minutes%20thirty%20nine%20seconds%20remaining.&visimes=true http/1.1"" 500 290
2020-10-23 19:23:29.591 | error    |  3890 | mycroft.audio.speech:mute_and_speak:131 | backend returned http status 500
2020-10-23 19:23:29.592 | debug    |  3890 | mycroft.audio.speech:mimic_fallback_tts:146 | mimic fallback, utterance : the timer for twenty five minutes has seven minutes thirty nine seconds remaining
"
122,productivity to do tasks and lists,skill suggestions,"
skill name: todo-lists
user story:

as a busy person, i want to be able to add to do items to various lists so that i don’t forget them
as a busy person, i want to be able to add different tasks to different lists so i can be more organised

what third party services, data sets or platforms will the skill interact with?
unknown at this stage. there are several to do list platforms around, but we may also want to consider having a skill that is usable offline - ie without an internet connection or an api call to a third party service.

remember the milk - https://www.rememberthemilk.com/

todoist - https://en.todoist.com/

google keep - https://keep.google.com/

evernote - https://evernote.com

habitica (open source) - https://habitica.com


are there similar mycroft skills already?

https://github.com/carstenagerskov/skill-the-cows-lists

what will the user speak to trigger the skill?



“hey mycroft, add {{item}} to {{list}}”




“hey mycroft, what items are on {{list}}”




“hey mycroft, how many items are on {{list}}”




“hey mycroft, what are the highest priority items on {{list}}”



what phrases will mycroft speak?

on {{list}} there are {{count}} items. they are {{item_list}}
i have added {{item}} to {{list}}

what skill settings will this skill need to store?

the items and the list
priorities
categories or similar
authentication details if a third party api is being used

other comments?
i’d really like to know what people use for their to do lists, or how they use them.
","
as i already mentioned for the calendar skill, it would be helpfull for me if mycroft would support caldav as open protocol for calendar and todos especially in connection with own- / nextcloud.

again, as with the calendar, spot on. privacy aware applications would be great here. i’d buy google home if i wanted google keep e.g.

do you know if exist a skill that read/write a to do list on evernote.
"
123,picroft wakeword detection stopped working,support,"
hello,
i am running picroft on pi3b+ with respeaker 4 mic array and precise listener. i am really struggling with the wake word detection. it was working initially, but not extremely. during one of my debugging sessions i spoke the wake word 5+ times and since then mycroft stopped responding to wake words at all.
the micrphone and the language processing seems to be working as i can set timers if i initiate communication via cli. this looks similar to the issue described here.
please advise.
","
have you changed wake words?  which listener are you using?  what’s in the voice.log?

false alarm, it seems. i cannot reproduce this any more. wake word with precise is working again, but i am pretty sure that this morning, when i botted mr m i wasn’t able wake him with my utterances.
thanks for a reply and for a tip to check voice.log! to answer haven’t changed the wake word but i played with the listener and moved between pocketsphinx and precise prior to the freeze. i restarted couple of times too.
"
124,new setup with picroft bluetooth,general discussion,"
hi all, just want to say thumbs up to the mycroft team. using it for a few days and loving it. my setup is the following:

raspberry pi 3 with pycroft
logitech c310 webcam (input sound)
blackweb soundpebble speaker (output sound, using bluetooth and/or audio jack)
created an sh script to autoconnect the bluetooth speaker at startup.
homeassistant integration (works great with my setup)

so far so good! looking forward to try some more skills and other things !
","
hi there @redpotatoes, welcome to the community and glad you’re enjoying mycroft! 

hey, that is great to hear.
most people have had a pretty rough time working with bluetooth audio devices. if there’s anything you can share about your setup that would assist others please do 




 redpotatoes:

created an sh script to autoconnect the bluetooth speaker at startup.


could you share? would be interested in seeing this.

first,
at the command line do this once:
bluetoothctl
power on
agent on
default-agent
scan on
pair xx:xx:xx:xx:xx:xx (mac)
trust xx:xx:xx:xx:xx:xx (mac)
connect xx:xx:xx:xx:xx:xx (mac)
quit
once this is done, create a sh script like this:
nano bluetooth.sh (add the following)
#!/bin/bash
echo “power on \nagent on \ndefault-agent \nconnect xx:xx:xx:xx:xx:xx \nquit” | bluetoothctl
at the command line do this:
crontab -e (add the following)
@reboot sh /home/pi/bluetooth.sh &
***this does not work everytime
alternative:
nano .bashrc (add the following at the end, note that this file gets overwritten if update.sh is ran)
sh /home/pi/bluetooth.sh &
i also installed the cmdskill so i can run scripts with my voice, i modded .mycroft/mycroft.conf:
“cmdskill”: {
“alias”: {
“connect bluetooth”: “sh /home/pi/bluetooth.sh”
}
}

thank you for the sharing.
can you specify better what do you mean as modded the mycroft.conf?
do you appended the cmdskill at the end of the file??
thanks
"
125,i want to make your skill,mycroft project,"
do you have an idea for a skill? well, i have good news, i am experienced with python, want to learn more about mycroft, and am extraordinarily bored. if there is any skill you want, please say, and i will try to make it.
","
generous offer.   how about a skill that allows an interface to my shark ninja (roomba clone by shark)
i really need to learn python… 

just pick one from the skill suggestions forum.

second what @j1nx said!

make a skill that will allow the integration of more voices!

what i really miss is a skill i would call: ‘what can i say?’
then mycroft would ask ‘regarding what skill?’
then you would either say: ‘the last used’, ‘a specific skill’ or ‘i don’t know’ (the last one mycroft would ask you if you would like to hear the list of skills)
furthermore, i think it would be nice if you could ask mycroft to repeat the last thing she said…
lastly i would really be interested in being able to ask mycroft to tell me the ‘state’ of devices/entities in my home assistant setup (but probably this could already be achieved?)

i saw this skill a few days ago which can list some commands accepted by a specific skill (i haven’t tested it though).
and this skill should enable repeating the last response.

take a look at my help skill, that is exactly what it is for.




testing and feedback for help-skill skill feedback


    how to install help skill


install help skill by … 

msm install https://github.com/pcwii/help-skill.git

there are no additional dependencies required for this skill



help skill services … 

the help skill will retrieve a list of installed skills by collecting all the skill directories in the skills folder.
when “more” help is requested for the listed skill, examples will be provided by retrieving the utterance from the readme.md file in the skills folder.
this skill will only work if the re…
  



i installed the help-skill, would be awesome tho if you could say: ‘what can i say’ to trigger it atop of the other ‘triggers’…

i can definitely do that . i have not played with that skill in a while, definitely time for an update. i will likely add a web setting to enable or disable help on fallback skills. let me know if anything else can make it better.

hi we are developing a project of a humanoid robot with mycroft skills.
we would like to create this skill: tweet this message and the robot says ok, and after you say the message and the robot tweets it

a skill that monitors for smoke alarm noises and emails or texts you and sounds another alarm

thats one of the best offers i ve ever heard of… so i would love to have the skill of being able to add commands such as sentences that trigger answers and behaviours… such as -  i say - where are my keys - and the ai is responding with (annoyed phrases ) statements that i can add …
moreover i would love to have a simple mp3 player that reads files from a usbdrive and plays them if i say the names

could you look at fixing the medlineplus medical skill? people can search medlineplus.gov for basic health information. the files are at
https://github.com/christiankaye/medlineplusskill




 vonduesenberg:

moreover i would love to have a simple mp3 player that reads files from a usbdrive and plays them if i say the names


i got this one! i think this would be a cool skill too, i will add it to my todo list. ie: usb-music-player-skill 

i just recently happened by an interesting linux game server manager project: https://linuxgsm.com/
it appears to provide a somewhat standardized cli interface to a few popular games’ dedicated servers.
i think it would be pretty cool to have some kind of mycroft interface to a gsm installation. because of the compute power necessary, i am not proposing that gsm be installed on mycroft, but on a separate server. therefore, i presume this would have to be done over ssh or something like that, and the skill would have to gracefully handle the mediation. but wouldn’t it be great to:
“hey mycroft, start up the terraria server.”
“hey mycroft, how many players are connected to the terraria server?”
“hey mycroft, stop the terraria server.”
“hey mycroft, update the minecraft server.”
settings.json file would probably include the server dns name or ip, a place to insert or specify an ssh key, and specify supported games for gsm to manage.

i believe it would be very useful if you could make an inventory system skill, this is what i mean by that: i could ask “mycroft where is x item?” and mycroft would say on shelf 72 (or whatever), or i could say “mycroft store y item on shelf 23” and then go back and say “mycroft where is y item?” and mycroft would say “on shelf 23”, i think you get the idea. as for setting the shelves, you would say “mycroft add location shelf 99” (or you know). and remove items by saying “mycroft remove z item from shelf 11”.
so to summarize, you could add locations, and items to a location, check where an item is, or remove an item from a location.
if you like my idea and make it, thank you very much.




 kangaroogoo:

i believe it would be very useful if you could make an inventory system skill,


i would like to give this a go. i will keep you appraised.

check out https://github.com/renayo/inventory-skill.git




 walter_giacovelli:

we would like to create this skill: tweet this message and the robot says ok, and after you say the message and the robot tweets it


this can be done pretty easily, but i think you need a dev-level twitter account (which is straight-forward to get). let me know if, after that, you are still interested.
"
126,multiple remote mic speaker,mycroft project,"
so i’m wondering how mycroft handles multiple remote mics. say if i were to have a small pi + mic-array in each room, and feed back the audio over the network (probably via pulseaudio), and feed each into mycroft on my more powerful home nas/server. could that work? what about the possibility of mycroft figuring out which room i’m in based on which of the mic audio streams was the loudest?
i did see picroft multiple rooms, which is obviously quite similar, but from close to a year ago so things may have changed .
","
it doesn’t currently.  they’d each be separate instances.  what you’re asking about is on the long-term road map, but not here yet.

gotcha, thanks. was just checking in as to the current state. as i said, last update was nearly a year ago.
which side of the long-term road map? “we’ll get to it eventually”, or “it’s important to us and we’re trying to get to it soon™”? the former implies some day, maybe. the later is maybe in the next year or two (in my mind at least).

it’s important to enough community members that there’s efforts outside of mycroft.ai, and after mark 2 push is over, it may get attention.

oh awesome, ok, thanks!
"
127,msm tmp lock file is broken,support,"
when i try to use the msm app in the virtual enviroment then this happens.
traceback (most recent call last):
file “/usr/share/mycroft-core/.venv/bin/msm”, line 8, in 
sys.exit(main())
file “/usr/share/mycroft-core/.venv/lib/python3.8/site-packages/msm/main.py”, line 96, in main
msm = mycroftskillsmanager(
file “/usr/share/mycroft-core/.venv/lib/python3.8/site-packages/msm/mycroft_skills_manager.py”, line 109, in init
with self.lock:
file “/usr/share/mycroft-core/.venv/lib/python3.8/site-packages/fasteners/process_lock.py”, line 174, in enter
self.acquire()
file “/usr/share/mycroft-core/.venv/lib/python3.8/site-packages/fasteners/process_lock.py”, line 151, in acquire
self._do_open()
file “/usr/share/mycroft-core/.venv/lib/python3.8/site-packages/fasteners/process_lock.py”, line 123, in _do_open
self.lockfile = open(self.path, ‘a’)
permissionerror: [errno 13] permission denied: ‘/tmp/msm_lock’
permissions look like this
0 drwxrwxrwt 19 root    root     720 oct 23 13:23 .
4.0k drwxr-xr-x 19 root    root    4.0k sep 30 17:10 …
0 drwxrwxrwt  2 root    root      40 oct 23 12:34 .font-unix
0 drwxrwxrwt  2 root    root      40 oct 23 12:34 .ice-unix
0 drwxr-xr-x  4 mycroft mycroft  140 oct 23 12:35 mycroft
0 drwxr-xr-x 82 mycroft mycroft 2.0k oct 23 12:36 .skills-repo
0 drwx------  2 daniel  wheel     60 oct 23 12:35 ssh-jmqcw98c2f64
0 drwx------  3 root    root      60 oct 23 12:35 systemd-private-89bb8633b58046ea9815bf5b5ee1bb8b-colord.service-zsw5fi
0 drwx------  3 root    root      60 oct 23 12:35 systemd-private-89bb8633b58046ea9815bf5b5ee1bb8b-geoclue.service-v11zuh
0 drwx------  3 root    root      60 oct 23 12:34 systemd-private-89bb8633b58046ea9815bf5b5ee1bb8b-systemd-logind.service-t8igef
0 drwx------  2 daniel  wheel     40 oct 23 12:36 temp-87570ecc-f8c3-45cf-b031-d2bbc2698907
0 drwx------  2 daniel  wheel     40 oct 23 12:36 temp-cb00f3eb-ad93-45cf-a488-687a9394861e
0 drwxrwxrwt  2 root    root      40 oct 23 12:34 .test-unix
0 drwxr-xr-x  6 mycroft mycroft  220 oct 23 12:58 tmp0tgtnzcn
0 drwxr-xr-x  6 mycroft mycroft  180 oct 23 13:01 tmp811_z34_
0 drwxr-xr-x  4 mycroft mycroft  200 oct 23 12:44 tmp872_u38m
0 drwx------  2 daniel  wheel     60 oct 23 12:59 .vifm-trash-1000
0 drwxrwxrwt  2 root    root      60 oct 23 12:35 .x11-unix
0 drwxrwxrwt  2 root    root      40 oct 23 12:34 .xim-unix
0 -rwxrwxrwx  1 mycroft mycroft    0 oct 23 12:35 identity-lock
0 prw-r–r--  1 daniel  wheel      0 oct 23 12:35 mpd.fifo
0 -rwxrwxrwx  1 mycroft mycroft    0 oct 23 12:35 msm_lock
0 -rwxrwxrwx  1 mycroft mycroft    0 oct 23 12:35 mycroft-msm.lck
0 prw-r–r--  1 daniel  wheel      0 oct 23 12:35 polybar_mqueue.934
4.0k -rw-------  1 daniel  wheel    159 oct 23 12:35 serverauth.sh0qbgkkb9
92k -rw-r–r--  1 mycroft mycroft  91k oct 23 13:19 .skills-meta.json
4.0k -rw-------  1 mycroft mycroft   22 oct 23 12:35 tmpe9ykd_wg
4.0k -rw-------  1 mycroft mycroft   22 oct 23 12:39 tmpgxa_nzpi
4.0k -rw-r–r--  1 mycroft mycroft 2.9k oct 23 12:40 tts.wav
0 prw-------  1 daniel  wheel      0 oct 23 12:45 vifm-ipc-vifm
0 prw-------  1 daniel  wheel      0 oct 23 12:49 vifm-ipc-vifm1
0 prw-------  1 daniel  wheel      0 oct 23 13:11 vifm-ipc-vifm2
0 prw-------  1 daniel  wheel      0 oct 23 13:11 vifm-ipc-vifm3
0 prw-r–r--  1 daniel  wheel      0 oct 23 13:11 vifm-ueberzug-27753
0 prw-r–r--  1 daniel  wheel      0 oct 23 13:11 vifm-ueberzug-28041
4.0k -r–r--r--  1 daniel  wheel     11 oct 23 12:35 .x0-lock
","
who is the owner of the packages in



 daniel-radloff:

/usr/share/mycroft-core/.venv/lib/python3.8/site-packages/


this is a weird combination daniel/mycroft/root and mycroft-core in /usr/share/

everything there is owned by user group mycroft.
i used the aur to install it.

works on mine btw.
check htop. something’s not running under mycroft
you can easily check the permission:
su mycroft
nano /tmp/msm_lock
"
128,mycroft on lxqt,none,"
mycroft normally uses kde programs to run, but can you also run it on lxqt (in example lubuntu on lxqt) ?
lxqt tends to be a lot lighter then kde (huge difference!), so it seems much more appropriate, if it can be done that is.
","
mycroft doesn’t currently include gui components, so run it on whatever dm you want.
"
129,translate lingua franca on github,languages,"
hello there,
i just want to ask how can i translate lingua franca into galician language on github. i am not a professional of this so i ask for help here. i f you could give some simple instructions of how to do it and how to put folders with this new language, i will be very grateful.
","
is galician that different from portuguese that you can’t use googlecloud stt gl-es, but running mycroft/google tts language pt/pt-pt? is such mix even possible?
if this is a “off the grid” kind of direction, oh boy. hopefully got some vacation time left. i guess you have to expand on the purpose of this endeavour.
"
130,mycroft roadmap,mycroft project,"
hey all, the old mycroft roadmaps that were linked from our docs had become extremely outdated so recently we did some work to map out what we think the current priorities for mycroft are.


docs.google.com



mycroft ai: master roadmap 2020
mycroft ai: master roadmap 2020  current priority: mark ii production key milestones: ship mark ii dev kit all hardware tested and operational alpha software available finalize gui api beta software released for testing manufactured mark ii...






the aim is for the new master roadmap to always start with the highest priority project followed by individual roadmaps for secondary priorities.
each of these are collaborative google docs and anyone can comment on them. so let us know what you think!
sidenote: did you know that the plural of priority didn’t really exist until ~1940.
but enough history, what do you think about these priorities?
what’s missing, and what could be bumped down?
",
131,privacy preserving ai andrew trask mit deep learning series,machine learning,"

",
132,mark ii update phase 2 build video,general discussion,"
originally published at:			http://mycroft.ai/blog/mark-ii-update-phase-2-build-video/
there have been many requests to show the making of the new rockchip based prototypes. so we put together a quick montage showing some assembly of the proof of concept prototype and the phase 2 prototype. the proof of concept utilizes a combination of laser cut panels and 3d printed parts, and the phase 2 prototype is almost entirely 3d printed. in the video that sam (our multi-talented designer) put together you can see our production guru, darren courtney, assembling the prototypes.
these prototypes are optimized for our tools, so they are designed to work with a formlabs sla printer and an epilog laser cutter. our goal is to do a quick revision that is better suited for fdm printing. the fdm printed version should easily fit on a printer with an 8” x 8” build platform.
as a preview of the upcoming documentation here is how we wired the phase 2 prototype. once we validate a few other systems we will begin sharing more documentation on github including the 3d files and parts list so that others can recreate our prototypes. stay tuned for more updates.
","
just in case people are looking for the video…


have you been able to recoup any of your “investment” in the hardware that you had to abandon?
in other words, did you get any money back from the vendors that failed to perform?

yep, that has all been taken care of, now it’s full steam ahead on the mark iir 

yes can happen some stop, but now there shall be a new due date, to regain credibility.
a date that you won’t miss.

and, please: this time avoid to bomb us of “fantastic” and thrilling announcements. just facts and percentages to goal completion. please let me (and all that asked for refound) have confidence in you again. by the way i did not asked for refound, but now i claim some humble and sound work by your side.




 derick-mycroft:

our goal is to do a quick revision that is better suited for fdm printing. the fdm printed version should easily fit on a printer with an 8” x 8” build platform.


i got a respeaker core v2 and a fdm printer waiting for some work… can you provide the stl for the mark-iir prototype, please?

hello, can you send me the mark ii stl for corev2 respeaker please?

check https://github.com/mycroftai/hardware-mycroft-mark-ii

are you talking about the mark iir from phase ii?
@derick-mycroft provided the stls for respeaker corev2 to me, as i don’t know if it is o.k. if i forward them i think it is best you contact him directly…
"
133,picroft an error occurred while processing a request in mozilla iot gateway,support,"
hello i am new to mycroft and am enjoying it but i am not sure what is wrong with the webthings skill. i am able to control the hue when i am in the webthings gateway page, but i get the error “an error occurred while processing a request in mozilla iot gateway” even after rebooting both the webthings (in docker container) and the entire raspberry pi/picroft. i did the auto setup and the skill says it worked and i see the authorization token for the pi in webthings.
here are the relevant logs for the picroft skill.log:
2020-06-23 18:45:18.230 | info     |   720 | mycroft.skills.skill_updater:_log_next_download_time:248 | next scheduled skill update: 2020-06-23 19:45:18.223656
2020-06-23 18:45:18.236 | info     |   720 | mycroft.skills.skill_updater:update_skills:163 | skill update complete
2020-06-23 18:45:26.934 | error    |   720 | mycroft.skills.mycroft_skill.mycroft_skill:on_error:835 | an error occurred while processing a request in mozilla iot gateway
traceback (most recent call last):
file “/home/pi/mycroft-core/mycroft/skills/mycroft_skill/event_container.py”, line 66, in wrapper
handler(message)
file “/opt/mycroft/skills/mozilla-webthings-gateway-skill.mozilla-iot/init.py”, line 71, in handle_command_intent
data = response.json()
file “/home/pi/mycroft-core/.venv/lib/python3.7/site-packages/requests/models.py”, line 897, in json
return complexjson.loads(self.text, **kwargs)
file “/usr/lib/python3.7/json/init.py”, line 348, in loads
return _default_decoder.decode(s)
file “/usr/lib/python3.7/json/decoder.py”, line 337, in decode
obj, end = self.raw_decode(s, idx=_w(s, 0).end())
file “/usr/lib/python3.7/json/decoder.py”, line 355, in raw_decode
raise jsondecodeerror(“expecting value”, s, err.value) from none
json.decoder.jsondecodeerror: expecting value: line 1 column 1 (char 0)
2020-06-23 18:55:04.495 | info     |   720 | mycroft.skills.settings:_emit_settings_change_events:411 | emitting skill.settings.change event for skill dismissal-skill|20.02
2020-06-23 18:55:04.500 | info     |   720 | mycroft.skills.settings:_emit_settings_change_events:411 | emitting skill.settings.change event for skill telegram|20.02
2020-06-23 18:55:04.506 | info     |   720 | mycroft.skills.settings:_emit_settings_change_events:411 | emitting skill.settings.change event for skill laugh
2020-06-23 18:55:07.563 | info     |   720 | mycroft.skills.mycroft_skill.mycroft_skill:handle_settings_change:308 | updating settings for skill dismissalskill
2020-06-23 18:55:07.730 | info     |   720 | mycroft.skills.settings:save_settings:111 | skill settings successfully saved to /home/pi/.config/mycroft/skills/dismissal-skill.chancencounter/settings.json
2020-06-23 18:55:08.715 | info     |   720 | mycroft.skills.mycroft_skill.mycroft_skill:handle_settings_change:308 | updating settings for skill laughskill
2020-06-23 18:55:08.792 | info     |   720 | mycroft.skills.settings:save_settings:111 | skill settings successfully saved to /home/pi/.config/mycroft/skills/laugh.jarbasal/settings.json
2020-06-23 18:55:08.927 | info     |   720 | mycroft.skills.mycroft_skill.mycroft_skill:handle_settings_change:308 | updating settings for skill telegramskill
2020-06-23 18:55:08.937 | info     |   720 | mycroft.skills.settings:save_settings:111 | skill settings successfully saved to /home/pi/.config/mycroft/skills/telegram.luke5sky/settings.json
2020-06-23 18:55:55.316 | error    |   720 | mycroft.skills.mycroft_skill.mycroft_skill:on_error:835 | an error occurred while processing a request in mozilla iot gateway
traceback (most recent call last):
file “/home/pi/mycroft-core/mycroft/skills/mycroft_skill/event_container.py”, line 66, in wrapper
handler(message)
file “/opt/mycroft/skills/mozilla-webthings-gateway-skill.mozilla-iot/init.py”, line 71, in handle_command_intent
data = response.json()
file “/home/pi/mycroft-core/.venv/lib/python3.7/site-packages/requests/models.py”, line 897, in json
return complexjson.loads(self.text, **kwargs)
file “/usr/lib/python3.7/json/init.py”, line 348, in loads
return _default_decoder.decode(s)
file “/usr/lib/python3.7/json/decoder.py”, line 337, in decode
obj, end = self.raw_decode(s, idx=_w(s, 0).end())
file “/usr/lib/python3.7/json/decoder.py”, line 355, in raw_decode
raise jsondecodeerror(“expecting value”, s, err.value) from none
json.decoder.jsondecodeerror: expecting value: line 1 column 1 (char 0)
2020-06-23 19:00:19.921 | error    |   720 | mycroft.skills.mycroft_skill.mycroft_skill:on_error:835 | an error occurred while processing a request in mozilla iot gateway
traceback (most recent call last):
file “/home/pi/mycroft-core/mycroft/skills/mycroft_skill/event_container.py”, line 66, in wrapper
handler(message)
file “/opt/mycroft/skills/mozilla-webthings-gateway-skill.mozilla-iot/init.py”, line 71, in handle_command_intent
data = response.json()
file “/home/pi/mycroft-core/.venv/lib/python3.7/site-packages/requests/models.py”, line 897, in json
return complexjson.loads(self.text, **kwargs)
file “/usr/lib/python3.7/json/init.py”, line 348, in loads
return _default_decoder.decode(s)
file “/usr/lib/python3.7/json/decoder.py”, line 337, in decode
obj, end = self.raw_decode(s, idx=_w(s, 0).end())
file “/usr/lib/python3.7/json/decoder.py”, line 355, in raw_decode
raise jsondecodeerror(“expecting value”, s, err.value) from none
json.decoder.jsondecodeerror: expecting value: line 1 column 1 (char 0)
the logs for the webthings only have status updates let me know if you want those as well.
thank you!
update
so i uninstalled the webthings gateway and tried to reinstall it but it is saying its not found when using msm command line and with voice. i think the first time i installed it i pulled it down from github. does webthings gateway not work with picroft even though it says it does on the skill page?
","
you’d probably have to use git to pull it down again if that’s how you did it the first time and it doesn’t install via msm or voice request.
the jsondecode error sounds like there’s a misconfiguration of something.

does the skill settings page auto configure not work?

hey snukkums,
the way they built the skill forwarded all intent parsing to the mozilla webthings gateway however they removed the intent parsing module in v0.12.0 so any gateway running v0.12.x or higher won’t run properly.
i thought we’d removed the skill from the marketplace so will look into that and get it taken down.
there are some community members looking at building a new skill that uses mycroft’s intent parsing and interacts with the new webthings api, but it’s not ready yet. if you’re interested in helping out let us know and i can connect you.

i would be willing to test it out for sure. i am limited on my programming ability and haven’t built a skill yet so i don’t know how helpful i would be other than testing it.
thank you for your response as well!

i am interested in connecting with community members building a new skill for webthings api.

hey there, welcome to mycroft!
we’ve had another community member james start to tackle this too. not sure that he’s on the forums but you can find the skill at:



github



jamesmf/mycroft-mozilla-iot-skill
commoniot implementation for interacting with mozilla iot gateway from mycroft - jamesmf/mycroft-mozilla-iot-skill






to test it out, you will need to first install the iot control skill:
mycroft-msm install https://github.com/mycroftai/skill-iot-control

this requires python3.6+
"
134,mycroft raspberry pi generic usb webcam from amazon,none,"
hey guys. just discovered this project and trying to get it going. i’m installing mycroft on a pi4 running manjaro.
the pi pics up the microphone in the os but doesn’t work with mycroft. the microphone level just says 0. any ideas
","
hello @marcoby and welcome to the forums.
as i can understand, you have a webcam and yet it is detected -and working?- on the os, mycroft mic level is at 0. isn’t it?
please take a look into documentation, there is a comprehensive guide to audio troubleshooting. and tell us back if you still have problems.

it worked out. simple mistake. i was connecting audio through 3.5 speakers but the audio was outputting on a different connection that was off.
"
135,mycroft defeats patent trolls again for now,mycroft project,"
mycroft is excited to announce that the company’s legal team beat the patent troll.  again.  for now.  the federal court of the western district of missouri dismissed the troll’s case against mycroft on the grounds that the troll didn’t specify which features of mycroft’s technology infringed on which claims of the patent.
see the whole story at:



mycroft – 15 oct 20



mycroft defeats patent trolls…again…for now - mycroft
mycroft is excited to announce that the company’s legal team beat the patent troll.  again.  for now.  the federal court of the western district of






","
make’em bit the dust! law should be very punitive with those patent trolls, and make them pay with all their personal wages
"
136,picroft with i2s audio,mycroft project,"
hi there,
i’m planning on building a picroft powered “smart radio” using an old radio. i’d like to output the sound using an i2s dac/amplifier combo like this one:  https://www.adafruit.com/product/3006
has anybody used something like this before? 
all the best,
moritz
","
that sounds like a fun project and could be made 
i only have experiance whith google voicehat, but if the adafruit thing is working on the pi i cant see why it couldnt work on a pi whith mycroft installed.
the picroft image is basicly a raspbian strech installation whith mycroft preeinstalled and a script helping you to get started.
so i would revomend installing picroftimage, and then the adafruit and get that to work by changeing relevant alsa and config settings when you get sound out and in by testng whith aplay and arecord mycroft should work to.

hi @schmolmo,
i haven’t used that exact breakout board before but i have used an i2c dac with picroft and hooked that to a separate amplifier. i did consider using the one you mentioned but i already had this other one on hand. the small purple board in top left of the attached image is the i2c dac that i used. from memory it doesn’t take too much configuration to get it going.

img_20180202_0540261000×821 254 kb


great, i’ll try that. thanks!

hey there,
i’ve just started messing around with the i2s chip and the ps3 eye mic and can’t get it working 
i’ve set the i2s up using `this tutorial. the soudn playback in the setup wizard is working, but when i get to the mic test, i receive this:
playing wave ‘/tmp/test.wav’ : signed 16 bit little endian, rate 16000 hz, mono
aplay: set_params:1305: channels count non available
an error occured while playing back audio (1)
any ideas?
all the best, schmolmo

i’ve had similar problems, that i’ve yet to solve. if i attempt to record with arecord with format of s16_le i get a wav file, but the contents are all 0. if i use arecord but with a format of s32_le i get good sound data. i notice that doing the mycroft_mic_test the audio file (which has the right number of bytes but they’re all null bytes) the format is s16_le which might be the problem i’ve encountered using the adafruit i2s microphone.
i don’t know if there is anyway to tell it to use the s32_le format for recording, but if there’s another solution someone has found i’m happy to try it.

@schmolmo - it’s possible to set the listener channels manually using mycroft.conf
here’s a fairly long example configuration but roughly half way down you’ll see the “listener” attributes can be set with:
""listener"": {
    ""channels"": 1
}


it’s easy to set the format of playback via mycroft.conf but i don’t think we have support for different recording formats. let me see what i can dig up…
so, have talked to ake and he’s confirmed that the mic format comes directly from the microphone base class in the python package we use. it should be possible to override this, however no one has tried this so big disclaimer - editing source code can break things…
if you wanted to do this and don’t mind breaking mycroft if it doesn’t work, try adding the following after the init method of that super class:
self.format = self.pyaudio_module.paint32
self.sample_width = self.pyaudio_module.get_sample_size(self.format) # size of each sample


thanks for the reply, we’ve decided to use a different audio approach and it’s working much better. we’re using the seeestudio mic array and it’s much better than the previous i2s. recently got the websocket monitoring the mycroft messages to integrate the light ring on the mic array card and it works and looks great.
keep up the good work!!

i’m working on embedding picroft in an animatronic, so i added the same i2s amp as the op (https://www.adafruit.com/product/3006) to my rpi 3b+. i also added a usb mic. i can’t get pulseaudio to play nicely.
alsa (aplay -l) shows:
(.venv) pi@picroft:~ $ aplay -l
**** list of playback hardware devices ****
card 0: sndrpihifiberry [snd_rpi_hifiberry_dac], device 0: hifiberry dac hifi pcm5102a-hifi-0 []
subdevices: 0/1
subdevice #0: subdevice #0
but pactl info shows:
(.venv) pi@picroft:~ $ pactl info
server string: /run/user/1000/pulse/native
library protocol version: 32
server protocol version: 32
is local: yes
client index: 29
tile size: 65496
user name: pi
host name: picroft
server name: pulseaudio
server version: 10.0
default sample specification: s16le 2ch 44100hz
default channel map: front-left,front-right
default sink: auto_null
default source: alsa_input.usb-c-media_electronics_inc._usb_pnp_sound_device-00.analog-mono
cookie: b116:decf
i’ve been through all of the config files dozens of times, reloaded alsa and pulseaudio, etc. i’m missing something simple. if i turn off pulseaudio, i can force output through the speaker, but then the usb mic doesn’t work.
any assistance would be appreciated.

does aplay successfully play audio? if so you can set the play commands in mycroft with:
mycroft-config set play_wav_cmdline ""aplay %1""
mycroft-config set play_mp3_cmdline ""mpg123 %1""

alternatively, do any sinks show up if you run:
pactl list sinks short

if so you can set the default sink with:
pactl set-default-sink 1

replacing 1 with the relevant number

“aplay” works only if i kill pulseaudio.
“pactl list sinks short” replies with:
0  auto_null    module-null-sink.c    s16le 2ch 44100hz  suspended
alsa works. pulseaudio runs, but can’t grab the sink.
i wonder if it’s a permissions issue?

fixed. appears to have been a permissions issue that was exacerbated by running updates on my own.

awesome, glad you worked it out 

hi there
i’m facing the same issue described in this topic, except that my hardware configuration is different (maybe i should open a new topic …)
mycroft is running on a nvidia jetson nano with two i2s mems microphone (adafruit sph0645) . i’m able to record and play sounds on both channels with arecord/aplay with the  s32_le format used by the i2s mics.
when doing some testing with mycroft-mic-test, the sound file is encoded in 16 bits format, and is empty :
$ aplay /tmp/test.wav
playing wave ‘/tmp/test.wav’ : signed 16 bit little endian, rate 16000 hz, mono
i tried to modify the mic.py code as suggested by @gez-mycroft :
def __init__(self, device_index=none, sample_rate=16000, chunk_size=1024,
             mute=false):

    microphone.__init__(self, device_index=device_index,
                        sample_rate=sample_rate, chunk_size=chunk_size)

    self.format = self.pyaudio_module.paint32
    self.sample_width = self.pyaudio_module.get_sample_size(self.format) # size of each sample
   
    self.muted = false
    if mute:
        self.mute()

and i ran mycroft-mic-test again.
the result file is now encoded in 32 bits :
$ aplay /tmp/test.wav
playing wave ‘/tmp/test.wav’ : signed 32 bit little endian, rate 16000 hz, mono
but the sound file is still empty.
do you have an idea on what is missing ?
thank you for your answers
stephane

good news, i’ve added the line
default-sample-format = s32le
in the /etc/pulse/daemon.conf file and now mycroft is using the correct format, without having to modify the mic.py file !
i can now use the adafruit sph0645 mems with mycroft.
stephane
"
137,customized speech models for deepspeech,machine learning,"
i’ve been fiddling with deepspeech a bunch of late, trying to improve its accuracy when it listens to me.
tl;dr: fine-tune the mozilla model instead of creating your own.
if this is in your field of interest,  start with this post over on the mozilla discourse:



mozilla discourse – 1 dec 17



tutorial : how i trained a specific french model to control my robot
tutorial how to build your homemade deepspeech model from scratch adapt links and params with your needs…  for my robotic project, i needed to create a small monospeaker model, with nearly 1000 sentences orders (not just single word !)  i recorded...
reading time: 15 mins 🕑
likes: 37 ❤





an extraneous list of steps i used is below.  more than a few are duplicated from the above. this isn’t definitive, and will certainly need to be redone over time.  you do not need massive resources to compile a small (<10k clips) model, but the larger your gpu the better.  you will need deepspeech, the native_client, kenlm, and python3 installed.  i used a lot of bash for loops to do the majority of the bulk processing steps.
first, i built up my library of clips.  mimic recording studio can be used to do this.  i had recorded approximately 800 source clips using a combination of common commands i say to mycroft, the top 500 words in the english language, and several papers from arvix on computing topics.  these were recorded in 16 bit, 48khz stereo. start with the best quality you can, it’s easier to make that worse than try and fix bad source material.  if you have saved clips from mycroft, or can easily record noisy or bad voice quality clips, then you should do so.  described here is a way to augment your data with lower-quality clips.  i have a pair of small diaphragm condensers connected to a usb audio interface.  one mic channel is set for -10db and oriented in approximately 90 degrees from the other in order to make for a slightly different recording on each channel.  i used a short shell script to record a sentence twice, and write out the filename and the transcription to a csv file.  in the csv file you make, it is vitally important that you limit the amount of odd characters, punctuation, and the like.  also helps to run it through an upper to lower step as well.
super annoying recording script:
#!/bin/bash

if [[ $# -lt 1 ]] ; then
  echo ""usage: $0 sentencesfile""
fi

if [[ $# -eq 2 ]] ; then
  sec=$2
else
  sec=5
fi
red='\033[0;31m'
nc='\033[0m'
blue='\033[0;34m'
yellow='\033[1;33m'
green='\033[0;32m'
c1=1
ds=$(date +%s)
sfn=$(echo $1 | tr -s '\057' '\012' | tail -1)
echo $sfn

if [[ ! -d tmp ]]; then
  mkdir tmp
fi

record() {

for i in 1 2  ; do
  echo -e ""${nc} 2....""
  sleep 1
  echo -e ""${blue} 1....""
  sleep 1
  echo -e ""${red} recording for $sec seconds!""
  iterationname=""${ds}-${c1}-${i}""
  #echo -e ""${red} $iterationname""
  arecord -f dat -d $sec tmp/${iterationname}.wav  
  echo -e ""${green} done recording. ${yellow} ""
  echo ""${iterationname},${line}"" >> tmp/metadata.csv
done

}

echo ""reading lines from file $1""
sleep 2

while read line ; do
  echo -e ""${red}___________________""

  echo -e ""${green}current sentence: ""
  echo "" *************** ""
  echo -e "" ** ${yellow}${line} ""
  echo -e ""${green} *************** ""
  echo -e "" line $c1${nc}""
  record 
  c1=$((c1 + 1))
done < $1
echo -e ""${nc}done!""

after recording, i used webrtcvad to trim the silence (https://github.com/wiseman/py-webrtcvad/blob/master/example.py):
python3 example.py 1 yourwavefilehere.wav
this sometimes results in two wave files emerging, usually just one if you’re recorded cleanly enough. you should watch for multiples and pick the correct one as needed.  i then used sox (http://sox.sourceforge.net/) to split the files into left and right channel wavs at 16 bit,16khz mono.
$ sox $i l-$i remix 1
$ sox $i r-$i remix 2
additionally, i recorded a short clip of background noise in my house (hey it’s where i’ll use this most).  with one channel’s wav files, i combined the noise and saved those as an additional set of clips.
$ sox -m $j noise.wav n-$j.wav trim 0 $(soxi -d $j)
after combining, i used the chorus function of sox to make the quality of the clips slightly worse for another set of clips (play with the values to make it work for you).
$ sox $k c-$k chorus .8 .1 25 .4 .1 -t
from the other channel, i recorded a fifth set of clips with bad quality (without extra noise).  now i have left, right, noisy left, noisy bad quality left, and bad quality right.  if you were so motivated, you could also throw in some speed variations on these for good measure.
on the training machine (local or cloud), i made a directory (/opt/voice/dsmodel/), and sub-directories within for wavs, test, dev, and train.  i placed all my wav files into the wavs directory.  the csv file full of names and transcriptions in the dsmodel dir.  for bonus points, you can run the csv file through shuf to randomize the clip order.  for deepspeech, you want to put 10% into test, 20% into dev, and the remainder into train.  ds wants a particular format for the csv, so i passed each line through a short script to get the filename, find the size of the file in question, and write the file name, size, and transcription to the relevant csv (train.csv/test.csv/dev.csv, each in their respective directory).  you end up with something like:
wav_filename,wav_size,transcript
c-r-1550042834-10-1.wav,86444,we are going to turn before that bridge
c-r-1550042834-13-2.wav,78764,four five six seven eight
c-r-1550042834-15-2.wav,69164,nine ten eleven twelve
c-r-1550042834-17-2.wav,88364,thirteen fourteen fifteen sixteen
c-r-1550042834-20-2.wav,69164,twenty thirty forty fifty

don’t knock my sample sentences til you try 'em.    the header line is necessary in each csv.
to make the alphabet.txt file, you’ll want to grab all the transcriptions and sort into unique characters.
cut -d, -f3 test/test.csv >> charlist ;cut -d, -f3 train/train.csv >> charlist; cut -d, -f3 dev/dev.csv >> charlist
i found the charparse.cpp* file on the web, and can’t find the source now, will edit if i do.  compile that (g++ -o charparse charparse.cpp), then you can do:
charparse < charlist >alphabet.txt
important:
review your alphabet.txt and make sure it only has letters and minimal punctuation (i have period and apostophe, then lower case a-z. end on a blank line. 29 lines total for mine, you should be similar.  non-latin character sets will certainly differ.
now we build files to model with. not going to pretend i know what each of these is for, feel free to look up yourself. start in your dsmodel directory and run:
$ lmplz --text vocabulary.txt --arpa words.arpa --o 3
$ build_binary -t -s words.arpa lm.binary
$ generate_trie alphabet.txt lm.binary trie
in your deepspeech folder (i used /opt/deepspeech), edit your run file.  here’s mine for fun:
#!/bin/sh
set -xe
if [ ! -f deepspeech.py ]; then
    echo ""please make sure you run this from deepspeech's top level directory.""
    exit 1
fi;

python3 -u deepspeech.py \
  --train_files /opt/voice/dsmodel/train/train.csv \
  --dev_files /opt/voice/dsmodel/dev/dev.csv \
  --test_files /opt/voice/dsmodel/test/test.csv \
  --train_batch_size 48 \
  --dev_batch_size 40 \
  --test_batch_size 40 \
  --n_hidden 1024 \
  --epoch 64 \
  --validation_step 1 \
  --early_stop true \
  --earlystop_nsteps 6 \
  --estop_mean_thresh 0.1 \
  --estop_std_thresh 0.1 \
  --dropout_rate 0.30 \
  --default_stddev 0.046875 \
  --learning_rate 0.0005 \
  --report_count 100 \
  --use_seq_length false \
  --export_dir /opt/voice/dsmodel/results/model_export/ \
  --checkpoint_dir /opt/voice/dsmodel/results/checkout/ \
  --decoder_library_path /opt/deepspeech/native_client/libctc_decoder_with_kenlm.so \
  --alphabet_config_path /opt/voice/dsmodel/alphabet.txt \
  --lm_binary_path /opt/voice/dsmodel/lm.binary \
  --lm_trie_path /opt/voice/dsmodel/trie \
  ""$@""

two things to keep in mind here are batch_size and n_hidden.   batch_size is basically scaling how much of the data to load per training step.  i have an 8gb gpu, and on my dataset this worked.  on yours it might be bigger or smaller.  more data is usually smaller.  try and keep n_hidden even.  the larger you can scale n_hidden, the better your model may be. try powers of 2 type numbers (256, 512,1024, etc).  higher can be better.  if you don’t keep batch_size even, you may experience a tiresome warning on inference later.  the early stop parameters are there to help prevent overfitting.  i’d recommend keeping them on for any small training set.   the learning rate, dropout rate, stddev bits you can use if need be, review after first model completes.
after all of that…start a screen session and run your script.  in another screen session, set up tensorboard on the output directory.  switch back to your training script and see what error it’s popped up. it’s fairly good about indicating what it’s working on when it errors, ie, it parses the csv files and will indicate what character it doesn’t like in them.  fix anything that comes up, and try again.
depending on your data’s size and your compute resources, go to sleep for the night or check back in ten minutes.
mycroft@trainer:~/deepspeech$ ./run-me.sh 
+ [ ! -f deepspeech.py ]
+ python3 -u deepspeech.py --train_files /opt/voice/dsmodel/train/train.csv --dev_files /opt/voice/dsmodel/dev/dev.csv --test_files /opt/voice/dsmodel/test/test.csv --train_batch_size 48 --dev_batch_size 40 --test_batch_size 40 --n_hidden 1024 --epoch 64 --validation_step 1 --early_stop true --earlystop_nsteps 6 --estop_mean_thresh 0.05 --estop_std_thresh 0.05 --dropout_rate 0.30 --default_stddev 0.046875 --learning_rate 0.0005 --report_count 100 --use_seq_length false --export_dir /opt/voice/dsmodel/results/model_export/ --checkpoint_dir /opt/voice/dsmodel/results/checkout/ --decoder_library_path /opt/deepspeech/native_client/libctc_decoder_with_kenlm.so --alphabet_config_path /opt/voice/dsmodel/alphabet.txt --lm_binary_path /opt/voice/dsmodel/lm.binary --lm_trie_path /opt/voice/dsmodel/trie
preprocessing ['/opt/voice/dsmodel/train/train.csv']
preprocessing done
preprocessing ['/opt/voice/dsmodel/dev/dev.csv']
preprocessing done
i starting optimization
i training epoch 0...
 15% (7 of 46) |########################################                                                                                                                                                                                                                                | elapsed time: 0:00:04 eta:   0:00:29

each epoch took about a minute for me, validation on each epoch about one-tenth that.
pull up tensorboard from the training machine and watch it make cool graphs.
and after modeling finished, it looks pretty good:
--------------------------------------------------------------------------------
wer: 0.000000, cer: 0.000000, loss: 0.003964
 - src: ""wiki search pineapples""
 - res: ""wiki search pineapples""
--------------------------------------------------------------------------------
wer: 0.000000, cer: 0.000000, loss: 0.004020
 - src: ""the quick brown fox jumped over the lazy dog""
 - res: ""the quick brown fox jumped over the lazy dog""
--------------------------------------------------------------------------------
wer: 0.000000, cer: 0.000000, loss: 0.004198
 - src: ""sum three and two""
 - res: ""sum three and two""
--------------------------------------------------------------------------------
wer: 0.000000, cer: 0.000000, loss: 0.004204
 - src: ""a large fawn jumped quickly over white zinc boxes""
 - res: ""a large fawn jumped quickly over white zinc boxes""
--------------------------------------------------------------------------------
i exporting the model...
i models exported at /opt/voice/dsmodel/results/model_export/

from here, you can copy your model to your deepspeech server host, and start doing asr to your voice’s content.
to make an mmapped model (from https://github.com/mozilla/deepspeech):
$ convert_graphdef_memmapped_format --in_graph=output_graph.pb --out_graph=output_graph.pbmm

so how does it work? eh…depends.  largely due to my limited training set, it can work on those lines pretty well. anything beyond that it tends to get way off course.
“could you tell me the weather in san francisco” resulted in…
initialize: initialize(model='/opt/voice/models/output_graph.pb', alphabet='/opt/voice/models/alphabet.txt', lm='/opt/voice/models/lm.binary', trie='/opt/voice/models/trie')
creating model /opt/voice/models/output_graph.pb /opt/voice/models/alphabet.txt...
tensorflow: v1.12.0-14-g943a6c3
deepspeech: v0.5.0-alpha.1-67-g604c015
warning: reading entire model file into memory. transform model file into an mmapped graph to reduce heap usage.
model is ready.
stt result: i'm able girls able ship water hallway best surface

charparse.cpp
#include <iostream>
#include <set>

int main() {
    std::set<char> seen_chars;
    std::set<char>::const_iterator iter;
    char ch;

    /* ignore whitespace and case */
    while ( std::cin.get(ch) ) {
        if (! isspace(ch) ) {
            seen_chars.insert(tolower(ch));
        }
    }

    for( iter = seen_chars.begin(); iter != seen_chars.end(); ++iter ) {
        std::cout << *iter << std::endl;
    }

    return 0;
}

","
hey baconator, really interesting to hear your experience with it, thanks for the detailed write up.
it’s also an excellent warning for others who are considered training their own model about what to expect, and how much work goes into it. are you planning to continue building up your training data set and retrain, or was this more of an experiment in understanding how the whole process works?
i think most importantly, we need to keep shipping water down the best surface of that hallway! 

fine-tuning is the definite way to go for any non-specific, non-size restricted command set.   the common voice 2.0 dataset would allow someone to try training such a thing, but also requires some massive resources (4xgtx teslas or more).
i saw a branch of ds where there’s additional hidden layers to improve recognition, which might be interesting to pursue at some point, particularly if it works better with smaller datasets.
once i get fine tuning down a bit i’ll try and add the differences here for good measure.

fine-tuning and you: standing on the shoulders of giants.
the deepspeech repo’s readme (https://github.com/mozilla/deepspeech#continuing-training-from-a-release-model) pretty much covers this.   download the relevant pre-trained model (1.5gb or so compressed–you will want to run this on an ssd for sure).   verify that your transcription character set (alphabet.txt) matches the one included in the model.  if not, you will have to adjust your transcriptions or you’ll run into problems.  as per the readme, you can then just point the following at your csv’s you created earlier:
python3 deepspeech.py --n_hidden 2048 --checkpoint_dir path/to/checkpoint/folder --epoch -3 --train_files path/to/my-train.csv --dev_files path/to/my-dev.csv --test_files path/to/my_dev.csv --learning_rate 0.0001   --export_dir /opt/deepspeech/results/model_export/ 

the epoch -3 means three more epochs.  this appears to skip validation between epochs as well.
each step takes significantly longer to train (23 minutes vs. 55 seconds).  this is run on a gtx1070 backed by a ryzen 1600 and 32gb of ram on ssd’s.

preprocessing ['/opt/deepspeech/train/my-train.csv']
preprocessing done
preprocessing ['/opt/deepspeech/dev/my-dev.csv']
preprocessing done
w parameter --validation_step needs to be >0 for early stopping to work
i starting optimization
i training epoch 0...
 99% (3584 of 3615) |################################################################################################################################################################################################################################################################   | elapsed time: 0:22:32 eta:   0:00:24

that’s…it.  oh yeah, make your resulting model mmapable (see first post) for good measure.  the resulting model is much larger than the customized ones i’ve created, but also much improved on wer and generalized recognition.

minor update. deepspeech has added data augmentation to their .6 release.  see the main repo for more, but you can skip altering your own clips with that functionality now.

hello!
i need some help with creating the csv files, if you can  share the script that helped you creating the csv… i have around 1500 .wav and i wanted to create the csv manually but it is a real struggle
thank you


for i in $(ls *.wav); do
fs=$(stat --printf=""%s"" $i)
ts=$(cat $i.txt)
echo “$i,$fs,$ts” >> corpus.csv
done

this assumes the transcript is in a wavfile.wav.txt file.   other locations are left as an exercise for the reader.

hello, can you provide me a contact info (email), please help me, i need to ask you some questions please

better to post them here, more people who have knowledge can answer them.

actually i need to build an ai model for deepspeech for a less used language. can you please advice me on how to proceed the faster way. can i used a pre trained model from deepspeech and train it with my language or will i have to proceed like you did?
i already have all the wavs files for my language though.

i think you are looking for “transfer learning”. for that specific question i suggest to consult the mozilla deepspeech forum
"
138,call for testers mycroft with pending parser formatter refactor,general discussion,"
this is coming to mycroft, unless something goes terribly wrong and the core devs object strongly. it’s not hypothetical 
also, and importantly, i don’t work for mycroftai. i’m a collaborator on lingua franca, and the person responsible for a big refactor that’s finally happening.gif
lingua franca is mycroft’s algorithmic nlp library (as opposed to ml.) most of the parsing, formatting, and time-related functions that used to live in mycroft.util have moved, or are moving, to that package (though you can still import them via mycroft.util for backwards compatibility.)
jarbas has standardized the names and locations of various functions and data, and i’ve taken those naming conventions and used them to implement dynamic function loading. this means, among other things, that you’ll no longer be loading all the functions for every language into memory, just the ones for the languages you’re using.
now that it’s basically done, we need testers! i’ve got an early, lazy integration at my repository, on this branch.
you can test it without cloning my fork by adding me as a remote and checking the branch out, like so (from your mycroft-core directory):
git remote add chance https://github.com/chancencounter/mycroft-core
git fetch chance
git checkout chance/lf-refactor
mycroft-pip uninstall lingua_franca
./dev_setup.sh

and then starting mycroft. please report any issues you find 1) here, 2) in chat, channel ~languages, or 3) at the relevant pr on github.
known issues:
* datetime skill has problems with certain requests
* weather skill fails, complaining about the number of positional arguments (reason known, issue filed)
known breaking changes for skill devs:

don’t pass lang=none anymore
ideally, don’t pass a language at all unless you have to. that won’t break anything, but mycroft is now responsible for making sure the user’s language is chosen. you can still pass a language if you need it to use a specific language, but this may cause problems until this is addressed. in the meantime, you can ensure that your language is loaded by calling lingua_franca.load_language(<lang_code: str>)


update: the known datetime-related problems have been patched, credit @forslund
","
wanted to give it a try but git checkout chance/lf-refactor gives me following error:

 error: pathspec 'chance/lf-refactor' did not match any file(s) known to git.

don’t know if this matters but i am trying this on my own fork of mycroft-core…

i forgot to have people fetch the remote! i’ll edit the instructions. thanks and sorry.
you’ll have to do git fetch chance and then the checkout should work.

git fetch ...  worked, thanks for quick response…

followed the updated instructions, everything looks to be working.
i am using a git install on a rpi 3b. but i don’t use mycroft alot.

thanks very much for testing!
"
139,default boot volume,mycroft project,"
hi.
i can’t seem to change my default(boot) volume. after a reboot it’s aways too loud and i have to manually set it to 5. and no, i don’t want to use the autovolume skill.
i am using rpi 3 model b + google aiy v1 kit
does anybody have any clues on how to achieve that?
so far i tried and failed using:
(.venv) pi@picroft:~ $ cat $home/.mycroft/mycroft.conf
{
  ""max_allowed_core_version"": 20.8,
  ""volumeskill"": {
    ""default_level"": 5,
    ""min_volume"": 0,
    ""max_volume"": 83
  }
}
","
there are several ways:

(the intended way - yet never used it) edit audio_setup.sh in your base user folder by adding:
amixer set pcm 50% > /dev/null 2>&1
amixer set master 50% > /dev/null 2>&1
my pr approach (where comment 3 has to be factored in)  

change this line in auto_run.sh (base user folder) to your liking (not recommended!)


hi and thanks for the reply.
wouldn’t audio_setup.sh get overridden on updates?

not to my knowledge. there are 2 scripts that should remain untouched, custom_setup.sh and audio_setup.sh
(and if not, this would be considered a bug)

yup, i did some research and you are right. i added the settings, rebooted and it worked. thanks alot!
"
140,common iot framework development video series,mycroft project,"
hey all, @stratus and i are trying a new thing - video tutorials!
the idea started with wanting to go through the common iot framework and figured we may as well record it. which raised the question of why not do a whole video series while we’re at it 
part 1 goes through getting mycroft, setting up a development environment with a specific version of python, and installing the common iot control skill and the homeassistant skill on the commoniot branch.

be great to get any feedback.

what did you think of our first attempt?
will more of these types of videos be useful?
if so - what other topics you’d like to see covered?

",
141,hey its me,general discussion,"
hidee ho. name’s dan. i’ve been lurking for a while now. figured since i just asked a question in the support area, i should probably be a neighbor and say hello as well.
i started on mycroft recently after becoming more interested in data privacy and being generally annoyed with creepy, over-entitled tech companies.
mycroft is super exciting. i jumped on the python bandwagon and even managed to write a cute little skill that reads the astronomy picture of the day description. (which is impressive for an old guy like me.)
my goals are to get my wife off of alexa, teach my kids how to install lineage os, and then solve world hunger.
i like classical guitar, especially leo brouwer. my guilty pleasures include oreos. it’s a pleasure to meet all of you.
salud!
","
hey dan, welcome to the forums. glad you jumped in from the sidelines 
great to hear you’re dipping your toes into skill dev. it’s definitely a gateway, and amazing what you can achieve with even modest python experience.
i am so on board with the oreos, but i’m a classic guy, not so sure about all these new fancy flavours they have going on.
"
142,testing and feedback jellyfin skiil,skill feedback,"
this skill plays music from your jellyfin server. it is a fork of the emby skill.
how to install jellyfin-skill

clone the skill into your skills folder
git clone https://github.com/tuxfoo/jellyfin-skill.git


how to setup and test jellyfin-skill

configure the skill settings in home.mycroft.ai
you can generate a api key from the settings in your jellyfin server
“play song poison” will use the common play framework
“play song poison from jellyfin” will be played using the jellyfin-skill
you can play artists and ablums too
“play artist blackmore’s night”
“play album alive”

where feedback on jellyfin-skill should be directed
general feedback and questons can be placed in this thread.
for feature requests and issues; please use github https://github.com/tuxfoo/jellyfin-skill
","
the interesting thing would be to be able to tap into the live tv m3u tuner.
"
143,fairytalez skill let mycroft tells fairytles,skill feedback,"
how to install fairytalez skill

install fairytalez skill by …

msm install https://github.com/andlo/fairytalez-skill


fairytalez sill connects to fairytalez.com…

this skill do web scraping on www.fairytalez.com to get content. please go to the site if you like fairytales



how to test fairytalez skill
specify the steps the user should take to test the skill, such as;


does it install


can you get any fairytales when asking “hey mycroft - tell a fairytale” ?


does he find the tales you want ? (see list here https://fairytalez.com/fairy-tales/)


can you stop him from telling by stop command or button on mark_1 ?


can you continue the storry by saying “hey mycroft - continue story”?


where feedback on fairytalez skill should be directed a
please give feedback here or through issues on github, or via mycroft chat.
interesting in this skill is

use of beautifulsoup for webscraping

        soup = beautifulsoup(requests.get(url).text,""html.parser"")
        lines = [a.text.strip() for a in soup.find(id=""main"").find_all(""p"")[1:]]
        lines = [l for l in lines if not l.startswith(""{"") and not l.endswith(""}"")]
        return lines


use og match_one from mycroft.util.parse to get best match from users spoken answer against the story index.

 index = self.get_index(""https://fairytalez.com/fairy-tales/"")
        result = match_one(response, list(index.keys()))

","
i would be happy if someone did test this skill so it could go to the market….
just added author info and information about fairytalez.com aswell as put in some smale sleep to get it sound a little more natural.

i will try and look at it this week.




 andlo:

https://github.com/andlo/fairytalez-skill


the skill seems to install correctly, and also will tell a fairy tale.  the problem that i see, in my first 5 minutes of use, is the search.  i think that it should take a title or subject as a variable when asking to tell a fairy tale.   ex. “hey mycroft tell me the fairy tale aladdin”  at this point, it does not understand when you ask that.  it does however understand when i ask it to tell a fairy tale.  this is a good start, and i think it would be a great addition.  thank you for your work.

thanks for input.
you have a good point, and i will look into extending the skill as you describe.




 builderjer:

i think that it should take a title or subject as a variable when asking to tell a fairy tale. ex. “hey mycroft tell me the fairy tale aladdin”


i just added this as described.
you can test by “hey mycroft. install fairytalez beta version”

just installed this version, and indeed it lets me specify a fairy tale.  next issue, if it cannot find the correct one, ex “read me the fairy tale the three bears”, it correctly asks if the closest one found is right, but, it will not take no as an answer.  also, you might let the skill accept the stop command.  currently, while the skill is telling the fairy tale, that’s all it does.  it will not accept me to tell it to stop.

good points 
i see i didnt respect no for an asnwer. that were a mistake, and easyly fixed 
him not respecting the stop i dont understands. it is implemented, and he should stop. but you are right he dosnt.
my code says:
        lines = self.get_story(url)
        for line in lines[bookmark:]:
            if not self.is_reading:
                return
            self.speak(line, wait=false)
            self.settings['bookmark'] += 1
            time.sleep(1)

and the stop is
def stop(self):
        if self.is_reading is true:
            self.is_reading = false
            return true

as far as i understands that says that if variable is_reading is true he will go throu al lines in the story one by one and if it is false, he stops. and he checks after each line.
but i will look into it. thanks for feedback 

i have been dooing lots of work here and some rewrite to make the stop work better.
that includes splitting up the text in smaller bites and be sure that loop is only sending one sentense to the tts at a time.
also fixed problem with mycroft didnt take no for an answer 

i just heard back from the makers of www.fairytalez.com who were happy about the skill. but as he didnt have acces to a mycroft, he couldnt test and try it.
so i will try to make a short video so he can have a look 

nice!! it kinda makes it worth it when the makers of something enjoy when their product is improved upon.  i will try and do some more testing soon.  i am hoping to have it for my granddaughter.  i think it would be great.  the only other issue, that has nothing to do with your skill, but maybe somebody will see, i would like my voice to read it a little bit slower.  is there a way to slow down the speech for a specific skill?  i am using the mycroft mimic2 online voice.

well, i just installed the beta version, and i can get nothing to work with it.
i have tried all of the phrases in the fairytalez.intent file to no avail.  should i go back to the release version?
also, while testing other skills, i noticed that fairytalez responds to every stop command.  maybe all of the skills do, but this one logs it.

huu - i am sorry - i hassnt been testing the beta fully, and maybe something went wrong. i were trying and testing lots of ways to fix the stop problem.
ill look at it as soon as posible.
regarding to speed of speech, there isnt anything i can do in the skill. well i have putin some sleeps between parts, but each line is not posible to slow down. that has to do with mimic2.
regarding to the stop - it is correct that all skill react to it, but only my skill does log - i did that to make sure and to locate if it got the stop. and it does, but there maybe is a problem in the messagequeue, that gives delays, and then the skill dosnt get the stop when it is issued.
it is beeing looked at  by åke-mycroft.




 builderjer:

i am hoping to have it for my granddaughter.


i would realy hope it would be usefull. my 8 yr daughter dosnt understand english, so i cant test it with her  and use google translate for a fairy tale dont give usefull result 
i have fixed the start issue - you can do
(tell|read (me|) a (good|) (story| fairy tale)
(tell|read) (me|) the story {tale}
so this should work again.
“tell a good fariry tale”
“read me the story aladin and the wonderful lamp”
the stop isnt fixed yet, but as earlier written, it isnt the skill - i does handle the stop command.

“if you want your children to be intelligent, read them fairy tales. if you want them to be more intelligent, read them more fairy tales.”
albert einstein

the latest update of mycroft-core do include fix for stop, so the skill now stops when soppused to.
there are some minor fixes to the skill aswell.
to get those updated to the market i need someone other than me saying the skill works 
please try the skill by
“hey mycroft install betaversion fairytalez”
and then when intalled
“hey mycroft tell me the story aladin adn the wonderfull lamp”
and then
“hey mycroft stop”
thanks 

i know its been a while since this thread has been active, but i just wanted to give an update. i’ve just recently built my first picroft and installed this skill. i cannot get it to work at all. i asked if fairytalez is installed, and mycroft verifies it is. any guidance?

i got it to work this morning by asking “tell me a story.” when i asked for a fairytale, it did not work.
i like the skill. it told me the three billy goats gruff. however, it does not find stories very well. i found a title from the website and read it precisely. mycroft even repeated it perfectly when it was looking it up. however, most times it returns with a different result. this is odd. are results being sorted oddly?
thanks




 johng:

i cannot get it to work at all. … “tell me a story.”


a wise man once said…
this is why i strongly advocate to expand the msm manager to voice out something like “skill installed, ask “tell me a story” or …” (pre mk2 if you ask me) and by extension adding to the development framework that every author has to add such feature seperately. it gets fiddly as the library grows.
like that tale
(sorry for hijacking)
"
144,cant see any mic level,mycroft project,"
hi all,
using the headphone jack and a cheapo usb soundcard mic input on a pi 3b.
levels of both have been set with alsamixer.
i set the default input/output with pactl and checked the setup with
mycroft-start audiotest -l
both record and playback work fine.
when using mycroft-cli, i can’t see anything under “mic level”.
i can hear the responses though, if i type questions.
any ideas?
thanks!
","
does it hear you if you ask questions via voice?

no response via voice.
i have followed all the steps here and i can successfully record and play audio with the testing steps. it just doesn’t seem to work in mycroft itself.
"
145,cant pair device,none,"
hey there,
i recently built on linux (raspberry pi)  and for some reason i’m not able to pair the device.
but everything seems to boot up ok,   starting with ./start-mycroft.sh debug
startup seems to go through all neccesary steps, and then i see:


i’m connected to the internet and need to be activated. open your browser and visit  home dot mycroft dot a i to register this device. in the history.


also i get “impossible to update configuration because device isn’t paired”   in the log.
note:  i don’t have audio up and running yet, so i’m just using the cli.
it does seem to respond when i type in something such as “what time is it”
but typing in “pair my device” does nothing.
:skills  shows me that the pairing skill seems to be installed.  not sure how to proceed?
thanks much++
","
did you install picroft or did you install via git?
you probably need to install the default skills.  activate the venv, then try “mycroft-msm default”?

i installed via git
i’m pretty sure all the skills are loaded, i can see in the startup log, where it says, pairing has been loaded successfully.    i can ask what time it is, date, my ip address, etc, (all the skills that do not require pairing, seem to work ok).
anyway, tried as you said,     mycroft-msm default  within an venv, and it did’nt help.
12:56:53.976 | info     |  1145 | mycroft.messagebus.client.client:on_open:114 | connected
12:56:53.982 | info     |  1145 | main:_start_message_bus_client:231 | connected to messagebus
12:56:54.005 | info     |  1145 | mycroft.skills.msm_wrapper:create_msm:90 | acquiring lock to instantiate msm
12:56:54.009 | info     |  1145 | msm.mycroft_skills_manager | building skillentry objects for all skills
12:56:59.635 | info     |  1151 | mycroft.client.speech.hotword_factory:load_module:429 | precise is taking too long to load
12:56:59.638 | info     |  1151 | mycroft.client.speech.hotword_factory:load_module:403 | loading “hey mycroft” wake word via pocketsphinx
12:56:59.990 | info     |  1151 | mycroft.client.speech.listener:create_wakeup_recognizer:365 | creating stand up word engine
12:57:00.022 | info     |  1151 | mycroft.client.speech.hotword_factory:load_module:403 | loading “wake up” wake word via pocketsphinx
12:57:00.160 | info     |  1151 | main:on_ready:175 | speech client is ready.
12:57:00.176 | info     |  1151 | mycroft.messagebus.client.client:on_open:114 | connected
12:57:06.991 | info     |  1151 | mycroft.client.speech.hotword_factory:on_download:254 | downloading precise executable…
12:57:09.882 | info     |  1145 | mycroft.skills.msm_wrapper:create_msm:106 | releasing msm instantiation lock.
12:57:09.886 | info     |  1145 | mycroft.skills.skill_updater:_log_next_download_time:265 | next scheduled skill update: 2020-10-11 13:53:23.607991
12:57:09.892 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-pairing.mycroftai
12:57:09.925 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-pairing.mycroftai/settings.json
12:57:10.023 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-pairing.mycroftai loaded successfully
12:57:10.031 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-volume.mycroftai
12:57:10.077 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-volume.mycroftai/settings.json
12:57:12.014 | info     |  1151 | mycroft.client.speech.hotword_factory:during_download:266 | still downloading executable…
12:57:10.779 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-volume.mycroftai loaded successfully
12:57:19.741 | info     |  1145 | main:_update_system:154 | attempting system update…
12:57:22.772 | info     |  1145 | mycroft.enclosure.display_manager:_write_data:70 | display manager is creating /tmp/mycroft/ipc/managers/disp_info
12:57:22.778 | info     |  1145 | main:_ensure_device_is_paired:133 | device not paired, invoking the pairing skill
12:57:22.783 | info     |  1145 | main:on_ready:177 | skill service is ready.
12:57:22.857 | info     |  1145 | mycroft.skills.skill_manager:_load_on_startup:270 | loading installed skills…
12:57:22.868 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-version-checker.mycroftai
12:57:22.909 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-version-checker.mycroftai/settings.json
12:57:23.035 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-version-checker.mycroftai loaded successfully
12:57:23.042 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-installer.mycroftai
12:57:23.104 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-installer.mycroftai/settings.json
12:57:23.349 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-installer.mycroftai loaded successfully
12:57:23.368 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: fallback-wolfram-alpha.mycroftai
12:57:23.534 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/fallback-wolfram-alpha.mycroftai/settings.json
12:57:23.611 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill fallback-wolfram-alpha.mycroftai loaded successfully
12:57:23.618 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-npr-news.mycroftai
12:57:23.831 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-npr-news.mycroftai/settings.json
12:57:24.932 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-npr-news.mycroftai loaded successfully
12:57:24.937 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-stock.mycroftai
12:57:24.974 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /opt/mycroft/skills/mycroft-stock.mycroftai/settings.json
12:57:24.979 | error    |  1145 | mycroft.skills.skill_loader:_create_skill_instance:271 | skill init failed with exception(‘skill has been disabled by mycroft’)
traceback (most recent call last):
file “/home/pi/mycroft-core/mycroft/skills/skill_loader.py”, line 268, in _create_skill_instance
self.instance = skill_module.create_skill()
file “/opt/mycroft/skills/mycroft-stock.mycroftai/init.py”, line 127, in create_skill
return stockskill()
file “/opt/mycroft/skills/mycroft-stock.mycroftai/init.py”, line 87, in init
raise exception(‘skill has been disabled by mycroft’)
exception: skill has been disabled by mycroft
12:57:24.987 | error    |  1145 | mycroft.skills.skill_loader:_communicate_load_status:327 | skill mycroft-stock.mycroftai failed to load
12:57:24.990 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-playback-control.mycroftai
12:57:25.015 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-playback-control.mycroftai/settings.json
12:57:25.252 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-playback-control.mycroftai loaded successfully
12:57:25.263 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-date-time.mycroftai
12:57:26.669 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-date-time.mycroftai/settings.json
~~~~ |  1145 | timeskill | registering resting screen <bound method timeskill.handle_idle of <mycroft-date-time_mycroftai.timeskill object at 0x74289cd0>> for time and date.
12:57:26.898 | info     |  1145 | timeskill | registering resting screen
12:57:26.924 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-date-time.mycroftai loaded successfully
12:57:26.927 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-joke.mycroftai
12:57:27.024 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-joke.mycroftai/settings.json
12:57:27.156 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-joke.mycroftai loaded successfully
12:57:27.162 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-configuration.mycroftai
12:57:27.194 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-configuration.mycroftai/settings.json
12:57:27.575 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-configuration.mycroftai loaded successfully
12:57:27.577 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: fallback-query.mycroftai
12:57:27.648 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/fallback-query.mycroftai/settings.json
12:57:27.783 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill fallback-query.mycroftai loaded successfully
12:57:27.790 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-hello-world.mycroftai
12:57:27.842 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-hello-world.mycroftai/settings.json
12:57:28.035 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-hello-world.mycroftai loaded successfully
12:57:28.041 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-support-helper.mycroftai
12:57:28.073 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-support-helper.mycroftai/settings.json
12:57:28.172 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-support-helper.mycroftai loaded successfully
12:57:28.188 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-personal.mycroftai
12:57:28.237 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-personal.mycroftai/settings.json
12:57:28.419 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-personal.mycroftai loaded successfully
12:57:28.424 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-ip.mycroftai
12:57:28.550 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-ip.mycroftai/settings.json
12:57:28.649 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-ip.mycroftai loaded successfully
12:57:28.655 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-spelling.mycroftai
12:57:28.704 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-spelling.mycroftai/settings.json
12:57:28.835 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-spelling.mycroftai loaded successfully
12:57:28.843 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-alarm.mycroftai
12:57:28.898 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-alarm.mycroftai/settings.json
removing event mycroft-alarm.mycroftai:nextalarm
12:57:29.331 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-alarm.mycroftai loaded successfully
12:57:29.336 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-wiki.mycroftai
12:57:30.237 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-wiki.mycroftai/settings.json
12:57:30.339 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-wiki.mycroftai loaded successfully
12:57:30.343 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-fallback-duck-duck-go.mycroftai
12:57:30.431 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-fallback-duck-duck-go.mycroftai/settings.json
12:57:30.526 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-fallback-duck-duck-go.mycroftai loaded successfully
12:57:30.529 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-singing.mycroftai
12:57:30.565 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-singing.mycroftai/settings.json
12:57:30.650 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-singing.mycroftai loaded successfully
12:57:30.657 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-naptime.mycroftai
12:57:30.690 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-naptime.mycroftai/settings.json
12:57:30.757 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-naptime.mycroftai loaded successfully
12:57:30.761 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-weather.mycroftai
12:57:32.127 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-weather.mycroftai/settings.json
12:57:41.321 | error    |  1145 | weatherskill | failed to prime weather cache (httperror(’{“error”:“device not authorized”}\n’))
removing event mycroft-weather.mycroftai:precache1
removing event mycroft-weather.mycroftai:precache2
removing event mycroft-weather.mycroftai:precache3
12:57:44.662 | warning  |  1145 | weatherskill | could not prepare forecasts. (httperror(’{“error”:“device not authorized”}\n’))
12:57:44.679 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-weather.mycroftai loaded successfully
12:57:44.683 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: fallback-unknown.mycroftai
12:57:44.716 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/fallback-unknown.mycroftai/settings.json
12:57:44.784 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill fallback-unknown.mycroftai loaded successfully
12:57:44.787 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-timer.mycroftai
12:57:45.130 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-timer.mycroftai/settings.json
12:57:45.659 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-timer.mycroftai loaded successfully
12:57:45.671 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-audio-record.mycroftai
12:57:45.742 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-audio-record.mycroftai/settings.json
12:57:46.055 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-audio-record.mycroftai loaded successfully
12:57:46.063 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-reminder.mycroftai
12:57:46.108 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-reminder.mycroftai/settings.json
removing event mycroft-timer.mycroftai:showtimer
12:57:46.516 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-reminder.mycroftai loaded successfully
12:57:46.527 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-stop.mycroftai
12:57:46.572 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-stop.mycroftai/settings.json
12:57:46.758 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-stop.mycroftai loaded successfully
12:57:46.767 | info     |  1145 | mycroft.skills.skill_loader:load:161 | attempting to load skill: mycroft-speak.mycroftai
12:57:46.806 | info     |  1145 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/mycroft-speak.mycroftai/settings.json
12:57:46.907 | info     |  1145 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill mycroft-speak.mycroftai loaded successfully
12:57:46.914 | info     |  1145 | mycroft.skills.skill_manager:_load_on_startup:272 | skills all loaded!
12:57:46.989 | info     |  1145 | mycroft.skills.padatious_service:train:100 | training… (single_thread=false)
12:57:50.892 | info     |  1145 | mycroft.skills.padatious_service:train:102 | training complete.
12:57:50.894 | info     |  1145 | mycroft.skills.padatious_service:train:106 | mycroft is all loaded and ready to roll!

yeah, it is totally unresponsive to any attempt to ask it to pair, such as pair device, pair my device, etc.   literally nothing happens, no errors, nothing, if i normally ask something it doesn’t understand or is not working, such as weather, it tells me the error, or if it doesnt understand the question, it says, it doesn’t understand or try asking in a different way.    when asked to pair, it just returns the line, and nothing happens. huh, kinda wierd. tnx for suggestions so far++

any reason you didn’t try picroft?

i had a pi around that i already had some dev stuff on it, and just wanted to try it out.  i ended up downloading picroft and installing it, and it paired fine.  it was a bit tricky to get my seeed 4-mic linear array working but eventually got it.  tnx++
"
146,looking for compact microphone on a budget,mycroft project,"
hello everyone,
i’m building a compact picroft and i am having a rough time trying to find a microphone and speaker that is the right form factor and works well.
i’m looking for:

takes up less than 2 inches inches square (~60mm). i’m okay with hacking it up with a dremel to make it smaller if i have to. it will be in a 3d printed case.
can hear well from at least 2 (preferably 3) meters away in any direction
is under usd$ 30

the little tiny microphone that keeps coming up here is certainly cheap and certainly small but from the reviews it’s clear it doesn’t have the range.
i am toying with an omni-directional lavalier microphone.
i’m also not opposed to some soldering or custom pcbs.
if anyone has any ideas or stuff they tried that works i’d love to hear it!
","
this is the best mic for me wihout array and low cost. https://www.sunfounder.com/product-usb-mini-microphone-1876.html . you can talk with mycroft over 4 meters.i think you can destroy the case if you have less space. you just have to turn on auto gain control.

@gras64 thanks. that’s the one i keep seeing everywhere. it’s certainly small enough but the amazon reviews are pretty brutal. they say it barely works at all. i guess for the price i can try it with auto gain control and see how it goes. good to hear someone got it working.

that is the same one i have been using for two years without issue.
"
147,no audio output just the signal tone input works,support,"
i installed mycroft on my ubuntu 20.04 server and everything worked fine. the only thing that doesn’t work is the audio (tts) output, so i’m not able to hear what the ai answers. i installed pulseaudio and scrolled through many faqs and documentation sites but nothing helped.
the debug console shows the answers from mycroft as a text and they match my questions but there’s no audio output…
do you guys have an idea?
","
check the audio.log in /var/log/mycroft and see if it has anything useful?

okay, i checked the audio.log and it says f.e.
“| error    |  3613 | main:on_error:34 | audio service failed to launch (typeerror(‘can only concatenate str (not “nonetype”) to str’)).”
and
“the tts could not be loaded” …
the voice.log has some errors too:
2020-10-02 04:13:54.347 | info     |  2819 | mycroft.messagebus.load_config:load_message_bus_config:33 | loading message bus configs
alsa lib pcm.c:2642:(snd_pcm_open_noupdate) unknown pcm cards.pcm.rear
alsa lib pcm.c:2642:(snd_pcm_open_noupdate) unknown pcm cards.pcm.center_lfe
alsa lib pcm.c:2642:(snd_pcm_open_noupdate) unknown pcm cards.pcm.side
alsa lib pcm_route.c:869:(find_matching_chmap) found no matching channel map
alsa lib pcm_route.c:869:(find_matching_chmap) found no matching channel map
alsa lib pcm_route.c:869:(find_matching_chmap) found no matching channel map
alsa lib pcm_route.c:869:(find_matching_chmap) found no matching channel map
alsa lib pcm_oss.c:377:(_snd_pcm_oss_open) unknown field port
alsa lib pcm_oss.c:377:(_snd_pcm_oss_open) unknown field port
alsa lib pulse.c:242:(pulse_connect) pulseaudio: unable to connect: access denied
alsa lib pulse.c:242:(pulse_connect) pulseaudio: unable to connect: access denied
alsa lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) invalid type for card
alsa lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) invalid type for card
alsa lib pulse.c:242:(pulse_connect) pulseaudio: unable to connect: access denied
alsa lib pulse.c:242:(pulse_connect) pulseaudio: unable to connect: access denied
2020-10-02 04:13:54.555 | info     |  2819 | mycroft.client.speech.listener:create_wake_word_recognizer:328 | creating wake word engine
2020-10-02 04:13:54.556 | info     |  2819 | mycroft.client.speech.listener:create_wake_word_recognizer:351 | using hotword entry for hey mycroft
2020-10-02 04:13:54.569 | info     |  2819 | mycroft.client.speech.hotword_factory:load_module:403 | loading “hey mycroft” wake word via precise
2020-10-02 04:13:56.587 | info     |  2819 | mycroft.client.speech.listener:create_wakeup_recognizer:365 | creating stand up word engine
2020-10-02 04:13:56.588 | info     |  2819 | mycroft.client.speech.hotword_factory:load_module:403 | loading “wake up” wake word via pocketsphinx
expression ‘painvalidsamplerate’ failed in ‘src/hostapi/alsa/pa_linux_alsa.c’, line: 2048
expression ‘paalsastreamcomponent_initialconfigure( &self->capture, inparams, self->primebuffers, hwparamscapture, &realsr )’ failed in ‘src/hostapi/alsa/pa_linux_alsa.c’, line: 2718
expression ‘paalsastream_configure( stream, inputparameters, outputparameters, samplerate, framesperbuffer, &inputlatency, &outputlatency, &hostbuffersizemode )’ failed in ‘src/hostapi/alsa/pa_linux_alsa.c’, line: 2842
2020-10-02 04:13:56.961 | info     |  2819 | main:on_ready:175 | speech client is ready.




 ltm:

audio service failed to launch (typeerror(‘can only concatenate str (not “nonetype”) to str’))


can you expand on that? just dump the last 300 lines on pastebin.com
most valueable are the traceroutes
there’s a none passed where it shouldn’t. the rest could be just aftermath.
and the last 1000 lines from skills.log

that’s from the audio.log
and this one from the skills.log

a quick search coming up with this troubleshooting guide to get the permission problem right.
there are some vlc problems, but you should look at the permissions first.

also alsa-info.sh from the alsa-utils package (not baseline) should give good information. i wonder why this isn’t recommended given the mounting audio problems reported

mimic is optional in the setup process, yet resides in /etc/mycroft/mycroft.conf (or the user equivalent) or is defaulted to if nothing is set - correct me if i’m wrong. mimic isn’t found on your system, hence the tts problem.
log.info(""failed to find mimic at: "" + bin) bin is the none thing
reinstall it or switch to google or … (this would be a way to conf google up)




 sgee:

alsa-info.sh


thank you all for helping!
after i ran the command “snap remove pulseaudio”, the last tts outputs ran through. now everything works! 

snap, snap, hurray 

hello.
i upgraded from 18.04 to 20.04 and am having this same problem. my log files also look similar to yours.
i downgraded python to version 3.7.5 (ubuntu upgraded it to 3.8). and i was able to start mycroft.
when i start mycroft, all services seem to come up normally without issues. the wake word works, but there is no follow up audio.
it also didn’t occur to me that i would have to pair the device again. but i can’t get a pairing code since i don’t have any audio.
i tried uninstalling pulseaudio, but with apt remove instead of snap.
does anybody have any other ideas?
thank you,
dan.
"
148,make a web request,skill suggestions,"
skill name: make a web request
user story:
_as a (type of user) i want this skill to be able to make simpme web requests so that i can integrate it with a tool i use for home automation.
what third party services, data sets or platforms will the skill interact with? it literally just needs to make a web request. for my purposes, it just needs to run curl and the url on the command line and it will make the request.
are there similar mycroft skills already? not that i know off.
see https://github.com/mycroftai/mycroft-skills for a list. if so, how could they be combined?
what will the user speak to trigger the skill?
the user will be able to define names to each web request and set up a custom trigger.
what phrases will mycroft speak?
just a confirmation
what skill settings will this skill need to store?
just the different requests and the trigger word.
see https://mycroft.ai/documentation/skills/skill-settings/ for more information
other comments?
not really. i just think this would be a powerful, yet simple to implement skill.
put any other comments you think are relevant in here
thanks!
","



 gotmax:

for my purposes, it just needs to run curl and the url on the command line and it will make the request.


if i understand correctly you to perform http-requests to a web-application by a mycroft skill? for that you may want to have a look at requests python library. otherwise you need to be more specific, do you have a example requests? what is your “home automation tool”?

hello there
i’ve the same request, so i’ll try to clarify:
i have a smart switch that turns the light on by making a get request to

http://192.168.1.123/relay0?cmd=on

having mycroft react to “hey mycroft! turn light on” by wgetting the above url would be pretty sweet (of course, the basic idea here is to make keywords/urls customizable)
thanks!

url looks familiar, is this for a shelly device?
python code with requests library could look like following:
import requests
response = requests.get('http://192.168.1.123/relay0', params={'cmd': 'on'})


thanks @dominik
yep, you guessed right 
thanks for the code
i think it would be useful for anyone to have it inserted in the marketplace as a generic solution, since many iot devices (tasmota being the most used) accept http commands for simple tasks (on/off) without requiring an home automation backend.
thanks!

hey, you piqued my interest and i had a little free time, so:



github



krisgesling/make-get-request-skill
a configurable skill for making simple get requests. - krisgesling/make-get-request-skill






currently only space for a single parameter, but this should be sufficient for dominiks example above.

hey @gez-mycroft, thanks!
i’ve cloned the repo in to /skills, and edited the
/mycroft-core/skills/make-get-request-skill.krisgesling/settingsmeta.yml
it now looks like this:
skillmetadata:
  sections:
    - name: trigger
      fields:
        - name: trigger_phrase
          type: text
          label: trigger phrase
          value: ""lights on""
          placeholder: this should not include the wake word
        - name: response_on_success
          type: text
          label: response on success
          value: ""as you command""
        - name: response_on_failure
          type: text
          label: response on failure
          value: ""i can't sir""
[...]

i have also restarted mycroft with:
./start-mycroft.sh restart all
giving the command :skills  in the cli correctly lists make-get-request-skill.krisgesling  among the installed skills.
but when i say “hey mycroft! … lights on”, mycroft responds with :
          lights on                                debug output
          >> i don't know what that means.  

is there anything else i need to do?
again, thanks for your support!

the relevant part for this to trigger is @intent_handler(‘make.request.intent’), thus saying “test this thing for me”. this is just a framework to be fleshed out.
bullshit, haven’t recognized update_intent_file
can you nano in make.request.intent and post the content?
(i expect a newline problem)

thanks @sgee, here is the nano of make.request.intent
test this thing for me

i don’t think it’s very evident from the code above, but there is a newline after “test this thing for me”… (ie: the file is composed of the above, followed by a new,empty line)
i’ve tried removing the newline and restarting mycroft tho, and still the command is not recognized
i also tried “test this thing for me” as the command, but it is not recognized either.
i deleted and re-created the file with just a simple word in it (“lights”) and it is working!

given (the intention of) the code “lights on” should be written to  make.request.intent
with the new content
test this thing for me\n
lights on






 viros:

i deleted and re-created the file with just a simple word in it (“lights”) and it is working!


sorry, just updated my last post:
i deleted and re-created the file with just a simple word in it (“lights”) and it  is  working!

yeah, but this doesn’t resolve the core problem. this should automatcally be written as you change the config. (given the home config fetching algorithm got juiced)

yep.
and i tought to play the smart guy and duplicate the skill (since as of now, i need at least two parameters: on and off) and so have a skill for the “on” command and the other for the “off” command but alas, this isn’t working either 

you could make something with “lights {status}” intent
and processing like
status = message.data.get(‘status’)
if status == “on”:
if status == “off”:

thanks @sgee, but i’m really new at mycroft 
when you say:

make something with “lights {status}”

you mean writing in make.request.intent exactly: lights {status} ?
i tried doing this, and putting the logic in __init__.py  but  lights {status}  isn’t recognized anymore as a command. (of course by speaking “lights on”)
again, pardon me but i’m a total noob at this
update: i also tried with double curly braces ({{status}}) as explained here:



mycroft – 21 jun 19



guest blog - all about intents - mycroft intents - mycroft
in this guest blog, mycroft community member jarbas ai explains mycroft intents and the nlu engines that handle them in skills.






but still the command is not recognized

lets stay with the intention of the code and iron out possible errors



 sgee:

make something with “lights {status}” intent


i mean
fields:
       - name: trigger_phrase
         type: text
         label: trigger phrase
         value: ""lights {status}""

with
@intent_handler('make.request.intent')
    def handle_request(self, message):
        """"""make the actual get request.""""""
        status = message.data.get(‘status’)
        key = self.settings.get('param_key')
        if status == ""on"":
             value = self.settings.get('param_value')
        if status == ""off"":
             value = self.settings.get('param_value2')
        ...

i’ve found that mycroft won’t react to the command specified in the settingsmeta.yaml, but just to the one in make.request.intent
maybe something else is broke… elsewhere?
just to be sure, i rm -fr the skill and re cloned it with
mycroft-msm install https://github.com/krisgesling/make-get-request-skill
then edited the above files one at a time, same results 

maybe you are doing something wrong, you really should not edit settingsmeta.yaml, you should edit settings.yaml  , the meta is for usage by the backend to display web settings
or maybe its the known selene bug, the backend is known to overwrite any changes you make locally to skill settings.

so installed the skill and added a trigger phrase
short time later (ckeck cli)   make.request.intent looks like
 turn lights {status}
you are running mycroft with lang=en-us?
since i had to copy the locales to de-de first to take effect (there are only en-us locales yet)
intent_file = /opt/mycroft/skills/make-get-request-skill.krisgesling/locale/de-de/make.request.intent` (in my case)

@sgee: yes, running with en-us
@jarbasal: there’s no settings.yaml in this repo, i’ve tried updating settingsmeta from the device settings in the website, but it takes long for the settings to apply (if ever) so i was changing them direcly on disk
"
149,noob question about wake word,none,"
hi,
i am very, very new here. i have 2 questions:
1: “hey mycroft” works great, but “hey jarvis” does not seem to work. how do i fix this?
2: when i say “hey mycroft” it beeps to acknowedge it is listening. any way to customize this to say “yes joe” or sir""
","
the easiest option would to be using pocketsphinx to set up a custom wake word (yet pocketsphinx isn’t as sophisticated). there is a option to use mycroft-precise to create a model from scratch, but this requires a pretty good audio setup and knowledge.
the wake word is a bit picky with accentuation, … which might be smoothed if tagging (on home) will be implemented. try changing the pronunciation slightly.

if you can record at least twenty wake words, and preferably twice that in relevant not-wake-words, you can upload to the precise-community-data repo and ask, and i’ll try modeling you a precise model.  audio quality not as important as quantity to start with.




 joe_somerville:

2: when i say “hey mycroft” it beeps to acknowedge it is listening. any way to customize this to say “yes joe” or sir""


never tried it myself, but you can get/set wave sound for some system “sound” events.
mycroft-config get sounds
{
  ""start_listening"": ""snd/start_listening.wav"",
  ""end_listening"": ""snd/end_listening.wav"",
  ""acknowledge"": ""snd/acknowledge.mp3""
}

maybe this is worth a look.

i was going to set my wakeword to “spy-device” just to mock people who has google/alexa/siri listening to them 24/7 
that or “hal”, but mycroft doesnt sound like hal 9000, and there is no hal 9000 theme for the gui, that would have been cool 




 thorsten:

“start_listening”: “snd/start_listening.wav”, “end_listening”: “snd/end_listening.wav”, “acknowledge”: “snd/acknowledge.mp3”


thank you. another noob question:
when i was having a hard time with the wake word i deleted the device from my profile. now i am having a difficult time getting the registration code for the device so i can pair it again. how do i get that?
i was not able to find anything in the documentation or the forum.

since the pairing skill is baseline, i think you can pair it with “pair my device” (never had to, so…)

douglas rain passed on a few years back, as well.  but candice bergen is still around, wonder if she’d be interested in reprising sal…
"
150,compatible smart devices,none,"
hi,
i am new here and still getting the hang of everything. i have been setting up my picroft and want to put together a christmas list. i looked and could not find a list of devices that work with the skills out there. here is what i am looking for:
smart bulbs
thermostats
smart plugs
i would like to be able to control the temp, the lights and various electronic devices like tvs etc with voice commands.
","
hi joe.
welcome to our community.
do you already use a smart home software? maybe this is the way to go. integrate several smart components in tools like fhem, home assistant or open hab.
for these smart home software components is an existing mycroft skill integration for controlling items by voice.
best regards
thorsten

i am not currently using smart home software. what do you reccomend?

that’s not easy to say, since it depends on …

check that the components you have (plan to buy) are supported natively or by modules, extensions, …
check how automation logic is implemented (yaml config, perl, php, ui clicking, …) - whatever you like more
are you a fan of scripting or ui clicking (business logic)
opensource, free, commercial products

i use a commercial software called ip-symcon (automation logic with php) for many years now but currently no skill exists for it, so it’s potential not the right software for you. i plan on implementing a skill for it wherever i’ve time for it.
maybe you should check some websites which compare available smarthome software solutions.
as far i know mycroft skills exist for

fhem
home assistant
openhab





 thorsten:

home assistan


thank you. i have a lot of research to do.
"
151,mycroft listens but doesnt respond,support,"
“hey mycroft pair my device” sound nothing else
how do i fix this?
as an easier explanation, i speak mycroft plays a sound then nothing else happens ( also it’s not because i have to wait before saying the command.)
help please
","
hey royster.
welcome to our community.
if your device isn’t paired yet maybe your tts config isn’t working.

which device do you use?
is mimic build locally?

do you see your pairing code in mycroft cli (mycroft-cli-client)?
"
152,precise collect not recording any sounds,support,"
hi all,
i have built a picroft on a raspberry pi 4 using a google aiy voice hat as the mic and speaker. this works very well with the default ‘wake word’ but i decided to try and create my own custom ‘wake word’, first i tried using pocketsphinx defining the phonemes for my ‘wake word’ and this worked reasonably well. i now decided to try and using the precise engine and train my “wake word”. i started with a fresh install of picroft (picroft_buster-keaton_2020-01-10) applying all the latest updates. i installed mycroft precise using the source install method, downloading the source using ‘git clone https://github.com/mycroftai/mycroft-precise’ and then running setup.sh in the mycroft-precise directory. this worked fine. i then set ‘source .venv/bin/activate’ and used ‘precise-collect’ to record some sample utterances of my ‘wake word’. the records seem to work but when i checked the wav files they all seem to just be flat lines as if the mic was mute. the default ‘wake word’ still works so the mic configuration would seem to be correct.
can anyone suggest why i may not be getting any successful recordings?
ernie
","
if you try “arecord -d 3 test.wav” (say something, of course) and then play that back, does it work?

hi baconator,
thanks for your reply. i have tried what you suggested and it does work but the play back is at a very low amplitude. i loaded the wav file into audacity and the wave form is only just noticeable. if i use ‘precise-listen /home/pi/.mycroft/precise/hey-mycroft.pb’ that seems to work as i would expect.
ernie

that seems to suggest you might need to check the levels in alsamixer.

hi,
i tried alsamixer and pressed f6 and selected the google sound card i believer, but it says ‘this sound device does not have any controls’.
ernie

the “default” is master. just hit f4 as you enter alsamixer.

hi sgee,
thanks for you reply.
when i enter alsamixer is shows a single column marked ‘master’ with a value of 99. if i hit f4 the program drops out back to the command prompt.
ernie

do you ssh in? the f-keys act weirdly in this case ( the 99 is the output level btw)

hi sgee,
thanks for your reply.
i didn’t realise there was a problem with ssh. i connect a hdmi monitor and keyboard so i could run alsamixer without a problem.
i didn’t know what a lot of this means but i think it looks ok. top left corner shows card: pulseaudio, when i press f4 / f5 i get the levels for ‘master’ & ‘’, both set to 100.
f2 gives a lot of system information
version:	k5.4.51-v71+
card:		snd-rpi-googlevoicehat-soundcard
devices:	0:[ 0]: control
		16: [0- 0]: digital audio playback
		24: [0- 0]: digital audio capture
		33:	: timer

timer	g0:	system timer: 10000.000us (10000000 ticks)
		p0-0-0:	pcm: playback 0-0-0 slave
		p0-0-1:	pcm: capture 0-0-1 slave

pcm		00-00 googlevoicehat soundcard hifi voicehat-codec:0  ……….

does this help at all? as i say the default mycroft voice capture works as it recognises the default ‘wake-word’ so the sound card is working. so it seems strange that the ‘precise-collect’  command fails to record any sound since i assume it’s using the same method.
thanks again for any help you can give.
ernie

maybe this is something for you - if using aiyv2. and if using aiyv2 there is another question if the setup process covered all things needed. since i think its taylored to aiyv1. but that is all guesswork. @andlo might be able to add something substantial.
if using v1 there is a similar skill in the marketplace.
another member described a somewhat similar problem, yet there was no answer. maybe pm him?
about precise/mycroft: do you use the stable or the dev version of mycroft-core? since the resulting precise model (mycroft-precise default branch; precise v0.3) wouldn’t be compatible with mycroft-core stable (precise v0.2). it’s not relevant to the problem (i guess) but it’s good to know further down the line.

hi sgee,
thanks again for your reply.
as you should see i took your advice and contacted randal to see if he made any progress with his problem. i also checked what version of the google aiy i was using, i was not familiar with the version 2, but mine is a hat that attaches directly on a raspberry pi 4 so i think this a version 1.
i noticed from the post you directed me to by chiisaa, that someone called andreas lorensen wrote the code for aiy v1, would this be the code used in the current release of picroft? if so do you think it may be worth contacting andreas himself to see if he could help?
thanks again ernie

you should file issues on the github repos as first step.




 erniep:

that someone called andreas lorensen wrote the code for aiy v1,


this is why i called @andlo (klick  ) to have a look.

hi sgee,
sorry if i misunderstood that you. so you have already been in contact with andreas. before i file an issue on the github repos as suggested by baconator should i wait to see what andreas says or should i do it now?
ernie

haven’t seen him around lately, so go ahead and file an issue. always beneficial to ring the bell on more than one place  you might also consider engaging with the team/community on https://chat.mycroft.ai

hi sgee,
i posted an issue for this problem on the forum over a week ago, first one i have done. would expect to get a response by now even if it is a negative one?
have you seen andreas about yet?
ernie

on my pi4
maybe you can use arecord -l or -l and find the capture device.
then the /etc/asound.conf is kind of like
(you’ll have to sudo “vi”, or “emacs”)
pcm.!default
{
playback.pcm
{
type hw
card 2
device 0
}
playback.capture
{
	type hw
	card 0
	device 0
}

}
i was also not able to record.
i think this is what did it for me, but i’m not sure.
it’s basically combining mu usb mic from best buy into the default pcm.
i think you need to reboot.
nate

raspberry pi4 doesn’t come with a microphone

no haven#t seen him around.
during your installation you should’ve came across this setup option, right? if so and followed this path, check if all steps under 4) got applied. and i would guess the firmware applied in this setup is rather old and may conflict with some newer software. lastly, is there a way to make sure you are running v1? just to rule out that you’re not running the wrong drivers.
i’m speculating that there are not much googlehats around in this community, thus unsufficient feedback.


img_20201002_114837143a4096×3072 714 kb
 hi sgee,
thanks for your comments. yes when i was installing picroft i took the option 4 to install the google aiy device. using drivers the ‘arecord –l’ command seems to recognise the device, see output below.
arecord -l
**** list of capture hardware devices ****
card 0: sndrpigooglevoi [snd_rpi_googlevoicehat_soundcar], device 0: google voicehat soundcard hifi voicehat-codec-0 [google voicehat soundcard hifi voicehat-codec-0]
subdevices: 0/1
subdevice #0: subdevice #0
also it’s the only device with a mic & speaker that is attached to my raspberry pi and in normal working picroft recognises the wake word even from several metres away (3m to 4m)
on the aiy version question my understanding the v1 connects to standard raspberry pi 3 / 4 and v2 connectors to a raspberry pi zero. see attached photo of mine.
thanks again for your help.
ernie
"
153,voice recognition does not work after mycroft upgrade,general discussion,"
i recent perfomed mycroft upgrade.
after, rebooting, the voice recognition is nowr working. also the led panel does not dispalay the hour or show any respone to wake up command or any other voice command
i used mycroft-cli-client… and any word phrase is captured, the debug client does nto display anything when send the wake up command or other voice command
","
you have to give more information about your system (software/hardware,…) although i suspect mark1, yet my crystal ball is dusty.

hi… it is mark 1
i received the message, of “major mycroft is available” and send the yes to update.
this happened on sunday.
below the mycroft version
mycroft-core/unknown,now 20.8.0 armhf [installed]
pi@mark_1:~ $
i am with stable branch, i verified using mycroft-use stable
attached you will find the device (how it looks like). commands outputs and log files!
as you could see no voice command is received in cli-client, also the device display led does not display when “hey mycroft” wake command is sent
you can check here
https://drive.google.com/drive/folders/1ukyifjzyvvrtzhrsameslhckxbsfmu5b?usp=sharing
mark1|678x500

hey grover, welcome to the forums 
sorry to hear the update seems to have caused some issues. can i check that you’ve already tried a reboot using the button on top?
does the device respond to commands typed into the cli?
can you also check whether skills are all loaded by typing :skills into the cli. this should provide a list of all skills that mycroft attempted to load. any in red have thrown an error when loading.

please see my  reply below

can i check that you’ve already tried a reboot using the button on top?
yes, i used… i see a message “speech recognition could not understand audio” (see attached image)


does the device respond to commands typed into the cli?
yes, it responds using the cli-client


can you also check whether skills are all loaded by typing :skills into the cli
i run and returns a list of skills, 4 are in red

mycroft-stock.mycroftai
mycroft-youtube-audio.mcdruid
pandora-skill.ethanaward
translate-skill.jcasoft
now, i retried, sending “play pandora” from cli-client… it does not sounds anything from mycroft (no speech from mycroft), but it is working, and is reproducing my radio station
regards,
grover

cli-client1349×737 283 kb


cool, mycroft-stock.mycroftai is expected - it’s been intentionally disabled. the other three aren’t in the marketplace so they didn’t get tested against the breaking changes from 20.02 > 20.08.
i find the easiest way to see what’s causing the issue for a skill that fails to load is to try and reload it. you can either touch a file in that skill eg:
touch /opt/mycroft/skills/translate-skill.jcasoft/__init__.py

or in the cli:
:deactivate translate-skill.jcasoft
:activate translate-skill.jcasoft

this will cause mycroft to attempt to reload the skill, and the cause of the error will get shown in your log output.
you can also find these at /var/log/mycroft/skills.log
in terms of the “speech recognition could not understand audio” error, this indicates that mycroft isn’t detecting any speech in what it’s hearing. can you tell if the microphone is working at all?
does the “mic level” indicator in the bottom right of the cli jump around?
if you type “set a timer”, you will then be prompted to what length timer, if you speak then, does it transcribe what you’re saying?

i tried the tips in cli-client
:activate translate-skill.jcasoft

then, i send the order “set a timer” and it recognizes and set timer (see attached image) and i could see in the display, the counting according to timer.
however, speaking “hey mycroft” wake command does not  work, no message displayed in the cli. also i tried type hey mycroft and then speak an order and it fails, voice is not recognized and no message.
i verified my device is connected and the wake word is hey mycroft
regards,
grover

set-timer1343×703 184 kb


hi,
i did some configuration to sound
pi@mark_1:~ $ cat .asoundrc
defaults.pcm.card 0
defaults.ctl.card 0
then i tried the mycroft-mic-test, and it was possible to record and reproduce sound.
then, i opened cli and retried type “set a timer” and now, it does not recognize, any order i sent can speak. only “play pandora” can reproduce sound
i run mycrot-speak and has no sound
i run mycroft-speech-client and got this
pi@mark_1:~ $ mycroft-speech-client
2020-10-08 04:32:41.263 | error    |  3540 | mycroft.client.speech.main:on_error:183 | audio service failed to launch (permissionerror(13, ‘permission denied’)).
i do not know what to do…

i have also had problems with audio, but only with pianobar, which i see you are trying to use too.  i’m not using mark, but raspberry pi4 with picroft.
i have complete audio other than pianobar/pandora.
when you run pianobar outside mycroft, do you get audio?  mine is distorted through the analog raspberry pi  4 card.
i am using picroft, so i’m thinking that the audio form pandora is not at the right rate for picroft’s pulseaudio settings.
sorry to kind of hi-jack your post.
i am totally willing to go into the pianobar skill and tweak it.  pianobar uses c and libao i saw.

as for no sound and permission denied , do you have an ~/.libao file?
mine is
default_device=pulse
i’ve had permission issues when it was
default_device=alsa
"
154,picroft does not understand home assistant commands,support,"
i decided to install the picroft image and it works fine for now, but then i added the home assistant skill and filled in my credentials but when i say commands like “turn on the livingroom lights”, mycroft always responds something like “there are no running timers” and when i say “read bedroom temperature”, the answer is “weather forecasts are currently not available”…
","
sounds like it’s not understanding you, or not matching the intents correctly.  for the first, check the voice.log (in /var/log/mycroft/) and the second, the skills.log.
"
155,trademark usage guidelines,mycroft project,"
hey all,
we know that lots of people are building products and projects that either incorporate mycroft or package it in some way. as an open source project this is exactly what should happen!
however we also need to be clear about the use of mycroft trademarks like the name and logo. so we’ve put together some guidelines for this to hopefully clear things up. we tried to keep it brief and included a range of scenarios to show what it means “irl”.
consider this v0.1 of the guidelines so any questions or feedback are warmly welcomed.



mycroft



trademark usage guidelines - mycroft
trademark usage guidelines - mycroft





",
156,improve wakeword detection,mycroft project,"
hy everyone,
i finally got my rpi3b+ working with the respeaker 2-mics pi hat. there are only two issues left:

the wakeword “hey mycroft” is not recognized all the time. would “precise” be a way to improve it?
the wakeword never works when music is playing. i read that there is no noise canceling etc, but how do you stop picroft from playing music?

thank you in advance.
br
eyk
","
a “hey mycroft, stop” would stop the music playing, but mycroft won’t hear you because music is playing…
maybe you can attach a push button via gpio and write a little skill that checks if button is pressed and then issue a “stop” over the message bus?

i have also thought about this solution, but i think  that it is not comfortable. if i am not mistaken, the same behaviour is also true for the mycroft mark1. how is it behaviour solved with this device?

it’s not.
there’s a few things to try like pulse aec or reinforcing the mic vs. speaker areas, but these aren’t a total fix.

thank you for the hint. could you please describe what to do?

first make sure you’re using precise. (ask what your listener is)  https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/troubleshooting/wake-word-troubleshooting
for the second thing, make sure your mic and speakers are far enough apart to minimize interference.  if you can isolate them from each other as well through whatever means you have (a case? positioning? sound proofing material?) that could be helpful.
"
157,mycroft home assistant integration,mycroft project,"
hi there
i’m using a raspberry pi 4b to run mycroft and the same for home assistant. i had to move the ha pi and the ip address has changed…can anyone please tell me on which mycroft file i update the ip address?
many thanks,
ed
","
hello.
not sure since i’m not using this skill, but can you configure ha hostname in webui within home.mycroft.ai in skill configuration?

thanks for the reply thorsten.
sorry for the noob question but could you tell me the command i should type into the web terminal window?
if not, on which page of the configuration pages can i find the skills configurations…do you mean integrations?
thanks again,
ed

hello ed.
if you’ve installed the home-assistant skill do the following steps:
1.) open home.mycroft.ai
2.) login
3.) click on the yellow circle icon in the upper right corner
4.) choose “skils”
5.) expand entry “home assistant”
6.) set parameter for your installation

bildschirmfoto 2020-10-07 um 17.33.57322×640 14.5 kb
 
bildschirmfoto 2020-10-07 um 17.32.442388×1640 246 kb

hope that helps.
thorsten
"
158,is there a way to wake mycroft with a press of the keyboard instead of mycroft listening for hey mycroft,general discussion,"
is there a way to make it so when i press a certain key, it wakes mycroft instead of mycroft listening for ‘hey mycroft’? if not, how would i make this?
","
not just by configurering mycroft. but you can make a skill that does that.
the trick will be to send to the messagebus that it has to start listning.
self.bus.emit(message(""mycroft.mic.listen""))

look at the picroft-google-aiy-skill for an example. it listen for press on button connected to the gpio and when that happen it emit the message to the messagebus
https://github.com/andlo/picroft-google-aiy-voicekit-skill
se more about messagebus here
https://mycroft-ai.gitbook.io/docs/mycroft-technologies/mycroft-core/message-bus

awesome tips thx andlo !

this is an awesome idea. thank you andlo, i didn’t know it was that simple to just send it via the message bus, that’s awesome!
i rewrote one of the cl tools (mycroft-say-to) and made mycroft-listen that just activates it. (i will send it up to see if i can’t merge it, or at least if it can be used somewhere else.).
on kde you can then make a custom shortcut to run this new script. (from the “custom shortcut”)
i am struggling to figure out git issues and keyboard issues tonight, so i’ll just post the contents of script here so you can check it out.
bin/mycroft-listen
#!/usr/bin/env bash

# copyright 2018 mycroft ai inc.
#
# licensed under the apache license, version 2.0 (the ""license"");
# you may not use this file except in compliance with the license.
# you may obtain a copy of the license at
#
#    http://www.apache.org/licenses/license-2.0
#
# unless required by applicable law or agreed to in writing, software
# distributed under the license is distributed on an ""as is"" basis,
# without warranties or conditions of any kind, either express or implied.
# see the license for the specific language governing permissions and
# limitations under the license.

source=""${bash_source[0]}""
cd -p ""$( dirname ""$source"" )""
dir=""$( pwd )""

# enter the mycroft venv
source ""$dir/../venv-activate.sh"" -q

# send a message to be spoken
output=$(python -m mycroft.messagebus.send ""mycroft.mic.listen"")

nice work, i’m very interested to see your progress on this topic. it would be very handy to be able to ‘double tap’ the command key and that wakes mycroft. i’m gonna look into how to achieve this. any ideas? @fruitywelsh

if you find anything on a double tap shortcut please let me know. that’s not anything i’ve heard of before, but i could see the use for sure! (i guess i know of things that use it, like windows sticky keys for example). the kde and gnome reddits maybe a good place to ask though!
pull request has been made.

awesome thanks @fruitywelsh !

i am new to mycroft and have a very poor recognition rate of the wake word.
this script is perfect - thanks.
if anyone is as new to bash as me:-
put the script in /home/pi/graham/mycroft_listen.sh and run with
sudo /home/pi/graham/mycroft_listen.sh
this change made it run
#source “$dir/…/venv-activate.sh” -q
source “/home/pi/mycroft-core/venv-activate.sh” -q
i intend to run the script by waving a hand close to an ultrasonic range module or use a clap detector.
"
159,date time not working,support,"
hi, i can’t trigger the date & time skill on my picroft.
this is what i get in the cli:
21:12:19.219 | info     |   805 | questionsanswersskill | searching for what is the time
removing event fallback-query.mycroftai:questionquerytimeout
removing event fallback-query.mycroftai:questionquerytimeout
removing event fallback-query.mycroftai:questionquerytimeout
removing event fallback-query.mycroftai:questionquerytimeout
21:12:28.208 | info     |   805 | questionsanswersskill | timeout occured check responses
21:12:28.236 | info     |   805 | unknownskill | fallback type: question
21:13:01.005 | info     |   805 | configurationskill | remote configuration updated
it looks like the skill isn’t triggered at all – could someone help me with this?
","
the intent handler couldn’t route the intent. in general the handler only needs “what time” (adapt), but adapt seems wonky to me (as the library grows).
try one of those (padatious)
(the|) (time|clock) ((right |)now |)(please|) = “time please” (ex.)
do you have the (current|) time =“do you have the time” (ex.)
current time
to get a better understanding: | = or, |) = or nothing
always a good thing to look at the vocabs (.intent are easier to read  )

thanks! unfortunately “time please” or “current time” give exactly the same result. “do you have the time” results in “there are no active timers”, so it’s routed to a timer skill. it looks like the date & time skill is ignored… is there a way to debug that?

hm… just tested it with lang=de and it works. both adapt and padatious

hey there, if you’re in the cli can you type:
:skills

this will show a list of loaded skills, any with an error will be shown in red. just want to confirm if the skill exists and is loaded correctly

and i’m assuming you are running with lang=en-us

thanks both for the hints – looking at :skills showed the date & time skill marked red because it didn’t load properly. i was then able to find in /var/log/mycroft/skills.log that this was due to a missing libf77blas.so.3 library required for numpy.
it’s strange that numpy didn’t complain on installation, but it’s ok now – simply installing libatlas-base-dev  fixed the problem.

thanks, that fixed date-time skill on my picroft as well.
on my mark-i i got a new error on "" _multiarray_umath.cpython-37m-arm-linux-gnueabihf.so missing"", after re-installing numpy as suggested here date-time skill now works on mark-i again.

thanks @tuetschek for your tip. now date-time skill is working again.


github.com/numpy/numpy








libf77blas.so.3: cannot open shared file: no such file or directory



        opened 01:52pm - 24 oct 19 utc


          closed 02:50pm - 08 dec 19 utc




          jamestcameron
        





-operating system: raspbian stretch (9)
-how python was installed: python 3.5.3 is default python3 on stretch
-how numpy was installed: pip installed under...


32 - installation







"
160,repeating last response when using rasa skill,support,"
i installed a skill described here (https://mycroft.ai/blog/connecting-rasa-to-mycroft-a-guide/) that connects mycroft and rasa. it works quite nicely but i face a usability problem that i cannot locate in the code.
i use the mycroft client interface to interact with mycroft  which runs in a docker container on a server. when i type the utterance “connect to diamond”, i receive the expected response “hi, i am diamond” and can use the bot.
when i stop interacting with the bot for 5-10 seconds, mycroft utters the last bot response again once. i could imagine that this is a feature meant to remind the user what was said last. on the other hand, it could also be a bug that breaks the user experience in my case. i could not find out why mycroft repeats the last utterance in this case. the problem seems to happen only when i use the rasa-mycroft skill. the log i extracted during a test conversation are below.
does anyone have an idea why this happens?


bus log
2020-09-07 11:40:36.067 | info     |    25 | questionsanswersskill | timeout occured check responses
2020-09-07 11:41:06.599 | info     |    25 | mycroft.skills.skill_loader:load:161 | attempting to load skill: skill-rasa-chat.wel
2020-09-07 11:41:06.607 | info     |    25 | mycroft.skills.settings:get_local_settings:78 | /opt/mycroft/skills/skill-rasa-chat.wel/settings.json
2020-09-07 11:41:06.617 | info     |    25 | mycroft.skills.skill_loader:_check_for_first_run:300 | first run of skill-rasa-chat.wel
2020-09-07 11:41:06.618 | info     |    25 | mycroft.skills.settings:save_settings:111 | skill settings successfully saved to /root/.config/mycroft/skills/skill-rasa-chat.wel/settings.json
2020-09-07 11:41:06.620 | info     |    25 | mycroft.skills.skill_loader:_communicate_load_status:320 | skill skill-rasa-chat.wel loaded successfully
2020-09-07 11:41:06.620 | info     |    25 | mycroft.skills.skill_manager:put:80 | updating settings meta during runtime…
2020-09-07 11:41:06.622 | info     |    25 | msm.mycroft_skills_manager | invalidating skills cache
2020-09-07 11:41:06.623 | info     |    25 | msm.mycroft_skills_manager | building skillentry objects for all skills
2020-09-07 11:41:09.858 | info     |    25 | mycroft.skills.skill_manager:send:64 | new settings meta to upload.
2020-09-07 11:42:00.473 | info     |    25 | skills | invalid level provided:
2020-09-07 11:42:00.473 | info     |    25 | skills | bus logging: true
2020-09-07 11:42:00.473 | info     |    25 | skills | bus: {“type”: “mycroft.debug.log”, “data”: {“bus”: true}, “context”: {}}
2020-09-07 11:42:01.207 | info     |    25 | mycroft.skills.settings:save_settings:111 | skill settings successfully saved to /root/.config/mycroft/skills/skill-rasa-chat.wel/settings.json
2020-09-07 11:42:01.211 | info     |    25 | skills | bus: {“type”: “mycroft.skill.handler.complete”, “data”: {“name”: “rasaskill.handle_talk_to_rasa_intent”}, “context”: {“client_name”: “mycroft_cli”, “source”: [“skills”], “destination”: “debug_cli”}}
2020-09-07 11:42:10.210 | info     |    25 | skills | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:42:15.889 | info     |    25 | skills | bus: {“type”: “recognizer_loop:utterance”, “data”: {“utterances”: [“connect to diamond”], “lang”: “en-us”}, “context”: {“client_name”: “mycroft_cli”, “source”: “debug_cli”, “destination”: [“skills”]}}
2020-09-07 11:42:15.895 | info     |    25 | skills | bus: {“type”: “skill.converse.request”, “data”: {“skill_id”: “skill-rasa-chat.wel”, “utterances”: [“connect to diamond”], “lang”: “en-us”}, “context”: {“client_name”: “mycroft_cli”, “source”: [“skills”], “destination”: “debug_cli”}}
2020-09-07 11:42:15.898 | info     |    25 | skills | bus: {“type”: “skill.converse.response”, “data”: {“skill_id”: “skill-rasa-chat.wel”, “result”: false}, “context”: {“client_name”: “mycroft_cli”, “source”: “debug_cli”, “destination”: [“skills”]}}
2020-09-07 11:42:16.097 | info     |    25 | skills | bus: {“type”: “skill.converse.request”, “data”: {“skill_id”: “fallback-unknown.mycroftai”, “utterances”: [“connect to diamond”], “lang”: “en-us”}, “context”: {“client_name”: “mycroft_cli”, “source”: [“skills”], “destination”: “debug_cli”}}
2020-09-07 11:42:16.101 | info     |    25 | skills | bus: {“type”: “skill.converse.response”, “data”: {“skill_id”: “fallback-unknown.mycroftai”, “result”: false}, “context”: {“client_name”: “mycroft_cli”, “source”: “debug_cli”, “destination”: [“skills”]}}
2020-09-07 11:42:16.298 | info     |    25 | skills | bus: {“type”: “skill.converse.request”, “data”: {“skill_id”: “mycroft-hello-world.mycroftai”, “utterances”: [“connect to diamond”], “lang”: “en-us”}, “context”: {“client_name”: “mycroft_cli”, “source”: [“skills”], “destination”: “debug_cli”}}
2020-09-07 11:42:16.303 | info     |    25 | skills | bus: {“type”: “skill.converse.response”, “data”: {“skill_id”: “mycroft-hello-world.mycroftai”, “result”: false}, “context”: {“client_name”: “mycroft_cli”, “source”: “debug_cli”, “destination”: [“skills”]}}
2020-09-07 11:42:16.507 | info     |    25 | skills | bus: {“type”: “skill-rasa-chat.wel:startchat”, “data”: {“intent_type”: “skill-rasa-chat.wel:startchat”, “skill_rasa_chat_welchatwithrasa”: “diamond”, “target”: null, “confidence”: 0.5, “tags”: [{“match”: “diamond”, “key”: “diamond”, “start_token”: 2, “entities”: [{“key”: “diamond”, “match”: “diamond”, “data”: [[“diamond”, “skill_rasa_chat_welchatwithrasa”]], “confidence”: 1.0}], “end_token”: 2, “from_context”: false}], “utterance”: “connect to diamond”}, “context”: {“client_name”: “mycroft_cli”, “source”: [“skills”], “destination”: “debug_cli”}}
2020-09-07 11:42:16.511 | info     |    25 | skills | bus: {“type”: “mycroft.skill.handler.start”, “data”: {“name”: “rasaskill.handle_talk_to_rasa_intent”}, “context”: {“client_name”: “mycroft_cli”, “source”: [“skills”], “destination”: “debug_cli”}}
2020-09-07 11:42:16.516 | info     |    25 | skills | bus: {“type”: “speak”, “data”: {“utterance”: ""hi, i am diamond "", “expect_response”: true, “meta”: {“dialog”: “hi, i am diamond.”, “data”: {}, “skill”: “rasaskill”}}, “context”: {“client_name”: “mycroft_cli”, “source”: [“skills”], “destination”: “debug_cli”}}
2020-09-07 11:42:16.559 | info     |    25 | skills | bus: {“type”: “enclosure.eyes.blink”, “data”: {“side”: “b”}, “context”: {“destination”: [“enclosure”]}}
2020-09-07 11:42:16.559 | info     |    25 | skills | bus: {“type”: “recognizer_loop:audio_output_start”, “data”: {}, “context”: {}}
2020-09-07 11:42:16.560 | info     |    25 | skills | bus: {“type”: “recognizer_loop:audio_output_end”, “data”: {}, “context”: {}}
2020-09-07 11:42:16.561 | info     |    25 | skills | bus: {“type”: “mycroft.mic.listen”, “data”: {}, “context”: {}}
2020-09-07 11:42:16.564 | info     |    25 | skills | bus: {“type”: “enclosure.eyes.blink”, “data”: {“side”: “b”}, “context”: {“destination”: [“enclosure”]}}
2020-09-07 11:42:16.819 | info     |    25 | skills | bus: {“type”: “active_skill_request”, “data”: {“skill_id”: “skill-rasa-chat.wel”}, “context”: {}}
2020-09-07 11:42:20.222 | info     |    25 | skills | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:42:22.227 | info     |    25 | skills | bus: {“type”: “mycroft-reminder.mycroftai:reminder”, “data”: {}, “context”: {}}
2020-09-07 11:42:22.229 | info     |    25 | skills | bus: {“type”: “mycroft-configuration.mycroftai:configurationskillupdate_remote”, “data”: “updateremote”, “context”: {}}
2020-09-07 11:42:30.239 | info     |    25 | skills | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:42:31.823 | info     |    25 | skills | bus: {“type”: “speak”, “data”: {“utterance”: ""hi, i am diamond "", “expect_response”: true, “meta”: {“skill”: “rasaskill”}}, “context”: {“client_name”: “mycroft_cli”, “source”: [“skills”], “destination”: “debug_cli”}}
2020-09-07 11:42:31.825 | info     |    25 | skills | bus: {“type”: “active_skill_request”, “data”: {“skill_id”: “skill-rasa-chat.wel”}, “context”: {}}
2020-09-07 11:42:31.830 | info     |    25 | skills | bus: {“type”: “recognizer_loop:audio_output_start”, “data”: {}, “context”: {}}
2020-09-07 11:42:31.861 | info     |    25 | skills | bus: {“type”: “recognizer_loop:audio_output_end”, “data”: {}, “context”: {}}
2020-09-07 11:42:31.863 | info     |    25 | skills | bus: {“type”: “mycroft.mic.listen”, “data”: {}, “context”: {}}
2020-09-07 11:42:40.252 | info     |    25 | skills | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:42:46.824 | info     |    25 | skills | bus: {“type”: “mycroft.skill.handler.complete”, “data”: {“name”: “rasaskill.handle_talk_to_rasa_intent”}, “context”: {“client_name”: “mycroft_cli”, “source”: [“skills”], “destination”: “debug_cli”}}
2020-09-07 11:42:50.267 | info     |    25 | skills | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:42:52.270 | info     |    25 | skills | bus: {“type”: “mycroft-reminder.mycroftai:reminder”, “data”: {}, “context”: {}}
2020-09-07 11:43:00.284 | info     |    25 | skills | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:43:10.297 | info     |    25 | skills | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:43:20.311 | info     |    25 | skills | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:43:22.316 | info     |    25 | skills | bus: {“type”: “mycroft-reminder.mycroftai:reminder”, “data”: {}, “context”: {}}



voice log
2020-09-07 11:42:00.472 | info     |    31 | voice | invalid level provided:
2020-09-07 11:42:00.473 | info     |    31 | voice | bus logging: true
2020-09-07 11:42:00.473 | info     |    31 | voice | bus: {“type”: “mycroft.debug.log”, “data”: {“bus”: true}, “context”: {}}
2020-09-07 11:42:01.210 | info     |    31 | voice | bus: {“type”: “mycroft.skill.handler.complete”, “data”: {“name”: “rasaskill.handle_talk_to_rasa_intent”}, “context”: {“client_name”: “mycroft_cli”, “source”: [“skills”], “destination”: “debug_cli”}}
2020-09-07 11:42:10.210 | info     |    31 | voice | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:42:15.888 | info     |    31 | voice | bus: {“type”: “recognizer_loop:utterance”, “data”: {“utterances”: [“connect to diamond”], “lang”: “en-us”}, “context”: {“client_name”: “mycroft_cli”, “source”: “debug_cli”, “destination”: [“skills”]}}
2020-09-07 11:42:15.895 | info     |    31 | voice | bus: {“type”: “skill.converse.request”, “data”: {“skill_id”: “skill-rasa-chat.wel”, “utterances”: [“connect to diamond”], “lang”: “en-us”}, “context”: {“client_name”: “mycroft_cli”, “source”: [“skills”], “destination”: “debug_cli”}}
2020-09-07 11:42:15.898 | info     |    31 | voice | bus: {“type”: “skill.converse.response”, “data”: {“skill_id”: “skill-rasa-chat.wel”, “result”: false}, “context”: {“client_name”: “mycroft_cli”, “source”: “debug_cli”, “destination”: [“skills”]}}
2020-09-07 11:42:16.097 | info     |    31 | voice | bus: {“type”: “skill.converse.request”, “data”: {“skill_id”: “fallback-unknown.mycroftai”, “utterances”: [“connect to diamond”], “lang”: “en-us”}, “context”: {“client_name”: “mycroft_cli”, “source”: [“skills”], “destination”: “debug_cli”}}
2020-09-07 11:42:16.101 | info     |    31 | voice | bus: {“type”: “skill.converse.response”, “data”: {“skill_id”: “fallback-unknown.mycroftai”, “result”: false}, “context”: {“client_name”: “mycroft_cli”, “source”: “debug_cli”, “destination”: [“skills”]}}
2020-09-07 11:42:16.298 | info     |    31 | voice | bus: {“type”: “skill.converse.request”, “data”: {“skill_id”: “mycroft-hello-world.mycroftai”, “utterances”: [“connect to diamond”], “lang”: “en-us”}, “context”: {“client_name”: “mycroft_cli”, “source”: [“skills”], “destination”: “debug_cli”}}
2020-09-07 11:42:16.303 | info     |    31 | voice | bus: {“type”: “skill.converse.response”, “data”: {“skill_id”: “mycroft-hello-world.mycroftai”, “result”: false}, “context”: {“client_name”: “mycroft_cli”, “source”: “debug_cli”, “destination”: [“skills”]}}
2020-09-07 11:42:16.507 | info     |    31 | voice | bus: {“type”: “skill-rasa-chat.wel:startchat”, “data”: {“intent_type”: “skill-rasa-chat.wel:startchat”, “skill_rasa_chat_welchatwithrasa”: “diamond”, “target”: null, “confidence”: 0.5, “tags”: [{“match”: “diamond”, “key”: “diamond”, “start_token”: 2, “entities”: [{“key”: “diamond”, “match”: “diamond”, “data”: [[“diamond”, “skill_rasa_chat_welchatwithrasa”]], “confidence”: 1.0}], “end_token”: 2, “from_context”: false}], “utterance”: “connect to diamond”}, “context”: {“client_name”: “mycroft_cli”, “source”: [“skills”], “destination”: “debug_cli”}}
2020-09-07 11:42:16.511 | info     |    31 | voice | bus: {“type”: “mycroft.skill.handler.start”, “data”: {“name”: “rasaskill.handle_talk_to_rasa_intent”}, “context”: {“client_name”: “mycroft_cli”, “source”: [“skills”], “destination”: “debug_cli”}}
2020-09-07 11:42:16.518 | info     |    31 | voice | bus: {“type”: “speak”, “data”: {“utterance”: ""hi, i am diamond "", “expect_response”: true, “meta”: {“dialog”: “hi, i am diamond.”, “data”: {}, “skill”: “rasaskill”}}, “context”: {“client_name”: “mycroft_cli”, “source”: [“skills”], “destination”: “debug_cli”}}
2020-09-07 11:42:16.526 | info     |    31 | voice | bus: {“type”: “enclosure.eyes.blink”, “data”: {“side”: “b”}, “context”: {“destination”: [“enclosure”]}}
2020-09-07 11:42:16.526 | info     |    31 | voice | bus: {“type”: “recognizer_loop:audio_output_start”, “data”: {}, “context”: {}}
2020-09-07 11:42:16.558 | info     |    31 | voice | bus: {“type”: “recognizer_loop:audio_output_end”, “data”: {}, “context”: {}}
2020-09-07 11:42:16.560 | info     |    31 | voice | bus: {“type”: “mycroft.mic.listen”, “data”: {}, “context”: {}}
2020-09-07 11:42:16.563 | info     |    31 | voice | bus: {“type”: “enclosure.eyes.blink”, “data”: {“side”: “b”}, “context”: {“destination”: [“enclosure”]}}
2020-09-07 11:42:16.819 | info     |    31 | voice | bus: {“type”: “active_skill_request”, “data”: {“skill_id”: “skill-rasa-chat.wel”}, “context”: {}}
2020-09-07 11:42:20.223 | info     |    31 | voice | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:42:22.227 | info     |    31 | voice | bus: {“type”: “mycroft-reminder.mycroftai:reminder”, “data”: {}, “context”: {}}
2020-09-07 11:42:22.229 | info     |    31 | voice | bus: {“type”: “mycroft-configuration.mycroftai:configurationskillupdate_remote”, “data”: “updateremote”, “context”: {}}
2020-09-07 11:42:30.238 | info     |    31 | voice | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:42:31.823 | info     |    31 | voice | bus: {“type”: “speak”, “data”: {“utterance”: ""hi, i am diamond "", “expect_response”: true, “meta”: {“skill”: “rasaskill”}}, “context”: {“client_name”: “mycroft_cli”, “source”: [“skills”], “destination”: “debug_cli”}}
2020-09-07 11:42:31.825 | info     |    31 | voice | bus: {“type”: “active_skill_request”, “data”: {“skill_id”: “skill-rasa-chat.wel”}, “context”: {}}
2020-09-07 11:42:31.830 | info     |    31 | voice | bus: {“type”: “recognizer_loop:audio_output_start”, “data”: {}, “context”: {}}
2020-09-07 11:42:31.861 | info     |    31 | voice | bus: {“type”: “recognizer_loop:audio_output_end”, “data”: {}, “context”: {}}
2020-09-07 11:42:31.863 | info     |    31 | voice | bus: {“type”: “mycroft.mic.listen”, “data”: {}, “context”: {}}
2020-09-07 11:42:40.252 | info     |    31 | voice | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:42:46.823 | info     |    31 | voice | bus: {“type”: “mycroft.skill.handler.complete”, “data”: {“name”: “rasaskill.handle_talk_to_rasa_intent”}, “context”: {“client_name”: “mycroft_cli”, “source”: [“skills”], “destination”: “debug_cli”}}
2020-09-07 11:42:50.267 | info     |    31 | voice | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:42:52.269 | info     |    31 | voice | bus: {“type”: “mycroft-reminder.mycroftai:reminder”, “data”: {}, “context”: {}}
2020-09-07 11:43:00.284 | info     |    31 | voice | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:43:10.296 | info     |    31 | voice | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:43:20.311 | info     |    31 | voice | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:43:22.315 | info     |    31 | voice | bus: {“type”: “mycroft-reminder.mycroftai:reminder”, “data”: {}, “context”: {}}
2020-09-07 11:43:22.319 | info     |    31 | voice | bus: {“type”: “mycroft-configuration.mycroftai:configurationskillupdate_remote”, “data”: “updateremote”, “context”: {}}
2020-09-07 11:43:30.327 | info     |    31 | voice | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:43:40.342 | info     |    31 | voice | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:43:50.356 | info     |    31 | voice | bus: {“type”: “mycroft-date-time.mycroftai:timeskillupdate_display”, “data”: {}, “context”: {}}
2020-09-07 11:43:52.360 | info     |    31 | voice | bus: {“type”: “mycroft-reminder.mycroftai:reminder”, “data”: {}, “context”: {}}

","
i am still looking for a fix to this issue. i realized that the mycroft client logs two listen events triggered from outside. they are 5 seconds apart and i think they are related to the issue.
 13:59:35.152 | debug    |    25 | mycroft.skills.intent_service:handle_utterance:346 | utterances: ['connect me to diamond']
 13:59:35.159 | debug    |    25 | mycroft.skills.intent_service:handle_utterance:366 | padatious intent: {'name': 'mycroft-date-time.mycroftai:date.future.weekend.intent', 'sent': 'connect me to diamond', 'matches'~~~~
 13:59:35.160 | debug    |    25 | mycroft.skills.intent_service:handle_utterance:367 |     adapt intent: {'intent_type': 'skill-rasa-chat.wel:startchat', 'skill_rasa_chat_welchatwithrasa': 'diamond', 'target': none~~~~
 13:59:35.211 | debug    |    31 | mycroft.client.speech.mic:trigger_listen:527 | listen triggered from external source.
 13:59:50.522 | debug    |    31 | mycroft.client.speech.mic:trigger_listen:527 | listen triggered from external source.

finally i could fix the issue here. it is a default setting in the get_response method that infinitly retries to get a response from the user. it does so by uttering the last response again, it seems. if i set this to 0, i have no repetitions.
https://mycroft-core.readthedocs.io/en/latest/source/mycroft.html?highlight=get%20response#mycroft.mycroftskill.get_response

hey thanks for posting the solution!
i’ve updated the code on that blog post to include num_retries=0 on the get_response method so others hopefully don’t run into the same thing.

i see from the code that this is deliberate:


github.com


mycroftai/mycroft-core/blob/dev/mycroft/skills/mycroft_skill/mycroft_skill.py#l454

    is_cancel (callable): function checking cancel criteria
    validator (callbale): function checking for a valid response
    on_fail (callable): function handling retries


""""""
num_fails = 0
while true:
    response = self.__get_response()


    if response is none:
        # if nothing said, prompt one more time
        num_none_fails = 1 if num_retries < 0 else num_retries
        if num_fails >= num_none_fails:
            return none
    else:
        if validator(response):
            return response


        # catch user saying 'cancel'
        if is_cancel(response):
            return none







i wonder if the docs should be more specific as this confused me too. how does this sound:
num_retries ( int ) – times to ask user for input, -1 will retry indefinitely until a validated response is received.
note: user can say “cancel” to stop the retries. also, if there is no response, an additional retry will be attempted. set num_retries to 0 to avoid this.
i feel that a more ‘expected behaviour’ solution would be to remove the retry on no response - especially if there is no validator. ignoring a prompt feels like a valid way to cancel. thoughts?

i like the wording update, did you want to do a pr for it?
i’d be hesitant to change the default behaviour without thinking through all the cases and putting it out for broad consultation. it would affect a large number of skills.

indeed a discussion about this would be useful. i felt that the default behavior should be that the assistant does not ask again by itself. no user input can have so many reasons and the most likely to me is that the user simply intended to remain silent.
what was the initial assumption to set the default to -1?

that was well before my time i’m afraid…
looks like it was included in the original implementation. so not sure how much thought went into it or not. perhaps the assumption behind -1 default was that if the user is responding to the prompt each time then they are attempting to provide a valid response.
in terms of 1 retry on no voice input, i’d say that’s a middle ground so that if a user misses the response window they don’t need to start all over again, but also that the device doesn’t end up prompting indefinitely.
"
161,how to hack a data stream pipe,general discussion,"
speaking of hitting a wall. this one is solid concrete to me.
i’m coding a skill for moonlight embedded (opensource streaming c compiled client for nvidias gamestream) and while the advancement is satisfying i’ve got a big problem i can’t wrap my head around.
the most crucial thing is to get the config / game lists / connection tests / pairing right if you want to solely rely on voice to have an appropriate experience. while these (game lists, connection tests, etc)  are easily manageable with subprocesses popen and grabbing stdout to process the data in mycroft, pairing is giving me serious headaches (since i have to grab pin to be voiced out mid-process).
it seems the pairing credentials are written to stdout only at the end of  the process (see strace log), eg you either cut off the pairing or enter a wrong pin. there seems no way to catch the data as it is appearing on screen. i’ve gone several python routes (threading, threading+queue, select.select, and so on. the list would be too long, this would be one viable set of code)
the deeper problem is - like stated in the topic - that i have to hack a data stream pipe that is directed to a special file (hopefully that assessment is right  ). i even can moonlight pair host > /dev/pts/0 and it will appear on the ssh-ing screen in real time, while moonlight pair host > out.txt ( and all redirections or script -c to file, …) will act like described above.
why is that? and does anyone know a (hacky) way to get around that? i simply don’t want to drop pair (to be handled in terminal), that wouldn’t be the whole package.

if someone wants to reproduce it (needs a nvidia-card-host with nvidia geforce experience installed and gamestream activated (geforceexp>settings>shield))
sudo apt-get install moonlight-embedded
and simply running the python code linked above (it will print out what’s happening ; change hostname/ip in .py as fit; threading/queue has to be pip’ed in)
in comparison do a moonlight pair hostname/ip
(have an eye on host)
",
162,mycroft skills merge conflict,none,"
is there a way to purge the skills and add the back in? tbh im not sure what i did but i have been hacking around with mycroft and now when i run msm update this happens
 20:07:17.989 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on mycroft-volume: mergeconflict
 20:07:18.189 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on mycroft-ip: mergeconflict
 20:07:18.482 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on mycroft-personal: mergeconflict
 20:07:18.661 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on fallback-query: mergeconflict
 20:07:18.952 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on mycroft-playback-control: mergeconflict
 20:07:19.170 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on mycroft-spelling: mergeconflict
 20:07:19.445 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on mycroft-hello-world: mergeconflict
 20:07:19.698 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on fallback-unknown: mergeconflict
 20:07:20.143 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on mycroft-reminder: mergeconflict
 20:07:20.216 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on mycroft-version-checker: mergeconflict
 20:07:20.426 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on respeaker-io-skill.domcross: mergeconflict
 20:07:20.460 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on mycroft-pairing: mergeconflict
 20:07:20.632 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on mycroft-naptime: mergeconflict
 20:07:20.766 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on mycroft-fallback-duck-duck-go: mergeconflict
 20:07:20.843 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on mycroft-stop: mergeconflict
 20:07:20.993 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on mycroft-audio-record: mergeconflict
 20:07:21.292 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on mycroft-singing: mergeconflict
 20:07:21.477 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on mycroft-alarm: skillmodified(uncommitted changes:
     d __init__.py
)20:07:21.706 | error    |   841 | msm.mycroft_skills_manager | error running install_or_update on mycroft-configuration: mergeconflict
","
not sure on that.
but maybe you can check your changes or rollback your local changes in skill folder by using git commands:


git diff (show local changes)


git reset


git reset --hard



for anyone who has this problem this is what i did to fix it.
i ran
msm update #to get the errors
msm remove ""skill"" #remove the skill that errored
msm install ""skill"" #installed it back 

there might be a quicker way but this is how i fixed it.

got this multiple times this weekend. even indicating all files were edited locally. yet this is fluctuating and most likely isn’t happening with the next boot.
the origin has to be found in the (update) code (or it’s a github thing), so i wouldn’t make a fuzz about it (at least for now)
"
163,cannot install speedtest skill,support,"

mycroft speedtest893×199 158 kb

i tried to install the speedtest skill with the “install” voice command, but it always thinks that “mycroft-release-test” is a possible option too. then i say that “speedtest” should be installed but that simply doesn’t work…
","
stt splits “speed” and “test” thus the low match percentage (53%) and alternative. quick tests, even with german as lang, gave good results if you speak them in one go.
if speech doesn’t work -> mycroft-msm install speedtest
"
164,error in audio log,support,"
i am testing this out right now and don't know why my speaker isn't working.  i am using the audio jack on the pi for connection with a usb microphone.  the speaker is old and junky but it does work.
    019-11-29 13:35:43.718 | info     |   760 | mycroft.messagebus.load_config:load_message_bus_config:33 | loading message bus configs
  carnegie mellon university, copyright (c) 1999-2011, all rights reserved
  mimic developers, copyright (c) 2016, all rights reserved
  version: mimic-1.2.0.2 ()
2019-11-29 13:35:52.840 | info     |   760 | __main__:main:38 | starting audio services
2019-11-29 13:35:52.856 | info     |   760 | mycroft.messagebus.client.client:on_open:67 | connected
2019-11-29 13:35:52.898 | info     |   760 | mycroft.audio.audioservice:get_services:56 | loading services from /home/pi/mycroft-core/mycroft/audio/services/
2019-11-29 13:35:52.919 | info     |   760 | mycroft.audio.audioservice:load_services:100 | loading chromecast
2019-11-29 13:36:02.906 | info     |   760 | mycroft.audio.audioservice:load_services:100 | loading mopidy
2019-11-29 13:36:02.920 | info     |   760 | mycroft.audio.audioservice:load_services:100 | loading mplayer
2019-11-29 13:36:03.111 | error    |   760 | mplayer__init__:<module>:20 | install py_mplayer with pip install git+https://github.com/jarbasal/py_mplayer
2019-11-29 13:36:03.116 | error    |   760 | mycroft.audio.audioservice:load_services:106 | failed to import module mplayer
importerror(""no module named 'py_mplayer'"",)
2019-11-29 13:36:03.119 | info     |   760 | mycroft.audio.audioservice:load_services:100 | loading simple
2019-11-29 13:36:03.152 | info     |   760 | mycroft.audio.audioservice:load_services:100 | loading vlc
2019-11-29 13:36:03.419 | error    |   760 | mycroft.audio.audioservice:load_services:121 | failed to load service. nameerror(""no function 'libvlc_new'"",)
2019-11-29 13:36:03.423 | info     |   760 | mycroft.audio.audioservice:load_services_callback:168 | finding default backend...
2019-11-29 13:36:03.427 | info     |   760 | mycroft.audio.audioservice:load_services_callback:172 | found local
","
i solved this.  i did an update and reboot.

hi good afternoon, how di you update your mycroft?
i am experimenting problems with audio… no sounds, it was working… but suddenly it is mute… i tried also mycroft-mic-test and no sound
i can try same you did, thanks
"
165,internet acrchive,skill suggestions,"
archive.org wrapper
i thought this would be in line with the maker space dynamic, and some of fun stuff in the creative commons is on there.    i want this skill to be able to access the archive.org documents and read them.  or play the music.  maybe upload literature or dication too.
what third party services, data sets or platforms will the skill interact with?
i don’t know of an api.  i am set to start this skill.
what will the user speak to trigger the skill?
“connect to the internet archive”
“checkout information on electric engines”
“upload the file "" "" to the internet archive”
what phrases will mycroft speak?
what skill settings will this skill need to store?
login information perhaps, genre interests for what type of material
see https://mycroft.ai/documentation/skills/skill-settings/ for more information
other comments?
put any other comments you think are relevant in here
",
166,devsync 20200928,general discussion,"

",
167,devsync 20200923,general discussion,"

",
168,devsync 20200921,general discussion,"

",
169,activation code only five letters,none,"
i was told that the activation code mycroft gave me was only five characters and
","
and?
there’s numbers and letters, you may have gotten both.  check the logs (/var/log/mycroft/*) to see what it was as well.

hey all let’s try to keep this civil please. i’m going to close this thread as it doesn’t seem like we’re getting anywhere quickly.
in case other stumble across this. it can sometimes sound like you are only getting 5 characters but we’ve never seen a verified case of it happening. most likely there was a sixth character but it was hard to hear.
the code is read out, and output to the cli so be sure to check there. it’s also logged in /var/log/mycroft/skills.log so in your terminal you can run:
grep ""pairing code"" /var/log/mycroft/skills.log

and it should return only the lines containing your pairing code. if there are multiple, always take the last one. if pairing fails for any reason mycroft will try again but the pairing codes expire so a new one will be issued and any old codes won’t work.
if anyone has logs that show a five character code then please get in touch as we’d want to look into that.

"
170,no audio on on the snap version 20 2 4,support,"
hi all
i think i just downloaded the snap version and i get the gui, buy no sound of mike level indication. i am using ubuntu 16.04. what can i do?
thanks
aaron
",
171,halloween voice,none,"
if we could get a halloween voice?  that’d be great mmmkaaay?
","
funny thought. has someone tinkered around with librosas’ melspectrogram / pytorch spectrogram in line with other tools to change the characteristics of a model? given a rock solid multispeaker model, you should be able to create a respectable bandwidth of flavours.
stumbled over voice cloning lately and his spin off resembler where a whole buisiness model evolved around

there is this by @j_montgomery_mycroft and
@jarbasal




""hey, mycroft"" - laugh maniacally! general discussion


    does someone in the community have time to whip up a quick skill that makes mycroft laugh ( randomly ) like a maniac?  maybe call it the “maniac laugh skill”. 
example of laughs that would be great ( but without the doors soudtrack ) 
 
“install maniac laugh skill” 
“hey, mycroft, laugh like alexa” 
“hey, mycroft, random laughter” 
it’s worth a copper mycroft challenge coin.  if you work as a team, each team member gets a challenge coin ( up to 10 team members ).  needs to be production ready wi…
  



with tacotron you used to be able to get some witch-cackling level of creepiness early in training.  would be fun to work one of those up.
"
172,devsync 20200914,general discussion,"

","
37:00 qt gui card system discussed.
(posted this in the comments at youtube, but this is the better place for it)
to comment on the qt-gui card system. i don’t think it is that bad ! see my quick and dirty demo from may this year;

resolution of that screen is 720p and overall size is perhaps bigger as the smaller sized screen, but if it comes to the card system working yes or no, i don’t think it is bad at all. indeed some further tweaking would be nice, but nothing fancy.
look at the weather skill for multiple cards
look at the date for a simple one card without backgroud
look at the wiki skill for a one card including background.

oh and btw, (and this is more for @ken-mycroft) do you see those led’s lighten up as they should. the whole mycroft software stack and therefor the skill itself runs as a mycroft user not as root.
c’mon guys, really? sudo to control a couple of led’s… (the solution has been talked about multiple times and lies in udev, the right group for the gpio and make sure the user running the software is within that group. ping me and i will have a look how to help you)
*sarcasm with a wink mode off

just finished watching the last dev-sync of the 28th, but as there is no thhread yet, going to comment on it here where above video demostrates the performance as well.
it kind of suprises me that you guys doubt the performance of the gui while those parts are easily handled by the gpu’s nowadays.
you start the dev-sync about precise running at 45% continously for 4 cores and then debate the performance of the gui?
qt5 is being handled by the gpu. i don’t know kivy (yet) but almost sure it is also handled by he gpu.
above video is qt5.12 lts with the kf5 framework running on wayland fully hardware accelerated by the gpu. it hardly consumes either memory or cpu cycles or at least compared to precise. most (if not all) of the “thinking” time of mycroft is because of the actual python software and network polling/delays
yes, qml has to be compiled at runtime and might take some micro seconds, but even that is cached within the users home directory.
@gez-mycroft @j_montgomery_mycroft @ken-mycroft @chrisveilleux
more than happy to give you guys some numbers. just let me know what you guys would like to see/test/benchmark

so, i just saw this. i must admit i am a bit confused. first i don’t know what you mean by ‘do you see those leds lighten up’. now when you reference group permissions yes, but at the end of the day that group will need elevated privileges anyway to hit the geopixel code with the gating factor not being the gpios as i understand it but rather the use of /dev/mem. these are not your grandfather’s leds, these are basically a daisy chain of serial devices supporting 256 bit color. consider this a ping and feel free to email me at ken.smith@mycroft.ai. i would love nothing better than to not need elevated abilities to control a led.

regarding 28th dev meeting
qt runs fine even on a pi3, not even a b+ , it also runs on android…
i don’t understand why kivy was even made, this was the only part of the mark2 that was ready, why throw away 2 years of perfectly good work? not even to mention all the skills that wont be cross compatible…
i’d love to see the rationale behind the whole kivy debacle, but a dev meetings is probably not the place for it, as you said the performance issue mostly comes from precise, it should have been migrated to tflite or something instead of devoting resources to a new gui implementation

*crap, the sarcasm wink thingy did not get through. should have used smileys.
meant, the respeaker stuff just running under a normal user conteolling the spidev stuff from python without the need for sudo.
started of the wrong foot here. will email you.

no worries. i am not thin skinned and i appreciate the input. just that the respeaker leds go through the usb interface if i am not mistaken. the sj201 is directly controlled. but seriously, if there is a way to handle those leds without using sudo i would love to do that.

you can add the user to the gpio group, and that should allow them access to the relevant pin devices, but not to /dev/mem if that’s the only concern.

hey, we originally looked into the kivy framework late last year when we were having trouble getting qt to run reliably. things have changed a lot since then and a lot of the dependencies are now available in the mainstream repo’s which makes it all a lot more straight forward. as you’ve shown, i think the qt framework is running great now.
i’ve been having a quick look at system monitoring and reporting packages so we can get some real data on the resource usage. will probably also create a baseline against picroft with no gui at all. at the moment i’m leaning toward nmon but any other suggestions welcomed.

quick & dirty show case;



mycroft 20.8.0 (ae72ebd247f877f643573791650e63c4044604d1)


mycroft-gui 1.0 (2212941674de7f23fc0aa076b7051deb5ada0856)


hdmi screen in 1920x1080 resolution (worst case type of scenario)
linux: linux version 5.4.68-v8
build: (gcc version 9.3.0 (buildroot 2020.02.6-59-g85b5f2c6ff-dirty))
release  : 5.4.68-v8
version  : #1 smp preempt thu oct 1 08:14:22 cest 2020
cpuinfo: cpu architecture: 8
cpuinfo: bogomips : 108.00
cpuinfo: cpu part : 0xd08
cpuinfo:
number of cpus: 4
machine  : aarch64
nodename : mycroftos-7aadc9
/etc/*ease[1]: name=buildroot
/etc/*ease[2]: version=2020.02.6-59-g85b5f2c6ff-dirty
/etc/*ease[3]: id=buildroot
/etc/*ease[4]: version_id=2020.02.6
lsb_release: not found


basically the weather skill is the most demanding skill as it runs multiple cards at the same time all with animated pictures.
"
173,mark ii update september 2020,mark ii,"
originally published at:			https://mycroft.ai/blog/mark-ii-update-september-2020/
we got the second revision of the sj201 daughterboards delivered in late august. unlike the first run, these were printed as a double sided pcb, however they still required many components to be hand soldered. 
once assembled, kevin could flash the firmware and start testing the audio!

sj201 v0.66 – firmware flashing

testing the audio – a play by play from kevin
xmos and the usb sound card both come up as usb devices in windows. the usb audio device previously also came up on the last boards. and i just realized i flashed the usb adaptive, instead of the usb/i2s firmware.

usb audio devices detected from sj201 v0.66

and we have audio recording! i have 2 sets of mic, i’m using the spk0641 right now, i marked that on the backside of the board (641) so i’ll know which mic is best.

sj201 v0.66 – backside

i’ll give it a full set of testing later today, playing music and recording.  attached is my voice recording, from about 1 foot away, in a decently quiet room, but there is a fish tank and fan behind me, no music playing.


audacity screenshot showing audio waveform680×520

sj201 v0.66 – audio input waveform

another system test passed. the mic recording switch directly turns off the mics. i was talking during this time, then flipped the physical switch, and the mics stopped recording.

audio input waveform showing hardware cutoff

moved the switch back to this position, it allows recording. also, i’m now powered through the 12v. the update i did worked. so far, everything is working. soldering on the leds next and the raspi connector. 

sj201 v0.66 – hardware cutoff for microphone

findings
overall we’re very pleased with this new revision. we have addressed many of the problems discovered in the first prototype run, but there are still a few issues to resolve. 
first we discovered a problem with the output from the amplifier. an incorrect capacitor was spec’d, and replacing that solved the problem. fortunately that’s just a value change on the part.
we have also noticed some popping sounds when the audio is routed over i2s from the cm108b soundcard to the xmos 3510. this doesn’t happen when we don’t use the i2s, but we’ve decided we need to simplify the data path. fortunately xmos came through on a firmware update and we can reverse the data path to be xmos → soundcard, which will simplify the design and avoid this issue altogether. we’re testing that now.
 
sj230 – laser-cut enclosure for prototypes
for development purposes, we have created a relatively simple laser-cut enclosure with a 3d printed acoustic chamber. this provides a quick case for our development team while we move into the next phase of testing. 


laser cut device enclosure with pcb on top and screen on front.1024×768

laser cut enclosure for the mark ii prototypes

once we have cut a few and verified the design, we’ll upload the cad files to the mark ii hardware repo so that anyone playing from home can get up and running quickly whilst keeping your components safe.
 
what’s next?
our focus at the moment is ensuring that the sj201 meets all of our hardware requirements and getting an sj201 prototype out to everyone in our team. once we are confident that the hardware itself is solid, then we can start thoroughly testing running mycroft on this new hardware.
we’re also looking ahead at the third pcb print run. the next revision will be a fully assembled pcb from the board house.
","
just an idea: as volume make things cheaper. perhaps maybe people within the community would like to also order a next revision 3 full pcb print run version to work on mark-2 support.
if the price is right, i would…

hey j1nx,
our first goal is to get them out to all of our internal team. i’m also yet to get my hands on an sj201!
and the next milestone is getting the dev kits out to the community. first preference for this has to go to existing dev kit backers, but it could be a good idea to see if others want to order these also.
if others would be interested in this please shout out.




 gez-mycroft:

first preference for this has to go to existing dev kit backers


i have never heard of a “mark-ii dev kit” before - am i qualified as a regular mark-ii backer?

yeah it was one of the tiers you could select, it doesn’t include the case so was intended for the diy makers / hackers. but if you backed a dev-kit or above then i’m sure we can work something out 

@gez-mycroft perfectly understandable. i was just late for the game, but considering the kickstarter date in comparison of the current date, most of the current community members are 
"
174,devsync 20200916,general discussion,"

",
175,talksocket a voice initiated multi assistant mobile experience,general discussion,"
originally published at:			https://mycroft.ai/blog/talksocket-a-voice-initiated-multi-assistant-mobile-experience/
who wants to access mycroft on their phone? we all do, right? and while we want that for you as well, we aren’t there yet. but talksocket is. at least, they can provide a critical piece of the experience. that is why we are partnering with them to help make mycroft a supported option for their new hands-free device. it’s a great first step.
they are working with otterbox and popsockets to bring you voice access to your phone. we learned about them on kickstarter and immediately backed the project. then we reached out to see how we could get mycroft added as an option. they’ve agreed to support mycroft as the third voice assistant on the talksocket in the future. while we don’t have a timeline for an official mycroft app for either android or ios, this device solves a major technology roadblock to making that happen.

still_241920×1080 960 kb

when we’ve explored the possibility of developing mycroft for android and ios apps in the past, it became immediately obvious that the user experience would be a major issue. both google and apple deny anyone else the ability to continuously run a voice assistant. the app must be manually launched, and the listener triggered with the push of a button. this may not seem like much, but it’s a huge barrier for anyone who has come to expect a voice-initiated experience and significantly changes how you use it.
there are of course privacy and security concerns with letting any application have this type of access, however we believe that these can be effectively managed with the appropriate policies and procedures. the current stance of companies like google and apple are an abuse of their position as platform providers to prevent genuine competition at a critical time in a growing market.

still_081920×1080 818 kb

talksocket tackles this head-on, providing a voice-initiated multi-assistant experience that doesn’t exist on any other device today. your support for the talksocket project now will help to make mycroft for android and ios a development priority once the mark ii is released.
 
check it out on kickstarter
",
176,hey mycroft we have to talk talks without hey mycroft,general discussion,"
people can talk without key words. i thought that would be a great selling point for mycroft. here are my thoughts:

a request could start with “hey mycrft we have to start talking”.
“if you think” mycroft begins the audio recognition based on the level.
as long as volume is detected, and the server request starts.
during the playback of the answer, no detection is performed.
now mycroft is waiting for audio again. if you’re say “great talking” the end of the recognition.

much time could pass between inquiries. this is the mode when you alone in the room a book reads or you want to call several things in a row but takes time to think about the question
what do you think. would that be possible.
","
there’s the concept of a transaction within skills, ie, “hey mycroft turn on some music.”
and mycroft would ask what you want to hear/feel like listening to, and you respond with “my favorites playlist”.
there’d have to be a ridiculous decision tree attached to what you’re discussing to account for relevance, but not implausible.

i know the talk tree. however, i am always forced to an answer and i can not change the time for the answer.
of course, listening permanently is not so great in privacy, but with homeserver no problem

hey lately i got deepspech to work on my server in german. then i started a similar dictation skill https://github.com/gras64/deepspeach-dictation-skill . i don’t have the ability to set up mycroft for permanent listening for a certain period of time. deepspech can currently run from the resources on a “normal pc”. does my skill have to connect the server directly or can i do it in the future via a mycroft function.
"
177,overwrite amazon echo dot with mycroft,general discussion,"
hi,
i have a free echo dot arriving in the post after buying a new heating system. i don’t want alexa due to data privacy concerns. is there a known way to overwrite the echo with mycroft?
cheers,
joe
","
no.  there’s a previous thread about this, basically the hardware is not easily hackable and may not be sufficiently potent for mycroft.

what baconator said, but if you still want to try then a starting point could be this: https://github.com/echohacking/wiki/wiki

a shame, but thanks for the responses! guess i’ll hold onto it as a christmas present or sell it. cheers
"
178,hermod voice suite,general discussion,"
hi all,
excited to see the the discussion around an open hardware design.
i recall a comment about not being a hardware company but that is the thing i want most from mycroft.
i’ve been developing an open source voice dialog service suite https://github.com/syntithenai/hermod inspired by snips and integrating rasa. i put up a demo (https://edison.syntithenai.com) using voice to help fill crosswords.
my suite is designed for network distribution and web integration but can also be used voice only standalone similar to mycroft.
i’d started tinkering with this way back when i signed up as a mycroft backer and even then saw mycroft as an open hardware solution.
a pi4 with usb speakers and playstation mic array works ok but ‘barge in’ doesn’t work and it’s ugly.
as a backer i will completely understand if i am never sent hardware. i see that mycroft as an organisation has done much good in creating a software stack and a vision of an open source platform for developing voice applications where you can make choices about how you compromise your privacy.
snips offered a similar vision with offline recognition and developer friendly api but closed source and ultimately sold to sonos.
picovoice is doing some great work with webassembly solutions with asr and nlu in the browser but again closed source and no local training of models.
hopefully options for mutual pollination. i’ve certainly gained context from your source tree.
i couldn’t get past the possibilities of network distribution using the mqtt bus .
last i checked you’re not using streaming speech recognition which is in my mind essential for responsiveness.
thanks for your great work. keep the open source light shining.
cheers
steve
","
looks like there’s options for streaming stt’s already: https://github.com/mycroftai/mycroft-core/blob/6f33cc0553235df483dab109778798f0e2e9fbdc/mycroft/stt/init.py#l352
using deepspeech (not streaming) i have unnoticeable latency as well.

my ignorance, ta for the link
s

about one year ago i got deepspeechstreamserverstt working with an older ds-version (0.5 iirc). besides the proof that it is working my takeaway was that latency did not improve significantly. as far as i understand inference performance was improved with current ds-version. with a beefier machine running ds-server the latency will probably drop… (…note to myself: add deepspeech to my todo-list again…)
(i am still using google-stt as models for my native language - german - are not feasible for every day use, wer >15%)

gpu definitely helps after loading.

?
if you don’t stream then you have a latency of at least the wav duration.
deepspeech is a streaming model, though.

thanks for the words of encouragement.  it is an interesting project.
quick question, how are you adding dialog?  we’ve been giving some thought to piping missed queries ( from opt-in community members only ) to a queue and encouraging the community to tag them as a group effort.
we’re glad to have you as a backer and are looking forward to seeing what you come up with.

hi joshua,
i’m not clear what you mean by adding dialog. i’ll take a punt and guess you dialog training data/example sentences for the nlu engine.
the rasa philosophy seems to be that real input from end users is the gold.
i’m taking a varied approach using a combination of

initial hand carved rasa training data
chatito to build rasa training data especially for integrating large numbers of entities.
capturing all end user nlu requests as rasa training data to database.
providing a tool to end users that lets them fix incorrect nlu results by highlighting section of the last transcript text and assigning intent/entities.

i’m still coming to terms with the art of building an nlu/dialog model with rasa.

less is more. fewer intents means less possibility of conflict between intent examples. rasa offers some great tools for validating and finding conflicts in models.
integrating many independant plugins/vocabulary packages is problematic because of potential for uncoordinated overlap. google/alexa have the annoying plugin preposition. “hey alexa, ask meeka music to play some pop”
be generous with entity examples. you don’t have to get all possible values but hundreds are required to do a good job of picking up entities that are not in the training data.
transcription accuracy particularly of uncommon words used in entities can vary based on the asr engine being used. action handlers can assist by using fuzzy matching of the value provided from the transcription to a legal set of values.
allow for fallback in action handlers. for example, in my crossword model, asking for an  of a  without providing a value for  results in falling back to a general search rather than one based on wikidata. even though the intent was incorrect, there is still a useful outcome.
with rasa nlu, entity matches(and corresponding values in session)  are considered when selecting an intent match. some intent examples may only include the entity. this increases the prospect of overlap but is helpful to user engagement if used cautiously.

in general, don’t go too general. pick a small set of critical intents to support your application and expand cautiously from there.

i’d like to feed all of the intents from opt-in members to an engine where they can then be marked for the appropriate skill.  i’d like to give the community access to the engine so that members of the community can come in and mark intents.
for example: play “huey lewis and the news” should play music by the band, not trigger the news skill.
there are dozens and dozens of examples like this that we need to figure out how to deal with.  in some cases ambiguity needs to trigger a clarification question from the ai.
i’d also like to start looking at how we can engage in more meaningful dialog ( like replika.ai ) where members can use the dialog engine to simply hold a conversation.
the way i see it the path to success here ( a more natural conversational agent ) is to use data from opt-in users ( as you said - golden ) and effort from the community ( platinum ) to build learning loop.
i wrote a blog post on the overall approach a few years ago.  would be awesome to have some folks trying to put it into practice.

hey joshua,
the premier open example of what you are talking about is surely common voice (https://voice.mozilla.org/) that is collecting validated open licenced recordings for deepspeech (or any other engine). users can record text snippets and vote on other users text recordings. two positives without negative means inclusion of the recording in the data set.
vital for speech recognition where a huge amount of data is required for good results. the common voice team are suggesting 10,000 hours of validated audio as a target.
with 1500 hours, the current english language model struggles for accuracy with names and less common words.
possibly less so for a hotword engine where a small amount of data can be used to train a model for a single speaker although a general hotword model still takes a lot of data.

i see nlu as a different beast. my understanding is that an nlu model needs hand crafting to avoid overlap and balance accuracy vs features and can only be pushed so wide before the user experience suffers. domain specific. less is more. kiss.
large data sets still help.
audio to text is a one to one mapping. converting a sentence into meaning is one to many depending on context. “tell me more”… about what?
feeding context with dictated text into an nlu model can help with overlap.
google and alexa have imposed switching context in requiring a skill name after the hotword. ok for voice apps that continuously engage but “ok google, ask meeka music to …” for music control is a pain.
rasa uses a secondary machine learning model based on example stories of sequential nlu intents and actions to select between scored possibilities from the nlu model.
session context is also used in routing stories and decisions. an intent “stop” is interpreted by the news reader rather than the music player because the session remembers that it was the last used skill in the dialog history.
mycroft and jovo support explicit switching on and off intents using a state variable held by the skill server session. i recall dialogflow had something similar.
there is also potential context from

speaker identification
location tracking
my cloud data eg recent searches and messages and contacts
a range of hotwords
my social network

maybe pessimistic about the contraints of nlu engines but i think mycroft integrators will find they have to be careful with what skills they combine.
i’ve certainly seen plenty of applications start to fall over as i load up the plugins.
your goal is a broad ecosystem of skills which will inevitably overlap. curation/certification would help. skill naming like google/alexa is a possibility. (or multiple hotword switching)
session inclusive context like rasa, combined with other technologies like speaker identification would be helpful.
lots of examples is probably the best medicine.
i think your suggestion of an open licenced repository of sentence/context to meaning maps would be a great resource for the voice developer community to source initial training data for their application domain.
a challenge is finding a format and tools so that central data can be converted to various nlu training data formats.
it would need to allow for context filters to be attached to training data.
categorised by application. music. search. news. weather. joe’s brainbender skill.
curation to flag overlap using additional context filters expecting that users will select their own training data as needed.
include some tools to assist selecting from the data set, merging training examples with entity data using chatito and converting to various destination nlu training formats.
from another angle, categorised lists are very useful in generating training data. a model can get by on few intent examples but good entity recognition requires significantly more data. lists of animals, fruits, people names, movie stars or code for scraping that data would be very useful to improve intent recognition in a restricted selection of domains. rhymezone shows how wordnet can be used to find related words based on frequency of closeness in an english corpus.
the rasa team have a github repository https://github.com/rasahq/nlu-training-data.
scraping the mycroft community skills repository would certainly be a nice chunk of nlu training data.
google put up lots of public domain examples for dialog flow.
there’d be heaps of starter material.
there is also lots of folks out there who have shared thoughts and code related to voice development in blogs and repositories.
tools to encourage capture and collation from live systems like rasa-x generate lots of data that can be particularly valuable in providing sentence structures for intents that may not have been considered in generated initial data.
a website with search tools would be nice but a github repo would do.
readme with useful links

skills
  music player
    tools
      scrape
      chatito expand entities
    responses
      - library of spoken responses in ?? format
    examples
      - many files in json format that specify example, nlu parse, related contexts ??
        - allow for entity expansion from scrape sources
  news
  joe's brainbender skill
  
tools
  conversion
  scrape
  
lists
  readme with links to open data sources
  famous_people
    - plain text lists of entity values

i’d have some time to feed an nlu sources repository if anyone else is interesting in owning and promoting it.
cheers
steve

you’re right.  is a tough problem.  i do believe it can be solved with careful thought and enough effort.
as a rule, i generally start simple and work out from there as i get a deeper understanding of the problem.  in this case i think that is essential because the problem is so complex that even framing it gives me a bit of a headache.
i think it would be great to start with a simply capturing real world intents, tagging the intent and object and dropping them into buckets.  stuff we can’t work out would go in a “tbd” bucket.
we can then use the data to train our basic intents using padatious ( or rasa if that makes more sense ).
once we have that working, we move on to a deeper effort?

hey joshua,
i’ve had a crack at realising our discussion about a place to share nlu training data. (https://opennludata.org)
reads and writes mycroft/jovo and rasa  format files as well as importing text.
progressive web application keeps all data local (until publish) and works with many thousands of records.
all published skills are committed to an mit licenced github repo.
no ranking/scoring as yet as i’m trying to avoid/minimise dependance on an api endpoint and i suspect there is better value in  algorithmic scoring of skills based on size and example distribution.
i’ve raided a sample of skills from the various systems. would love to see contributions.
cheers
steve
"
179,mic works wake word doesnt,support,"
hi all, first post and i’m less than a week into mycroft so bear with me if this is poorly formatted or phrased.
i’m running picroft on a raspberry pi b3+ with a matrix voice hat with audio output coming through the 3.5mm jack on the pi. i flashed picroft onto the sd card with an image from the mycroft site on my pc.
as the title describes, i am able to get my microphone to pick up fine, tried setting timers and it can hear me decently, even without speaking loudly or directly next to it and always picks up what i’ve said nearly perfectly. however i have only managed to trigger the wake word twice in the last 8 or so hours of fiddling i’ve done, even speaking softly and directly into the microphone from 6 inches to a foot away.
i have attempted to change the volume via alsamixer and tested maybe 8-10 different input level combinations, as well as tried “hey mycroft” and “hey jarvis” as my wake words. i’ve tried changing from pocketsphinx to precise, and i’ve tried programming in my own basic sphinx wake word, with no luck. i’ve used ‘mycroft-config edit user’ though admittedly i don’t know what settings are optimal there. i’ve used arecord -d 10 test.wav and aplay test.wav to test, as well as the mycroft-mic-test and both indicated the microphone is recording, albiet somewhat quiet, even though the capture setting in alsamixer for my matrix voice is at 75% currently. i’ve checked to see if any skills are malfunctioning, and the only one that fails to load is the stock skill, so i don’t see how that could factor in. i have had some undervoltage warnings from the rpi, the new psu is on the way, but i don’t think this should be causing any issues if the mic is still being picked up.
does anyone have any advice for this? i’ve browsed the forum fairly extensively over the week and tried the various solutions listed above as recommended in various similar situations. i’d include logs but honestly i’m too new to mycroft to know which ones are important here, so let me know which logs could help.
thanks in advance.
","
i had the same problem with the wake words. i am using a pi4.  i ended up uninstalling and reinstalling. my device end up waking and working fine. it may have be something in software or i may have done something. i’m sure there is a way to fix the issue with coding,but i didn’t want to reinvent the wheel. it worked and  i’m happy.

the voice documentations’ trobleshooting section rerouting to hal code examples.
https://matrix-io.github.io/matrix-documentation/matrix-hal/examples/microphone/
maybe you find an answer with these. at least you can determine where the problem is coming from. mycroft or hardware.
what comes to my mind is the sensitivity (precise) resp. treshold (pocketsphinx) option in the conf.
""hey mycroft"": {
        ""module"": ""precise"",
        ""phonemes"": ""hh ey . m ay k r ao f t"",
        ""local_model_file"": ""~/.mycroft/precise/models/something.pb""
        ""sensitivity"": 0.5,  // higher = more sensitive
        // ""trigger_level"": 3   // higher = more delay & less sensitive

        //""lang"": ""en-us""
        //""threshold"": 1e-90
        },


sorry for late reply, first week back at class has me busy.



 sgee:

what comes to my mind is the sensitivity (precise) resp. treshold (pocketsphinx) option in the conf.


i tried fiddling with this setting a little but not knowing a great deal about what i was doing i only tried altering the threshold setting to: “1e-50” and “1e-30”, i believe that would be the proper adjustment but i did have difficulty understanding the guide on the topic that i read.
i’m going to try a fresh install today to see if that resolves the issue, and then if not i suppose i will try working with the threshold setting more and failing that, see if it’s a problem with the matrix voice itself, though i previously had it running flawlessly with a snips.ai installation previous to this, so i’m a little unsure why it would be having issues now.
thanks again, i’ll report back whatever i find.

if you use the default (initial) settings, you should adjust first and foremost sensitivity, since pocketsphinx (which uses the phonemes; with the tuning option “threshold”) is only the fallback option. precise is way more acurate, hence the better wake word listener.
if sensitivity resembles the trained model sensitivity (i’m still waiting for a confirmation on that) then the default is “0.2”. work your way up from there.



 sgee:

“sensitivity”: 0.5, // higher = more sensitive


never played around with trigger_level, but if sensitivity fails, you might want to give it a try

the threshold is for pocketsphinx.

after the fresh install, everything seems to be working, though i’ve had some weird occasional failures to connect to the internet. seems to have fixed the problem.
"
180,rasa as a fallback skill,support,"
i’ve been running mycroft on a respeaker core v2 and have integrated it with my robot running ros .  i intend to do a writeup on how to integrate ros with mycroft… wasn’t very hard but i did have to modify mycroft source code a small amount.  but at this time, mycroft won’t startup properly unless ros is already up and running on the main machine, so i need to figure out how to handle that before i post my solution (mycroft starts quicker than the main machine starts ros).
as for the ros integration, i have a mycroft skill that accepts commands (examples below), sends them to ros and plays back the responses that ros returns:

turn around
to to twin’s room
what is your battery level
move forward 1 meter

this works really well, but the robot doesn’t have much to offer at the moment other than to follow commands.  i am still very new to mycroft and not entirely sure i understand how it all works, so please bear with me if i’m off-base on things.  i’m interested in using rasa to provide a level of conversation capability to the robot, but jamesmf’s skill requires that the user enter chat mode and then exit it when done.



github



jamesmf/skill-rasa-chat
mycroft skill for chatting with a rasa agent. contribute to jamesmf/skill-rasa-chat development by creating an account on github.






before i embark on trying to do this, is there any reason that the rasa skill could not become a fallback skill?  that is, when talking to mycroft, it will pass my robot commands (e.g., turn around) to the ros skill and everything else would get passed to the rasa skill as a fallback?  if so, would this remove the need of having to start and stop a chat?
","
i suggest that you look here for a similar but (in my view) cleaner solution:



mycroft – 17 oct 19



connecting rasa to mycroft: a guide - mycroft
mycroft recently attended an open source hackathon hosted by the royal bank of canada. one team focused on integrating mycroft with rasa, and have






i implemented this in a rasa-x docker stack + mycroft container. it works nice except this: mycroft repreats the last bot response after receiving no input for 5 secs (which is annoying). if you find a similar problem, please drop a message in the thread i made regarding this issue.

thanks for that link.  however, i ran into a great deal of complexity with just getting rasa running and see it has a pretty big learning curve.  i also think i misunderstood it’s use case… i just needed a basic chit-chat bot and its clearly much more than that.  after getting the aiml chatbot from the mycroft marketplace installed and working, it sort of does the job… better than nothing by far.
"
181,call pass to a fallback skill from another skill,support,"
if i create a skill that gets activated on a “what is …” utterance and the skill can’t answer the question, is it possible to pass the utterance on to a fallback skill?
",
182,resource cancel voc for lang pt br not found,support,"
i’ve tried to install wiktionary and play spotify from the skills marketplace, but keep getting a skill error. i’m using it on arch, with pt-br lang, . tried to answer yes, but since it only recognizes portuguese i guess it won’t work anyway.
this is what appears in the cli client:

   11:45:08.249 | error    | 27069 | mycroft.skills.mycroft_skill.mycroft_skill:on_error:835 | skill.error
  traceback (most recent call last):
    file ""/home/dinhego/mycroft-core/mycroft/skills/mycroft_skill/event_container.py"", line 66, in wrapper
      handler(message)
    file ""/opt/mycroft/skills/mycroft-installer.mycroftai/__init__.py"", line 86, in install
      if not self.confirm_skill_action(skill, dialog):
    file ""/opt/mycroft/skills/mycroft-installer.mycroftai/__init__.py"", line 379, in confirm_skill_action
      resp = self.ask_yesno(confirm_dialog,
    file ""/home/dinhego/mycroft-core/mycroft/skills/mycroft_skill/mycroft_skill.py"", line 489, in ask_yesno
      resp = self.get_response(dialog=prompt, data=data)
    file ""/home/dinhego/mycroft-core/mycroft/skills/mycroft_skill/mycroft_skill.py"", line 436, in get_response
      return self._wait_response(is_cancel, validator, on_fail_fn,
    file ""/home/dinhego/mycroft-core/mycroft/skills/mycroft_skill/mycroft_skill.py"", line 459, in _wait_response
      if validator(response):
     file ""/home/dinhego/mycroft-core/mycroft/skills/mycroft_skill/mycroft_skill.py"", line 425, in validator_default
  return not is_cancel(utterance)
file ""/home/dinhego/mycroft-core/mycroft/skills/mycroft_skill/mycroft_skill.py"", line 421, in is_cancel
  return self.voc_match(utterance, 'cancel')
file ""/home/dinhego/mycroft-core/mycroft/skills/mycroft_skill/mycroft_skill.py"", line 582, in voc_match
  raise filenotfounderror(
filenotfounderror: could not find cancel.voc file
11:45:08.228 | info     | 27075 | __main__:handle_utterance:72 | utterance: ['sim']

","



 diegorapoport:

with pt-br lang


there are only pt-pt ressources (in that regard)
(yet it should fall back to en-us if ressources aren’t there)
"
183,volumeskill and setvolume at initialisation,support,"
maybe this flew under the radar (or i misinterpreting things), but given your system volume configuration isn’t the same as the default_level (set up by mycroft.config/volumeskill itself) there is no adjustment at the initialisation of the skill. as i understand the code _setvolume is only event driven. am i wrong?
and vol_before_mute would keep it that way, never really setting default_level unless you fire up the event (eg set it yourself).
in my case the state got stored at 75 vol. no matter the default (volumeskill; default_level=6) nor the custom entry (mycroft.conf; default_level=10) makes any difference.
","
created a pr on github
"
184,suggested vernacular for patent troll nsfw,general discussion,"
i’m writing an update for the community about our patent troll and need some help.
i’m trying to figure out how to best refer to the company and the trolls that are suing us.
my preference so far is “leaking colostomy bag of putrefying feces” or “shitbag” for short, but i want to open the floor to the rest of our community to see if there are any other ideas.
so if you have an idea for a phrase and short name that accurately encapsulates the giant sucking blowhole that is our friendly neighborhood patent troll, please drop it here so we can consider it.
","
they’re oxygen thieves.

thank you @baconator
really?  no one else?  these folks are putting the entire project, community and future of open voice tech in jeopardy.  if they win this bs lawsuit they could potentially zero out all of the investment and crowdfunding that has gone into the project.  all for their own personal gain.
and you don’t have any creative ways to describe them?

insults are childish, i dont get the point of this post
no one agrees with the trolls, all of us would have a thousand insults for them. but “help me insult these guys” is a weird thing to ask of a community
i don’t think its smart either, didn’t you get into issues already because of your previous “call to arms” ? it sucks to see your 1st amendment ignored, but you already had to remove your previous insults once


techdirt.



patent troll gets court to order startup it sued to 'edit' blog post; troll...
in february, we wrote about how a patent troll, voice tech, had sued a small open source voice assistant company, mycroft ai, claiming infringement. mycroft ai and its founder/ceo joshua montgomery had put up a blog post about the situation, which...





to be clear i’m on your side, these trolls deserve the insults! i’m just confused with the purpose of this post

if you need help with your insults, here is an api for that
https://insult.mattbas.org/api/insult

while i strongly disagree with how the patent troll is operating, i think mycroft ai also needs to be careful around how their interaction in this matter is being perceived in the wider open source, and open voice community, and how that reflects on how mycroft ai approaches relationships in general.

i agree with jabas here. i don’t think it’s beneficial to create an environment where they can gather bias.
isn’t the whole open source project prone to be patent trolled? how is this handled earlier on?

@jarbasal and @kathyreid are spot on here. i don’t think there is anyone who is in this community that sides with the patent troll, but name calling is a waste of energy, effort, and blood pressure. and also plays into their hands. we all have invested in this project creatively, and / or financially and don’t want the project to be railroaded by this troll, but we need to take the high road on this one.

@jarbasai, @kathyreid & @sgee - taking that line of reasoning, why don’t we just pay them?  they only wanted $30k and we’re more than $100k into the case already.
if we’re just going to do the same thing as all of the buttoned up, professional companies out there, lets go ahead and just pay.  that would be the professional and responsible thing to do after all, yes?
the only thing mycroft gets out of the case is, maybe, some positive pr.  the only reason the main stream media and the eff are tracking this case at all is because of my over-the-top rhetoric in setting our policy.
if we’re just going to be staid and boring with our communications we might as well settle because we won’t get any additional coverage and pursuing the case is a waste of money that would be much better spent developing the mark ii.

thinking of this a bit deeper, i want to add a couple of additional thoughts.
-1. @jarbasai - you understand that they are going to sue you, yes?  they’ve asked for your name, your company’s name and your address.  it’s the full time team here that is protecting you.
if i’m asking for some help here it is probably for a reason.  i’m an extremely busy guy and don’t waste time on things like this unless i’ve got a plan.
if you want input into the patent troll strategy, feel free to have your legal team reach out to our legal team and we’ll set up a call under privilege.  we could use additional paid counsel.
-2. controversy drives media coverage.  boring patent filings don’t.  if we want to get anything out of this case we need to amp up the controversy and make damn sure people are reading about it.
the end state of all of the spending we’re doing on this patent case should be a reputation in the patent troll community similar to newegg’s.  “don’t mess with the mycroft  they will bring the pain, and they will do it in public.”
-3. i feel like i’m being shushed by a nanny here.  when did the mycroft community become so adverse to a bit of fun?
( edited for formatting and some stray words because of some weirdness with newlines and list items )

have you talked over this with your attorneys? i’m assuming you haven’t, because given the history of your case and how the patent trolls have reacted, this seems like the type of conduct that might be frowned upon. i know you guys are a small shop and might not have the money to hire polsinelli and a pr firm, but i’d hate to see your actions come back to bite you in the butt.

we’ve retained maschoff brennan ( lee cheng of newegg fame ), lathrop gpm ( ip ), maneke law group ( state patent troll case ), mark brown law ( ip ) and the avant law group ( ip ).
we’re on track to spend hundreds of thousands of dollars defeating these trolls.  that is money that could go into software development, hardware development, marketing, inventory, r&d or sales.  instead, we’re wasting it fighting against this sack of deformed monkey toenails ( thanks for the api link @jarbasal ).
on the pr side: we’ve worked with pr firms in the past and they’ve universally under-performed.
we built our company, brand, community and reputation by being authentic.  we are known for privacy because we believe in privacy and walk the walk.  we are known for openness because we believe in openness and operate transparently.  pr may be needed for companies that hide things and engage in nefarious activities ( like the deformed mass of odious anal puss patent trolls ), but mycroft hasn’t needed pr because we’re up-front with our community.
the judge was wrong to order the editing of our original post, but we chose not to fight it because we didn’t want to piss her off.  that doesn’t change the fact that she was wrong.  if we wanted to press the issue we could easily get her overruled by a higher court.  we’ve just chosen not to.  i’m sure she had the best of intentions, but if she tries to infringe on my first amendment rights again i’ll appeal the ruling and i’ll win.  but i’ll do so at the time and place of my choosing.
now pissing off the ignorant snot bag (still loving this api ) patent abusers at tumey llp?  that i’m more than happy to do.  frankly i hope they die of complications related to genital herpes.
we are, despite the over the top rhetoric, being strategic.  strategic doesn’t mean staid and polite, it means developing an overarching strategy and following it through to it’s conclusion.
a good strategy can be summed up in a single sentence.  ideally it can also be published in full view of the enemy and still succeed.  ours is simple:
“fight all patent abusers robustly both in the court of law and the court of public opinion in order to deter future abuse.”

i don’t think anyone disagrees that you should be spend the money defending the suit. that’s exactly the right course. you can’t cave to these guys.
and the judge was absolutely, extraordinarily wrong to order you to edit your post.
but do you really want to goad the plaintiffs? if it’s about goading the judge to do it again (which it obviously isn’t) then there wouldn’t be much point, because you’ve already got grounds for appeal. so it’s about goading the plaintiffs. i’m inclined to taunt them as well. they suck. don’t do it. the judge has already demonstrated that she’s willing to rule on their hurt feelings.
what’s the point? you’ve got expensive attorneys. let them focus on the defense. anything you do that prompts the plaintiffs to file another motion for this, that, or the other bullshit, is that much more cruft that your expensive attorneys have to address. they should be focused on the absurdity of the suit, the plaintiff’s interpretation of the patent, the patent itself, and the plaintiff’s history of what are plainly fraudulent, extortionate lawsuits, based on dubious patents, which the plaintiff knows or should know are not enforceable. and that’s to say nothing of the fact that, in this case, the dubious patent doesn’t even describe mycroft!
that’s plenty of work for the lawyers. you can speak forcefully about the issue without taunting the guy.




 chancencounter:

that’s plenty of work for the lawyers. you can speak forcefully about the issue without taunting the guy.


ah, but we can’t.  a forceful, well thought out argument that is above the fray…yawn.  no coverage.  speaking forcefully is only effective if the message is widely broadcast.
i fully expect tumey and his band of inbred cat nipples to file additional motions about the grievous nature of revealing ( shock ) their names, business addresses, professional affiliations and other information that is ( drumroll ) already in the public domain.
that’s kind of the point.  controversy drives coverage.  that’s why they want us censored in the first place.  they want to operate in the shadows.  controversy, colorful language and outside-the-box approaches to pr help to shine a media spotlight on them.

if they’re surprised that their names are on the intarwebz, they shouldn’t have put them out there.  

freeloading leach like scum.

i shouldn’t weigh in here, but scum is a good word. it might convey the original idea while also not selecting sexist type words.

hey, forum newbie here.
i just discovered mycroft due to the eff coverage of hon. ketchmark’s order. loving it so far.
i worked for about 6 years in politics and law, before transitioning to software engineering. i am certainly not a lawyer, but i’ve known a lot of them. and judges and journalists and lawmakers, etc. i’ve gotten a “backstage pass” to hot-button litigation.
wanted to chime in real quick here and remind people that these types of cases are not won by playing nice. insults are uncomfortable and ugly. i know that. when i was new to law, i had this starry eyed idea that lawyers make calm and reasoned arguments in front of an unbiased judge who would rule in a fair and honorable way.
ahahaha, nope.
sorry if i rub anyone the wrong way here. obviously, i am new and missing a lot of context, but. . . a stranger has just walked up and punched your project in the face.
don’t tell anyone to calm down and be reasonable. a stranger has just walked up and punched your project in the face.
cases like this can be won (sometimes) but only by showing the opponent that you are willing to do whatever it takes (staying legal, of course). if you don’t want to be personally involved in the ugliness, that’s fair. it is ugly. just please, do what you can to unite as a community. having viable alternatives to spyware voice assistants is something i care about a lot and i really want mycroft to survive this.

on the original topic of the post, though, i second @krisadair. nothing sexist/ableist/racist, etc. that’s a quick way to turn public opinion hard against you. like for serious, do a web search on any insults you pick, and make sure.

welcome @bat_fan - good to hear another perspective on it.
and couldn’t agree more. we’re ready to do whatever it takes, but don’t want to lose our morals along the way.
"
185,help with padatious matching ambiguous things,general discussion,"
hey!
i’m trying to parse commands you could use in a chatroom, like
ban hallowed for 1 week for trolling
this is my (simplified) .intend:
ban {user} for {time} for {reason}
the user and reason entities are just set to :0, but time is defined like this:
#(hours|days|weeks|months|years)
expected output would be:
user: hallowed
time: 1 week
reason: trolling
but instead i get:
user: hallowed
time: 1 week for trolling
reason: none
in this case, using regex would be probably better, but i’m still confused why i get that as a response because for trolling is nowhere near the .entity specified.
thanks a lot in advance:)
","
i believe it is the ‘for {time}’ that is causing the issue everything after the for is considered {time}. agreed regex is likely better. you may have to parse the second part separately.
"
186,system audio working skill audio not working,support,"
i have picroft running on a rpi 4 with an amp hat. system audio works perfectly, both input and output. however, when i try to use some of the audio skills (emby and pandora), playback begins but i get no sound. this is not a problem for other skills (such as podcasts, which works perfectly). as part of my install i changed the system default output device to the hat, but that is the only audio setting i have messed with.
any guidance would be appreciated.
","
maybe skills.log or audio.log logged something suspicious?

logs reside in /var/log/mycroft/

no errors during playback, but since i restarted the system to get the log i did see some errors when booting:
audio.log (last boot is at the bottom):
https://termbin.com/a7e0
skills.log (again, last boot at the bottom):
https://termbin.com/u223
i think the error about py_mplayer is the issue, that the skills use py_mplayer for playback but podcasts/the system don’t? i’m not really familiar with how picroft is set up.
thanks for your help!

mplayer is phased out by now, this isn’t the issue. “simple” is the default audio service and since the skill isn’t specifically hammering down another service, this is the one that is called. mycroft only preloads existing audio services at this moment. the stopped stream is likely caused by
traceback (most recent call last):
  file ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  file ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  file ""/home/pi/mycroft-core/mycroft/audio/__main__.py"", line 73, in <module>
    main()
  file ""/home/pi/mycroft-core/mycroft/audio/__main__.py"", line 69, in main
    audio.shutdown()
  file ""/home/pi/mycroft-core/mycroft/audio/audioservice.py"", line 473, in shutdown
    self.bus.remove('mycroft.audio.service.play', self._play)
  file ""/home/pi/mycroft-core/mycroft/messagebus/client/client.py"", line 213, in remove
    self.emitter.remove_listener(event_name, func)
  file ""/home/pi/mycroft-core/.venv/lib/python3.7/site-packages/pyee/_base.py"", line 136, in remove_listener
    self._events[event].pop(f)
keyerror: <bound method audioservice._play of <mycroft.audio.audioservice.audioservice object at 0xb38d0df0>>

since _play thrown this error. maybe @forslund can shed some light on this.
yet the logs also show errors with timezonefinder and wolfram (not related) wich is worth inspecting

thank you for pointing that out … installing the dependencies for wolfram and dateandtime via pip actually seems to have fixed the playback issue for pandora. i’m getting a new error for emby but that seems to either be a skill-specific issue or a configuration issue:
main: [src/mpg123.c:685] error: access to http resource http://10.0.0.18:8096/audio/13467/stream.mp3?api_key=f52078cf7f4947b9be1e3a050dde7c53 failed.
if anyone has any ideas i’m all ears, but i think i’m going to make a separate thread and ping the skill author since this seems to be a problem with that specific skill.
thanks again!

you might want to use the diagnostic intent (given you’re running lang en-us)
yet, at this stage (the skill allready knows what to call) i think it’s an other part that’s failing

ok, so this actually may or may not be a skill issue. most of my music is in flac, so when i was testing with random stuff playback didn’t work, but also no errors. i do have a few mp3s, so i tried one as a test and it played perfectly! so it’s an issue with flac playback through simple audio.
is this an issue with the emby skill being hard-coded to call mpg123 to resolve the media stream? or is there a larger issue with flac support on picroft? i could theoretically have the emby server transcode the stream but that would defeat the purpose of having all of my music in lossless …
thanks again for your help.

most likely flac-simple related. there is no equivalent in audio_utils for flac
i’m curious what would happen if you push everything to vlc (mycroft.conf)
  ""audio"": {
    ""backends"": {
      ""local"": {
        ""type"": ""simple"",
        ""active"": true
      },
      ""vlc"": {
        ""type"": ""vlc"",
        ""active"": true,
        ""duck"": true
      }
    },
    ""default-backend"": ""vlc"" #default=local
  },

figured it out! i set vlc as the default, but that didn’t work. however, looking over the python for the skill, the skill assumes the files are mp3s by using the “stream.mp3” handler. i attempted to use the generic “stream” handler, but vlc wasn’t able to determine what format it was. for the time being i’m going to leave it with the “stream.flac” handler, since that’s 90% of my music. in the meantime, i’m going to look into the new stream api that emby has and push an update to the skill when i’ve got it figured out.
thank you for helping me get to the bottom of this!
"
187,wolfram alpha not responding,mycroft project,"
hi all,
we are aware that the wolfram alpha skill is currently not responding to queries.
our api key has been blocked and we’re reaching out to get it back online. this means that mycroft may not have answers to everything that it can usually respond to, particularly general knowledge questions.
this should be a quick resolution once we’re able to get through to the wolfram team so i don’t anticipate this to be a long outage.
will post again once we have more details.
",
188,pr and code review process,mycroft project,"
hi all,
i wanted to post an early draft of some updated processes that i’ve been looking at. the first is diving into our process for code contributions - taking pull requests, reviewing the code and hopefully merging the the changes.
mycroft is a big project, with a lot moving parts. we have over 350 individuals contributing to over 150 repositories on github. whilst not all of these are active repos, it can be a real challenge keeping up with the incredible contributions of our community. this isn’t just hard for us, it can also be incredibly frustrating for the contributor. i know how much time and effort goes into the pull requests we receive. i also know how much it sucks not knowing if or when it will get merged, and why.
open source comes with a lot of benefits, and a lot of expectations. by improving these processes, we hope to better meet the expectations you have of us, and make clear the expectations that we believe are reasonable to have of our contributors.
so without further ado, here is the first of a series of new process docs. it’s an early draft because i believe strongly that these processes need to be created together with the community. it has had a little input from some of our long term contributors already, because i wanted to get a decent skeleton together first, but everything us up for discussion.


docs.google.com



process: community code contributions
process: community code contributions this document is intended to outline (or develop) a new process for end-to-end management of code contributions from the mycroft community. aims: provide an efficient process for all code contributions to the...






please have a read and let me know what you think.

are we covering all the major scenarios?
do we need to be more explicit / more flexible?
are there more processes we can automate or improve?

also, anytime i make a substantive change to the document i’ll post here so that people can get notifications that a change has been made (unfortunately you can’t do this directly in google docs). once it feels like we’re coming to agreement, i’ll merge the contents into our main documentation.
",
189,mycroft and ducking pulseaudio,support,"
so i have enabled two features in the config:

“pulse_duck”: true
“duck_while_listening” : 1.0

i somehow don’t understand what happens. so when i say hy mycroft it does not duck the currently played music. even while listening to my question or order it does not duck the music. however once mycroft gives the answers it does duck the music.
how can i duck the whole process from the beginning of recognizing the wake word till the answer has been given?
it must be a mycroft problem since the pulseaudio stream seems correctly configured (see music and phone icon):

image1264×422 35.2 kb

edit:
when i edit the https://github.com/mycroftai/mycroft-core/blob/bcb426ab6980a5c221a963ca44bb3c1c88791771/mycroft/util/audio_utils.py#l64
file and change it to “phone” instead of “music” it will at least also duck when the “i’m listening now” sound from mycroft is played. but still does not duck while listening to my question…
","
those looks like the old ducking controls. sadly those didn’t work reliably (streams that were ducked could stay ducked if they exited while ducking). what should work is tts ducking, while mycroft is speaking.
""tts"": {
  ""pulse_duck"": true
} 

should enable that part. ducking while listening i think was removed back in 19.08.
the environment you changed to phone should be music so the tts can duck media played by mycroft skills.
i had a hack that implemented ducking while listening but it was a bit dodgy so it was never included.

hmm ok… it would be a great feature… so i need to do it by myself by controlling the audio player that mostly just plays through this audio sink…

can report a ducking problem, too. (with only ```“pulse_duck”: true`)
not only he stays ducked despite
 18:56:54.971 | info     |   790 | __main__:handle_wakeword:67 | wakeword detected: hey-mycroft
 18:56:55.443 | debug    |   784 | volumeskill | muting!
 18:56:55.444 | debug    |   784 | volumeskill | finding alsa mixer for control...
 18:56:55.429 | info     |   790 | __main__:handle_record_begin:37 | begin recording...
 18:56:55.477 | debug    |   784 | volumeskill | volume before mute: 75
 18:56:55.478 | debug    |   784 | volumeskill | 75
 18:56:55.478 | debug    |   784 | volumeskill | 0
 18:56:57.797 | debug    |   784 | mycroft.skills.settings:download:337 | no skill settings changes since last download
 18:56:59.515 | debug    |   784 | volumeskill | finding alsa mixer for control...
 18:56:59.555 | debug    |   784 | volumeskill | 75

vlc (used by the skill tunein in this example) throwing an error
18:57:00.922 | debug    |   784 | mycroft-skill-tunein_johnbartkiw:find_station:122 | aliased search_term: antenne bayern lounge
 18:57:01.177 | info     |   784 | mycroft.audio.services.vlc:stop:79 | vlcservice stop
[ade4c958] http stream error: local stream 1 error: cancellation (0x8)
[adf0c290] http stream error: local stream 1 error: cancellation (0x8)
[adf94738] prefetch stream error: unimplemented query (264) in control

i can definitely say that this has to do with pulse_duck, because i’m switching back and forth trying to find the culprit.
does this have something to do with this? where is the vlc configuration stored?
or is this just an aftereffect?
i get the impression that he’s not muting after hearing the wakeword, he is silenced to begin with (beep should’t be muted)




 sgee:

i can definitely say that this has to do with pulse_duck


cough or maybe not. made the error to add high-priority = yes
to the daemon.conf. module documentation
(“the decision whether a stream has high or low priority is made based on the stream role”)
a comment on the feature: the ducking (if you listen to a radio stream for instance) is a little late and transitions into the text read (the first second or so)

still had problems with the ducking… sometimes. news, emby, much less tunein not recovering after some tts action. and i can’t make a rhyme out of it since the whole audio thing is a big black box to me.
so, desperate times need desperate measures, i thought to myself… the volume skill has an option “ducking while listening”. since i turned that off things works smoothly (not tested long-term), still ducking due to conf settings when i want him to duck (tts, speech input; given that the backend option syncronized properly. although i don’t get why he is ducking at speech input since ducking is only mentioned in the {“tts”:…} section )
is he performing some double duck here   that kicks s*** south?

there is a “pause” feature that is always active with the “simple backend” so when mycroft is listening the audio playback is paused.
i’ve seen it stick muted when using module-role-cork, if you’ve got that module loaded try replacing it with module-role-ducking

i’ve unloaded module-role-cork as i set up the ducking module, as suggested.
is there a difference between loading and then unloading a module on the one hand and commenting out said module on the other? i’ve experienced some unexpected silence as i commented out 2 and 3 instead of
load-module module-role-ducking
unload-module module-role-cork
unload-module module-suspend-on-idle

that should be the same result 
gonna setup ducking here and see if i can replicate it…

this might be interesting in that regard:
he’s not able to restore the volume and stays dark after  this
2020-09-08 11:56:37.388 | info     |   801 | audioservice_vlc:stop:79 | vlcservice stop
2020-09-08 11:56:43.634 | error    |   801 | concurrent.futures | exception calling callback for <future at 0xa07f2f90 sta                                                                                     te=finished raised attributeerror>
traceback (most recent call last):
  file ""/usr/lib/python3.7/concurrent/futures/_base.py"", line 324, in _invoke_callbacks
    callback(self)
  file ""/home/pi/mycroft-core/.venv/lib/python3.7/site-packages/pyee/_executor.py"", line 60, in _callback
    self.emit('error', exc)
  file ""/home/pi/mycroft-core/.venv/lib/python3.7/site-packages/pyee/_base.py"", line 111, in emit
    self._emit_handle_potential_error(event, args[0] if args else none)
  file ""/home/pi/mycroft-core/.venv/lib/python3.7/site-packages/pyee/_base.py"", line 83, in _emit_handle_potential_error
    raise error
  file ""/usr/lib/python3.7/concurrent/futures/thread.py"", line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  file ""/home/pi/mycroft-core/mycroft/audio/audioservice.py"", line 338, in _restore_volume_after_record
    restore_volume()
  file ""/home/pi/mycroft-core/mycroft/audio/audioservice.py"", line 330, in restore_volume
    self.current.restore_volume()
attributeerror: 'nonetype' object has no attribute 'restore_volume'
2020-09-08 11:57:06.742 | info     |   801 | mycroft.audio.speech:mute_and_speak:127 | speak: spiele jetzt station antenne                                                                                      bayern lounge von tune in

bit of context: this happened after stopping the stream from emby and calling for a stream “antenne bayern lounge” from tunein

i got from the code that this shouldn’t be an issue since logically there shouldn’t be a stream present at that time - in my case - so nothing to restore. but is the follow up stream “lounge” not present at that time?
but since this is only a volume thing, i should be able to set this via speach. which i can’t.
another thing that’s bothering me is this line here. is he mixing things up here?
btw, if someone wants to reproduce this, this is with the audio backend pushed to “vlc”
  ""audio"": {
    ""backends"": {
      ""local"": {
        ""type"": ""simple"",
        ""active"": true
      },
      ""vlc"": {
        ""type"": ""vlc"",
        ""active"": true,
        ""duck"": true
      }
    },
    ""default-backend"": ""vlc""
  },

thanks for pointing to the vlc backend. i’ve been trying with the “simple” backend so far. have you tried to disable the vlc backend ducking (and just using the pulse ducking for tts)?

i’ve gone the mycroftos way of dealing with streams: pulses’ module combine_sinks.
that setup seems stable in regards of my broken (or misguided) stream problem. and the streams seem to duck just right, instead of warping into each other with the ducking one respond too late (most of the time).
i still consider this as a kind of bandaid, yet remedied my problem that i dragged over several weeks with no conclusion.
disableing vlc ducking wouldn’t have been an option since i want my node-red reporting my sensory stuff as they trigger (with music playing most of the time)
another thing: after applying the mycroftos approach (combine_sinks) i realized that due to vlc pre-flags cork i’ve ended up with a corked (filter start_corked; to 40% vol) stream (despite unload cork on my end), which was overcome by commenting out load-module module-stream-restore. also, the stream is sent with a video role applied.
maybe this is throwing things off, so maybe a useful lead

you could be on to something;


github.com


mycroftai/mycroft-core/blob/dev/mycroft/util/audio_utils.py#l63

                  "" supported formats are {keys}.""
                  .format(uri=uri, keys=list(extension_to_function.keys())))
        return none




# create a custom environment to use that can be ducked by a phone role.
# this is kept separate from the normal os.environ to ensure that the tts
# role isn't affected and that any thirdparty software launched through
# a mycroft process can select if they wish to honor this.
_environment = deepcopy(os.environ)
_environment['pulse_prop'] = 'media.role=music'




def _get_pulse_environment(config):
    """"""return environment for pulse audio depeding on ducking config.""""""
    tts_config = config.get('tts', {})
    if tts_config and tts_config.get('pulse_duck'):
        return _environment
    else:
        return os.environ









i am not a  python dev, but somethings i read as, the stream should be “music” to be able to be ducked.

this should pass the role:music per the deepcopied _environment which is chosen (_environment or os.environ) if the config holds pulse_duck: true
and got called by _play_cmd (play_wav/play_ogg/play_mp3)
either this (the call) is not the case in regard of vlc processing or isn’t working at all (processing with simple suggest otherwise), since the role remains video.
that said, i wouldn’t know why this should be overcome by a combined sink. makes no sense to me. the bigger lead in my estimation is that vlc sets cork=true (which means asks for being muted and paused) and the stream arriving at a muted state from tts processing resulting in a paused stream.
from https://www.videolan.org/developers/vlc/modules/audio_output/pulse.c
pa_stream_flags_t flags = sys->flags_force
                            | pa_stream_start_corked
                            | pa_stream_interpolate_timing
                            | pa_stream_not_monotonic
                            | pa_stream_auto_timing_update
                            | pa_stream_fix_rate;

btw the whole shebang seems not necessary with mycroftos (rpi4 64-alpha9). i suspect the config entry ""listener"": { ""mute_during_output"": false, ...
to circumvent the vlc problem. (besides the pulse setup)

as i recall from way back the pulse audio role ducking by default only ducks the music stream. that’s why it’s set there so skills and audioservice using those commands can be ducked. vlc should play using the “music” pulse role (atleast on my setup).
there is some pulseaudio config for ducking all roles but i have not been able to get that to work.

would be interesting to see your config. i would like to revisit this at some point and see if a good simple (and stable) setup can be achieved.

i might have a proper look at pulseaudio as well. considering the gui and therefor video playback being present, only ducking audio roles doesn’t cut it all aymore.
will try to reproduce this using vlc and the youtube (video) skill and report back.
"
190,mycroft read sharing,general discussion,"
hey everyone fairly new to programing here and work exclusively in bash at the moment so i’ve piggybacked on the inbuilt mycroft features. however, i wanted to share this little script that i’ve made that sets mycroft as a voice reader (which i use to read my textbooks) because i haven’t found a simple reader done up anywhere.
set -o nounset # treat unset variables as an error
set -e # exit on error
_readpath=""$1""
while read -r line; do
  mycroft-speak <<<""$line""
done <""$_readpath""
unset _readpath

simple but it works usage mcre /path/to/file
","
if you’re planning on doing significant amounts of tts like this you should set up your own local (or cloud, your choice) tts server.  it’s frowned on (and possibly will get you rate limited) to do against the mycroft tts server.

hey i tried setting up my-croft locally initially buy i just couldn’t get it to work (that being said i’ve only been programing and on linux for 4 months and it was over a months ago) so i may re look into that installing the “preferred way” for my distro didn’t work anyways (arch linux) so ill look back into it (any recommended reading?) i have the hardware to run locally either on my computer or an external mini server. to be honest i never really thought about that. also are you suggesting i run all of mycroft locally or just the text to speech portion? or are they inseparable? thanks in advanced.

if you’re feeding it lots of text, then running tts locally would be a good idea. you can configure it to use a local tts server separate from the default choices.
there’s a few different choices for tts, from really basic (mimic) to more human-sounding choices (mimic2, mozilla tts, tacotron[2], etc).  they all have some level of instruction on setting up locally now, though may require a bit of digging.

hey thanks i’ve had to sideline the the project for a little bit i’m back in school full time and i couldn’t get it to pause reading one i started however ideally i’d like to run the whole mycroft project locally but i’d prefer to keep the more pleasant if possible google voice if possible i’ll have a look at those two when i have some time i was also looking at some separate standalone tts software as well i was mostly using the read to read from my bash scripted to do list
"
191,mycroft gui on my mark ii first light,mark ii,"
hi,
well maybe this is all easy and obvious for the people here but i had a hard time to setup the software stack on my build of the mark ii prototype and did not find to much info in this forum.
in short:

i used picroft as basis
made it work with pulseaudio
built a slightly modified version of the mycroft-gui based on a custom qt 5.12 and custom build of the kde components.

i will not copy all the steps to this forum post but you can find my complete build docu/notes here: my mycroft-mark-ii build wiki
this is the first light on my mark-ii:

as said, maybe this is all just obvious for the pros here but for me it was a steep learning curve.
i hope it is useful to someone.
have fun - guhl
","
that is amasing. i like it, and your wiki is helpfull. i were thinking on how to add mycrfot-gui to picroft and you got it. would be nice to have it in default or as an option so picroft with an screen had a gui. (i mainly wants that for my kitchen pi as it has a 7"" screen and is built into the wall.

amazing!!! bookmarked.
check out these;



github



mycroftai/mycroft-gui-mark-2
contribute to mycroftai/mycroft-gui-mark-2 development by creating an account on github.








github



mycroftai/skill-mark-2-pi
contribute to mycroftai/skill-mark-2-pi development by creating an account on github.








github



mycroftai/skill-mark-2
control of the mycroft mark 2 enclosure. contribute to mycroftai/skill-mark-2 development by creating an account on github.






hi j1nx,
interesting, i’ll look at that. the question is why did i not find that?
but on the other hand i am happy that i did not and did take the road to get the quickview/qml stuff running.
have - fun guhl

hey andlo,
thanks for the feedback.
my next steps will be to provide the qt and the kde stuff as deb packages so that one can install them without compiling them. let’s see if i can get webview compiled too.
and probably i will fork the mycroft-gui repository and provide the modified gui and a working dev_setup.sh from there.
i have been using gentoo linux for 15+ years so compiling stuff and fixing things that are not compiling is a routine for me and i have been doing porting of sailfish os to various mobile phones for quite a while (which is totally eglfs, wayland, qml based).
the downside is that i do have very little experience with the debian packaging and binary installations in general but i think i can learn that.
have fun - guhl

basically. the skill-mark-2-pi will take care of your volume control and some initial framebuffer drawing before the gui kicks in.
search the mycroft github repositories, becauswe there is also an enclosure repo, showing you how they set up certain things.
for your screen being rotated;
https://jumpnowtek.com/rpi/pitft-displays-and-qt5.html
dtoverlay=pitft35-resistive,rotate=90,speed=32000000,fps=60

set the rotation parameter to what you want. values of 90 or 270 orient the display in landscape mode.




 guhl:

well maybe this is all easy and obvious for the people here but i had a hard time to setup the software stack on my build of the mark ii prototype and did not find to much info in this forum.


congratulations!
as far as i can tell you are among the first users  outside the mycroft dev team to get mark-ii/gui software stack running on actual hardware. keep up the good work.

hi j1nx,
thanks for the rotation tip.
for the waveshare that i am using (as of bom) it is described here waveshare 4.3 orientation raspeberry pi
and for me this works with:

# waveshare screen
max_usb_current=1
hdmi_group=2
hdmi_mode=87
hdmi_cvt 800 480 60 6 0 0 0
display_rotate=1 #1:90; 2: 180;  3: 270


in /etc/config.txt

hi,
it seems that there is a install able deb for qt5.12 (including webengine) available here https://github.com/koendv/qt5-opengl-raspberrypi
did not check and install it but this might save a lot of time.
have fun - guhl

in the meantime i built qt5.12 with webengine myself on the pi using distcc to cross-compile it on regular amd64 linux hosts.
i did update the instructions on my-wiki on how to setup distcc and compile qt5.12 with webengine.
have fun - guhl

@guhl is this on github at all  if not could you post it on github.

hi,
at the moment this is mainly software building instructions and they are posted on my trac wiki (see the op) and i constantly update them. as soon as there would be code changes i will probably fork the mycroft-gui project and add the building inbstructions there.
have fun - guhl

this is so cool!
you’re very welcome to contribute back any build documentation to the mycroft-gui project.
also i might point out the https://github.com/mycroftai/mycroft-devices repo with the debos build recipe for our wip image. this is currently arm64 architecture and will only work with the pi4. we’ll likely be switching back to the armhf to get support for some hardware acceleration.

instead of building/using the entire plasma stack you can look at deploying mycroft-gui on top of plasma-nano https://github.com/kde/plasma-nano also adding specific shell configuration https://github.com/mycroftai/mycroft-gui-mark-2
scripts from these should also help if building with above:



github



mycroftai/mycroft-devices
contribute to mycroftai/mycroft-devices development by creating an account on github.






https://github.com/mycroftai/mycroft-devices/tree/master/overlays/mark2/etc/xdg // add to your image at same paths
https://github.com/mycroftai/mycroft-devices/tree/master/scripts //to run at time of build
the skill that controls volume and gui enclosure: https://github.com/mycroftai/skill-mark-2
if building on top of picroft it would be easier to upgrade picroft buster to debian testing so you can use system qt packages without having to rebuild qt. http://www.linuxandubuntu.com/home/upgrading-debian-from-stable-to-testing

@aiix am i correct in understanding that; “mycroft-gui” is what you currently use on bigscreen and therefor for normal landscape screens? while “mycroft-gui-mark-2” is the portrait versioni what is used on the mark-2? both doing the same / being the same, the gui for mycroft?
i am also looking into implementing the gui within mycroftos. what would you recommend for such a “normal” os that just utilizes a widescreen landscape hdmi screen?

mycroft gui consist of two parts one an application that houses the mycroft skillview and second the library which provides the skillview. both are part of the mycroft-gui repository.
the mark-2 prototype runs plasma-nano with the above mycroft-gui-mark-2 shell package that embeds the skillview directly into the shell so it runs as part of the shell and not inside the mycroft-gui application.
to better explain the “mycroft-gui-mark-2” package is a shell package for plasma nano that embeds the mycroft skillview onto the shell itself and also provides configuration to set the orientation, it provides support for a virtual keyboard, and provides some basic gui interface for settings like wifi. all of this is provided in this package as plasma-nano is a much reduced package with lesser dependencies compared to what plasmashell provides on the desktop, the orientation is basically of the skillview and the shell, it shouldn’t be needed on a normal screen unless someone wants to build some custom enclosure with a rotated screen.
there really isn’t a direct concept of landscape/portrait mode since the skillview simply can utilize the whole screen if embed in a shell or whatever window like the gui application, the skills should simply adjust to the screen/resolution, (it largely depends on skill devs to create response designs with qml, all of the current skills that implement a gui are responsive so they should have no issues being displayed in any form factor / orientation )
plasma bigscreen is also based on plasma-nano but we use the mycroft-gui application to display the voice apps/skills in a window rather than embedding the skillview into the shell itself as bigscreen has a larger usecase of being able to use multiple applications.

thanks for the quick response. much appreciated.
i think with “running directly into shell” you mean, straight onto the linux framebuffer instead of a full fledged x11/wayland kde desktop right?
in that case, that is exactly what i also have in mind for mycroftos as everything runs on the framebuffer. no window manager or desktop environment what so ever.  so if that is the case; mycroft-gui-mark-2 it is then!



 aiix:

provides some basic gui interface for settings like wifi


this intrigues me. so you basically the whole “wifi setup wizard” stuff is not needed anymore if you use this gui? if i remember correctly, plasma uses networkmanager for it’s network setup right? or does it talk to and / or configure the wpa_supplicant stuff straight away?
sorry for bugging you, but now you are here… 

by the shell i mean plasmashell or the reduced version of it plasma-nano, it still requires x/wayland. window management is optional but for something like virtualkeyboard it could be required.
this as well uses network manager but the wifi setup gui has been added to the gui containment on the mycroft-gui-mark-2 package and doesn’t require the systemsettings or plasma network manager applet which would normally be found on desktop.

perfect!
enough information to start cracking (although it might take a while before i can start timewise).
will report back if i run into problems.

hi aiix,
thanks for the response and all these very important insights.
i will have a thorough look at all this and rebuild my software stack based on that.
i’ll probably keep keep my self build qt/kde but the plasma-nano is something that i was not aware of.
have - fun guhl
"
192,test your mic outside of mycroft,none,"
hey, great project! my first post.
is there a way to change the mic record test:

arecord -d 10 test.wav
so that we may adjust the test for different hz and mono/stereo?

when i run the test i am only able to record a sample as:

recording wave ‘test.wav’ : unsigned 8 bit, rate 8000 hz, mono

similarly when i run:

aplay test.wav

i get:

playing wave ‘test.wav’ : unsigned 8 bit, rate 8000 hz, mono

i would like to run this test at 16000hz & 44100hz mono/stereo
thanks! 
","
this command uses resources “outside” of mycroft. arecord (alsa) parecord (pulse audio sound server)
correct me if i’m wrong, the 8k seem to be dictated by your input device. you have to tell pulse audio to resample your stream using ffmpeg (or other services)
although i don’t expect a noticeable better quality through upsampling.
the linked article expands on some configuration options, that might be interesting in general.
play it with paplay

hey there, my onboard audio device inputs and out are both 44100hz

can you specify that hardware? but regardless you have to make some configuration changes.

thanks for the great community, help, and introduction.
so, it appears i can do this:
arecord -d 10 -f cd -t wav test.wav
and it will record in signed 16-bit little endin, rate 44100hz, stereo
aplay test.wav plays it back as the above.

if you want to stay in alsaland, config asound.conf

yes, you can specify all the parameters on recording.
arecord -d (duration) -f presetformat -r samplerate -c channels
arecrd -d 3 -f s16_le -r 16000 -c 1 16khz16bitmono.wav
"
193,picroft rpi4 respeaker mic array v2 0 audio issues,mycroft project,"
hey there. i would like to run 2 own build mycroft devcies and i had some issues.
my config is:
-raspberry pi 4 - with 4gb
-respeaker mic array v2.0 (https://www.seeedstudio.com/respeaker-mic-array-v2-0.html)
-picroft unstable 2019-11-01 buster image
-mycroft-core 19.8.3
the problem:
-when i play something from youtube-audio (youtube audio skill - testing and feedback) or other sources, i got static noise. listen to a sample here:
https://mega.nz/#!gopcqaya!0olwtoke3nopbjxva4tpswhfzz7f4rgw019hdhxt1eo
at the moment i use the 3.5 inch jack from my respeaker module. i read the this should be done, because of the nc feature.
strange is that i am not able to set the default audio output to hdmi or the rpi-3.5 jack. when i do test in the wizard there is no sound.
is the respeaker mic array v2.0 not supported by picroft for the rpi4?
greetings by suisat
","
static noise can be an issue of resampling / samplerate.
without any information about your sound configuration files, perhaps you could get some pointers from their mark2 enclosure code here;



github



mycroftai/enclosure-mark2
this repository holds the files, documentation and scripts for building mark 2 pi device images. - mycroftai/enclosure-mark2





have a look at how they flash a certain respeaker firmware and configure pulseaudio.

i use the default setup of the picroft unstable. so i just flashed the card and run the mycroft-setup. there i just choosed respeaker as default mic and output.
i try yesterday to get another setup, so i install the latest buster image for another rpi4. then i install mycroft and i got the same results.
perhaps you are right that my sound settings are wrong. i found this:


github.com


mycroftai/enclosure-mark2/blob/ac31a99a81f07fca168c1b8200f13395ba8e316e/image_recipe_script.sh#l31

apt-get install apt-transport-https -y
apt-key adv --keyserver keyserver.ubuntu.com --recv-keys f3b1aa8b
bash -c 'echo ""deb http://repo.mycroft.ai/repos/apt/debian debian main"" > /etc/apt/sources.list.d/repo.mycroft.ai.list'
apt-get update -y


# dev tools
apt-get install vim -y
apt-get install tmux -y


# set default pulseaudio sample rate
sed -i.bak 's|; default-sample-rate = 44100|default-sample-rate = 44100|' /etc/pulse/daemon.conf


## disable kernel boot tty on gpio uart
systemctl stop serial-getty@ttyama0.service
systemctl disable serial-getty@ttyama0.service


## enable systemd-timesyncd
timedatectl set-ntp true


## edit boot configuration settings
sed -i.bak s'|dtparam=i2c.*|dtparam=i2c_arm=on|' /boot/config.txt







i guess i should use pulseaudio and change this samplerate. i thought alsa was the default audio-manager.
did i need to change first from alsa -> pulseaudio?
greetings by suisat

looks like you are going to use this rpi4 only for this purpose so this is what i should do;
first, make sure that you use pulseaudio for audio playback. edit your “/etc/mycroft/mycroft.conf” or “~/.mycroft/mycroft.conf” to add/change to these two lines;
""play_wav_cmdline"": ""paplay %1""
""play_mp3_cmdline"": ""mpg123 %1""

pay good attention on the proper json format, using the comma’s where you should etc.
then we can also force alsa userspace to use pulseaudio as well by editting the “/etc/asound.conf” with the following content;
# use pulseaudio by default
pcm.!default {
  type pulse
  fallback ""sysdefault""
  hint {
    show on
    description ""default alsa output (currently pulseaudio sound server)""
  }
}

ctl.!default {
  type pulse
  fallback ""sysdefault""
}

now all sound setting are pushed towards pulseaudio. now it is matter to setup pusleaudio properly similar as the mark-2 enclosure;
edit “/etc/pulse/daemon.conf” and at the bottom add the following content;
resample-method = ffmpeg
default-sample-format = s24le
default-sample-rate = 48000
alternate-sample-rate = 44100

edit “/etc/pulse/default.pa” and at the bottom add the following content;
unload-module module-suspend-on-idle
unload-module module-role-cork
load-module module-role-ducking

that last file is most likely not needed for your issues, but if all works after this, we can easily enable ducking of audio later on.
give it a go and let us know.

wow! it seems to solve my issue. you are genius! thx
the youtube-audio skill works finde and i can hear the mycroft voice. but spotify seems to be broken. i am not sure if this come from the audio-changes to pulseaudio or if this a issue by the spotify skill itself.
can you explain what you mean with “enable ducking of audio later on.”?
greetings by suisat

tts is now duckable. have a look at my mycroft.conf


github.com


j1nx/mycroftos/blob/develop/buildroot-external/rootfs-overlay/etc/mycroft/mycroft.conf
{
  ""play_wav_cmdline"": ""aplay %1"",
  ""play_mp3_cmdline"": ""mpg123 %1"",
  ""ipc_path"": ""/ramdisk/mycroft/ipc/"",
  ""enclosure"": {
    ""platform"": ""mycroftos"",
    ""platform_build"": 1
  },
  ""listener"": {
    ""mute_during_output"": false
  },
  ""tts"": {
    ""module"": ""mimic2"",
    ""mimic2"": {
      ""lang"": ""en-us"",
      ""url"": ""https://mimic-api.mycroft.ai/synthesize?text="",
      ""preloaded_cache"": ""/opt/mycroft/preloaded_cache/mimic2""
    },
    ""pulse_duck"": true
  },


  this file has been truncated. show original





within the tts section you can configure “pulse_duck = true”
with that enabled. start a playback with the youtube skill. during playback say “hey mycroft”. the playback mutes for it to listen to your command. ask about for example the weather. playback continues and than during the tts the music volume get’s reduced.

i will check this duck feature, sound like the feature on google home or alexa.
could there be a reason why my spotify give me no sound output?

i don’t have spotify, but from the top of my head, reading about it here and there in relation to mycroft, you need a “spotify connect” type of client.
i believe that client uses “libao” for it’s playback, which you need to tell to use pulseaudio as well.


github.com


mycroftai/enclosure-mark2/blob/master/etc/libao.conf
default_driver=pulse
quiet






but that is as far as i followed, run into, read about spotify.

sadly i still found no solution yet.
i am really unsure if this is a problem with raspotify/librespot or with mycroft for the rpi4.
@andlo he had some issues with the google-aiy and setting the volume but. i can change the volume.
here is somebody changing the /etc/asound.conf but this does not change ynthing on my picroft.
some other users had some issues after playing 5 songs with raspotify here. i create the .pulse/client.conf without any success.
@andlo or @forslund any tips or tricks, how i can enable the spotify skill to play on my configuration? i would be very thankfull.

looking at the last link, am i correct that raspotify/librespot is running as a seperate user?
with the asound config, forcing to forward all stuff to pulseaudio, there should not be any tweaking required for those systems, however if the program is running under a different user that user should be added to the pulse group to be able to spawn a connection.
or you should run pulseaudio in systemwide deamon modus and allow anonymous acces to either the bus, tcp or both.

@suisat could i ask you for a favour? as you have a rpi4 and the respeaker, i wonder if you have a spare sdcard and would like to test my rpi4 build of mycroftos?
coded it “blind” without access to a rpi4, so fair big change it doesn’t even boot, but just wanted to know.

perfect! cool and many thanks. best is to give you the link to my threa here on the forums;




mycroftos - a bare minimal (production type of) os based on buildroot modding


    i have uploaded a new alpha version for people to test. this is the fourth alpha version. 
information 

linux kernel 4.19.x
buildroot 2019.11.x
mycroft 19.08.4
raspberry pi 3/3b/3b+/(4)

i have a raspberry pi 3b, but i believe it should also boot and work on the normal rpi3 and rpi3b+. if you have one of those, please by all means give it a go and report back to me. 
with buildroot 2019.11.x there is also raspberry pi 4 support. i compiled it blind for the rpi4, so untested. if you feel adventu…
  

the rpi4 version can be downloaded here;
https://j1nx.stackstorage.com/s/nkmnt1ongmtzn2c
or from github;



github



j1nx/mycroftos
mycroftos is a bare minimal linux os based on buildroot to run the mycroft a.i. software stack on embedded devices. the software stack of mycroft a.i. creates a hackable, privacy minded, open sourc...










 j1nx:

looking at the last link, am i correct that raspotify/librespot is running as a seperate user?


this could be the problem. i will check that and write you soon if this brings me closer to a runnable mycroft-system with working spotify.

hey there, i have a similar setup and also ran into some problems using the spotify skill.
i installed raspotify as a client and first tried it with and without the --device hw:1,0 option to select the mic array for sound output, however i never got any sounds. then i found this fix for a similar issue, related to different hw: https://github.com/dtcooper/raspotify/issues/40#issuecomment-346436761
following the instructions, it removes the raspi internal sound card from the aplay -l list and in my case leaves only one option, the mic array. doing so and removing the --device option from the raspotify config, then finally worked out for me.

i wonder if there is any downside of flashing the 48k firmware? the sound output is pretty much subpar maxxed at 16k, especially if you want to want to hear radio or else.
i’ve seen mycroft setting config to 48000/44100 but only flashing the 16k firmware. did i miss something here?

to my knowledge the mark-ii dev team runs the respeaker mic array with the 48k/1-channel firmware.

yes it is, confirmed here
thanks
can someone please explain to me the device name parameter of aplay/mpg123/…
aplay -dplughw:arrayuac10,0
doesn’t this mean the device with the device.id “arrayuac10” which is card0?
so, basically the card which is fetched by
cat /proc/asound/cards
the same you find (dev= ) on the output from aplay -l
default
    playback/recording through the pulseaudio sound server
sysdefault:card=arrayuac10
    respeaker 4 mic array (uac1.0), usb audio
    default audio device
front:card=arrayuac10,dev=0
    respeaker 4 mic array (uac1.0), usb audio
    front speakers

i refer specifically to the device number




 sgee:

aplay -dplughw: arrayuac10,0


i asked, because as far as i understand things right, with pi+respv2 (or other usb audio for that matter) there is no default scenario where respv2 is card0 (even if bcm* is blacklisted; as in my case) . and thus a chance to fail because of that. am i wrong?
"
194,mycroft instead of picroft on a pi 4,mycroft project,"
so, i have a raspberry pi  4 (4gb) with the raspian os.
i’m really new to mycroft, like, this is my first post and i was wondering if, instead of the picroft image for raspberry pi, i could use the linux version of mycroft on the pi.
the problem i have with the picroft image is that it overwrites the raspbian os and only comes with a “lite” version. i’m planning not only to use my pi for picroft but also to build guis and control an arduino. (which, without the full raspbian os, is quite a strain)
so i was wondering if i could use mycroft with the pi, are there any hindrances, and most importantly, whether it completely resigns my pi to be just a virtual assistant and nothing more?
help would be appreciated!!!
","
there’s a recipe how to brew your own picroft. this should work with your current os (you might have to chose “boot into command line” in the raspi-config, but personally i do think even booting into pixel shouldn’t be an issue)
note that the os installed should’t be too populated at the time of the mycroft installation, simply because it would make backtracking issues more complex.
ps: before you’re doing the final steps make sure mycroft’s working and you have the additional programs set up and configured. this way you got an image that can be easily applied to other pis’

wouldn’t it be simpler to just get the linux version of mycroft? 

marginally, and you won’t have the benefits of a somewhat automatic uprade and extended installation process. but, your call 
in short, of course you can.

thanks! and speaking of margins, is the interface/code etc. of mycroft the same as picroft in case i decide to switch?
and do you think people get hindered by the “lite” version of raspbian?

same base
as i suggested. simply stay with your os instead of using lite (which the how to uses). (if this question was about selfbrewing)

right! i’m starting to lean on picroft a bit more because of the ‘support’ but let’s see…
thanks anyway!
"
195,is there a plan to sell markiis audio card sj201 separately,support,"
as documented/open sourced the mark2 will come with the daughterboard (sj201) to the pi4 holding (snapshot):

usb soundcard (cm108b)
audio front end (xmos xvf-3510) - for microphone input processing
20w amplifier (maxim integrated max9744)
usb 4-port hub controller (fe1.1s)
2 digital mems microphones (st micro mp34dt05)
12 rgb leds (worldsemi ws2812b-mini)
3 momentary buttons (volume up, volume down, action)
1 toggle switch (mic mute)

with audio being to most freqeuently issued part i wonder if this is not an opportunity to sell them as an off-shoot as this card is maturing.
streamlining that essential part should be beneficial to the accessablility of the whole project.
(maybe i missed some communication on that)
","
hey, that’s certainly the idea however we don’t have specific plans for what that will look like just yet.
given how much we’ve looked around for something exactly like the sj201 and it just doesn’t exist, i’m sure that many others are in the same boat.
the board can be used in two modes:

via gpio on a raspberry pi
as a usb sound card connected to any computer

this flexibility i think makes it more attractive as a general development platform.
so yeah, our focus is still on delivering the mark ii’s, but it would be good to hear if people are keen to get their hands on an sj201 for other development purposes?
"
196,news skill issues,skill feedback,"
there seem to be a few things going on with the news skill and this isn’t comprehensive.


for many of the news selections, i get a message from mycroft saying “…technical issue.  i couldn’t start that news feed…”  but it plays anyway.    this occurred on npr, bbc, pbs news hour and ap hourly news.


fox news gets the same “…technical issue…” as above, but the news never plays.
the code looks reasonable:
  ‘fox’: (‘fox news’, ‘http://feeds.foxnewsradio.com/foxnewsradio’,
image_path(‘fox.png’)),  


the log shows this:
    file “/home/pi/mycroft-core/.venv/lib/python3.7/site-packages/requests/adapters.py”, line 516, in send
raise connectionerror(e, request=request)
requests.exceptions.connectionerror: httpsconnectionpool(host=‘foxnewsnetwork.mc.tritondigital.com’, port=443): max retries exceeded with url: /omny_foxnewsradionewscast_podcast_p/media/d/clips/4633a30a-0821-43b2-b233-aab201108451/23ee2b9d-080a-4e2b-8d54-aada012d111e/f3cb3556-d1f0-47e6-8e24-ac1c013b48c8/audio/direct/t1597864494/3pm_edt_08_19_2020_newscast.mp3?t=1597864494&in_playlist=b3bd4b0c-63b7-4140-ae0d-aada012d112c&utm_source=podcast (caused by newconnectionerror(’<urllib3.connection.verifiedhttpsconnection object at 0x44ac4290>: failed to establish a new connection: [errno -2] name or service not known’))  


ap hourly news plays news from (probably) early april.   code may be specifying an episode.
    ‘ap’:  (‘ap hourly radio news’,
“https://www.spreaker.com/show/1401466/episodes/feed” 


georgia public radio doesn’t work at all.  message is “i’m not sure how to play news.”  looking at the code (and i am no python expert), it looks to have no link to that, so perhaps it should be removed from the menu choices in the skills settings.


""gbp"": (""georgia public radio"", gbp, none),
i didn’t check any of the other feeds.
","
are you still experiencing this, there was an issue previously but it was resolved 5 days ago

item number 1 is no longer an issue.
items 2 - 4 are still not working properly.  (although #2 doesn’t have the “…technical issue …” message.)

thanks for the update. will test those out and see if i can find the issue(s).

i’ve verified 3 and 4 and can see what’s wrong there, the feed pages has stopped updating and georgia gets 404 errors when resolving the links in the feed. alternative methods for finding the episodes needs to be added.
the fox feed looks ok at first glance, but i haven’t done any serious experimentation yet…will look into finding alternative feeds for the ap and georgia public radio stations.
thanks for reporting!

oops.  i must apologize for reporting #2.  it turns out it is not playing due to a router app that blocks some sites (malware and ads).  once one of the domains needed was whitelisted, the news began playing.
"
197,devsync 20200904,general discussion,"

",
198,devsync 20200831,general discussion,"

",
199,devsync 20200826,general discussion,"
0:00 intro from michael
0:35 ken and chris on wake word tagging and training progress
12:33 derick on mark ii enclosure and audio service issue
23:00 michael on mark ii hardware
27:40 ken on precise training skill

",
200,devsync 20200824,general discussion,"
sprint planning session.

",
201,devsync 20200819,general discussion,"
aug 19, 2020
0:00 hardware update
1:10 precise wakeword tagging
2:50 jira ticket review - starting with chris on precise uploads
8:30 jira management discussion
17:10 ken on wake word training
23:10 gez on the 20.08 major release
30:40 derick on mark ii enclosure design
36:10 precise tagger user stories

",
202,playing bbc cuts off,mycroft project,"
twice today, i listened to the bbc news hour and both times it cut off in the middle of the broadcast, same goes with npr
","
had a stream break due to a ntp sync (sadly no output captured, but seen it in the cli) that got a fix lately (link), but that was a singulary event

hey trekjunky, thanks for reporting this.
i replicated it and the worst thing is there doesn’t seem to be any error thrown when it happens. will need to dig into it more to work out is happening.
i’ve created an issue on the news skill repo for this.
"
203,mocking mk2 on picroft total noob here,mycroft project,"
i backed the mk2 kickstarter back when it was running but they still aren’t ready and won’t be for a while from what i can gather, so in the mean time i want to attempt setting  something similar up.
i have picroft running on a raspberry pi and things seem to be working so far. but one feature i’d like to get it’s the ui screen thing going. after a few days of fiddling i figured out that i won’t be able to get mycroft-gui project working as it’s 64bit while picroft is 32bit (also  when attempting to install the qt5 packages i was told that apt-get couldn’t find them).
i cant future out how else to handle this. has anyone got any suggestions how to get a gui on my picroft setup? 
","
check mycroftos which comes with the gui

that did the trick, thank you very much 
"
204,hacking alexa or google devices to install mycroft,none,"
has anyone attempted to install mycroft on an amazon echo or google device?  obviously it it void the warranty but who cares!  the mic/speaker and form factor are great.  seems like something fun to do with one of these things grandma gave you for christmas.
","
i have to echos sitting here, dooing nothing, and would love to hack them to get linux and mycroft running on it. best information i found were this article



medium – 2 jan 17


exploring the amazon echo dot, part 1: intercepting firmware updates
tl;dr: i have an amazon echo dot to poke at. i had no idea what the device consisted of when i bought it. it’s just amazon fire os…
reading time: 28 min read







but reading that tells me it isnt easy, and requere lot of work to be usable 

the echo dot/home mini had 256mb of ram, which is rather low to run mycroft on. for budget applications a pi 3 and the google aiy voice kit v1 would be a reasonable approach, though not nearly as pretty or compact.  the google mini has a similar two-mic system in it as well.



ifixit – 19 apr 16



amazon echo dot teardown
we tore down the amazon echo dot on april 20, 2016.









medium – 25 oct 17



google home mini teardown, comparison to echo dot, and giving technology a voice
google just announced some new products — one of which was the google home mini, a smaller, cheaper version of their voice assistant…
reading time: 14 min read








not exactly what i was was going for… but pretty creative.



the verge – 15 jan 19



this project hacks amazon echo and google home to protect your privacy
google, you can’t hear me now.






step 1 - open it up
step 2 - take the hardware out
step 3 - put a picroft inside

apart from the warranty getting void, are there any other specs?
as in this: https://techyhost.com/ways-to-highly-compress-files/
i wanna try this too.

just had a closer look at that mini tear down and it says 256mb of nand flash. it’s actually been beefed up to 4 gig of ram



marvell 88de3006-btk2  — (unchanged) this is the multimedia processor soc, an armada 1500 mini plus with a dual-core arm cortex a7, supporting 1080p hd content.

toshiba tc58nvg1s3hba16  —(unchanged) 256 mb nand flash

texas instruments  tas5720  — (unchanged) the audio amplifier

marvell avastar  88w8887  wlan/bt/nfc soc  — (changed from the 88w8897 used in the google home) the main difference with this soc is that instead of a 2x2 wifi radio (meaning it can send and receive on two antennas simultaneously and do wifi beamforming), it only has a “1x1”. this is most likely a cost reduction, also requiring less thermal dissipation and hence a smaller, cheaper design, for a product that may not need the extra bandwidth and antenna strength.

sk hynix h5tc4g63cfr-pba  — this is 4gb ddr3l sdram (changed from the samsung k4b4g16 512mb ddr3 sdram used in the google home)


picroft is still an easier and safer way to go! 

oh damn, was clearly still asleep, definitely a lower case b there…
no 4gb’s of ram sorry folks.

i got amazon to confirm that all the echo devices run fireos, a version of android. i bet that the new echo show has lots of ram, and could probably run mycroft…

hi guys and lads,
does anyone have updates about it ?
i’m stuck with à bunch of minis and would like to upgrade them 

your best bets are to check hackaday and xda. no update that i could find.

no one’s tried it that we know of…

im stuck with a 1. gen, which has 2 gb of ram. hopefully i can find something, definatley posting an update in case.

looks like if you have an echo v1 you should be able to install debian on it: https://github.com/echohacking/wiki/wiki/echo. it does require attaching to the debug pads on the bottom of the device. i’ve not yet found anyone who has actually used this for anything other than listening into the device itself (would love to see if you can still access the speaker and mic etc…). it appears that the echo v1 runs on linux while all later ones run on android.
"
205,testing and feedback email commands skill,skill feedback,"
email commands skill
control mycroft by email
about
allows you to send commands to mycroft by email, inspired by this blog post
want to turn on the tv before you get home? or maybe you want to impress your friends and play your intro song just as you enter your house? use you phone and tell mycroft to do it
skill configuration
i did not use mycroft home backend for skill settings, it is irresponsible
to get your email credentials in there. instead you need to edit mycroft.conf
this is a security measure, i will not change this behaviour
i recommend you create a new email just for this
add the following section to your mycroft.conf

""email"": {
   ""mail"": """",
   ""password"": """",
   ""address"":""imap.gmail.com"", 
   ""port"":993,
   ""folder"":""inbox"", 
   ""time_between_checks"":30, 
   ""whitelist"": [""thisemail@isallowed.com"", ""email@thatcansend.commands""]
}

only mail, password and whitelist are required, other parameters use the
defaults for gmail, if using a different provider you might need to change address and port
time_between_checks is in seconds, it is the time to sleep between checking
for new emails, the “lag” between a command and its execution
whitelist is a list of emails allowed to send commands to mycroft, if the
sender is not in this list then the emails will be ignored
email configuration
you might need to change some settings in your email provider, for gmail you need to

enable less secure apps https://myaccount.google.com/lesssecureapps

enable imap  https://mail.google.com/mail/u/1/?tab=mm#settings/fwdandpop


appstore
please rate and leave feedback in the pling appstore


pling.com



email commands skill
allows you to send commands to mycroft by emailwant to turn on the tv before you get home? use you phone and tell mycroft to do itnote: you need to edit mycroft.conf before using this skill, there...





github



github



jarbasskills/skill-email-commands
control mycroft by email. contribute to jarbasskills/skill-email-commands development by creating an account on github.





credits

jarbasal

category
configuration
tags
#configuration
#remote-control
",
206,siri controll skill,mycroft project,"
hello! i’m thinking about controlling picroft by siri…
simply with imap gmail notes …
here is full explain https://medium.com/@thesanjeetc/want-to-control-something-with-siri-heres-how-bae98aceb586 
it will be so nice if somebody did that!
","
doubt anyone in the community will want to do that, 99% of us are here to avoid siri/alexa/google/cortana/arbitrary_spy_company
the docs are good and its not hard to make mycroft skills, why dont you try ?
https://mycroft-ai.gitbook.io/docs/skill-development/voice-user-interface-design-guidelines

see skill configuration here




testing and feedback - email commands skill skill feedback


    email commands skill
control mycroft by email 
about
allows you to send commands to mycroft by email, inspired by this blog post 
want to turn on the tv before you get home? or maybe you want to impress your friends and play your intro song just as you enter your home? use you phone and tell mycroft to do it 
skill configuration
i did not use mycroft home backend for skill settings, it is irresponsible 
to get your email credentials in there. instead you need to edit mycroft.conf 
this is a secu…
  

mail -> your email
password -> your password
folder -> notes
whitelist -> [your_email]
"
207,usb music skill testing and feedback,skill feedback,"
 usb music
play music from a usb device with mycroft.ai
about
play local music by inserting a usb drive into your mycroft device. upon inserting a usb device mycroft
will scan the device for mp3 music and add it to a temporary library that you can play.
autoplay
if autoplay is enabled in the websettings then the usb music will begin playing imediatly when inserted.
unplugging the usb will automatically stop and unmount the device.

settings711×380 9.82 kb

command examples

“play the artist elvis presley”
“play all shook up”
“play the song blue suede shoes”
“play the album appeal to reason”

credits

pcwii (20200709 covid-19 project)
some elements borrowed from https://github.com/preston-sundar/raspberrypi-pyudev-usb-storage-detector/blob/master/usbdev.py


category
media
tags
‘#music, #usb, #mycroft.ai, #python, #skills, #mp3, #cps’
require
tested on platform_picroft (others untested)
other requirements

mycroft
mutagen
pyudev

installation notes

ssh and run: msm install https://github.com/pcwii/usb-music.git


todo

add “next/previous” commands
add “random” selection
add thumbnails for display
…?

feedback

i have only tested this on picroft so feedback on other devices is very much welcome.
provide feedback in this topic
provide feedback in github
https://github.com/pcwii/usb-music


","
nice skill, wondering if it would be possible to make it run from an smb share instead of the usb drive (with a settings to select the path). mixed with cmdskill to mount the path it could auto play. i havent looked at the code yet but i am pretty sure this could be done




 redpotatoes:

wondering if it would be possible to make it run from an smb share instead of the usb drive


so i got this working as a test… sort of… be warned it took a bit of time to load the library in my test case, i don’t know how many songs you have but i am going to need to optimize things and clean up the code a bit. if you are adventurous, update to the latest build “msm update” then update your websettings with your smb share and then issue the command “load network music” and be prepared to wait if you have a large library (maybe try a smaller share first)


image711×638 14.8 kb


great i will give it a try tomorrow. thumbs up !

cool skill, i have some questions:



 pcwii:

mycroft will scan the device for mp3 music and add it to a temporary library that you can play.


that means it will copy locally the items or it creates an m3u pointing to the right place? (usb, smb/whatever future path)
what about other formats like flac, ogg, etc?
what about, if you are willing to create a similar skill for videos, to create a meta-skill “usb-load” which scans the usb (or network path, but we’re speaking of usb) and determine how many audio files and video files are present on the device and playing it accordingly? my ideal skill would be one which scans the media and you can tell something like “play 12 monkeys movie on living tv’s kodi” or “play the artist elvis presley (locally)”




 malevolent:

that means it will copy locally the items or it creates an m3u pointing to the right place?


i do not copy the files to a temporary area. currently my approach for the “library” is to create a mount point then scan the mount point for mp3 data. i then create a dict of the data that i index when a request is made. the dict list consists of the following data for each found item.
info = {
                        ""location"": song_path,
                        ""label"": self.song_label,
                        ""artist"": self.song_artist,
                        ""album"": self.song_album
                    }

it does take some time to index all the items. and i am not sure at this point if this is a blocking event. i will need to do some testing to be sure. i will look into the flac, ogg, as far as the movies i am just about ready to release my revamped kodi skill and it should cover off some of those items.

for those interested, i have updated this skill with the following new features.
about
play local music sources (usb, smb, local)

inserting a usb drive into your mycroft device. upon inserting a usb device mycroft
will scan the device for playable music and add it to a temporary library that you can play upon request.
adding an smb source will let you play music from that source.
adding a local path source will let you play music from that source.
supports the following music formats ‘mp3’, ‘m4a’, ‘flac’, ‘wav’, ‘wma’,‘aac’

autoplay
if autoplay is enabled in the websettings then the usb music will begin playing immediately when inserted.
unplugging the usb will automatically stop and unmount the usb device.
autoplay only functions with usb sources, not smb or local path.

i have made another small update to this skill.
this change lets you add a command that will automatically be pushed to the messagebus (invoking another skill) when the usb has been inserted.
automatic insert command request
if enabled the skill will automatically send a command to the messagebus
when the usb is inserted.
why?
why not?
this will let you do all sorts of things with this skill by invoking other skills when the usb is inserted.
example:

you have a running playlist that is stored on a usb stick.
you insert the usb before you jump on the treadmill.
the insertion automatically issues the following command to the messagebus
“set a timer for 20 minutes”
now you have workout music and a timer to notify you when your workout is complete.

as always feedback / bug reports can be added to this thread.
enjoy!
"
208,making our development syncs public,general discussion,"
originally published at:			https://mycroft.ai/blog/making-our-development-syncs-public/
the mycroft ai development team is constantly working to improve existing software, develop new software, and improve hardware compatibility.  the team meets three times a week to review the previous week’s accomplishments, plan out tasks for the coming week, discuss outstanding issues, and review progress on existing tasks.  until recently these meetings have been entirely internal meetings.  we recently decided to start recording and publishing these development syncs.  we made this decision in order to increase openness, transparency, and collaboration with the community.
in publishing our dev syncs we are able to gather feedback from the community which allows us to tailor our development sprints to what is important to the end user.  this also allows us to quickly and consistently publish updates about the ongoings here at mycroft ai.  we deeply appreciate everyone who has watched or listened, taken the time to comment, and become involved with some of the development efforts.  the community of fantastic developers is a key to bringing private and open voice technology to everyone.  we look forward to reading your questions, comments, compliments, and concerns on the forums and in the comments section of the dev syncs.
all of our dev syncs can be found in this youtube playlist.
to leave some feedback, head on over to the mycroft community forum.
",
209,need help in raspberry pi and cpu,none,"
hello everyone,
i am new to this forum and i wish if anyone can help me.
i have a raspberry pi b+ and i have installed linux (raspbian) on it. could i download mycroft without burning picroft disk image.
i also have a problem in my ubuntu 16.04 desktop. when i run mycroft the cpu get overloaded as you can see in the attached picture.
img-20200909-wa00061080×1920 493 kb

","
check the load numbers at the top line of your pic  (.91, .31, .20).  you’re on a four core machine, you can have 1.0 per cpu before getting to cpu-overload.  .91/4 isn’t much load at all.
eta: pic you posted is not from a pi 3b+…




 baconator:

.91/4 isn’t much load at all.


that’s why he’s called baconator. 
"
210,silent utterance possible,general discussion,"
hello, i use a combination of voice and screen outputs via the companion app for a recent project. is it possible to utter something silently, i.e. the text is visible but mycroft does not read it?
","
hey try this with messagebus
https://mycroft-ai.gitbook.io/docs/mycroft-technologies/mycroft-core/message-types#recognizer_loop-utterance
and
https://mycroft-ai.gitbook.io/docs/mycroft-technologies/mycroft-core/message-types#speak
would you like to use the mycroft without sound?
you could simply switch it off via the configuration tts.

no, the audio needs to be on. i want to avoid that mycroft reads links that i want to display to the user.

there is a text output option for gui / enclosure system that can be triggered via message bus. if you want, i’ll see if i can find something. with this you can only display texts and not pronounce them, but not globally across all skills.
https://mycroft-ai.gitbook.io/docs/skill-development/displaying-information/mycroft-gui

thanks for your help, it would be great if you can point me to a code snippet or similar.

do you use a mycoft gui device or something else to display

i only use the android companion app to display and speak to the user. i think that one relies completly on the message bus.

something like that



github



mycroftai/mycroft-android
android companion app, sends commands from your android device to your mycroft system and returns the output as speech or other medium to the android device. - mycroftai/mycroft-android






i try to understand what you want to do exactly because there are many ways to reach your goal

yes this is the app. it connects to the message bus via websocket and google stt reads anything that the app receives via the websocket (as far as i understand).
what i am looking for is a way that the sent message is visible only without the stt part. i thought there is a markup for the message to indicate that the tts should not trigger. probably, in this case the solution involves changing the android app directly (turn of the tts for some messages).

you want to create a skill that sends a “link” to the android app and the user only has to click on it or do you want to change the app so that you can’t hear anything and only see?

the user should see the link (url) only.
hearing it is a bad user experience because it’s cryptic and quite long - mycroft will annoy the user.
i do not know if i must change the skill or the app or both for this. naturally, i would assume to change my skill (e.g. that i can mark messages as “write only” => they are inaudible).

you could look into emiting a custom message from the specific skill that would send the utterance as a data dict over the messagebus and capture its kind via type key or data key over a socket from the application

thanks for the comments. i implemented something now with minimal changes like this:
there is a new message type “write”.
mainactivity.kt
if (voxswitch.ischecked && !mycroftutterance.silent) {
    ttsmanager.addqueue(mycroftutterance.utterance)
}

messageparser.kt
if (obj.optstring(""type"") == ""write"") {
    val ret = utterance(obj.getjsonobject(""data"").getstring(""utterance""), utterancefrom.mycroft, silent=true)
    callback.call(ret)
}

utterance.kt
data class utterance(val utterance: string, val from: utterancefrom, val silent: boolean = false)
"
211,there is a complete lack of linux voiceai interoperability,design,"
its fubar to have proprietary audio satellites locked into certain systems.
you have your own mechanisms and that has its purpose but if you boil it down a mic & speaker is basically all a voiceai common denominator needs to be.
mycroft the asr server is all good for me, as so is rhasspy the asr server and the many that are available but i should be able to use base functionality of a satellite, with any ‘open source’ voiceai server and having to put ‘open source’ in quotes due to dubious choices in interoperability is such a shame.
its such a shame that in reality mycroft, rhasspy and others have zero interoperable functionality and common hardware of simple satellites can not be reused without complete firmware changes.
the base function of a satellite is audio only as without a voiceai doesn’t work and all else are just additional additions that should be attributed to that device but not exclusive.
a pixel ring on a device is a pixel ring device, a screen is very much the same and so is any further satellite hardware.
i have created a repo that currently is bare so that its not rhasspy, mycroft or others and has no intention other than being a simple interoperable satellite platform.



github



1satellite/1satellite
1 satellite is a platform and framework interoperable simple audio point for voiceai - 1satellite/1satellite





satelites should be like keyboards & mice and we just plug them in and if anyone wants add input then please do.
i have my fave audio rtp of snapcast and wondering if anyone has any airplay experience or other to bring into initial scope. gnomecast, https://roc-streaming.org/ and others … ?
its extremely easy to setup an array of servers and clients that play on a loopback sink and so present a source and some small containers would cover it.
avahi, audio-rtp & docker and i think you are getting gist…
its 1satellite to work with all not to rule so kws and whatever is purely choice, but is there any interest as its very possible to create with some simple tools that already exist.
thinking maybe kickstarting it with precise or raven and seeing if you guys might be open to allowing the rhasspy functionality which is little more than alsa hooks.
going to do the same @ mycroft and see if anything is occurring 
","
as you seem to be familiar with rhasspy you most likely came hermes-audio-server and its successors rhasspy-speakers and rhasspy-microphone which allow audio transport over mqtt.
shouldn’t be overly complicated to integrate this into mycroft core/audio-service…
there is also a variant of hermes-server that runs on matrix-voice-esp32 (played with it, but never got this running though - but this is how i actually got aware of hermes…)

to be honest its hermes-audio that has prompted my call for audio-rtp as to start with audio transport over mqtt is just a strange concept.
no please don’t ignore the many specific audio protocols that provide qos, latency and function of streaming audio that are audio protocols specifically designed for audio not a light weight bi-directional message queue that in terms of streaming audio is probably the worst protocol you can use.
i quite like rhasspy post kws in fact i used to like rhasspy but where its going is their decision not mine but in terms of interoperable satellite that uses standard methods you have just killed it in one fell swoop by adding a proprietary protocol such as hermes-audio.
the matrix-voice-esp32 sums up very much what i think of hermes audio as its a $75 sound card/pixel ring that is a wtf of required functionality and apparently doesn’t work.
so definitely no to integrating an absolutely ridiculous idea that voiceai is special in terms of streaming audio and that a project is going to design its own protocol just for the sake of it.
mycroft core uses standard alsa/pulse audio which both suck for reliable network streaming but work perfectly as default linux audio systems and that is exactly my point there should be no integration needs into mycroft core as standard alsa/pulse audio should be used.
also for the audio-rtp it would seem wise to use an audio-rtp protocol not a bidirectional messaging and monitoring protocol and that is why i posted as was interested in what audio-rtp protocols and was wondering what others would throw up as choice?
there are already many wireless speaker systems out there and using hermes-audio will play on none of them and is a totally non interoperable proprietary protocol with ridiculously low adoption as without snips its now only used in rhasspy!

to get back on track with interoperable linux standards alsa (advanced linux sound architecture) as far as i am aware can have any number of devices it just depends on kernel setting and if alsa is a module as it can also be configured as a modprobe setting.
but when you start to think of what is fit for purpose on an individual voice server even the common configured max of 32 covers many.
i am not exactly sure as have never scaled up to this amount but an audio server is usually a one way stream of multiple channels but on the theme of matrixmodules aloop can support 8 devices with 8 sub-devices where clients can present a source by play into an aloop sink.
that is max devices before we get to pcm streams.
that allows all audio to be externalised from the mycroft core and ensures interoperability as mics & rooms can be exposed as standard alsa sink/sources.
there is no need for proprietary protocol over mqtt, but down the line on larger systems could use it to create server clusters but at this level we have absolutely no need.
i was wondering what audio/streaming curve balls would be thrown in terms of interoperability now and do the likes of spotify, sonos even chromecast come into play and to gather advocates as they can give details of their preferred protocol requirements.
interoperability is about bringing common format(s) to mycroft so that it can co-exist with multiple systems without being forced in to singular operation of uncommon protocols.
hermes-audio can be one of those interoperable systems working via standard linux architecture , but it definitely should not be integrated into the core or have need to be.

i have not really dug into it much yet, but are you asking about something like this for rtp multicast streaming: http://www.pogo.org.uk/~mark/trx/
it kinda looks like it would fit the parameters of what you are attempting to do…

that is one out of many what i am suggesting is that a common input linux patchbay should be the alsa  snd-loopback on a asr server.
then if http://www.pogo.org.uk/~mark/trx/ , airplay, snapcast, roc , pulseaudio-rtp, gnomecast or whatever… can be installed and play into the loopback so on the corresponding sub-device on snd-loopback will be present as a standard alsa source.

not 100% sure if i understand what you want, but isn’t pulseaudio able to do what you would want?
loop device sink and you can add whatever playback you want.
you can then pass on the loopback sink back into alsa userland.
but again, not sure if this is what you mean.

yeah exactly what i mean as yes with a single satellite it would work now as you just select the right alsa snd_ loopback sub-device.
we don’t have a method for multiple mic/speaker satellites and as above the majority can be done with standard linux methods.
but exactly how and what many work actually beaks what is a simple function.
a kws would work as rtp should only broadcast from kw trigger to vad silence so its not constantly broadcasting and also only triggered mics offer rtp.
there is more to think about than just that but actually not all that much and having interoperable satellites  to give choice of preference of kws system (satellite) to asr server (mycroft asr>>>server) or linto or rhasspy… could be beneficial to all and provide choice of preference via mix and match.
the only thing really missing is the kws trigger value as the highest value on the server should be the one that is used and instantly you have created a wide distributed mic array without what seems in many projects a huge amount of unnecessary bloat to create something as simple as a kws mic/speaker satellite.
here with a similar project like rhasspy they use hermes protocol which is a ton load of unnecessary proprietary bloat but really satellites should become common devices that work plug and play with any voiceai server.
"
212,add a command from settings,mycroft project,"
hello everyone,
i’m busy making my first skill. the idea is that you specify a short command, like ‘hello’, ‘bye’ or ‘good night’ in the skills configuration and if you then trigger this command a specified mqtt payload will be send to a specific topic. you can also set a response that mycroft will speak out.

i’m gone use it to trigger openhab scripts. so if i say “mycroft, bye” al the light go out and my alarm will turn on. or when i say “mycroft, good morning” a light will turn on and openhab wil tell me schedule of that day.
i have all the mqtt stuff working bud have one last hiccup. i have it working when i put the commands in separate intent files. bud this will mean that you will have to change the commands in those files. is it possible to get a command from a config file. the same as that you can use .speak.(“hello world”) to let mycroft talk bud then the other way around.
thank you in advance, and if you have any other tips or suggestions, i’m all ears.
greetings,
sander
","
have you seen my mesh-skill? might be something there you can use. others are already using the mesh-skill with openhab.
https://community.mycroft.ai/t/testing-and-feedback-for-mesh-skill-mycroft-to-mycroft-communicator

i did not see that before, looks great.
bud i don’t think that it will help me. as i can see in the short time i have read about it you are still using the keyword “send”. it is the keyword i want to make variable by config.

i have it working now bud i don’t think it is like the creators of mycroft intent it. when i change the settings of my skill the skill will just write directly to the .intent files.

"
213,no playback or mic input but all audio tests succeed,support,"
hi,
i installed mycroft on a raspberry pi 4 (8gb) with raspberry pi os installed (with desktop). i used the normal “linux install” via github. the problem is, that there is no output from mycroft and i get no mic input level in the mycroft-cli-client. all other tests like

arecord -d 10 test.wav
aplay test.wav
mycroft-start audiotest -l

were succeeding. i can record some stuff and playback is working. i am using a usb conference mic which is working fine and i use hdmi/jack for audio output. everything seems to work - except for mycroft…
i tried to troubleshoot the issue but i don’t know where to start. from the audio.log i have this:

2020-09-06 13:00:45.048 | info     |   573 | main:main:50 | starting audio services
2020-09-06 13:00:45.072 | info     |   573 | mycroft.messagebus.client.client:on_open:114 | connected                                                                                                                                                                 2020-09-06 13:00:45.098 | info     |   573 | mycroft.audio.audioservice:get_services:61 | loading services from /home/pi/mycroft-core/mycroft/audio/services/
2020-09-06 13:00:45.106 | info     |   573 | mycroft.audio.audioservice:load_services:105 | loading chromecast                                                                                                                                                        2020-09-06 13:00:46.709 | info     |   573 | mycroft.audio.speech:mute_and_speak:127 | speak: this unit is not connected to the internet.
2020-09-06 13:00:48.010 | info     |   573 | mycroft.audio.speech:mute_and_speak:127 | speak: either plug in a network cable or setup your wifi connection.                                                                                                           verbindungsfehler: verbindung verweigert
pa_context_new() fehlgeschlagen: verbindung verweigert
verbindungsfehler: verbindung verweigert
pa_context_new() fehlgeschlagen: verbindung verweigert

it seems like there is a connection error with pulseaudio but additional info is missing… one strange thing is the “not connected to the internet” log - my device is connected to the internet (via lan) and this is working fine?..
the voice.log is showing some smiliar output:

alsa lib pulse.c:243:(pulse_connect) pulseaudio: unable to connect: connection refused
alsa lib pulse.c:243:(pulse_connect) pulseaudio: unable to connect: connection refused
alsa lib pcm_a52.c:823:(_snd_pcm_a52_open) a52 is only for playback
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition ‘cards.bcm2835_hdmi.pcm.iec958.0:card=0,aes0=6,aes1=130,aes2=0,aes3=2’
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm iec958:{aes0 0x6 aes1 0x82 aes2 0x0 aes3 0x2  card 0}
alsa lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) invalid type for card
alsa lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) invalid type for card
expression ‘alsa_snd_pcm_hw_params_set_period_size_near( pcm, hwparams, &alsaperiodframes, &dir )’ failed in ‘src/hostapi/alsa/pa_linux_alsa.c’, line: 924
2020-09-06 13:00:42.977 | info     |   581 | mycroft.client.speech.listener:create_wake_word_recognizer:328 | creating wake word engine
2020-09-06 13:00:42.982 | info     |   581 | mycroft.client.speech.listener:create_wake_word_recognizer:351 | using hotword entry for hey mycroft
2020-09-06 13:00:42.991 | info     |   581 | mycroft.client.speech.hotword_factory:load_module:403 | loading “hey mycroft” wake word via precise
2020-09-06 13:00:43.016 | error    |   581 | mycroft.client.speech.hotword_factory:update_precise:205 | precise could not be downloaded(connectionerror(maxretryerror(""httpsconnectionpool(host=‘github.com’, port=443): max retries exceeded with url: /mycroftai/precise-data/raw/dist/armv7l/latest (caused by newconnection$2020-09-06 13:00:43.038 | info     |   581 | mycroft.client.speech.hotword_factory:install_model:242 | couldn’t find remote model.  using local file
2020-09-06 13:00:43.072 | info     |   581 | mycroft.client.speech.listener:create_wakeup_recognizer:365 | creating stand up word engine
2020-09-06 13:00:43.079 | info     |   581 | mycroft.client.speech.hotword_factory:load_module:403 | loading “wake up” wake word via pocketsphinx
2020-09-06 13:00:43.425 | info     |   581 | main:on_ready:175 | speech client is ready.
2020-09-06 13:00:43.448 | info     |   581 | mycroft.messagebus.client.client:on_open:114 | connected
expression ‘ret’ failed in ‘src/hostapi/alsa/pa_linux_alsa.c’, line: 1736
expression ‘alsaopen( &alsaapi->basehostapirep, params, streamdir, &self->pcm )’ failed in ‘src/hostapi/alsa/pa_linux_alsa.c’, line: 1904
expression ‘paalsastreamcomponent_initialize( &self->capture, alsaapi, inparams, streamdirection_in, null != callback )’ failed in ‘src/hostapi/alsa/pa_linux_alsa.c’, line: 2171
expression ‘paalsastream_initialize( stream, alsahostapi, inputparameters, outputparameters, samplerate, framesperbuffer, callback, streamflags, userdata )’ failed in ‘src/hostapi/alsa/pa_linux_alsa.c’, line: 2840

can anyone help me out here?
thanks
","
i think you should tackle your lan connection before diving in the audio stuff. the sound broblems seem to be a fallout from not being able to connect.
just curious, do you run on 64-bit kernel?

hi there,
the connection is totally fine. i can browse, watch youtube, ping google etc… that is not the problem - there must be another problem related to mycroft itself. do i need to change iptables config or something? maybe the connection is blocked by any firewall?
the pi is running an updated 32-bit pi os:
linux raspberrypi 5.4.51-v7l+ #1333 smp mon aug 10 16:51:40 bst 2020 armv7l gnu/linux

oops edited the wrong one.
it surely is for mycroft, since he can’t reach out, hence no precision update or tts processing 

despite these strange connection errors i get “audio service is ready” in the logs:

2020-09-06 14:36:49.636 | info     |   555 | mycroft.audio.audioservice:load_services:105 | loading mopidy
2020-09-06 14:36:49.644 | info     |   555 | mycroft.audio.audioservice:load_services:105 | loading mplayer
2020-09-06 14:36:49.685 | error    |   555 | audioservice_mplayer::20 | install py_mplayer with pip install git+https://github.com/jarbasal/py_mplayer
2020-09-06 14:36:49.687 | error    |   555 | mycroft.audio.audioservice:load_services:114 | failed to import module mplayer
modulenotfounderror(“no module named ‘py_mplayer’”)
2020-09-06 14:36:49.688 | info     |   555 | mycroft.audio.audioservice:load_services:105 | loading simple
2020-09-06 14:36:49.699 | info     |   555 | mycroft.audio.audioservice:load_services:105 | loading vlc
[a6a41958] vlcpulse audio output error: pulseaudio server connection failure: connection refused
[a6a58bc0] vlcpulse audio output error: pulseaudio server connection failure: connection refused
2020-09-06 14:36:49.891 | info     |   555 | mycroft.audio.audioservice:load_services_callback:177 | finding default backend…
2020-09-06 14:36:49.892 | info     |   555 | mycroft.audio.audioservice:load_services_callback:181 | found local
2020-09-06 14:36:49.894 | info     |   555 | main:on_ready:30 | audio service is ready.

i really don’t know what to do - and i don’t find information about ports needed for mycroft…

yeah, he might be ready, but since mycroft can’t connect (for whatever reason) mycroft servers can’t process the utterance (what you are saying)

i just tried to let him speak manually by using the commandline tool mycroft-speak. this is the output in the logs:

2020-09-06 16:16:21.788 | info     |   555 | mycroft.audio.speech:mute_and_speak:127 | speak: hello                                                                                                                                                        verbindungsfehler: verbindung verweigert                                                                                                                                                                                                                   pa_context_new() fehlgeschlagen: verbindung verweigert

so mycroft can’t connect to pulseaudio (pa_context_new()) but pulseaudio is up and running?

what is
pactl info
saying?


server-zeichenkette: /run/user/1000/pulse/native
bibliotheks-protokollversion: 32
server-protokollversion: 32
ist lokal: ja
client-index: 8
tile-größe: 65496
name des benutzers: pi
rechnername: raspberrypi
name des servers: pulseaudio
version des servers: 12.2
standard-abtastwert-angabe: s16le 2ch 44100hz
standard-kanal-zuordnung: front-left,front-right
standard-ziel: alsa_output.platform-bcm2835_audio.analog-mono
standard-quelle: alsa_input.usb-solid_state_system_co._ltd._usb_pnp_audio_device_000000000000-00.analog-mono
cookie: 544c:549d


you’ve set up mycroft as a service? which user is mentioned? i’m guessing pa is mixing up things there.

yes i did,
i use systemd for mycroft:

[unit]
description=mycroft ai
after=pulseaudio.service
[service]
user=pi
workingdirectory=/home/pi/
execstart=/home/pi/mycroft-core/bin/mycroft-start all
execstop=/home/pi/mycroft-core/bin/mycroft-stop
type=forking
restart=no
[install]
wantedby=multi-user.target

if i watch the output of htop i see that all mycroft processes are run by ‘pi’ as well as pulseaudio…

since you run pixel, i wonder if pavucontrol could get you on the right track? (troubleshooting wise)

which settings should i set via pavucontrol? i already have it installed, but didn’t change anything… i also experienced a strange behaviour while using it:
if i open it from the desktop my mouse and keyboard freeze and i need to set things up via vnc and close the window - only then mouse and keyboard are working again… pulseaudio is really messing things up but i don’t know how to change that…

i was just fishing for an error message from pavu.
is there a sound output in this environment?

how about tossing pulseaudio alltogether to be reinstalled through the mycroft requirements (assuming pa was already set up before mycroft)

yes i have sound output. i can use paplay for instance to play some .wav files. pavucontrol also shows audio level for input and output devices.
what do you think is the best method to reinstall pulseaudio through mycroft? i already reinstalled it completely using apt - there was no effect in doing so…

is this a clean installation (mycroft), or do you have additional skills set up?



 nknuelle:

2020-09-06 14:36:49.685 | error | 555 | audioservice_mplayer::20 | install py_mplayer with pip install git+https://github.com/jarbasal/py_mplayer
2020-09-06 14:36:49.687 | error | 555 | mycroft.audio.audioservice:load_services:114 | failed to import module mplayer
modulenotfounderror(“no module named ‘py_mplayer’”)



this is a clean installation. i only paired the device - i did not set up additional skills

yeah (looked it up), the mplayer is used since 2018 from core. why isn’t the py_mplayer not in the requirements.txt and what is/was it used for? if discontinued, the whole shebang should be tossed.
anyways, i don’t know how the new raspberry os is shipped. was pulseaudio still on there as mycroft got installed? (if not, the whole process would be completely unnecessary)
grasping for straws here, but what is ls -la ~/.config/pulse/ saying?

wait a sec. did you just say “paplay” is working? what is ~/.mycroft/mycroft.conf or /etc/mycroft/mycroft.conf using? (and is mpg123 playing mp3 files?)
"
214,dockerhub image outdated,support,"
hello, it seems the docker image in dockerhub is outdated (4 onths old). is it planned to update it?
",
215,is msm deprecated,none,"
hello, i am wondering why the documentation does not have a site for the mycroft skills manager anymore. https://mycroft-ai.gitbook.io/docs/mycroft-technologies/mycroft-skills-manager
can anyone give an advice what this means?
","
from the look of the change it seems like there was a page rename that didn’t carry through. the original msm page still exists. i assume it was intended to be renamed as part of the restructure that was made. or perhaps an updated version of the page is in the works.
mycroft and mycroft-skills-kit still use msm internally and i’ve seen no code in either of those repos indicating that a replacement is on it’s way in.

hey thanks for flagging that, this has now been fixed.
"
216,moira c1 open source hardware osh design for mycroft,mycroft project,"
note: the moira c1 is in beta/review phase. please see the status section below.
moira c1
the moira c1 is an open source hardware (osh) design for the mycroft software appliance.

moira c1 overview600×761 517 kb

upon release the following will be available:

scripts to update pycroft to set up hardware drivers and define default settings and preferences (github)
final parts list, pricing, and component retail locations (github)
3d printer ready models in stl (thingiverse)
3d cad files for editing (tbd)

design
the moira c1 is designed to make the most out of the selected components without incurring high costs. the average component cost minus shipping and pla/abs will likely be around $70 usd. features of the design include:

enclosed chamber system for the speaker
upward firing design to spread audio 360 degrees
bottom weight distribution for a solid foundation
vented grill flush wrapped in speaker cloth
beveled overlapping edges to match the finish of a google home
dual microphones
rbg led indicator that glows through the top of the case

hardware
the following hardware is the recommended selections and what the moira c1 has been designed around.

respeaker 2-mics pi hat
adafruit max98357 i2s class-d mono amp
2.244"" gf0573 8ohm 3/5w 450hz ~ 21khz 92db speaker
rbg clear common cathode led
micro usb interface board for power
gray speaker grill cloth
raspberry pi 3/4b
pla or abs for printing

what’s in a name?
moira was selected as the name as it is a female name that sounds like mycroft and will be the default trained wake word along with a female voice once the software setup scripts are complete (completely optional in setup).
c1 stands for cylinder 1, or the design/shape of the device.
status
i’m announcing this project because i would like feedback before i start printing betas and figuring out all the little bits that are going to break as it’s being assembled, etc. the 3d modeling is complete, but there is always room for improvement. as i am new to microft there may be other hardware related elements i haven’t considered yet, as well. so please, engage below in what you think is good, and what you think should change!
upcoming moira hardware designs


moira c2wb - a waterproof and battery operated cylindrical mycroft client

","
great project and fantastic design!
while working on my mark-iir a major problem i had was sourcing all the parts from the bom as some vendors do not deliver from us to eu (or if they do the shipping cost would be ridiculous high). maybe community members from all over the world can help checking availability of parts in their country? (i can offer to this for germany)

thanks! and thanks for the insight! i’m in the us, but i do know adafruit has the mouser arm, then digikey ships to what… 170 countries? the only thing i don’t know about is seeedstudio, so i’m not sure about the respeaker 2. also for the amp, i’m curious if the respeaker 2’s 1w would be enough. granted, the 3w amp is only $5.

love the project! i was a bit disappointed when i first read that mark 2 perhaps finally would have a square shape factor. i prefer round/cilinder forms.
i have some questions ideas:

do the microphones work well when they are placed upon the speaker? i mean, can the user speak to the c1 when there is sound? i guess respeaker has noise cancelation, but just wondering.
led light is just a dot? it would be great if it would were a “line” on the border around the top and/or bottom of the speaker, a la alexa ( i like their design of that light)

image1105×396 239 kb


hardware buttons, don’t see any, but what about 4: power, vol +, vol - and play|pause . buttons should be placed on top and be part of the enclosure, without standing out from it.
dark color option! i found black more elegant and “clean” than white or clear colors


can you drop stls here so we can take a peek?
a couple things we’ve learned:

the power needs to be extremely clean and grounding needs to be well thought out or the speaker will buzz.
the mic and speaker should be in physically separated.
the re-speaker stuff makes us of a proprietary binary for noise cancellation.  this may or may not work with the 2 mic setup ( i don’t know because i haven’t tested ).
the grill can be printed with fdm, but it is very hard to get it right.  the retract on the fdm can cause issues with alignment and make prints difficult.
making textiles work on a plastic speaker is more difficult than you might think.
heat.  pay attention to how air gets in/out of the microprocessor chamber.
hardware buttons are important.  precise works…(ish)…but in a loud enviroment it may become impossible to control the speaker which leads people to unplug it…which corrupts the cf…which bricks it until it is re-flashed.
the timing between the wake word detection and dropping the volume via the adafruit amp can be a problem.
noise canceling mic arrays need to be co-planer.  they don’t work as designed under a dome.

there are probably a few more gotchas, but i’m glad to see you’re working on it.  if you share the stls i’m happy to take a look ( or print/make one if you’re happy with the results ).

maybe it’s time to crowd source the mark ii? lots of talented people in the community, plenty of individual hw testing happening (speakers, microphones, noise cancelling, displays, etc.)

we had a meeting about getting people in the community more involved in the hardware ( and software ) development process.  more news on this to come, but…yes…we are going to start involving the community more in both the decision making and implementation.
i don’t want to hijack this thread tho, so we’ll have that discussion in a new thread soon.

wow, thanks for all the feedback! there is some super-helpful stuff in here.
@malevolent

do the microphones work well when they are placed upon the speaker? i mean, can the user speak to the c1 when there is sound? i guess respeaker has noise cancelation, but just wondering.

that is a very good question. see below. 

led light is just a dot? it would be great if it would were a “line” on the border around the top and/or bottom of the speaker, a la alexa ( i like their design of that light)

it is just a single led (for now). i’m trying to make this design simple for the community to print and assemble at home, but arbg is something i’ve considered.

hardware buttons, don’t see any, but what about 4: power, vol +, vol - and play|pause . buttons should be placed on top and be part of the enclosure, without standing out from it.

see below on this one, too!

dark color option! i found black more elegant and “clean” than white or clear colors

this will be completely up to the user. i don’t currently plan to distribute this, just to provide it to the community. if there is enough interest i would investigate a group-buy of professionally printed parts, though.
@j_montgomery_mycroft

can you drop stls here so we can take a peek?

i would love to get some feedback from you since you’ve been so intimately familiar with designing these things, but i’m rather enjoying designing on my own without influence from a whole bunch of people so i can continue to learn  would you be ok if i dm them directly to you?

the power needs to be extremely clean and grounding needs to be well thought out or the speaker will buzz.

do you have any specific suggestions here?

the mic and speaker should be in physically separated.
…
noise canceling mic arrays need to be co-planer. they don’t work as designed under a dome.

hrm, this may require a redesign of sorts to make things wider then. the second item also seems like two separate ideas. so you’re saying i need to lay the mic array flat (co-planar) but also don’t put it underneath the lid (dome)? just trying to follow the thought process.

the re-speaker stuff makes us of a proprietary binary for noise cancellation. this may or may not work with the 2 mic setup ( i don’t know because i haven’t tested ).

is there another hardware suggestion you would recommend?

the grill can be printed with fdm, but it is very hard to get it right. the retract on the fdm can cause issues with alignment and make prints difficult.

the “printed” grill is really a frame to wrap in speaker cloth. it’s a tiny bit narrower than the top and bottom which overlap the edges. it’s also why there is a gap across the back, to make it easier for assemblers to pull the cloth inside the cylinder and then attach it. i’ve actually already redesigned that some to simplify construction (removing the need for supports) and make the holes bigger.

making textiles work on a plastic speaker is more difficult than you might think.

i’ve done a lot of speaker design/construction in the past (i’ve designed crossovers that were balanced flat for a room/etc) however this will be my first time laying cloth onto plastic. i am also curious how this will go, but i think i’ve got a working idea with the pull-through attachment method.

heat. pay attention to how air gets in/out of the microprocessor chamber.

this was why i put the pi inside the speaker cloth area. i’m hoping there will be enough ventilation that way.

hardware buttons are important. precise works…(ish)…but in a loud enviroment it may become impossible to control the speaker which leads people to unplug it…which corrupts the cf…which bricks it until it is re-flashed.

yea, 2 votes already for hardware buttons. this is something that i’ll certainly find a way to implement, but maybe in v2.0 so i can get this thing out to the world more quickly. i started to look into capaciive buttons that i could lay under cloth like how the google home mini works.

the timing between the wake word detection and dropping the volume via the adafruit amp can be a problem.

good to know. same question as above, any hardware suggestion?

we had a meeting about getting people in the community more involved in the hardware ( and software ) development process. more news on this to come, but…yes…we are going to start involving the community more in both the decision making and implementation.
i don’t want to hijack this thread tho, so we’ll have that discussion in a new thread soon.

please post a link in here when it comes so we (i) can find it 
on a good note, the test print (rough) came out as expected:

testprint600×800 195 kb





 fmstrat:


the re-speaker stuff makes us of a proprietary binary for noise cancellation. this may or may not work with the 2 mic setup ( i don’t know because i haven’t tested ).

is there another hardware suggestion you would recommend?


the respeaker 2mic-pi-hat is a very simplistic mic board. the mic-array-v2 and the usb-mic-array are more sophisticated with hardware-suppported audio processing for  echo-cancellation etc.

hi
can you list what 3d printer and accessories you used or what you suggest using for prototyping the moira cylinder and parts?




 dominik:

the respeaker 2mic-pi-hat  is a very simplistic mic board. the mic-array-v2  and the usb-mic-array  are more sophisticated with hardware-suppported audio processing for echo-cancellation etc.


well, i like the look of that mic-array-v2, however the price is way up there compared to the 2mic. i don’t want to put this out of the price range of what some people may want to pay, so i’ll need to do some research on benefits. thanks for pointing it out!



 markc:

can you list what 3d printer and accessories you used or what you suggest using for prototyping the moira cylinder and parts?


not yet, sorry. my test prints are just rough cuts using pla on a monoprice maker select v2. once the design is complete and final hardware is selected, i will probably make slightly oversized versions of the slts, too, so that abs can be used and smoothed with acetone vapors. that would make for a nice clean finish with at-home prints.
i will be posting all of this type of progress here as time goes on. thanks!

very interesting thread. will follow the development.
as soon as the final part list is available, i will support them within mycroftos similar as the mark-2 device.

@j_montgomery_mycroft hey, don’t want to hound you, and i know there is a lot up there, but i’m on hold with the design until i hear back on this:



 fmstrat:


the mic and speaker should be in physically separated.
…
noise canceling mic arrays need to be co-planer. they don’t work as designed under a dome.

hrm, this may require a redesign of sorts to make things wider then. the second item also seems like two separate ideas. so you’re saying i need to lay the mic array flat (co-planar) but also don’t put it underneath the lid (dome)? just trying to follow the thought process.


any chance you could address that one part? does the array need to be “open air” at the top, or can it be under cover (dome) just flat at the top separated from the speaker?

so i’ve decided to go with the respeaker 6-mic array to stay in line with the feature set on the new hardware. it’s more pricey, but i have a feeling this group would prefer functionality over price.

poll for the group. with the new 6-mic array, the device will need to be wider, and can thus be a bit shorter. that being said, i have two options i can go down for audio. the “google home” model or the “alexa v1” model.
the first is dual full-range speakers. these would likely be 2"" full-rangers that could get down to (potentially) around 175hz, most likely one of the popular peerless models. these would be outward facing, similar to the google home to give a bit of “surround” presence.
the second is a single 2-way speaker design. this would consist of a 3"" woofer and <1"" tweeter, likely adding clarity in higher volumes and may be able to get down below 100hz. so while this setup would be mono, it may actually “sound better”.
thoughts?

this does not really hooks into your poll, but is more my view / idea /wish for an enclosure.

why not flipping the enclosure 90 degrees into a soundbar type of form factor. then you can use the top and bottom, which are now the left and right side of the enclosure for two speakers.
with the soundbar form factor you can use the 4-mic lineair from respeaker at the front for the mics and leds;
https://wiki.seeedstudio.com/respeaker_4-mic_linear_array_kit_for_raspberry_pi/
but to really kick ass (imho) you use below screen as the front of the soundbar formfactor;
https://www.waveshare.com/product/raspberry-pi/displays/7.9inch-hdmi-lcd.htm
witth, the 4-mic lineair as top (or just above the screen) and as described two nice speakers left and right. you could even use the back of the soundbar formfactor enclosure for proper openings / airflow etc.
with the right design, you could make the screen a option. if no screen, you just provide a (fabric) type of insert/wrap.
(really no clue if above makes sense for your guys? hard to speak about something that is visual in your brain. if i could draw it i would…)
edit; btw no clue why that ribbon connector is at the front, but the one from voicen (same company i believe) has it at the back;
https://wiki.voicen.io/hardware/

depends on your goal - is moira a voice assistant that can play some music (like amazon echo device) or is moira a music player with voice assistant capabilities (e.g. sonos)?
anyway some knowledge in acoustics desing should be helpful here…

hey @fmstrat  great design. i like the clean form. here are some quick thoughts. i don’t have much experience with the 6 microphone array, but it looks like it uses a software solution for the audio front end (librespeaker). we have been relying on a hardware solution for the audio front end (respeaker mic array v2.0 with xmos xvf-3000). the drawback of using a software solution is that it could impact the performance since they’re both using the rpi for resources, but i’ve been curious about some of the software solutions out there and how they perform. we have really only heavily evaluated one software based audio front end by alango.
it was a good idea to move your speaker drivers to an outward firing design because upward firing would likely impact your barge-in performance. having the speakers pointed at the microphones would impact the acoustic echo cancellation.
for a lot of the hardware-based audio front end solutions the designs are dependent on the microphones being planar with as little obstruction as possible. that is why you see a lot of designs with a flat top. you also want to get the mics as close to exterior as possible.
lastly, use some strategies to reduce vibration between the acoustic chamber and the microphones. for example, we mount everything on either foam tape or silicone. we are also using a separate enclosure for the acoustic chamber. all of this improves barge-in performance.
great project!

we are using a dual driver setup with two tc5fc07-04 that are rated at 146hz. we will drive these with 5w continous (7w peak) per channel or 10w continous (14w peak) total. i’ve found that loudspeaker designs are very subjective, it might be worth experimenting with a few different set ups and seeing how they sound. we went through almost a dozen different configurations before landing on the dual tc5fc07-04 drivers.

the software ec works really well due to diversification of process.
the actual ec algorithm only runs when audio is playing, so soon as your vad or kws kicks in and stops audio play essentially there is no load until media starts again.
the respeaker usb products are not that great the firmware isn’t really capable on multi mic and respeaker admit its better to use the single channel firmware.
also the audio out isn’t all that good, seems to be plenty of complaints.
the 2mic is a great piece of hardware but the drivers suck slightly, well for me a lot, don’t like at all seemed to had all sorts of conflicts my 2 mic now gathers dust.
the software ec works well on both pi3 & pi4 and think it does a slight better job on the pi4 which makes me think its just fiiting into the pi3 load.
many of the microphones are planar but that is due to doa (direction of arrival) and the beamforming you can produce.
as that part in software as far as i am aware (beamforming) doesn’t exist in a good working form.
the respeaker usb stuff doesn’t make a lot of sense for price.
anker do a jabra challenger https://www.amazon.co.uk/anker-speakerphone-microphones-conference-compatible-black/dp/b07znt7prl
which £69 free delivery from amazon makes it cheaper and its far better than a respeaker.
personally i have found with barge in you can do it with any usb sound card and the ec software that works reasonably well that doesn’t give a mycroft gump that refuses to stop that it does without.
there is an edimax stereo mic usb card that first thing to do is break that great little pcb out of its butt plug housing and link up 2x active mics.
the modules are really cheap mems or electret and have agc and programmable gain by gpio.
https://www.scan.co.uk/products/enermax-ap001e-dreambass-usb-soundcard-plus-earphones-genie-with-integrated-80-hz-plus6-db-bass-boos
the noise/buzz can often be solved with a dc blocking capacitor approx 4.7uf inline to your amp, but again there are amps that have a standby pull to gnd that a transistor on gpio could activate.
also on the input a relatively small for audio standards capacitor can help.
tpa3116 has a wide array of configs and this one was my budget one with standby https://www.ebay.co.uk/itm/tpa3116-2x50w-mono-100w-digital-audio-power-amplifier-board-module-12v-24v-amp/172479863207
i went the other way round with the speaker with a 4 ohm  8cm 30watt from tectonic as the bmr are supposed to have far less directional audio with this freaky radiator design.
the problems of isolating mics which makes a huge difference was i mounted the speaker in the base facing downwards and the fixing screws became pillars to raise a gap for sideways audio dispersal (approx 10mm or above seems to work great).
https://www.tectonicaudiolabs.com/audio-components/bmr-speakers/
i didn’t print a thing as used pvc tube which i have learnt order, receive and measure as the outside dimension will always be correct but the internal always seems to vary.
its really cheap and easy to get acrylic disks cut via ebay that i taped underside ran a bit of epoxy around the join and created an airtight seal.
i used some hex spacers for the next level pulled the speaker cable through with a bit of flex left and also sealed that.
i dremmeled an access slot again hex pillars and a final disk of the outside diameter went on the top.
mics where planar as they where easier to fit the top disk was clear in fact they all where as it made the order easier and also had one with the speaket cutout also.
mic could of gone on the sides with no doa beamforming choice is yours and never got round to testing effects on sensitivity.
4"" or 5"" pipe makes a relatively low profile puck than a tube, so actually don’t need access to 3dprint.
i was supposed to draw up some vinyl cutouts and add some leds but it was just a test.
if you can cope with the rather ropey 3.5mm pi audio out you can also just add x2 i2s as adafruit do drivers and have a howto on their site.
you could also add a hdmi2hdmi+audio and use with x2 i2s mics as the are less than $10 google hdmi audio extractor and yeah the cheap china ones are relatively decent.
the active mics on a soundcard are far more sensitive and you could put multiples on an input if that floats your boat.
my s-pipe mycroft got disassembled with me just playing with hardware and pi’s both the pi3a+ and pi4-2gb are best value.
pi0w just doesn’t make the grade for me anymore but there is a zero priced rockpi-s rk3308 pi3 equivalent with built in audio codec that maybe i might get round to doing some work with but sort of lost interest and been a bit lax of late.
with heat a 12v 3cm fan on 5v works surprisingly well and is extremely silent, which a 5v running at full speed will definitely not.
if i had access to 3dprint i would prob make an access cover for my dremeled port, maybe make some internal mic housings and really go to town on mic isolation, but that would prob be it.
the denser more solid pvc is miles better with higher wattage audio as from what i have seen of some of the 3dprints they are likely to flex and rattle and detract from audio quality as they often look extremely thin and the filament material is often low density.
but just to throw in a swerve ball the speaker can go on the bottom and it still sounds sweet.
there is also a syba usb stereo mic soundcard and i have purchased x2 and both turned up as fakes with a c-media mono mic input, but mono can be ok and as said its choice how many mics you feed it.
i don’t really see beamforming and far field as much loss as they work extremely well in either distributed noise with predominant volume voice or a quiet background.
if they have to compete with another single predominant noise source they can quickly return this vocoded synthetic noise that is bad for recognition.
in industry the beamformers prob work great but domestic wise hifi or tv depending on position to you can often totally negate its effectiveness.
a couple of i2s mics and 3.5mm of the pi is just a couple of $ with software ec.
soundcard and active mic modules £15 same approx for hdmi2hdmi+audio and i2s which gives you an upgrade from the 3.5mm but soundcard and active mics is prob way to go if you did not get i2s mics.
my opinion with respeaker is there is always gotchas or snakeoil and why i don’t like them i think its very much the latter.
the anker is supposed to be a great piece of kit but haven’t tried myself.



adafruit learning system



adafruit i2s mems microphone breakout
for many microcontrollers, adding audio input is easy with one of our analog microphone breakouts. but as you get to bigger and better microcontrollers and microcomputers, you'll find that you don't always have an analog input, or maybe you want to...





"
217,volume control for mark 1 rca stereo jacks,support,"
i ran into an unexpected issue with my mark 1 when setting it up to stream from google music on my command. i hooked it into my amplifier, only to discover that the rca output is a line-level output, and no volume control normally available to mycroft can adjust it. rather inconvenient to have a voice assistant you can’t tell to quiet down!
the solution is to create a software volume control audio path and profile set, and bind them to the mark 1’s audio output in pulseaudio:
# /usr/share/pulseaudio/alsa-mixer/profile-sets/mycroft-enclosure-mark1.conf

[general]
auto-profiles = yes

[mapping mycroft-enclosure-mark1]
device-strings = hw:%f
channel-map = left,right
paths-output = analog-output-software
paths-input = analog-input-mic

# /usr/share/pulseaudio/alsa-mixer/paths/analog-output-sofware.conf

[general]
priority = 99

[element master]
switch = mute
volume = ignore
override-map.1 = all
override-map.2 = all-left,all-right

.include analog-output.conf.common

# /etc/pulse/default.pa

# insert the following line before all other audio drivers are loaded.
load-module module-alsa-card device_id=sndrpiproto name=mycroft-enclosure-mark1 card_name=mycroft-enclosure-mark1 profile_set=mycroft-enclosure-mark1.conf profile=""output:analog-output-software,input:analog-input-mic""

add the two new files, make the change to the driver parameters, and reboot your mark 1 (or otherwise reload pulseaudio for the mycroft user), and presto: the pulseaudio master volume mixer will affect both the internal speaker and the output jacks.
","
so i just tried this, and it had no effect.
has anyone else tried it?  i feel like maybe an intervening update has broken this method, but i’d love to know if it’s just me…

i’ve been couch-surfing for a while and thus haven’t had my mycroft set up, but when i have time i’ll dig it out, update, and see what’s changed — assuming nobody beats me to it!
"
218,productivity time tracker,skill suggestions,"
skill name: time-tracker
user story:
as a remote worker i want this skill to track how much time i spend on different categories of tasks so that i can see where my time is being spent over different periods of time.
what third party services, data sets or platforms will the skill interact with?
it could interface with existing services, or just track data itself and report back to the user.
are there similar mycroft skills already?
not that i am aware of.
what will the user speak to trigger the skill?

“start time tracking for emails”
“i’ve finished emails for now”
“i’m starting work on code reviews”
“stop time tracking”

what phrases will mycroft speak?

“time tracking started”
“great, you spent xx minutes on emails”

what skill settings will this skill need to store?
could define categories via settings but this might be more limiting than doing so dynamically in the skill.
",
219,speech recognition failing,none,"
mycroft installed perfectly (well after some help) but now although the cli is working the speech recongnition shows this.
15:34:07.855 | error    |  6462 | mycroft.client.speech.listener:transcribe:239 | list index out of range
15:34:07.856 | error    |  6462 | mycroft.client.speech.listener:transcribe:240 | speech recognition could not understand audio
is there something wrong with my mic or is it mycroft
","
this happens if the stt backend doesn’t return any response. try the audiotest to verify the audio quality. i assume you get some audio since to get this far the wakeword must have triggered.
stop mycroft then run
./start-mycroft.sh audiotest
it will use the mycroft internals to record 10 seconds of audio and then play it back to you.
also in the bottom right corner of the cli you can see the mic input level. make sure that is moving.
"
220,fatal error when installing,none,"
i have two problems. one is that the script keeps installing pip 2.0.2 over my already installed 2.2.2.
so perhaps this is the problem leading to this error
","
**gcc:**   **fatal error:** killed signal terminated program cc1
compilation terminated.
make[1]: *** [makefile:4068: lang/vid_gb_ap/libttsmimic_lang_vid_gb_ap_la-vid_gb_ap_cg_01_params.lo] error 1
make[1]: *** waiting for unfinished jobs....
make[1]: leaving directory '/home/aryan/mycroft-core/mimic'
make: *** [makefile:4319: all-recursive] error 1




    error: command erroredout with exit status 1:
        command: /home/aryan/mycroft-core/.venv/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-q1llz2
    hw/fann2/setup.py'""'""'; __file__='""'""'/tmp/pip-install-q1llz2hw/fann2/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);
    code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /tmp/pi
    p-install-q1llz2hw/fann2/pip-egg-info                                                                                                    
            cwd: /tmp/pip-install-q1llz2hw/fann2/                                                                                           
       complete output (10 lines):                                                                                                          
       traceback (most recent call last):                                                                                                   
         file ""<string>"", line 1, in <module>                                                                                               
         file ""/tmp/pip-install-q1llz2hw/fann2/setup.py"", line 65, in <module>                                                              
           build_swig()                                                                                                                     
         file ""/tmp/pip-install-q1llz2hw/fann2/setup.py"", line 60, in build_swig                                                            
           swig_bin = find_swig()                                                                                                           
         file ""/tmp/pip-install-q1llz2hw/fann2/setup.py"", line 55, in find_swig                                                             
           raise exception(""couldn't find swig2.0 binary!"")                                                                                 
       exception: couldn't find swig2.0 binary!                                                                                             
       running swig                                                                                                                         
       ----------------------------------------                                                                                             
    error: command errored out with exit status 1: python setup.py egg_info check the logs for full command output.

these are the two errors.

and this as well. 
error: command errored out with exit status 1:
    command: /home/aryan/mycroft-core/.venv/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-onf
1sdw5/fann2/setup.py'""'""'; __file__='""'""'/tmp/pip-install-onf1sdw5/fann2/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file_
_);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pi
p-record-u3xmk94n/install-record.txt --single-version-externally-managed --compile --install-headers /home/aryan/mycroft-core/.venv/incl
ude/site/python3.8/fann2                                                                                                                 
        cwd: /tmp/pip-install-onf1sdw5/fann2/                                                                                           
   complete output (28 lines):                                                                                                          
   running swig                                                                                                                         
   running install                                                                                                                      
   running build                                                                                                                        
   running build_py                                                                                                                     
   creating build                                                                                                                       
   creating build/lib.linux-x86_64-3.8                                                                                                  
   creating build/lib.linux-x86_64-3.8/fann2                                                                                            
   copying fann2/__init__.py -> build/lib.linux-x86_64-3.8/fann2                                                                        
   copying fann2/libfann.py -> build/lib.linux-x86_64-3.8/fann2                                                                         
   running egg_info                                                                                                                     
   writing fann2.egg-info/pkg-info                                                                                                      
   writing dependency_links to fann2.egg-info/dependency_links.txt                                                                      
   writing top-level names to fann2.egg-info/top_level.txt                                                                              
   reading manifest file 'fann2.egg-info/sources.txt'                                                                                   
   reading manifest template 'manifest.in'                                                                                              
   writing manifest file 'fann2.egg-info/sources.txt'                                                                                   
   copying fann2/fann2.i -> build/lib.linux-x86_64-3.8/fann2                                                                            
   copying fann2/fann2_wrap.cxx -> build/lib.linux-x86_64-3.8/fann2                                                                     
   copying fann2/fann_cpp_subclass.h -> build/lib.linux-x86_64-3.8/fann2                                                                
   running build_ext                                                                                                                    
   building 'fann2._libfann' extension                                                                                                  
   creating build/temp.linux-x86_64-3.8                                                                                                 
   creating build/temp.linux-x86_64-3.8/fann2                                                                                           
   gcc -pthread -wno-unused-result -wsign-compare -dndebug -g -fwrapv -o3 -wall -march=x86-64 -mtune=generic -o3 -pipe -fno-plt -fno-se
mantic-interposition -march=x86-64 -mtune=generic -o3 -pipe -fno-plt -march=x86-64 -mtune=generic -o3 -pipe -fno-plt -fpic -dswig_compil
e -i./include -i../include -iinclude -i/home/aryan/mycroft-core/.venv/include -i/usr/include/python3.8 -c fann2/fann2_wrap.cxx -o build/
temp.linux-x86_64-3.8/fann2/fann2_wrap.o                                                                                                 
   g++ -pthread -shared -wl,-o1,--sort-common,--as-needed,-z,relro,-z,now -fno-semantic-interposition -wl,-o1,--sort-common,--as-needed
,-z,relro,-z,now build/temp.linux-x86_64-3.8/fann2/fann2_wrap.o -l/usr/lib -ldoublefann -o build/lib.linux-x86_64-3.8/fann2/_libfann.cpy
thon-38-x86_64-linux-gnu.so                                                                                                              
   /usr/bin/ld: cannot find -ldoublefann                                                                                                
   collect2: error: ld returned 1 exit status                                                                                           
   error: command '/usr/bin/g++' failed with exit code 1                                                                                
   ----------------------------------------                                                                                             
error: command errored out with exit status 1: /home/aryan/mycroft-core/.venv/bin/python -u -c 'import sys, setuptools, tokenize; sys.ar
gv[0] = '""'""'/tmp/pip-install-onf1sdw5/fann2/setup.py'""'""'; __file__='""'""'/tmp/pip-install-onf1sdw5/fann2/setup.py'""'""';f=getattr(tokeni
ze, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec
'""'""'))' install --record /tmp/pip-record-u3xmk94n/install-record.txt --single-version-externally-managed --compile --install-headers /h
ome/aryan/mycroft-core/.venv/include/site/python3.8/fann2 check the logs for full command output.

hi, what os are you running on?
the mimic error could be an out of memory issue or out of tmp diskspace. you can try editing the dev_setup.h l539 making it maxcores=1 which should limit the memory usage.
the second error indicates that libfann2 can’t be found, dev_setup.sh should have installed it…

and while you’re at it install swig >=2

i’m running manjaro. how much space does mycroft need?
thank you

how would i solve the other problem about pip. i have pip 20.2.2 installed but the script still asks me to upgrade it

the arch installation uses  libfann  which was removed from aur. therefore, it should be replaced with  fann

that’s not a real issue. pip just informs that there is a new version available (we lock the pip version since upgrades of pip has broken dev_setup.sh in the past)

any idea how i do this manually

do you mean upgrading pip or installing fann?

it seems to be done already on the dev_setup.sh so i don’t why it’s showing the error

i know to install it but how do make it use fann instead of libfann

sorry i know to little of arch/manjaro to help out here 

thank you very much anyway. i’m trying it again and will close this conversation if it’s suscessful
oh my god! you solved it, i’ll just swap that with the actual active one. perhaps you could make this change a real thing? https://aur.archlinux.org/packages/fann/
no that happens to me as well

just noticed the gitrepo the script tries to pull gives a 404: https://aur.archlinux.org/fann.git
edit: false alert, just my proxy acting up

if the package is directly available in the repo we should definitely use it instead of building locally. the arch / manjaro install is community contributed, none of the mycroft devs uses it so if you can get it up and running feel free to make a pr updating it. or open an issue on github so it’s tracked.
"
221,where is the message types csv mentioned in the documention repo,none,"
i was looking at finding ways to automate what i did by hand (with some error and inconsistency) in the mycroft-json-messages repo. i did that just by following the documentation, which from my understanding that page is autogenerated it’s self using this script (if i am way off base that wouldn’t be that surprising either though).
the main issue though is that the scripts mentions a message_types.csv as it’s source of truth to build the rest of document off of, but i haven’t had any luck find any .csv files in any of the repos.
","
@krisgesling will know for sure but it may be an export of https://docs.google.com/spreadsheets/d/1rsbpmogx4o8a_zf_uelyqslk8dlischgnfzj3isn0ae/edit#gid=0

yep, ake is spot on, the csv was exported from document linked above.
important to note that the linked document was last edited in 2019. updates were made as i was generating the docs when i came across incorrect or unclear information, and then further edits have been made to the page since it got generated.
so message types page in our main documentation should now be considered the best source of information and any edits or additions be made directly to it.
it’s still not complete. i was made aware today that we haven’t added any of our gui related message types as yet. but it is the most up to date that exists.

how is are the available message types determined before being added to the document page?

often they’re proposed through feature additions to mycroft-core. but you could also create a pull request on the documentation repo if you wanted to propose the addition of a new message type.
this means it’s open for others to see what’s being proposed and discuss if it fits within the existing schema. for larger changes we’d likely discuss it at an internal dev sync meeting.
"
222,less than expected audio from mimic2 tts,mimic,"
hello all was playing around more with sending mycroft things to say, and sent him “helloooooooooooooooo” this is what i got from him.
seems he doesn’t naturally get extended ooo sounds
","
how many of those occur naturally in common language again? 
(this is a tacotron synthesis issue, will check if it’s better on t2)

sorry for the delay: it mostly worked on an english tacotron2 model. see chat for more.
"
223,pandora playback issues,support,"
hmmmmm…i have an issue that i think i know what it is, but not sure how to fix it.  when i say “hey mycroft, play pandora”, pandora starts playing, but at super slow speed.  the news plays well, but it uses a different playback method.  the voice works well, but not with long utterances.  the biggest thing is pandora.  i use it often and would love to have it fixed.
","
i agree.  mine stops after a few songs.  @steve.penrod - when will this be fixed?

hey builderjer,
from memory you have a few devices running mycroft is that right? is the slow playback occurring across all devices or just one?
it sounds like this is a different issue to the playback stopping after a few songs. last time it happened, i believe there was an issue with the pianobar package that we use, but i will take a look and see what we can find.

this one is on a fresh picroft.

i found this old pianobar issue that describes a similar issue.
the first (easiest) thing i’d try is reducing the audio quality. you can do this by editing your pianobar config which should be located at:
~/.config/pianobar/config
and add/edit the following line
audio_quality = low
second, for at least some people it was an issue with libao. the fix there was to edit:
/etc/libao.conf
and change:
default_driver=alsa
to read
default_driver=pulse
if neither of those work i’d also be interested to see what your cpu load and memory usage look like at rest vs when pandora is playing. i’m assuming this is a raspberry pi 3?

yes, it’s a pi 3, i will try your suggestions when i get a chance. thanks for your research.

neither of these options worked for me. how do i show cpu usage while running mycroft?

htop should be installed. so if you open another ssh shell and hit ctrl+c to exit the cli.
you can just run:
htop
this provides a nice terminal gui to view the running processes and system resource usage.

i figured that out, and was getting ready to post here. it normally goes, with all 4 processors, between 48 and 58%. no matter if i’m playing pandora, or not. i will try it on my other machine, not pycroft, and see what happens

can you try running pianobar without mycroft and see if it’s the same?
exit the cli and run pianobar
it should have your login credentials configured already so will give you a list of stations, just use the station number to select.

same thing!! arghhh!!

tested on another machine and it works fine. must be a pi error

as in same thing just using pianobar?

yes, sloooooowwwww playback. works fine on my other machine. just not my pi. it sounds very robotic and jittery. is there a way i can post an audio file so you can hear?

you can record from the mic and save it to a file with:
arecord -d 10 -o test.wav
d = duration in seconds - ie 10 seconds here
i’ve done a callout in the picroft chat channel to see how widespread this is too.

i apologize for digging up this old thread but i am experiencing this problem right now and would like to know if you found the problem back in the days 
the very slow and distorted sound-playback does not only occur in pianobar but also when i play selfmade arecords via aplay.
but it doesn’t occur in all audio-outputs - i can listen to the news-skill (german wdr2-radio) without problems.
i use a raspi 3b+ with the latest stable release of picroft and a respeaker 4mic-hat.
the audio-output is configured to the raspberrys own 3.5inch plug in the raspi-config.
i replaced alsa with pulse and set the pianobar-quality to low as suggested above but the problem remains.
i can`'t see a change in htop with or without running a pandora-playback. one core goes to max. 50% while the others remain at 5-15%.

greetings,
i have had the same problem with pandora.  however i did the following and somehow it seems to have helped:

i updated and then upgraded my raspberry pi (version 3).
i rebooted my pi
i tried using a different skill (news skill) then retried the pandora skill

frankly i am not sure what really helped, but i can listen to pandora now.  i just hope this is a good fix - only time will tell.,

thank you for your help 
i already tried this last week (and now a second time) but sadly without success.
another user had similar problems which were originated within pulse-audio, so i will remove it later to check if the problem persists if i am only using alsa (it’s a shot in the dark but who knows ^^).
since my primary goal for mycroft is to control the lights and powersockets in my livingroom and my beloved kodi-mediacenter (and maybe learn a little python on the way ) i will focus on these tasks first.
if i find the time i will try a new, clean install of my rpi3 and then test pandora again next week.
this post will be updated if i find something useful.

changing /etc/libao.conf to default_driver=alsa worked for me?
the package i had to install was libao4 on debian 9. my understanding is that this is for programs that use alsa directly when pulseaudio is the system default? anyone?
linux sound…ugg

hello,
i seem to be having the same issues described in this post. all audio is crystal clear except for pandora. pandora music sounds slow and distorted.
i have tried settings “audio_quality = low” in the .config/pianobar/config file
i have also tried changing the “default_driver” to “pulse” in the /etc/libao.conf config file
resources look okay during playback while monitoring with htop.
if i launch pianobar independently, enter my pandora credentials, and launch a playlist, the audio quality is fine. the problem only occurs through the pandora skill.
update - if i manually launch pianobar from the cli without “sudo” i still have audio issues. if i manually launch pianobar from cli with “sudo” i do not have any audio issues.
update2 - aplay /usr/share/sounds/alsa/front_center.wav = audio issue. sudo aplay /usr/share/sounds/alsa/front_center.wav = no audio issue. i think i am getting somewhere…
any help would be very much appreciated!
"
224,mark 1 power supply,support,"
i seem to have waylaid my power supply for the mycroft mark 1. does anyone know the requirements (voltage, minimum amps and ideally plug-tip size/type)
","

20180611_1433352620×4656 442 kb

*assuming you’re in the us

could you share these details? i’ve lost mine as well. thanks

the power supply is 12v 1a dc.
"
225,testing and feedback inventory skill,skill feedback,"

per @kangaroogoo’s idea
no dependencies


install skill name by …

https://github.com/renayo/inventory-skill.git



feedback on the skill could be provided through issues on github, or preferably by direct reply to this post.
",
226,patent troll wrongly limiting what mycroft can post about,none,"
updates on the patent trolls and their bottom feeding behavior if you’re wondering what’s happening there:



electronic frontier foundation – 24 aug 20



courts shouldn’t stifle patent troll victims’ speech
in the u.s., we don’t expect or allow government officials – including judges--to be speech police. courts are allowed to restrain speech only in the rarest circumstances, subject to strict limitations. so we were troubled to learn that a judge in...






stopping mycroft from blogging about support is a terrible decision.
","
is there anywhere that is still actively posting about the state of things since mycroft is being censored?  also is the eff going to take any action in this case since it seems that the judge is bug-nuts crazy?

the blog posts were removed and they aren’t supposed to be asking for help from the community.  which is ridiculous.  also they’re not supposed to say patent trolls should be shot, stabbed, or hung.  they should be disbarred for crimes against humanity, though.

yea, i read that on the eff page.  i was wondering who is still actively covering all of this since mycroft is not allowed to comment.  if people do not know about this crap, these guys will keep doing it.  i was hoping that there was a tech site that was watching this closely that is not affiliated with mycroft directly and can post the progress of things beyond what the eff has already posted.

someone with pacer access could let the filings get into recap.  that’d be helpful.
"
227,what is the session id,support,"
i am playing with sending json messages to the mycroft websocket, but some of the messages need session ids.
what are these and how would someone get that data from the message bus?
i couldn’t find anything in the documentation, at least by just searching for session, so this maybe a blind spot in there as well (or i’m just bad at searching, an very possible option lol ).
","
sessionid is something that really isn’t used (anymore). a speech request still tags he input with a sessionid though.
what is used is the ident field in the context, which is forwarded all through an interaction (currently only used for gathering timing metrics to evaluate the performance of the stack)

ok, cool so is an optional field overall then?
is there a websocket request to get one assigned if it needed?

it’s definitely optional (borderline deprecated)
there is no way to request one over the messagebus at the moment.

sweet well i will file that under something not to worry about 
thanks!
"
228,testing and feedback euronews skill,skill feedback,"
euronews live stream
live news from euronews

about
european news for mycroft
supported languages:

english
portuguese
italian
spanish
french
german


622×780

examples

“play euronews”
“play news in portuguese”
“play news in spanish”

github



github



jarbasskills/skill-euronews
european news for mycroft. contribute to jarbasskills/skill-euronews development by creating an account on github.





credits

jarbasal
euronews

category
information
tags
#news
",
229,testing and feedback dagon a lovecraft story skill,skill feedback,"
dagon, a lovecraft story skill
adaption of dagon, by h. p. lovecraft

1308×541

a simple skill to figure out how to best play videos in skills
about
this skill brings to you an adaption of dagon, by h. p. lovecraft
an illustrated reading of ‘dagon’ by h.p. lovecraft. read by mike bennett, illustrated by christopher steininger.
“dagon” is a short story by american author h. p. lovecraft. it was written in july 1917 and is one of the first stories that lovecraft wrote as an adult. it was first published in the november 1919 edition of the vagrant. dagon was later published in weird tales.
illustrated by christopher edwin steininger

508×780

examples

“read dagon”
“play lovecraft video”
“play dagon”
“play dagon audio drama”
“play the dagon video”

github



github



jarbasskills/skill-dagon
dagon, by h.p lovecraft. contribute to jarbasskills/skill-dagon development by creating an account on github.





credits

jarbasal
dagon by h. p. lovecraft (youtube)

category
entertainment
tags
#audio
#video
#books
#lovecraft
#entertainment
",
230,skills loading but not responding to intents,support,"
i went to use mycroft this morning to find that certain utterances no longer properly triggered skills. here are some examples of utterances that previously worked with what they now return:

how are you
>> i am doing well, thank you — (works as expected)


set a timer for one minute
>> i dont know what that means — (answered by questionanswersskill)


set a timer
>> how long of a timer? — (works as expected)
one minute
>> timer started for one minute — (works as expected)


stop timer
>> sorry, i don’t understand. — (answered by questionanswersskill)


play queen on spotify
>> just a moment while i look for that  — (works as expected)
>> i’m not sure how to play queen on spotify — (cli info indicates playback control skill returned no matches)


spotify list devices
` no response — nothing in the cli  log output at first; however, whenever the next utterance is received, the log reads:

error    |   738 | mycroft.skills.intent_service:handle_converse_error:247 | skill id does not exist


it looks like many of these commands are wrongfully going to fallback skills, though i am not sure if that is the case for all of them (see spotify)
a look at the skill log seems to indicate that skills are loading properly, so my guess is that somehow the intent system is the culprit, but i do not know how to go about fixing it.
this is an unfortunate case where everything was working fine yesterday, and i woke up today to find it broken.
if you have any ideas, they would be much appreciated!
","
i strongly advise to look at the vocab directory from your skill(s). you must be pretty precise in what you say in order to get a proper response.
but dont’t begin with weather skill code… 
<<btw another feature request. read the intents (or at least one variation) out loud. this can be pretty confusing at times. especially with the library growing.

as far as i can tell, these should be valid ways of executing the desired commands. they seem to  match the required intents for their respective commands. i am giving confidence in this not only by how recently they worked for me, but also that in the case of set a timer for one minute , the command is one of the example commands in the skill’s readme file.
swapping one with 1 yields the same results, as does swapping spotify list devices with list spotify devices which is another command stated as working in the documentation.

lets take start timer:
(code)
@intent_handler(intentbuilder(""start.timer"").require(""timer"")
                    .require(""start"").optionally(""connector""))

which means:
required 1 of {start, set, create, begin, need, give}
required 1 of {timer, timers}
optionally 1 of {a, an, another, one more, the}
another entry point is @intent_file_handler('start.timer.intent'), speaking one of:
(another one|more|second|third|fourth|fifth| ) timer ((for ){duration} )
ping me in {duration}
timer
timer for {duration}
timer (for|called|named) {name}
timer (for|called|named) {name} for {duration}
{duration} (second|minute|hour) timer
{duration} timer (for|called|named) {name}
the rest could (and will) throw the intent handler off and the first has nothing to do with the second.

that is my interpretation as well. should that not mean then that set a timer for one minute is a valid intent that should result in a timer being started?

nope,
“set a timer” would be entry point 1
“timer for {duration}” would be entry point 2
start.timer.intent has to be edited a such
(set a|) timer for {duration}
to grant entry

in that case, should any of the following work?
timer for one minute
one minute timer
timer for one

i don’t know how extract_duration_en (parsing “one minute”) handling looks like (i’m german) but
check
not
not

i see. thanks for cross checking. unfortunately, none of those utterances were able to work.
i do potentially have an idea though: in this commit, the timer vocab was changed from including elements that would have handled set a timer for one minute to forgoing them, as they were redundant since the same utterances could be handled by adapt to the same effect. this leads me to believe that there is some failure in the adapt module, though i do not know if it could explain all the issues i am having.
@gez-mycroft, it looks like you made the commit. do you have any insights?

what is the output from the first of the three?
oh since gez added this . that commit keeps me puzzled




 sgee:

what is the output from the first of the three?


by the first three do you mean:



 kabi:

timer for one minute
one minute timer
timer for one


?
if so, the responses were, respectively:
sorry, i didn't catch that.
please rephrase your request.
i don't know what that means.
though i believe those are all fundamentally the same response

i just wanted to know if the first entered the method (the second and third shouldn’t be viable to begin with), but somehow the intent handler  doesn’t recognize {duration} as a part of the intent or is able to assign the utterance to it

but i have to add that i recently set  a timer myself (sucessfuĺly; german client) - which brings me back to the extract_duration_en problem which is probably somehow involved in this process.

making a list of other commands that result in a generic fallback misunderstanding:

restart
shutdown
install timer
uninstall timer
install tuya
uninstall tuya


another update:
i was taking a break from trouble shooting for a few hours, during which time mycroft was left on and booted up. i had previously set up a service to flash a light whenever a mycroft.ready message was intercepted. with neither warning nor my interaction, the light suddenly flashed! i sshed in to discover that some of the commands that had been failing before were working! commands i specifically tested successfully included set a timer for one minute and restart. i also tested spotify/common play commands, though to no avail. i do not believe common play was able to resolve a candidate for the commands, despite them explicitly containing the word spotify and spotify being the only audio skill installed.
this seems to suggest that the spotify issue may be separate from the others, though i still suspect it is related seeing as it began to fail at the same time. it also potentially points a finger at the padatious services, which is listed here as being the sole producer of the mycroft.ready message.
tragically, the positive results did not persist through a system restart, and mycroft is now as functional as it was prior to the indication of the mycroft.ready message. i am hopeful, and feel one step closer to a solution, but am unfortunately still without an actionable path forward.
any ideas are appreciated!

the spotify issue is most likely because of this

that’ll do it. i heard that was a risk, but i did not think it would happen so soon.
i guess we can wrap up the spotify issue as a red herring; the remainder of the problem, however, still stands.

spotify could be the cause of this. i noticed the way spotify disabled the credentials caused the skill to lock up during startup due to a wait in the spotipy module. i did some server trickery and the credentials should fail to fetch completely allowing everything to load.

mixed success! i restarted mycroft, and it was able to respond to commands as expected. invoking spotify resulted in an error about bad credentials—as expected.
i am worried that something may have gone wrong during the update. while i was testing what commands were working, mycroft said something about potentially bad internet connection, and that i should try again in three minutes. i did, and got no results, with my ssh connection freezing shortly after. i rebooted the pi, and am able to see the action light flashing, but have not been able to establish an ssh connection since.
edit: the pi eventually did connect to my router (it was not initially appearing as an attached device), the connection is extremely slow. as an example, took multiple minutes for the microft/picroft ascii art to show up completely on login, and it appeared in multiple bursts of a few lines rather than all at once
"
231,wake word will not work,support,"
i’ve tried everything and mycroft will not respond to “hey mycroft” at all.
its a laptop running kubuntu 20.xx
it easily responds to comments from even farther away when i type 'set a timer"". i’ve even purposely mumbled and it still got it right. it just wont respond to any wake word i’ve tried.
precise and pocketsphinx both do nothing even when i put my mouth directly up to the mic and yell the wake word.
i need help!!!
","
have you tried the other options (jarvis, christopher, ezra) - in manage device defaults?
you might want to tune the
“sensitivity = 0.6” (precise-specific) [between 0 an 1] or
""treshold = 1e-30 (pocketsphinx-specific)
like here

ok, i will try this. i am running mycroft as a snap and there are two identical mycroft config files in different spots. which one is the right one?
also, yes, i’ve tried other wake words, to no avail.

sounds like your mic isn’t the device it’s listening on.  check the audio troubleshooting guide and see if that’s it?

mycroft will read the config files from each location, for personal instances, use ~/.mycroft/mycroft.conf, for system wide stuff you can use /etc/mycroft/mycroft.conf

adjusted the settings in mycroft.conf file. it did nothing

i have no ~/.mycroft/mycroft.conf or /etc/mycroft/mycroft.conf folders or even directories…
sorry for the withdrawn posts. i have no idea how to work this forum all it ever says is “429 too many requests”

audio troubleshooting

mycroft can hear me perfectly fine but the wake word will not work

if it’s hearing you, then it’s hearing the wake word, unless you’ve made some changes.

it hears me when i type “set a timer” and that initiates the listening for me to say, for example, “5 minutes”. saying “hey mycroft” (or any other wake word) doesn’t work at all.
this is a completely default install of mycroft ai. i’ve also tried completely reinstalling it.

i would continue to tinker with sensitivity, threshold or other settings like shown here
if you want to dig deep it should be beneficial (it sounds like you have a somewhat unique speech pattern) to create a custom wake word. but that process is intense.
out of curiosity: what wake word listener is in use?

ok. thanks for the advice. by the way i have a normal us speech pattern with no lisp or any other impediment. it just baffles me that it won’t work.
i have tried both pocket sphinx and precise.

i tried installing the non snap version just to see, and it won’t even start listening. it shows the mic level that jumps up and down accordingly to my voice but it won’t even start listening.
ughhhhhh

there’s a script called mycroft-mic-test under /mycroft-core/bin which you might want to use to hear what mycroft is hearing.

i have seen strangeness with the wakeword similar to this. i can easily trigger the wake word, so can my daughter, but my wife cannot trigger it no matter what she tries. my understanding is this is due to the training data that was used to train the model. you may have success training your own wake word using one of the skills found on the forum.

and in this case opting in to the open data set would’t be beneficial (to those who have the problem) because mycroft is hearing everything but the wakeword (or what it identifies as such)  - or do i get something wrong there?

it would be beneficial.  if it’s hearing that many false positives, when tagging starts back up again they will get classified as such and used to improve the models.

does this apply to false negatives (which would be the case here)? i actually don’t know if they could track/classify that

yes. watch the developer sync calls, there’s mention of the classifications for tagging (wake word/not/close-to-wakeword) in the last two.
"
232,mycroft snap application does not start on zorin os 15 3 after installation,support,"
hello dear mycroft team,
i just installed mycroft on my zorin os 15.3 distro. i have updated the system prior, so it is as up-to-date as it gets.
things i did so far:

first i noticed a the release of mycroft on the snapstore (on snapcraft.io) and installed it.
after the installation was completed i tried to launch mycroft but nothing happens.
then i tried “mycroft” in terminal and i get this response:
username@asus-zorinos:~$ mycroft
/snap/mycroft/1078/bin/mycroft-launch: line 92: mycroft-cli-client: command not found
stopping all processes…
mycroft-skills: no process found
mycroft-enclosure-client: no process found
mycroft-speech-client: no process found
mycroft-audio: no process found
stopped

then i unsinstalled it via the software center of zorin and then reinstalled it using the command in terminal “snap install mycroft --beta” … no difference…
there is no process showing up in the system monitor, so i have no clue if it actually starts to run or doesn’t. is there a specific process name i should look for?
perhaps the installer is still running after the snap “finished installation”?

please help me find a way to start mycroft. so far i can’t even manage to connect mycroft to the online services.
previously i could run mycroft on this pc by cloning the git, and i had some experience, however at that time it froze more than not.
","
i don’t believe the snap is an actual release from mycroft itself yet.  this was still in progress…
you could use the desktop install instructions from docs.mycroft.ai, though, that should get you up and running.

thank you, but i suspect yo may not have fully explored the docs yourself since it begins with saying:
"" getting started
there are multiple ways to install mycroft for linux.
snap package
mycroft is now available as a snap package. providing a simple and secure means of installing mycroft on a broad range of linux distributions.
the snap package is currently considered an alpha release. find out more: link""
thank you nonetheless. sorry if i am being a bit cocky, i will try installing through git aswell, until then i do consider this topic still open…
ps.: i did in fact read the official snap documentation prior installing the package.

you got me curious about that snap thing just to know what is happening there. someone yesterday reported the wakeword not working (although it wasn’t working on a git install either) and not finding the user conf due to a complete different structure.

i’d just use the git install.  
"
233,issue with ubuntu 19 10 mycroft service,none,"
job for mycroft.service failed because the control process exited with error code.
see “systemctl status mycroft.service” and “journalctl -xe” for details.
i am unable to start the service, as i am facing the above issue.
will be thankful if any one can assist on this
","
post the details please (regarding mycroft)
"
234,detecting events from other skills,general discussion,"
hi!
i am working with picroft in a custom enclosure with hardware components that i would like to have respond to the actions of various skills. i would like to be able to do this in a general context with a variety of extant skills, but for the purposes of this question, take the example of turning on and off an led when a timer, set with the standard mycroft-timer skill, expires and is canceled respectively.
i have thought of a few solutions for this, the worst of which being modifying the code of the skill itself to include a line that switches the light. one step better would be modifying the skill’s code to send a message via the messagebus to an external script that handles hardware interactions.
the ideal solution, at least as i currently understand, would be to catch messages already sent by the skill in question with an external script that handles hardware interactions. the problem is that, short of shooting in the dark until something works, i do not know how to figure out what (if any) messages are created by methods in those skills.
in short, my question is: how can i check what messages are being sent by a specific skill and when?
thank you for any help you can give!
","
hey kabi,
skills don’t necessarily emit messages to perform functionality. so there isn’t currently a simple way to achieve what you’re doing however i think it is a good suggestion.
with the timer example we could emit a series of messages indicating the changing state of timer notifications. something like:
    self.bus.emit(message('skill.timer.set',
                          {""timer"": new_timer }))
    self.bus.emit(message('skill.timer.expired',
                          {""timer"": expired_timer }))
    self.bus.emit(message('skill.timer.silenced'))

?

thanks for the reply!
that’s unfortunate that it’s not a standard feature—it feels like a good standard to adapt.
in that case, what would you recommend to be the most responsible way to change the code in an existing skill? obviously, i could simply modify the code directly on my device, but that seems vulnerable to change unexpectedly. should i create a branch of the skill’s associated github repository? if so, how would i set my mycroft instance to recognize that specific branch as the one from which it should update?
also, in the interest of better understanding how skills use the message bus, is there a standard established for how skills can be invoked by messages, even though there is no standard for the opposite? i ask because when i was trying to interface with skills, i found that i was sometimes able to call specific methods from a skill with a sensibly named message (e.g. self.bus.emit(message('mycroft-spotify.forslund:list_devices')) would call the method list_devices() from the skill mycroft-spotify.forslund). notably, the function list_devices() took message as an argument in addition to self.
thank you for all your help!

fork, clone,  set upstream, (optional: atom with git and remote-sync to mycroft instance to make coding process easy) push your updates to a branch of your fork and monitor changes made in the original skill and pull upstream/merge if necessary.
your changes will be recognised and not overwritten
(and i definitely second the feature request)

hey sgee,
i’ve created a fork and made preliminary changes; the issue i am running into now is how to get my mycroft instance to identify my fork as the repository to grab the skill from. are you aware of how to do this?

hey, if you change a skill on your device, mycroft will not update it. this means you won’t lose your work, but also means you won’t get any automatic updates from the upstream skill. each skill in /opt/mycroft/skills is a git repo so you can manage them the same you would any other git project.
we welcome contributions to all our repositories, so if you add something that you think would be useful for others then please create a pr back to the upstream skill.
the challenge on automating this for all skills and intents is that we don’t know what the outcome of an intent is. imagine a user says:

set a timer for pasta

this will trigger the timer skill but it hasn’t yet created a valid timer as it doesn’t have a duration. the skill will ask for this additional information and if provided will then set a timer. but if the user stops responding, cancels the request, or says something that cannot be parsed as a duration then no timer will be set.
one possibility is that we add a message for which intent is called and whether that intent was successful or not. you could then assume that if a “set_timer” intent reported success, then a timer is active.
the expiration isn’t triggered by an intent however so a message would probably need to be manually emitted by the skill for that.
we do have a skill api feature in the works that allows skills to expose specific methods to other skills. the spotify list_devices method is a curious one, i’m not sure why that works. @forslund can you shed some light on that? i can’t see any event handlers setup for it.

currently the work around to call another skill is to emit an utterance as if the user had said something that you know will trigger the other skill.

this is why i suggested a slightly more intricate way but ultimately granting a better workflow.
another caveat: (i guess) some skills are hooked to the backend to provide global configuration options. i think these are tied to a specific codebase. so unless this is rewritten, the skill will be useless.

@gez-mycroft that makes sense. truly, i do not think pragmatically generating responses to intents makes much sense for any party, which makes this more of a “best practices suggestion” than a feature request. designing skills to manually emit messages containing relevant information when when important things happen would make interfacing with skills extremely easy. this totally relies on an honor system, which is not such a satisfying solution, but with enough caring contributors, anything is possible. either way, it’s food for thought when designing. in my opinion, the best thing about mycroft (and the thing that drew me to it to begin with) was the potential to control the web of cause and effect at every level, and designing to allow users to control things as they choose will give mycroft exponentially more potential in that regard.
a skill api  would be a godsend. allowing skills to expose methods would very neatly handle one direction of communication, and it would formalize the notion that skills do not exist in a vacuum.

i don’t think i fully understood your original response until i read



 gez-mycroft:

if you change a skill on your device, mycroft will not update it


were you suggesting using github for version control, but forgoing typical ‘git pull’ for updating the skill on the mycroft device in favor of atom?

in essence. my setup almost completely centers around atom. there is not much to do on github itself, but to send the pr’s when the changes are made and it’s time to potentially merge into the upstream skill. (and even that isn’t necessary)
the fork gets cloned to my local machine, edited with atom (github activated, so you can forget about cmd) and then pushed onto mycroft (with atom package remote-sync) with every change you save. this makes it easy to test your changes (live) right away and act appropriately.
always work on a seperate branch to keep the default clean which then gets pushed to your github fork. with that and the upstream setup (i linked) you can easily fetch changes (locally) of the default branch - or other branches - and incorparate them if you chose to do so or simply push them to your fork.
when the pr is granted you can fetch default and send it to mycroft and everything works as normal. on the contrary you can stick with your changes with an eye on the changes made since. (big) changes are not that frequent to keep up with.




 kabi:

… to include a line that switches the light. one step better would be modifying the skill’s code to send a message via the messagebus to an external script that handles hardware interactions.


i’m at the moment tinkering with jarbas skill node-red / hivemind  (had node red running on my homeserver prior which is dealing with my sensory stuff) in theory it may accomplish what you trying to do (essentially it hooks the message bus to a websocket communicating with node red) - but the other way around  (light -> skill)

all intent handlers are registered on the messagebus and their message type is more or less mangled / demangled when registered and received. technically any intent handler can be called but the message type and the actual parameter names may change at any time.
the list_devices method doesn’t use any data so it’s pretty easy to invoke since the message data doesn’t need any special format.
"
235,day identification is not working correctly,none,"
after the last update, mycroft is having issues ‘saying’ the correct day of the week.  i notice this most often in the weather app.  examples (weather data is generally correct with ‘day’ being the only issue)…
 request:  'hey mycroft.  will it rain today'
 response: 'it will most likely not rain yesterday'

 request 'hey mycroft.  will it rain on monday' (asked on sunday, so monday is 'tomorrow')
 response: 'it will not rain today'.

i thought it could simply be an issue with the starting day of the week, but other times, she will get the verbagge correct. and thus a different starting day of the week should not matter.
","
what platform are you on, and what time zone are you in? i’m especially interested in the time zone thing.

mark i us central time zone.
if i ask the ‘date’ or ‘time’, she gets them correct.  seems to be isolated more to referential days - yesterday, today and tomorrow.

i’m digging in the function, and it seems that ‘yesterday’ was an oversight, specific to english, most recently on my (non-professional foss guy) part. you can post an issue on github, if you like, or i’ll take care of it (posting an issue) next time i get a break.
i can’t replicate the problem with “today” or with future relative dts, but i have discovered a couple other quirks.
if you’re curious: the weather skill correctly recognizes that “yesterday” is a relative day (this comes from a vocabulary file) and then passes it along to a parser that turns words into a machine-usable datetime. unfortunately, the parser doesn’t currently know what to do with “yesterday,” so it spits out its default output when it thinks it’s been passed garbage: today’s date.
as for “will it rain today” outputting “it will not rain yesterday,” that’s gonna bear separate investigation.
having written this much, i should clarify that i don’t work for mycroft, i just jumped in because i recently touched the datetime parsers (not that one, though.)

i went ahead and posted an issue at the mycroft core repository regarding “yesterday” as well as a few other things i found while i was digging. the problem with “today” parsing to “yesterday” is less obvious, and might be down to the weather skill, as i can’t replicate it by invoking the datetime parser directly. the mycroft instance i’m working on thinks i’m in us/central and it’s telling me today’s weather just fine 

thanks for the information (i like knowing things like this).  i haven’t had a chance to look at it either, but thought it would be good to have the information in the wild so more eyes can check it out.

thanks for reporting these, and for logging the extract_datetime issue too.
going to get the weather timezone situation looked at this week, pretty sure it will be either retrieving the wrong report from owm due to timezones, or that it’s evaluating the date of the report from an incorrect “now”. eg if now is tomorrow then today is yesterday 

what time of the day does this happen?
can you test before and after 12am? there is some logic around this to handle a special case that might be causing this weirdness
also please test using cli directly to rule out wrong stt transcriptions

i think the problem might be somewhere in the nice_date utils, doesn’t sound like it is in extract_datetime because “yesterday” wasn’t handled at all, i need to look into the weather skill and see if i can find the issue

i was thinking the ‘time of day’ requested could be part of it as well.  this was around 8:00 pm which would be after midnight for utc.

at the moment i can’t figure out how it’s even assembling “most likely”

testing tonight to see if it has to do with the different days in utc and us central time and after 7:00 central, she is now saying “i don’t have that information” for “will it rain today”.  asking at 6:00 pm today and she answered correctly and asking at 7:00 pm she answers correctly for tomorrow (wednesday).
note - i also asked “will it rain on tuesday” thinking i might get a different response, but she stuck with “i don’t have that information”.

one more update.  now that another hours has passed, she says ‘yesterday’ for today again.  guessing the time change (hasn’t changed in the us yet) is also causing issues.

hey, we’ve tracked this one down, it definitely is the timezone issue. a fix should be coming shortly.

fantastic.  thanks for the help and the great work.

any update on when this fix might be coming out?  i just updated to 20.2.0b and it is still an issue.  while i know the information is correct, sad to have constantly hear the wrong day from a device designed to be helpful.

i had to re-image mycrofts sd card and start her from scratch.  after i had her setup, she did an update and then i manually updated the os and rebooted her.
when she came up, i ask her where she was and the time and she answered correctly.  i asked about tomorrow’s weather and she still incorrectly stated it as ‘today’.
i ran the ‘dpkg-reconfigure tzdata’ command after sshing into her and select the us - central timezone and rebooted her again.
when asked about tomorrow’s weather this time, she stated ‘tomorrow the weather…’.
in the past when i looked at her config to find her local, i did not run the above command.  i am wondering if this was the issue then as well.  seems like there might be two different ways she is getting date/time information and both need to be set for her to truly ‘know’ the correct date.

that’s a current problem with the weather skill.most of the methods use the internal method __extract_datetime(“today”) to determine the date (22,8,0,0.0) which has a to_utc method added in succession. which means depending on your location it might fall back to 21,8,… i’ve planned to make a pr in the next couple of days.
to be precise: the code of the return value from __to_utc() is wrong. instead of “when.replace(tzinfo=pytz.utc)” it should return “when.astimezone(pytz.utc)”. the fallback to “yesterday” happens elsewhere because of the replacement
"
236,how long of a time should it take to update a mark i,support,"
is there a time frame when an update has been ‘too long’?
i have told mycroft to update to the latest version 3 days in a row.  she said she is downloading it in the background, it might take a while, but should be running the latest version ‘soon’.  last night, i rebooted her and her eyes went yellow and they have been yellow since.
this is an update from 20.0.3 -> 20.0.4 as i keep her on the latest versions.
","
hey randy, there’s an issue with the mark 1 update service at the moment, caused by the expiration of a root ssl certificate.
it is possible to do a work around on the device but we’d advise to just hold on until we’ve resolved it properly.

hey randy, this has been resolved, let me know if you’re still seeing any issues.

looks like she is ok now, with a twist.  i asked her today what version she was on.  she let me know and that i wasn’t on the latest version and did i want to update.  i told her ‘no’.  she said ‘ok’ and a while later she rebooted and was on the latest version.
i like her to update, but since i said ‘no’, she really shouldn’t have (but i am glad she did).

the no response would have been honoured in terms of not updating immediately, however it doesn’t currently turn off future automatic minor updates. major updates eg 19.8.9 > 20.4.0 require user confirmation.
will have a think about how we should handle this.

hello - could this issue be resurfacing with the latest update?  she’s (mark i) been on yellow eyes for 10 hours so far.

hey randy, it was a pretty sizable update with a few new/changed dependencies so can take a while however definitely not 10 hours. did you already try power cycling the device?
if it is perpetually stuck on yellow eyes, the easiest option is probably to flash the sd card with the latest mark 1 image.
for the mark ii we are looking at better update systems to ensure this can’t happen. so in the event that an update does fail, it will roll back to the previously working state.

yes, i have power cycled it a couple of times with no success.  i have also run manual update (sudo apt-get update && sudo apt-get upgrade) via ssh and power cycled afterwards.  still no luck.  it has been another 10 hours prior to the ssh commands and 5 more after them for a total of about 25 or so hours.
i will look at flashing the sd card.  it is just a pain to remove/replace on the mark i.  hopefully, access on the mark ii (i am a backer on kickstarter) will be better.

 sorry to hear that
on the mark ii, we are switching to usb boot and making these easily accessible so flashing or running your own software will also be much easier.

no worries.  i pulled the sd card last night and it is being a pain to format/write with an image.   i is the original one, so it could be on the way out.  replacing it should solve the issue.  i will let you know if anything else comes up.
oh, and thank you for the prompt response and information.  it is appreciated.
randy

i have the sd card loaded back in to mycroft.  she did an update, i reconnected her and did os updates and she is going well.
"
237,picroft on raspberry pi 4b 8gb,none,"
hey,
i plan to try out picroft and therfore have some questions concerning the hardware i should use:

does picroft support the new raspberry pi 4b with 8gb ram?
can picroft use more than 4gb ram on a pi yet and does the increased ram improve performance?

cheers,
markus
","
the latest image is running buster so there is no reason it should not run on rpi, mine is running off a 3b using the googleaiy kit that came with the magpi.  i’m working on making it a bit ‘smarter’.  my daughter is keeping me on my toes with that!

according to the following issue on github it is not working yet:


github.com/mycroftai/enclosure-picroft








is picroft supported on pi4 with 8gb?



        opened 10:05am - 08 aug 20 utc




          robertsawko
        





hello,
i have only just started playing with picroft and i burned the picroft-buster-keaton-2020-01-10 stable on an sd card. after connecting all...










ootb,  no (means with picroft image). but you can copy the necessary files into /boot/.
8gb seems a bit over the top. maybe if you want to go big on the gui side, but even a full fledged bigscreenos (kde neon) is satisfied with 4. there would be some usecase in the machine learning departement… but mycroft itself? not in the foreseeable future
"
238,internet radio skill,general discussion,"
i just created a skill called “internet radio” for now it only exists in my own github repo.  i am looking for a few people to test it out.  https://github.com/normandmickey/skill-internet-radio
its a very simple internet radio player. the default station can be changed in the init.py script.
","
hi @norman_moore, that’s a pretty awesome skill, thanks so much for all your hard work.
let me flag this with our skills team in chat at;
https://chat.mycroft.ai/community/channels/skill-management-team
and we’ll get some peope to check it out for you.
thanks again!

thanks @norman_moore
non-techy here.  what command can do i use to install it via voice?  would love to check it out and give you my thoughts.

im still working on getting it included in the mycroft skills directory.  it should be availbe in a few days.  i will let you know when it is.  thanks.

you can install this skill now by telling mycroft to “install internet radio”.  please let me know what you think.

@norman_moore my unit is having conflicts with the install voice command.   whenever i say “install play some music”  it starts playing pandora.   any thoughts???

i will install the pandora skill and test it out. i may have to change my skills name and keywords.

@norman_moore any luck on this?

i renamed the skill internet radio to avoid conflicts with the keyword “play”. try asking mycroft to “install internet radio”.  you can also now manage the station url under your device settings at home.mycroft.ai.

@norman_moore i finally got pandora uninstalled and got yours working.  just to note, currently, the pandora skill lives in memory so you need a reboot to clear that cache.
here were my steps:

voice:  hey mycroft, uninstall skill pandora  – need @mn0491 to make the uninstall vocab a little more robust
voice:  (to confirm pandora was uninstalled)  – hey mycroft, play pandora – this worked and asked @mn0491 why.  he said it was due to memory on my mark 1.   need him to fix that.
reboot the device.
rechecked pandora and it was gone.
voice:  hey mycroft, install internet radio.  this worked.
voice:  hey mycroft, play some music – failed.
voice:  hey mycroft, play internet radio station - asked me to configure the settings.
8   went to home.mycroft.ai and went to found the settings.   everything was already there so i just hit save.
voice:  hey mycroft, play internet radio station - started playing.
went back to home and changed the genre from rock to rap.  (just checking out the feature) hit save.
stopped the stream and restarted the skill.   still same song.  verified my genre changed in home.  it did.
reboot.
voice:  hey mycroft, play internet radio - same results as before.

so it works.  my assumption is by changing the genre it would change the stream url and play different music.  it did not.
thank you for providing this.  great option to listen to random music.  needs a little work imo to get ready for the public.
nate

looking through the code you actually need to change the station url. the other one is simply just a label. @norman_moore, there is a label tag for the settingsmeta.json if you would like to use that instead. check it out and example here https://github.com/mycroftai/skill-weather/blob/master/settingsmeta.json

@mn0491 is correct you can either change the station url in the settingsmeta.json file or under the skills settings at home.mycroft.ai .  i plan to build upon this skill and add the ability to select a different station to listen by specifying the genre (ie. “internet radio rock” or “internet radio classical”).  i can set a default url for each genre but the user will be able to override it using the skills settings on mycroft’s website.  fyi. the default radio station is one that i setup.  it is licensed and does not have any commercials.  it’s a mix of rock music between the 50s and 80s.

that would pretty sweet! let me know when you are done or need any help with that. i’d love to test out the new features for you.

the internet radio skill has been updated to support multiple genres.  here are the new commands.  the station urls can be changed under the skills settings at home.mycroft.ai
""internet radio""
“web radio”
“play some music”
“rock radio”
“country radio”
“classical radio”
“country radio”
“jazz radio”
“top 40 radio”
“christmas radio”

i have the issue that when i ask to play something it plays, but after it says ‘an error occurred in x skill’

i will test it out on a clean install tonight.  did you just install it or have you had it for a while?

i changed the url for “favorite” and i just get silence.  i still get rock radio, etc.
i want: https://vpr.streamguys1.com/vpr64.mp3
do i say “favorite radio” or just “internet radio”?
a great addition might be some other tabs available that novices like me can change at mycroft.ai for my favorite stations.  i love the potential here.

i had not realized that only some stations allow streaming ip addresses outside of their own players (i.e., my local npr station).  i found ip addressed that worked–the problem is that, not the program.
it would be nice to have a second “internet radio” skill that has blank stations that i can fill with my own ip addresses.  imagine the six buttons on my car radio that i program.  currently, i say “childrens radio” to get the 80s station i want, but i don’t remember that easily.  saying “radio four” would be at least a tad less confusing.  i think i can remember six buttons and their matching stations.
even better, the genre, too, as choice.  i have no idea how it could learn both the name and station, although i am sure it can be programmed by someone with more talent that me (i.e., anyone).  so, in skill i put “new wave radio” into one box and the ip address below it.
i am really enjoying internet radio, though.  thanks!

hi there,
i’ve change https://vpr.streamguys1.com/vpr64.mp3 on the mycroft portal and i modify the file /skills/skill-internet-radio/settingsmeta.json. seems it doesn’t overwrite the setting.
to start just say hey mycroft, start internet radio
{
“name”: “internet-radio”,
“skillmetadata”: {
“sections”: [
{
“name”: “favorite station url”,
“fields”: [
{
“name”: “station_url”,
“type”: “text”,
“label”: “favorite station url”,
“value”: “https://vpr.streamguys1.com/vpr64.mp3”
}
]
},
{
“name”: “rock station url”,
“fields”: [
{
“name”: “rock_station_url”,
“type”: “text”,
“label”: “rock station url”,
“value”: “http://144.217.253.136:8564/stream”
}
]
},
{
“name”: “country station url”,
“fields”: [
{
“name”: “country_station_url”,
“type”: “text”,
“label”: “country station url”,
“value”: “http://50.7.70.58:8708/”
}
]
},
{
“name”: “classical station url”,
“fields”: [
{
“name”: “classical_station_url”,
“type”: “text”,
“label”: “classical station url”,
“value”: “http://174.36.206.197:8000”
}
]
},
{
“name”: “top 40 station url”,
“fields”: [
{
“name”: “top40_station_url”,
“type”: “text”,
“label”: “top 40 station url”,
“value”: “http://bbcmedia.ic.llnwd.net/stream/bbcmedia_radio1_mf_p”
}
]
},
{
“name”: “christmas station url”,
“fields”: [
{
“name”: “christmas_station_url”,
“type”: “text”,
“label”: “christmas station url”,
“value”: “http://144.217.180.30:8016/stream”
}
]
},
{
“name”: “children’s station url”,
“fields”: [
{
“name”: “childrens_station_url”,
“type”: “text”,
“label”: “children’s station url”,
“value”: “http://14123.live.streamtheworld.com/sam01aac213_sc”
}
]
},
{
“name”: “jazz station url”,
“fields”: [
{
“name”: “jazz_station_url”,
“type”: “text”,
“label”: “jazz station url”,
“value”: “http://149.56.155.209:80/live”
}
]
}
]
}
}

there was a problem with the “favorite radio” keyword which has now been corrected.  you should remove the internet-radio skill and install it again. i also found that the back-end media player (mpg123) will not play https streams.  when you update your favorite station at home.mycroft.com  enter the stream url without the https (http://vpr.streamguys1.com/vpr64.mp3).
"
239,different behaviour keras tensorflow model precise listen vs live,support,"
hey mycroftees,
i’ve (incrementally) trained some keras model on the mycroft-precise dev version with quite an extensive load of material with a result of 97% accuracy.
tested it with precise_listen with the expected outcome just dinging on “samira” (ww) or “samira”-ish words with 100% accuracy. tested it against tv and other common noises around here. everything as expected so far.
but with the implementation of that model things turned somewhat up-side-down.  the recognition on “samira” is about 10% and the church bell triggers it with
an incredible accuracy.
triple checked the config, which is in line with the ones given in the docs. the wake word is set to “samira” (mycroft_cli_client). and from what i can tell the voice.log not indicating some major problems.
2020-07-25 17:34:01.048 | info     |   706 | mycroft.client.speech.listener:create_wake_word_recognizer:323 | creating wake word engine
2020-07-25 17:34:01.050 | info     |   706 | mycroft.client.speech.listener:create_wake_word_recognizer:346 | using hotword entry for samira
2020-07-25 17:34:01.052 | warning  |   706 | mycroft.client.speech.listener:create_wake_word_recognizer:348 | phonemes are missing falling back to listeners configuration
2020-07-25 17:34:01.054 | warning  |   706 | mycroft.client.speech.listener:create_wake_word_recognizer:352 | threshold is missing falling back to listeners configuration
2020-07-25 17:34:01.060 | info     |   706 | mycroft.client.speech.hotword_factory:load_module:403 | loading ""samira"" wake word via precise
2020-07-25 17:34:03.368 | info     |   706 | mycroft.client.speech.listener:create_wakeup_recognizer:360 | creating stand up word engine
2020-07-25 17:34:03.371 | info     |   706 | mycroft.client.speech.hotword_factory:load_module:403 | loading ""wake up"" wake word via pocketsphinx
2020-07-25 17:34:03.672 | info     |   706 | mycroft.messagebus.client.client:on_open:114 | connected
2020-07-25 17:35:51.805 | info     |   706 | mycroft.session:get:74 | new session start: b49e73cc-c657-4611-8934-7a049a81546c

a couple of questions regarding that log:
why is pocketsphinxs’ wake word loaded (since none is set up in the conf)? fallback?
and why are phonemes and thresholds missing? (pocketsphinx stuff?)
(edit: samira.pb.params: ""threshold_config"": [[6, 4]], ""threshold_center"": 0.2)
what has gone so wrong that caused the live implementation to be that inaccurate? is there a known problem with dev? should i step back to master?
","
how much data?  how much wake word, not wake word?  97% means there’s some misses on your dataset, did you reinforce those items?  do the noises that are triggering the tf model trigger it in testing as well?

i know there’s some room for improvement in that regard and i think i know how to solve that.
what’s the problem here is that the (live) tensorflow model behaviour is completely off compared to the accuracy precise-listen indicates.

how are you testing each?

ok, i tested .net with precise-listen - statistics from precise-test - (the pb only in live conditions).now you’ve asked i tested .pb also with precise-listen and it behaves similarly.
would be nice to know were the loading/implementation problem stems from.

most likely it’s a sensitivity issue. just forgot to add the custom (.8) sensitivity to the hotword entry

are you testing on the same host you’re using it live on?

yes, to reduce uncertainty. and there is plenty for those who are not knee deep into this to be frank.
the model is trained at .8 sensitivity, i just forgot to adress that. but in that departement i still have to figure out how the threshold is altering the overall sensitivity of the listener.
and then back to the drawing board filling those gaps of (live) false postives. unluckily the german database is not nearly as good and despite running against multiple gigs of phonemes, common voice data and noises there are huge gaps - and logically apeaking these should have absolutely nothing to do with any sensitivity. the statistical number (even as high as 99%) is just as good as the data thrown at it.

to be clear, this is far from being resolved, even with the sensitivity added in subsequently. made another model, switched it and switched back and experienced a lot of silence (e.g. not recognizing the wake word) resp. triggering on planes, ventilators etc. whereas mycroft-precise (precise-listen) is pretty much on point.

that sounds more like data is the issue.
what version of precise are you training on?

has anyone else gotten around this yet? i went through the steps in the mycroft documentation. the freshly trained wake word did work, but also tripped on almost every noise it heard, as to be expected. training it against the public domain sounds to cut out the false positives yielded a wake word that did not work at all. i created a new wake word again, from scratch, and followed the tips section to fully train it (e 300 b 5000 s .8). after training it against both the public domain sounds and the google sounds, i have a 99.84% model on precise-test, and it works flawlessly in precise-listen. however, i have yet been able to get it to trip once i run mycroft with the pb file. if i change the file name in mycroft.conf to the mycroft.pb file, the mycroft wake word works. i think i am seeing the same thing as sgee.

what version of precise are you training on?

0.3.0 cloned from github

see if you can check out the 0.2.0 version and train with that instead.  that’s what mycroft-core is still using at the moment.

thanks. i did try checkout the 0.2.0 version, but it fails due to tensorflow 1.8.0 no longer being available. i downloaded the new precise-engine to .mycroft/precise/, renamed the old precise-engine directory, and unzipped the updated engine there, and that did correct the issue.

you can use 1.13 on .2 without issue.

mycroft could probably do with something like the linto hmg (hotword model generator)



github



linto-ai/linto-desktoptools-hmg
gui tool to create, manage and test keyword spotting models using tf 2.0 - linto-ai/linto-desktoptools-hmg





creating models is very much about the quality of the dataset and after playing, became to really like the layout of the hmg.
after you create your model you can do tests and validation and the false positives and negatives it lists and you can click to play each wav sample.
instantly answer why did that fail and often its extremely apparent.
until that point i had presumed the google command set was pretty good, but due to hmg and listening to almost 8% that where just extremely bad  trims where most of the hotword is clipped.
i thought that google command dataset was validated but wow its not true.
its an old adage with computers but especially true with datasets as input garbage, can drastically effect output.
non hotwords have lesser effect but you will be surprised how what can sound like crap can have similar phones.
after deleting a shed load of hotwords and some non-hotwords that kept appearing i massively increased the model accuracy.
the number of samples even for hws/kws can be quite a manage and a desktop tool like hmg can be a godsend.
don’t assume your dataset is good and if you find bad definately remove them from your dataset.
i found hmg quite interesting to how completely wrong samples can heavily weight accuracy the wrong way.
something like hmg would be a great plus to mycroft and also a user based web interface to share distributed datasets with some peer review method.
there are loads of asr datasets but word datasets are a rarer beast and all we need to do is share mydrive/googledrive datasets in a distributed database that can be queried by language, region, gender, age and word(s).

as disussed in the the dev syncs there soon should be an update of the selene ui with such functionality - though being unclear about the scope . therefor i stopped bothering with the custom ww for now to take a closer look at the (german) skill implementation.
but the hmg is a good call, will test this in my arch-vm, since it’s the only instance running tf 2 (there is a pre dev mycroft-precise pr regarding tf>2.0 btw)
the _val % are dubious at best and only as good as your test sample i each category, which quality is hard to determine. if trained incrementally the split between nonww/test-nonww is made regarding ratio, not quality - most of the time arranging cascaded snippets in one place.
since you followed the same tutorial; trim the “silence” before the ww with sox (great source of false positive environmental sounds)
that said, my best results were with 0,6 sensitivity trained and 0,55 set in the conf… (oh, and the safe best -sb flag during training)

it would be great if a dev could clarify the seleneui precise functionality coming. also i would love to hear from @wolfgange about the matter disussed.
"
240,dev sync 2020 08 19,none,"

0:00 hardware update
1:10 precise wakeword tagging
2:50 jira ticket review - starting with chris on precise uploads
8:30 jira management discussion
17:10 ken on wake word training
23:10 gez on the 20.08 major release
30:40 derick on mark ii enclosure design
36:10 precise tagger user stories
also fyi we’re moving to 3 syncs a week - monday, wednesday, friday.
",
241,dev sync 2020 08 18,general discussion,"

same day posting!!! we’re getting better at this 


0:00 proposed status/watchdog service

9:45 mycroft-core v20.08 release discussion

13:55 changes bumped to 20.08.1

17:35 mycroft-core v21.02 desired changes started

19:20 wakeword schema changes

42:00 mycroft snap package

52:52 josh - why we don’t want a battery.

","
their patent is bogus.   even if you can’t say it, anyone with any level of technical knowledge can see that.

@gez-mycroft is that document already available somewhere? as i see myself as the origin of that systemd/watchdog request, i would like to write out my view, thoughts, etc about the topic. explaining it more.
what i could read on the screen, i would say; what “i want” is basically the same as “mycroft a.i. dev team wants”.
i can typre out the different aspects of all the statuses. what they are for and how they could be used, basically creating that so called “supervisor” by completely rely on the linux kernel, os/systemd and even the hardware itself making it very robust as end consumer device.

thanks for the nudge j1nx. for speed and ease, here is the high level overview chris put forward. would be great to get your thoughts on it and what this service might look like:

the status/watchdog service would be a new systemd service running on devices that run mycroft core. the functionality of this service could include, but would not be limited to:

monitoring various aspects of the health of each core service (is service running, healthy, ready, etc.).
heartbeats could be emitted from each service, or the new service could check the pulse of each service on regular intervals.
actions could be taken if a service becomes unhealthy, such as attempting to restart a crashed service.
specific checks, such as the existing microphone watchdog, could be included in this service.
responsible for restarting services on a daily basis.
if a user opts in to data sharing, information about exceptions or crashes could be sent to mycroft for diagnostic purposes.


the current processstatus pr is here if you haven’t seen that: https://github.com/mycroftai/mycroft-core/pull/2648
to be clear, this pr is not intended to achieve all of the above.
"
242,how to increase mycrofts response rate,general discussion,"
hello everyone.
greetings form nepal
i am new to this forum and i find mycroft interesting.
if anyone knows how to increase response time of mycroft that would be great help.
thank you
shishir
","
what do the logs show as the slowest part of the response?
what hardware are you running on?
what voice are you using?
what config changes have you tried?
"
243,respeaker 2 mic without the horrible respeaker drivers,none,"
respeaker 2 mic without the horrible respeaker drivers.
install the kernel headers



github



pguyot/wm8960
wm8960 driver for raspberrypi. contribute to pguyot/wm8960 development by creating an account on github.





amixer -c2 cset numid=55 1
amixer -c2 cset numid=51 1

or whatever aplay -l gives
in fact so many settings amixer -c2 contents copy those somewhere from your respeaker settings and apply to the wm8960 driver.
amixer pi@raspberrypi:~ $ amixer -c2 contents
numid=12,iface=mixer,name='headphone playback zc switch'
  ; type=boolean,access=rw------,values=2
  : values=off,off
numid=11,iface=mixer,name='headphone playback volume'
  ; type=integer,access=rw---r--,values=2,min=0,max=127,step=0
  : values=110,110
  | dbscale-min=-121.00db,step=1.00db,mute=1
numid=17,iface=mixer,name='pcm playback -6db switch'
  ; type=boolean,access=rw------,values=1
  : values=off
numid=57,iface=mixer,name='mono output mixer left switch'
  ; type=boolean,access=rw------,values=1
  : values=off
numid=58,iface=mixer,name='mono output mixer right switch'
  ; type=boolean,access=rw------,values=1
  : values=off
numid=41,iface=mixer,name='adc data output select'
  ; type=enumerated,access=rw------,values=1,items=4
  ; item #0 'left data = left adc;  right data = right adc'
  ; item #1 'left data = left adc;  right data = left adc'
  ; item #2 'left data = right adc; right data = right adc'
  ; item #3 'left data = right adc; right data = left adc'
  : values=0
numid=19,iface=mixer,name='adc high pass filter switch'
  ; type=boolean,access=rw------,values=1
  : values=off
numid=36,iface=mixer,name='adc pcm capture volume'
  ; type=integer,access=rw---r--,values=2,min=0,max=255,step=0
  : values=195,195
  | dbscale-min=-97.50db,step=0.50db,mute=1
numid=18,iface=mixer,name='adc polarity'
  ; type=enumerated,access=rw------,values=1,items=4
  ; item #0 'no inversion'
  ; item #1 'left inverted'
  ; item #2 'right inverted'
  ; item #3 'stereo inversion'
  : values=0
numid=2,iface=mixer,name='capture volume zc switch'
  ; type=integer,access=rw------,values=2,min=0,max=1,step=0
  : values=0,0
numid=3,iface=mixer,name='capture switch'
  ; type=boolean,access=rw------,values=2
  : values=on,on
numid=1,iface=mixer,name='capture volume'
  ; type=integer,access=rw---r--,values=2,min=0,max=63,step=0
  : values=43,43
  | dbscale-min=-17.25db,step=0.75db,mute=0
numid=10,iface=mixer,name='playback volume'
  ; type=integer,access=rw---r--,values=2,min=0,max=255,step=0
  : values=246,246
  | dbscale-min=-127.50db,step=0.50db,mute=1
numid=23,iface=mixer,name='3d filter lower cut-off'
  ; type=enumerated,access=rw------,values=1,items=2
  ; item #0 'low'
  ; item #1 'high'
  : values=0
numid=22,iface=mixer,name='3d filter upper cut-off'
  ; type=enumerated,access=rw------,values=1,items=2
  ; item #0 'high'
  ; item #1 'low'
  : values=0
numid=25,iface=mixer,name='3d switch'
  ; type=boolean,access=rw------,values=1
  : values=off
numid=24,iface=mixer,name='3d volume'
  ; type=integer,access=rw------,values=1,min=0,max=15,step=0
  : values=0
numid=33,iface=mixer,name='alc attack'
  ; type=integer,access=rw------,values=1,min=0,max=15,step=0
  : values=2
numid=32,iface=mixer,name='alc decay'
  ; type=integer,access=rw------,values=1,min=0,max=15,step=0
  : values=3
numid=26,iface=mixer,name='alc function'
  ; type=enumerated,access=rw------,values=1,items=4
  ; item #0 'off'
  ; item #1 'right'
  ; item #2 'left'
  ; item #3 'stereo'
  : values=0
numid=30,iface=mixer,name='alc hold time'
  ; type=integer,access=rw------,values=1,min=0,max=15,step=0
  : values=0
numid=27,iface=mixer,name='alc max gain'
  ; type=integer,access=rw------,values=1,min=0,max=7,step=0
  : values=7
numid=29,iface=mixer,name='alc min gain'
  ; type=integer,access=rw------,values=1,min=0,max=7,step=0
  : values=0
numid=31,iface=mixer,name='alc mode'
  ; type=enumerated,access=rw------,values=1,items=2
  ; item #0 'alc'
  ; item #1 'limiter'
  : values=0
numid=28,iface=mixer,name='alc target'
  ; type=integer,access=rw------,values=1,min=0,max=15,step=0
  : values=4
numid=21,iface=mixer,name='dac deemphasis switch'
  ; type=boolean,access=rw------,values=1
  : values=off
numid=42,iface=mixer,name='dac mono mix'
  ; type=enumerated,access=rw------,values=1,items=2
  ; item #0 'stereo'
  ; item #1 'mono'
  : values=0
numid=20,iface=mixer,name='dac polarity'
  ; type=enumerated,access=rw------,values=1,items=4
  ; item #0 'no inversion'
  ; item #1 'left inverted'
  ; item #2 'right inverted'
  ; item #3 'stereo inversion'
  : values=0
numid=45,iface=mixer,name='left boost mixer linput1 switch'
  ; type=boolean,access=rw------,values=1
  : values=on
numid=43,iface=mixer,name='left boost mixer linput2 switch'
  ; type=boolean,access=rw------,values=1
  : values=off
numid=44,iface=mixer,name='left boost mixer linput3 switch'
  ; type=boolean,access=rw------,values=1
  : values=off
numid=9,iface=mixer,name='left input boost mixer linput1 volume'
  ; type=integer,access=rw---r--,values=1,min=0,max=3,step=0
  : values=3
  | dbrange-
    rangemin=0,,rangemax=1
      | dbscale-min=0.00db,step=13.00db,mute=0
    rangemin=2,,rangemax=3
      | dbscale-min=20.00db,step=9.00db,mute=0

numid=5,iface=mixer,name='left input boost mixer linput2 volume'
  ; type=integer,access=rw---r--,values=1,min=0,max=7,step=0
  : values=0
  | dbscale-min=-15.00db,step=3.00db,mute=1
numid=4,iface=mixer,name='left input boost mixer linput3 volume'
  ; type=integer,access=rw---r--,values=1,min=0,max=7,step=0
  : values=0
  | dbscale-min=-15.00db,step=3.00db,mute=1
numid=49,iface=mixer,name='left input mixer boost switch'
  ; type=boolean,access=rw------,values=1
  : values=on
numid=53,iface=mixer,name='left output mixer boost bypass switch'
  ; type=boolean,access=rw------,values=1
  : values=off
numid=37,iface=mixer,name='left output mixer boost bypass volume'
  ; type=integer,access=rw---r--,values=1,min=0,max=7,step=0
  : values=0
  | dbscale-min=-21.00db,step=3.00db,mute=0
numid=52,iface=mixer,name='left output mixer linput3 switch'
  ; type=boolean,access=rw------,values=1
  : values=off
numid=38,iface=mixer,name='left output mixer linput3 volume'
  ; type=integer,access=rw---r--,values=1,min=0,max=7,step=0
  : values=0
  | dbscale-min=-21.00db,step=3.00db,mute=0
numid=51,iface=mixer,name='left output mixer pcm playback switch'
  ; type=boolean,access=rw------,values=1
  : values=on
numid=35,iface=mixer,name='noise gate switch'
  ; type=boolean,access=rw------,values=1
  : values=off
numid=34,iface=mixer,name='noise gate threshold'
  ; type=integer,access=rw------,values=1,min=0,max=31,step=0
  : values=0
numid=48,iface=mixer,name='right boost mixer rinput1 switch'
  ; type=boolean,access=rw------,values=1
  : values=on
numid=46,iface=mixer,name='right boost mixer rinput2 switch'
  ; type=boolean,access=rw------,values=1
  : values=off
numid=47,iface=mixer,name='right boost mixer rinput3 switch'
  ; type=boolean,access=rw------,values=1
  : values=off
numid=8,iface=mixer,name='right input boost mixer rinput1 volume'
  ; type=integer,access=rw---r--,values=1,min=0,max=3,step=0
  : values=3
  | dbrange-
    rangemin=0,,rangemax=1
      | dbscale-min=0.00db,step=13.00db,mute=0
    rangemin=2,,rangemax=3
      | dbscale-min=20.00db,step=9.00db,mute=0

numid=7,iface=mixer,name='right input boost mixer rinput2 volume'
  ; type=integer,access=rw---r--,values=1,min=0,max=7,step=0
  : values=0
  | dbscale-min=-15.00db,step=3.00db,mute=1
numid=6,iface=mixer,name='right input boost mixer rinput3 volume'
  ; type=integer,access=rw---r--,values=1,min=0,max=7,step=0
  : values=0
  | dbscale-min=-15.00db,step=3.00db,mute=1
numid=50,iface=mixer,name='right input mixer boost switch'
  ; type=boolean,access=rw------,values=1
  : values=on
numid=56,iface=mixer,name='right output mixer boost bypass switch'
  ; type=boolean,access=rw------,values=1
  : values=off
numid=39,iface=mixer,name='right output mixer boost bypass volume'
  ; type=integer,access=rw---r--,values=1,min=0,max=7,step=0
  : values=5
  | dbscale-min=-21.00db,step=3.00db,mute=0
numid=54,iface=mixer,name='right output mixer pcm playback switch'
  ; type=boolean,access=rw------,values=1
  : values=on
numid=55,iface=mixer,name='right output mixer rinput3 switch'
  ; type=boolean,access=rw------,values=1
  : values=off
numid=40,iface=mixer,name='right output mixer rinput3 volume'
  ; type=integer,access=rw---r--,values=1,min=0,max=7,step=0
  : values=2
  | dbscale-min=-21.00db,step=3.00db,mute=0
numid=16,iface=mixer,name='speaker ac volume'
  ; type=integer,access=rw------,values=1,min=0,max=5,step=0
  : values=5
numid=15,iface=mixer,name='speaker dc volume'
  ; type=integer,access=rw------,values=1,min=0,max=5,step=0
  : values=4
numid=13,iface=mixer,name='speaker playback volume'
  ; type=integer,access=rw---r--,values=2,min=0,max=127,step=0
  : values=127,127
  | dbscale-min=-121.00db,step=1.00db,mute=1
numid=14,iface=mixer,name='speaker playback zc switch'
  ; type=boolean,access=rw------,values=2
  : values=off,off

","
hmmm, interesting. does it work with 5.4 64 bit? if so i might have a look for the 4-mic board as well.

haven’t tried but yeah works on 5.4 armhf didn’t role out the newer pios 64 but prob if i remember rightly they still haven’t included the 64bit kernel headers.
so you will have nothing to build against or any other module also.
no idea why they supply a 64bit os with the 32 bit kernel headers?
so it prob does work on a 64bit os but prob not the beta of raspios 64bit
if you read through there is a script to create the kernel headers.


github.com/raspberrypi/raspberry-pi-os-64bit








missing kernel headers



        opened 12:16pm - 28 may 20 utc




          alexreinert
        





the image contains the kernel package in version 1.20200527-1
but there is no matching header package available in the apt repository.








use ubuntu or other think even arch has a blazing fast 64bit now or i got a arch pi4 rootfs from somewhere.
or work out the kernel header script for 64bit.

kernel headers are not an issue for me as everything get’s build from source. both the kernel and headers. hence even the overlays.

should be yes then for the driver
"
244,as its says is this the best pi4 case,general discussion,"

they do a neo but apart from price the finished form factor of the one having everything on the back is just so good.
https://www.amazon.co.uk/argon-raspberry-heatsink-supports-accessible/dp/b07wmg27t7
https://www.amazon.co.uk/argon-one-raspberry-pi-case/dp/b082b4zy7k
","

having the io split across different sides is one of the things about rpis that has always pissed me off. makes them very untidy if you want to leave them on a shelf somewhere.

i guess if you are going touchscreen then



okdo



raspberry pi 4 touchscreen case – black – okdo
raspberry pi 4 touchscreen 7"" case - black
price: gbp 17.99






becomes less of a problem as its all behind the screen
"
245,dev sync 2020 08 14,general discussion,"

including

1:30 upcoming release schedule
3:20 processstatus for services
21:20 managing and processing community prs
39:10 code styling and formatting

",
246,dev sync 2020 08 13,general discussion,"

1:40 - processing of prs
3:00 - precise tagging, database schema, and storage of contributed wake words
40:30 - backwards compatibility of changes to core
note: we’re trying to take some timestamps as we go to make these more easily searchable etc. if anyone wants to update these as they watch the meetings please drop me a line as it would be very helpful for us 
",
247,mark ii update august 2020,mark ii,"
originally published at:			https://mycroft.ai/blog/mark-ii-update-august-2020/
we received our first set of prototypes of the sj201 board this week. as predicted there was some good news and some bad news.
let’s get the bad news out of the way first. unfortunately part of the shipment was lost coming from china to california. in order to expedite the prototypes we had the manufacturer make a set with the top half of the board assembled and a set with the bottom half of the board assembled. the idea was to sandwich the two boards together to create a fully working unit. unfortunately one set was lost, and we weren’t able to create a complete working prototype.


bottom of the sj201 board with hand soldered components800×519

bottom of the sj201 board with hand soldered components

 
the good news is that kevin was still able to test a lot of the systems from the half that did arrive, and identify some changes that need to be made. additionally he was able to populate the other half of the boards by hand for an almost working prototype.


boards being tested800×450

testing out the sj201

the power (5v and 12v), usb sound card, amplifier, buttons, and leds all tested successfully. the major untested system is the xmos xvf3510 and microphones which create the audio front end. even if kevin were to successfully hand solder the xmos chip there was an issue with the thermal and ground pad. a 1v pin interfered with the pad as it extended out further than anticipated. a keep out is necessary to fix the issue. there are many other smaller issues that have been addressed too, and we are nearly ready for the second spin of boards.


camera with macro lens pointed at pcb, with the output image displayed on a computer monitor.800×450

up close and personal with a 90mm macro lens

these types of issues were anticipated and we would have felt very lucky to have had a completely working prototype on our first spin. for the next batch we will use a board house that can fully assemble both sides of the board.
stay tuned for more updates on the progress of the sj201 boards, and jump into the ~mark2 channel on mycroft chat if you want to see all the granular detail as testing continues.
",
248,recording mycroft tts output,none,"
for a project i am working on i wanted to record out mycroft’s tts to a recorded audio file.
the current work around i have is adding ```“play_wav_cmdline”: “cp --backup=t %1 /home/andruid/mycroft.wav”```` to the system config (mycroft-core/bin/mycroft-config edit system
this will create a new wav file for every time mycroft would have talked.
i tried simply cat %1 >> /home/andruid/mycroft.wav but had no luck on that. are there any better ways to record out audio from mycroft?
","
i believe it dumps the wav into the /tmp/mycroft/ directory, check there for the uuid?
eta: this may just be for the american male voice currently.
eta2: for mimic2, at least, look at https://github.com/mycroftai/mycroft-core/blob/524a74ed5d5ae710e249dc5d896465982c91cd87/mycroft/tts/mimic2_tts.py#l240 and you can write it wherever you want.
"
249,system agnostic mycroft command line tool,none,"
hello all,
in an attempt to make the mycroft command line more featurefull i decided to rewriting parts of it into a python util.
currently you can send any message found on the message bus via it, but only speak, speak-to, and question-query messages actually sending any args or stdin data.
further i’ve only tested a few with speak working, and the mute unmute messages also working.
one of the major feature of this is that you can change the target mycroft by changing your env var mycroft_addr  to another address vs localhost   this could let you set it to say a mark 1 or 2.
reddit post
",
250,codequestion interagration,skill suggestions,"
skill name: codequestion-skill
user story:
as a devoloper it would be cool to be able to ask a coding questioning and have some useful tips be read back to you.
what third party services, data sets or platforms will the skill interact with?
https://github.com/neuml/codequestion is already a program for doing this via the command line by itterating over a parsed and local copy of stackexchange answers
are there similar mycroft skills already?
see https://github.com/mycroftai/mycroft-skills for a list. if so, how could they be combined?
what will the user speak to trigger the skill?
i guess it would be a question fall back system, because ideally it’s
“how do i itterate a list in python again?”
“how do i make a gui in python?”
“how do i read input as string with rust?”
etc
what phrases will mycroft speak?
it might read the question and ask if that works if the confidence is low:
“you want to know “how do i create a map from a list in a functional way?” is this correct?”
if the confidence is high it will just read the answer in chunks:
""use iterator::collect① :
[1] http://doc.rust-lang.org/std/iter/trait.iterator.html#method.collect
|  use std::collections::hashmap;
|
|  fn main() {
|      let tuples = vec![(“one”, 1), (“two”, 2), (“three”, 3)];
|      let m: hashmap<_, _> = tuples.into_iter().collect();
|      println!(""{:?}"", m);
|  }""
mycroft:“would you like me to repeat or go on?”
user: “go on”
mycroft:""collect leverages the fromiterator trait① . any iterator can be collected into a type that implements fromiterator. in this case, hashmap implements it as:
[1] http://doc.rust-lang.org/std/iter/trait.fromiterator.html
|  impl<k, v, s> fromiterator<(k, v)> for hashmap<k, v, s>
|  where
|      k: eq + hash,
|      s: hashstate + default,
said another way, any iterator of tuples where the first value can be hashed①  and compared for total equality②  can be converted to a hashmap. the s parameter isn’t
exciting to talk about, it just defines what the hashing method is.""
reading the lines with “|” slower and pausing between each a little may also be very helpful
what skill settings will this skill need to store?
you could let people set the db from here, but that isn’t implement yet in codequestion and maybe a little more advanced than needed
maybe coding languages preference, but this is also not yet implemented in codequestion which is where it should be put first
lastly one other obvious thing would be intergrate with the plasma applet (and i assume mark2 interface), as the actually coding bits would be nice to see i think for most people
",
251,mycroft does not work in ubuntu 20 04,support,"
i have just re-installed ubuntu 20.04 and installed mycroft and it does not work.
in audio.log:
2020-08-02 16:55:41.324 | info     | 19266 | mycroft.messagebus.load_config:load_message_bus_config:33 | loading message bus configs

2020-08-02 16:55:41.778 | error    | 19266 | mycroft.tts.tts:create:527 | the tts could not be loaded.
traceback (most recent call last):
file “/home/steven/mycroft-core/mycroft/tts/mimic_tts.py”, line 187, in validate_connection
subprocess.call([bin, ‘–version’])
file “/usr/lib/python3.8/subprocess.py”, line 340, in call
with popen(*popenargs, **kwargs) as p:
file “/usr/lib/python3.8/subprocess.py”, line 854, in init
self._execute_child(args, executable, preexec_fn, close_fds,
file “/usr/lib/python3.8/subprocess.py”, line 1583, in _execute_child
and os.path.dirname(executable)
file “/usr/lib/python3.8/posixpath.py”, line 152, in dirname
p = os.fspath§
typeerror: expected str, bytes or os.pathlike object, not nonetype
during handling of the above exception, another exception occurred:
traceback (most recent call last):
file “/home/steven/mycroft-core/mycroft/tts/tts.py”, line 517, in create
tts.validator.validate()
file “/home/steven/mycroft-core/mycroft/tts/tts.py”, line 435, in validate
self.validate_connection()
file “/home/steven/mycroft-core/mycroft/tts/mimic_tts.py”, line 189, in validate_connection
log.info(""failed to find mimic at: "" + bin)
typeerror: can only concatenate str (not “nonetype”) to str
traceback (most recent call last):
file “/home/steven/mycroft-core/mycroft/tts/mimic_tts.py”, line 187, in validate_connection
subprocess.call([bin, ‘–version’])
file “/usr/lib/python3.8/subprocess.py”, line 340, in call
with popen(*popenargs, **kwargs) as p:
file “/usr/lib/python3.8/subprocess.py”, line 854, in init
self._execute_child(args, executable, preexec_fn, close_fds,
file “/usr/lib/python3.8/subprocess.py”, line 1583, in _execute_child
and os.path.dirname(executable)
file “/usr/lib/python3.8/posixpath.py”, line 152, in dirname
p = os.fspath§
typeerror: expected str, bytes or os.pathlike object, not nonetype
during handling of the above exception, another exception occurred:
traceback (most recent call last):
file “/usr/lib/python3.8/runpy.py”, line 193, in _run_module_as_main
return _run_code(code, main_globals, none,
file “/usr/lib/python3.8/runpy.py”, line 86, in _run_code
exec(code, run_globals)
file “/home/steven/mycroft-core/mycroft/audio/main.py”, line 49, in 
main()
file “/home/steven/mycroft-core/mycroft/audio/main.py”, line 36, in main
speech.init(bus)
file “/home/steven/mycroft-core/mycroft/audio/speech.py”, line 182, in init
tts = ttsfactory.create()
file “/home/steven/mycroft-core/mycroft/tts/tts.py”, line 517, in create
tts.validator.validate()
file “/home/steven/mycroft-core/mycroft/tts/tts.py”, line 435, in validate
self.validate_connection()
file “/home/steven/mycroft-core/mycroft/tts/mimic_tts.py”, line 189, in validate_connection
log.info(""failed to find mimic at: "" + bin)
typeerror: can only concatenate str (not “nonetype”) to str
in voice.log:
2020-08-02 16:55:39.403 | info     | 19269 | mycroft.messagebus.load_config:load_message_bus_config:33 | loading message bus configs

alsa lib pcm.c:2642:(snd_pcm_open_noupdate) unknown pcm cards.pcm.rear
alsa lib pcm.c:2642:(snd_pcm_open_noupdate) unknown pcm cards.pcm.center_lfe
alsa lib pcm.c:2642:(snd_pcm_open_noupdate) unknown pcm cards.pcm.side
alsa lib pcm_route.c:869:(find_matching_chmap) found no matching channel map
alsa lib pcm_route.c:869:(find_matching_chmap) found no matching channel map
alsa lib pcm_route.c:869:(find_matching_chmap) found no matching channel map
alsa lib pcm_route.c:869:(find_matching_chmap) found no matching channel map
alsa lib pcm_oss.c:377:(_snd_pcm_oss_open) unknown field port
alsa lib pcm_oss.c:377:(_snd_pcm_oss_open) unknown field port
alsa lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) invalid type for card
alsa lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) invalid type for card
2020-08-02 16:55:41.054 | info     | 19269 | mycroft.client.speech.listener:create_wake_word_recognizer:323 | creating wake word engine
2020-08-02 16:55:41.055 | info     | 19269 | mycroft.client.speech.listener:create_wake_word_recognizer:346 | using hotword entry for hey mycroft
2020-08-02 16:55:41.057 | info     | 19269 | mycroft.client.speech.hotword_factory:load_module:386 | loading “hey mycroft” wake word via precise
2020-08-02 16:55:43.688 | info     | 19269 | mycroft.client.speech.listener:create_wakeup_recognizer:360 | creating stand up word engine
2020-08-02 16:55:43.689 | info     | 19269 | mycroft.client.speech.hotword_factory:load_module:386 | loading “wake up” wake word via pocketsphinx
2020-08-02 16:55:43.888 | info     | 19269 | mycroft.messagebus.client.client:on_open:67 | connected
traceback (most recent call last):
file “precise/scripts/engine.py”, line 32, in 
file “/home/tesla/mycroft-precise/.venv/lib/python3.8/site-packages/pyinstaller/loader/pyimod03_importers.py”, line 623, in exec_module
file “precise/network_runner.py”, line 17, in 
file “/home/tesla/mycroft-precise/.venv/lib/python3.8/site-packages/pyinstaller/loader/pyimod03_importers.py”, line 623, in exec_module
file “numpy/init.py”, line 151, in 
file “/home/tesla/mycroft-precise/.venv/lib/python3.8/site-packages/pyinstaller/loader/pyimod03_importers.py”, line 623, in exec_module
file “numpy/ctypeslib.py”, line 369, in 
file “numpy/ctypeslib.py”, line 358, in _get_typecodes
file “numpy/ctypeslib.py”, line 358, in 
modulenotfounderror: no module named ‘numpy.core._dtype_ctypes’
[19502] failed to execute script engine
exception in thread thread-17:
traceback (most recent call last):
file “/usr/lib/python3.8/threading.py”, line 932, in _bootstrap_inner
self.run()
file “/usr/lib/python3.8/threading.py”, line 870, in run
self._target(*self._args, **self._kwargs)
file “/home/steven/mycroft-core/.venv/lib/python3.8/site-packages/precise_runner/runner.py”, line 197, in _handle_predictions
prob = self.engine.get_prediction(chunk)
file “/home/steven/mycroft-core/.venv/lib/python3.8/site-packages/precise_runner/runner.py”, line 64, in get_prediction
self.proc.stdin.flush()
brokenpipeerror: [errno 32] broken pipe
","
have you made any config changes?
which voice do you have selected?
what have you tried to do to fix it so far? what was the result of that?

fresh install of everything, and there is nothing i can do, it just does not work.

thanks to ken-mycroft, now i know how make it works!

hi callofcthulhu, what was a cause and how did you fix it? i am going to upgrade my ubuntu 19.10 to 20.04 this week as 19.10 is no longer support updates. thank you

hi,
the reason is that ubunt 20.04 has python 3.8 by default and it does not work with mycroft.
so:
wget https://www.python.org/ftp/python/3.7.5/python-3.7.5.tgz
tar xf python-3.7.5.tgz
cd python-3.7.5/
./configure --enable-optimizations --enable-shared
sudo make install

can i clarify, are you using a manual install of precise rather than the binary that is downloaded by default?
using the standard installation instructions mycroft does work on ubuntu 20.04.

when i tried 11d ago as default installation did not work, but now it does.
so, my question is, does mycroft works with python 3.8?

the only actual issue that i can recall is the lack of an appropriate version of tensorflow for precise (issue #133).
currently we only test in python 3.5-3.7, so officially we don’t support 3.8 yet, but we do need to add it given it’s general availability now.

yes, but ubuntu 20.04 uses 3.8 and it seems that now mycroft works with that.

yes, it works because in a default installation precise gets downloaded pre-compiled with tf builtin and wouldn’t hit the issue above. so whilst we don’t officially support 3.8 mycroft does run fine on ubuntu 20.04 as you said.

now, i finally get it, the issue is with precise, so if you want to source install it you need python 3.7 right?

if you want to source install precise yes.
using the “git install” of mycroft-core uses the packaged version of precise.
(just wanted to be clear as i could see people calling this a source install)
"
252,solved mycroft ai skill loading failed,support,"
hey, so when i tried to say basic skills to mycroft it didn’t seem to load.
os: raspberry pi os
kernel: linux 5.4.51-v7+
mycroft core installation directory: /home/pi/mycroft-core
logs:
https://pastebin.com/09rdhuhj
nevermind, i got it solved.
",
253,mycroft spotify not working has no len,none,"
anything i try to play on spotify doesn’t work (albums, playlists, songs, etc). everytime i get this error in the console:

spotifyskill - error - object of type ‘nonetype’ has no len()

the only thing i can figure out is that mycroft is not showing up as a device in spotify. i have installed raspotify as well, but that didn’t change anything.
","
well, i’m not sure what the answer was, but i did get it working. i had to make sure raspotify had my login credentials in /etc/default/raspotify, and make sure raspotify was starting at boot. that let me see mycroft from a spotify client, but i would still get the same error when i told mycroft to play.
but going to the skills settings and typing mycroft as the spotify device has seemed to work. it is not successfully finding anything to play when left as default.

hi there,
thanks for letting us know how you got it working.
can i check, you had already authenticated with spotify through home.mycroft.ai but then manually added your credentials to raspotify as well?

yes, i first registered everything on home.mycroft.ai, then used the link from there to give mycroft permission on spotify’s side. when that wasn’t working i installed raspotify and still no luck (although in retrospect it’s possible i didn’t have the service running?) and so i entered my credentials in raspotify’s config. still nothing until i changed away from leaving default as the output on home.mycroft.ai. the console was spitting out a fair number of references to null this and that, so my guess was the last thing was the problem.

is it result from the network connection issue? spotify always wants me to launch online using my data on my phone. or another problem is that spotify goes wrong sometimes. it wants you to refresh and reinstall the application. check if it works. i found a new ways to listen to my songs from spotify from my friend. it is possible to download a spotify music converter on google like dumpmedia, noteburner or some other tools. convert the songs as you like. then you will not be worried about the issue you have encountered before.
"
254,dev sync 2020 08 10,general discussion,"

","
“i’ll get the hang of this one of these days.”
we’ve all been there, lately. 
"
255,testing and feedback hp lovecraft literary podcast,skill feedback,"
the h.p. lovecraft literary podcast
the h.p. lovecraft literary podcast has been creating podcasts and audio productions since 2009
listen to the podcasts or selected readings
about

each week, hosts chad fifer and chris lackey discuss a piece of weird fiction.
talented voice actors bring the text to life.
music and sound effects create atmosphere while occasional guest experts show up to make things classy.
full story readings are also available alongside podcast episodes

the haunter of the dark by h.p. lovecraft
from beyond by h.p. lovecraft
the picture in the house by h.p. lovecraft
the cats of ulthar by h.p. lovecraft
cool air by h.p. lovecraft
the call of cthulhu by h.p. lovecraft
the hound by h.p. lovecraft
the temple by h.p. lovecraft
pickman’s model by h.p. lovecraft
the statement of randolph carter by h.p. lovecraft
the yellow wallpaper by charlotte perkins gilman
the whisperer in darkness by h.p. lovecraft
one small, valuable thing by chad fifer

examples

“play lovecraft podcast”
“play lovecraft podcast episode #n”
“play the temple by lovecraft”
“read the call of cthulhu”

credits

jarbasal
the h.p. lovecraft literary podcast

github



github



jarbasskills/skill-hppodcraft
the h.p. lovecraft literary podcast . contribute to jarbasskills/skill-hppodcraft development by creating an account on github.





category
entertainment
tags
#audio
#books
#podcasts
#entertainment
",
256,fighting with pulseaudio aec,general discussion,"
i revisited pulseaudio webrtc aec for a final showdown and due to effect this module can have on mental health this is probably a pyrrhic victory.
unlike speexdsp ec, webrtc aec has drift compensation and actually seems to try and cancel the ‘far’ input noise from the ‘near’ input.
speexdsp ec is simpler as its just an ec attenuator rather than cancelation method as when it comes to loud close audio sources it actually still works.
webrtc aec falls down when the ratio of far input gets near to the levels of the near input.
no ec just playback with music noise, for reference. the alg also takes much longer to adapt than speex.
https://drive.google.com/file/d/1-ka0vlgtnkcaz9xwdj9osl5foatggumi/view?usp=sharing
pa ec but when it does its actually cancelation.
https://drive.google.com/file/d/1sjj19wfdu7vc-4py7ondwzps5xfagzbt/view?usp=sharing
you can not have the volume too loud and here same again with volume turned up.
no ec just playback with music noise louder as a reference
https://drive.google.com/file/d/1h96oe78oyxklpmgcqgdfwvfcsnusgfmw/view?usp=sharing
pa ec louder here you will tell things are starting to drop out and louder it only gets worse
https://drive.google.com/file/d/1twtxkbku2d4zketn18f3yld9ruhzubde/view?usp=sharing
because of the drift compensation of webrtc and the possibility of canceling remote input its always been hugely interesting to me but its been a long winded fight to get it actually working on a pi3.
what is confusing me is how susceptible it seems to be to latency and where is this supposed drift compensation but hey.
so i am not saying this is an exact config setup but if anyone wants to further this you can use the following as a start point as it took me some time to work out.
to blat latency to a minimum and do the following starting with  /etc/pulse/daemon.conf
  ; rlimit-rttime = 200000

default-sample-format = s16le
default-sample-rate = 16000
; alternate-sample-rate = 48000
; default-sample-channels = 2
; default-channel-map = front-left,front-right

default-fragments = 3
default-fragment-size-msec = 5

; enable-deferred-volume = yes

then in /etc/pulse/default.pa there is a section for module-udev-detect
### automatically load driver modules depending on the hardware available
.ifexists module-udev-detect.so
load-module module-udev-detect tsched=0 fixed_latency_range=1
.else
### use the static hardware detection module (for systems that lack udev support)

the the webrtc aec is just added to the end.
### enable echo/noise-cancellation
load-module module-echo-cancel use_master_format=1 aec_method=webrtc aec_args=""analog_gain_control=0\ digital_g$
set-default-source echocancel_source
set-default-sink echocancel_sink

the above will give you a starting point and i began to understand that having some of the config=0 is just as important as including it if you wish to activate it.
the only missing options are mobile=0 which is a totally separate aec for mobile webrtc and has options for  routing_mode=, comfort_noise=
i never got round to testing the mobile much but think its a lighter speex like attenuation ec with no drift compensation.
pulse will just use the default card from you asound.conf so just a simple /etc/asound.conf will do especially on a pi as you need a card like speex with playback/capture on the same clock and hardware.
usb soundcard to respeaker 2 mic or something.
defaults.ctl.card 2; # sets default device and control to third card (counting begins with 0).
defaults.pcm.card 2; # this does not change the data type.

start your recording in a console.
parec --rate=16000 --format=s16le --channels=1 --file-format=wav rec.wav
in another start your far noise.
wget https://file-examples-com.github.io/uploads/2017/11/file_example_wav_10mg.wav
paplay file_example_wav_10mg.wav
seems to help to resample to 16k
sox file_example_wav_10mg.wav -r 16000 file_example_wav_10mg-16k.wav
",
257,a skill that is open until closed,general discussion,"
hello, i would like to create a skill that remains active/open until the user closes it or after a long idle time of several minutes. how can i do this with mycroft? is there a config setting that allows me to define this?
","



 scienceguy:

remains active/open until the user closes it


does this mean it is waiting for a user response, or something else?

hi, here is the background. i use a skill to interact with a rasa chatbot. i need to keep the session with this bot open until the user decides to close it or several minutes passed - the user may interact with the bot in the meantime. currently, the session is lost after some idle time and i have to reopen the skill that connects me to the rasa bot. therefore, i thought its possible to adjust the “active/open” time window in the skill settings.
"
258,dev sync 2020 08 06,general discussion,"

",
259,missing part in mycroft mark 1 stl files,none,"
their is not volume knob stl file.  https://user-images.githubusercontent.com/35937408/89564013-9c659700-d7d9-11ea-81cf-3661a3b5c776.png
","
probably best to file an issue on github for this.

i filed one 5 days ago and their has been no response.

that’s not too unusual.  mark1’s not exactly a recent item.  did you check the history of that file to see if it was updated incorrectly?

i do not see the file at all in the changelog.

pinging @derick-mycroft here, might be a good idea to ask in mycroft-chat channel ~mark1 as well
"
260,testing and feedback let mycroft insult you like shakespeare,skill feedback,"
mycroft-msk install https://github.com/renayo/shakespeareinsult-skill
intent activated by insult me like shakespeare
i find it hilarious.



github



renayo/shakespeareinsult-skill
get insulted by shakespeare; for the mycroft open source voice-based ai - renayo/shakespeareinsult-skill





",
261,testing and feedback old world radio,skill feedback,"
old world radio
old world radio
about

examples

“play old world radio”
“play vintage radio”

credits

jarbasal
“old world radio”

category
entertainment
tags
#audio
#music
#entertainment""
github



github



jarbasskills/skil-old-world-radio
better radio for a bitter world. contribute to jarbasskills/skil-old-world-radio development by creating an account on github.





",
262,testing and feedback mouse jiggler,skill feedback,"
mouse jiggler
mouse jiggler prevents your computer from going to sleep
about
a mouse jiggler prevents your computer from going to sleep while you work or play.
this skill creates constant mouse activity so your computer won’t go idle and trigger screen savers or sleep mode—eliminating the need to log in repeatedly.

740×232

examples

“start mouse activity”
“stop mouse activity”
“enable mouse jiggler”
“disable mouse jiggler”

credits

jarbasal

“command line fu” by xkcd is licensed under cc by 2.0


category
configuration
tags
#configuration
github



github



jarbasskills/skill-mouse-jiggler
mycroft skill to prevent your computer from going to sleep - jarbasskills/skill-mouse-jiggler





",
263,mycroft kde plasmoid,mycroft project,"
well looks like it will build in kubuntu-17.04 now as long as you have the backports repo enabled

screenshot_20170812_105813.png1920×1080 603 kb
 
one quirk i have noted so far is the location , it seems to think it’s in  lawrence kansas in both the neon-/dev/stabel & the kubuntu-17.04 installs.
but the basic functions seem to work ok
vinny
","
so ,if one wanted to remove this (uninstall it ) at some time ,just how would you go about it ?
i do not see an uninstall script any where 
vinny

i think @aiix @aix should know more about this, since he is the main contributor on that project.

@vinnywright if you want to uninstall the plasmoid currently you have to remove it manually:
locate the plasma-mycroft directory (should be in the folder you ran the install script from or check inside the mycroft-core folder in /home/(yourusername)/)

cd plasma-mycroft
cd build
sudo make uninstall


@aiix
 thank you for the reply ,i was going to do just that after finding the “options” list in the make file .
but was waiting for a reply before doing it as i’v read that some times this will leave stuff behind.
when i do it i will do a
""locate mycroft > mycroft.txt"" (which seems to find every thing in ~/ and /opt and /usr) before and after to see if it’s all removed and let you know the result hear.
the one in neon-/dev/stable (actualy kubuntu-16.04 with the neon /dev/stable repo added ,jonathon sead it would eat the kittens and it did  , ) seams to work better than the one in kubuntu-17.04 ,in neon “hay mycroft open firfox” works , not in 17.04 ,in neon i can “hay mycroft search google for cat images” and firefox opens a google images page of cat picks , not in 17.04 ,so the “make uninstall” test will happen in 17.04
i ran the install script from in ~/documents/build/mycroft (the /documents/build is where i “try” to do such things when i feel frogy)
it created both a ~/mycroft-core and a ~/plasma-mycroft  directory in /home/me just so you know.

i would be happy to try/test whatever you would like if it would help you at all .

back soon with results of uninstall
vinny

ya ,no running “make uninstall” just removes the the files in the directories , code,images,and ui ,
in
/usr/share/plasma/plasmoids/org.kde.plasma.mycroftplasmoid/contents
and
/usr/lib/x86_64-linux-gnu/qt5/qml/org/kde/private/mycroftplasmoid
but not the directories them selves .
everything else is still hear
locate mycroft > mycroft.txt gives a line count of 4233 items still hear .
but that dose not include whats in my /home/me for some reason ,this is in the kubuntu-17.04 ,in neon the same
locate shows all even in ~/
thankfully everything thats left is in /opt/mycroft/* , ~/mycroft-core , ~/plasma-mycroft , ~/.mycroft and  ~/.virtualenvs
so removal of the rest should be relatively easy ,that is if all of it was in a “mycroft” dir .
after it’s all removed hear (i’m keeping the one in neon-/dev/stable as it’s working ok) i’m going to edit the ""install_plasmoid_kde.sh""
to call “checkinstall” insted of “make install” to see if that would be a more user friendly way of doing the install as theoretically it should be able to use the package management system to remove if necessary ,well we will see any way 
the trouble with the “locate” command not finding anything in ~/ ,and i mean anything not just the mycroft stuff may be that the system is installed to a btrfs file system as @17.04 and @/home17.04
in the neon-/dev/stable install on ext4 locate finds it all ,i’ll check the neon-lts install sitting next to kubuntu-17.04 on the btrfs partition and see if behaves the same .
more sleuthing on this is required.
vinny

ok just got back from the neon-lts install (also on the same btrfs partition as the kubuntu-17.04) and the “locate” command is working as expected ,so this makes me think that their is something amiss in the 17.04 install .
i’m just going to delete it’s subvolumes and do the “checkinstall” test with the “install_plasmoid_kde.sh” in the neon-lts
i do not have a lot of time invested in either one of them so no biggy (theirs 5 os’s on this box)
vinny

well ,i see now that “checkinstall” would half to be placed in sevrall locations ,and would add an ,unreasonable amount of user input during the process.
and theirs really on need as it all (just about"" goes into some form of “mycroft-” dir  and so is not that hard to remove if one wonts to .
vinny

mycroft is working well on my kde archos (actually endeavouros) system. however, installing the kde widget fails. anyone have any success with this?

what’s the error on installation exactly ?

hi aiix
see feedback from terminal install process:
warning: you are using pip version 20.0.2; however, version 20.2.1 is available.
you should consider upgrading via the ‘/home/paulh/mycroft-core/.venv/bin/python -m pip install --upgrade pip’ command.
building with 4 cores.
skipping mimic build.
-----------------------------------------------------
**whew, finally finished that!  now on to the plasmoid **
-----------------------------------------------------
cloning into ‘plasma-mycroft’…
fatal: repository ‘https://anongit.kde.org/plasma-mycroft.git/’ not found
sudo: apt-get: command not found
install_plasmoid_kde.sh: line 54: cd: plasma-mycroft: no such file or directory
mkdir: cannot create directory ‘build’: file exists
cmake error: the source directory “/home/paulh” does not appear to contain cmakelists.txt.
specify --help for usage, or press the help button on the cmake gui.
make: *** no targets specified and no makefile found.  stop.
make: *** no rule to make target ‘install’.  stop.
chmod: cannot access ‘/usr/share/plasma/plasmoids/org.kde.plasma.mycroftplasmoid/contents/code/startservice.sh’: no such file or directory
chmod: cannot access ‘/usr/share/plasma/plasmoids/org.kde.plasma.mycroftplasmoid/contents/code/stopservice.sh’: no such file or directory
chmod: cannot access ‘/usr/share/plasma/plasmoids/org.kde.plasma.mycroftplasmoid/contents/code/pkgstartservice.sh’: no such file or directory
chmod: cannot access ‘/usr/share/plasma/plasmoids/org.kde.plasma.mycroftplasmoid/contents/code/pkgstopservice.sh’: no such file or directory
everything is built and ready to go!
after the machine reboots, you will need to activate it by:

right-clicking on the desktop and picking ‘add widget…’
locating ‘mycroft’ and dragging that to the desktop
click on the plasmoid and press the ‘play’ button
register your devices at https://home.mycroft.ai with the pairing code
say ‘hey mycroft, what time is it?’
press the [enter] key to reboot…

however, on reboot, “add widget” does not present the mycroft plasmoid.
also, during the install process, a ""pip""update is recommended, which i did as instructed. however, on attempting to re-install the widget, it still wants the same “pip” update.
btw, re “sudo apt-get”; my system is arch linux based (i.e. endeavouros).
"
264,how to find load problems in skills,general discussion,"
it has happened to me several times that my pc goes to load 3. it is often the individual skills that i test that use up my pc. unfortunately i only see mycroft skills in htop and cannot assign it to a single skill. is there a way to monitor skills? or should i write a skill that only activates one skill and then monitors the load?
","
in mycroft-cli-client there is a command/option to disable all but one specific skill. type :help in the cli command line to get detailed information…

thanks for the hint unfortunately this is not so great with over 40 skills. i may blame myself for a skill to determine the performance of skills. maybe an live monitor would be great to optimize skills.
"
265,mycroft not installing any skills,support,"
mycroft isn’t installing any skills anymore since i updated. i am using a raspberry pi 3b. when i try to install the skill using the mycroft cli, it returns this error:
10:58:33.145 | error    |   744 | mycroft.skills.intent_service:handle_converse_error:234 | converse requested but skill not loaded.
does anybody know how i could solve this?
thanks in advance,
adutchman
",
266,respeaker 6mic array aec and noise spression,none,"
hi all,
i know this is not the correct forum to ask in, but the respeaker forum is pretty bad for finding answers. i am hoping someone here can help me. i have a respeaker 6 mic array, and it works pretty well. the last thing i want to do with it is enable acoustic echo cancellation and noise suppression, for better wake word detecting if music is playing in the background.
has anyone got this working? the documentation is confusing. it lists example code to enable these features, but it also says that processed audio is available through the loop back interface. does anyone know which is correct? i can’t see why they would provide code to process the signal, if it is already done on board itself? i’m confused.
hope someone can help me out here,
thanks in advance,
stephen
","
@stuartiannaylor is the current aec expert.

no expert as its dom with the respeaker usb things.
it needs to be set a limited channel mode all channels its just doesn’t work.
its all there on the respeaker wiki as it has a python properties control app where you set.
channels i think might be the firmware you flash.
if its 6 mic for pi dunno never played with it but quite likely like the 4 mic 4 pi the loopback channels are somewhere between outright bs or active imagination via seeed.
noise supression for a voiceai is sort of pointless really as it should be done via the mfcc process.
there are some excellent mfcc libs such as librosa https://librosa.github.io/librosa/ but what we have is quite poor in comparison.
https://github.com/juliadsp/mfcc.jl is godlike but julia and not python.



github



voice-engine/ec
echo canceller, part of voice engine project. contribute to voice-engine/ec development by creating an account on github.





is shown as an ec lib so presume it doesn’t have hardware ec as why mention the software one.



github



voice-engine/ec
echo canceller, part of voice engine project. contribute to voice-engine/ec development by creating an account on github.





just do as it says.
sudo apt-get -y install libasound2-dev libspeexdsp-dev
git clone https://github.com/voice-engine/ec.git
cd ec
make

install the alsa fifo
git clone https://github.com/voice-engine/alsa_plugin_fifo.git
cd alsa_plugin_fifo
make && sudo make install

copy the asound.conf to /etc/
run ec in cli console ./ec -i plughw:1 -o plughw:1 -d 75 -f 2048
in another cli console arecord -fs16_le -r16000 rec.wav
in another cli console aplay file_example_wav_10mg.wav
when you stop recording you will notice ec ends.
so to make it permanent you will just have to sort the pcm names and device names but using examples
sudo modprobe snd-aloop
arecord -d my-ec-output | aplay -d aloop,dev0

make aloop,dev1 the default capture and ec will always record but only kick on on media play.
i think there is prob a slight mistake in the code ec.c where the small 10ms filter_frame should be a power of 2 (128, 256 or 512) as fft like it that way. 10ms of 16000hz is between 128 & 256 but speex seems to think 20ms and why maybe i think 256 sounds a little better and 512 seems to work ok aswell.
he has actually set the filter_tail to a power of 2 and actually that doesn’t matter so much, but no harm in keeping to the power of 2  .
so maybe do a hard edit and make.
so it all works via alsa, kicks in when only media is played and with the loopback will not need a restart on each recording end.


speex.org



speex-manual.pdf
429.24 kb





only thing i did any different as noticed raspbian speex & speexdsp are quite old and an rc i just downloaded the tar and compiled and installed the 1.2.0 release before making ec.
ec_hw doesn’t seem to work with my 4 mic linear but not surprised as i am unsure if it has a loopback channel that is stated in the sales lit, think they are mixing it with the usb 4 mic.
if someone can provide a working latency measurement of that arrangement, it would be really appreciated as my 75ms delay is a total guess.
i tried the alsabat --roundtriplatency and couldn’t seem to get it to work but it does with straight hardware. also prob wouldn’t be correct because we have a pipe through a loopback after ec.
again once more my annoying voice but a default install of ec without any tinkering on a pi4 should sound like this.


drive.google.com



default-no-ec.wav
google drive file.







drive.google.com



default-ec.wav
google drive file.






@stephen_o_sullivan
if you have any problems with ec then give us a shout, its actually really easy to install.
also has a pulseaudio default.pa also.
needs a pi3a+ or pi4 as the poor old single core zero has neither neon or ooomph for software filters.
it requires the capture and playback to run from a single clock so its synchronized which means they need to be on the same card.
otherwise the clock drift just kills the ec process.
so use the output of the card not the pi3 3.5mm.

@stuartiannaylor,
thanks for taking the time to respond.
input and output are done the same card already, and i have a raspi 4 (4gb), so all requirements should be met.
my setup works very well with the respeaker card. just sometime, the wake word is not captured if the music is on the loud side.
so, what do i need to do to enable the echo cancellation?
thanks and regards,
stephen

grab a fresh image of raspbian.
ps on a pi4 64bit os is actually about 15-20% faster but just grab a 32bit raspbian lite and give it a fresh flash just to get to grip with things.
install that horrid respeaker driver.
i am not even going to bother with the loopbacks on the 6mic and just stick to what i know. i am presuming they are just the same but hardware loopbacks  that the kernel provides with ```
snd-aloop and you can play with those later.
i am pretty sure the echo mentions of respeaker are snake-oil as the silicon lacks dsp.
from the repo https://github.com/voice-engine/ec do as it says.
for some reason raspbian still includes the rc version of speexdsp and its up to you as the compile of the latest build is easy and if this is additional or not i will just run through here or just jump to the instructions of https://github.com/voice-engine/ec
git clone https://github.com/xiph/speexdsp
grab the latest and then cd speexdsp`` add some compile tools ``sudo apt-get install autotools-dev autoconf libtool pkg-config
think its ./autogen.sh
from memory
./configure -h   will show the options only one you need to add is the libs-dir= which is that funny /usr/lib/gnu-linux-whatever folder
so ./configure --lib-dir=gnu-whatever
don’t enable neon as it just causes errors and gets enabled anyway
then make
then sudo make install
that gives you the release version and installs speexdsp rather than the raspbian rc version it contains but both still work.
so if you didn’t compile install speexdsp as in https://github.com/voice-engine/ec#build
clone ec cd to the dir and make.
stay in that directory when you run ./ec as its not in any path anywhere.
but cd … and then do as https://github.com/voice-engine/ec#use-ec-with-alsa-plugins-as-alsa-devices
then copy https://github.com/voice-engine/ec/blob/master/asound.conf to either /etc/asound.conf (system wide) or ~/.asoundrc (user)
the cd back into the ec folder do as https://github.com/voice-engine/ec#ec-for-raspberry-pi and ignore the hardware loopback directions for now.
you need to do a aplay -l / arecord -l to get the index of the respeaker but likely
./ec -i plughw:0 -o plughw:1 -d 20
i tend to disable pi audio when not used as it just makes things more tidy
sudo nano /etc/modprobe.d/raspi-blacklist.conf
add blacklist snd_bcm2835  save reboot and now your respeaker will be index 0 for capture and playback and the 3.5mm soc_audio will of gone.
not essential but yeah a little more tidy when it comes to audio asound.conf and things.
so then it becomes  ./ec -i plughw:0 -o plughw:0 -d 20
leave that open in a terminal windows so you can see stndout in the window.
open another terminal and the asound.conf we set earlier will of set the defaults but just set the sampling rate, format and file to record to.
arecord -fs16_le -r16000 rec.wav
in another terminal playback an example wav whilst talking into the mic.
wget https://file-examples.com/wp-content/uploads/2017/11/file_example_wav_10mg.wav
aplay file_example_wav_10mg.wav
when its finished playing go back to the arecord terminal and press ctrl+c and that will stop recording.
ec ends when the recording ends so it will not be accessing the card and you can just aplay -dplughw:0 rec.wav to check your results.
thats just the test and wise just to do to make sure all is working as then we just need to add a few lines to the mycroft start service and script as ec only kicks in when media is playing and thats it done.
what i suggest you do is record a non ec via plughw:0 without ec running
arecord -dplughw:0 -fs16_le -r16000 no-ec.wav
in a seperate terminal
aplay -dplughw:0 file_example_wav_10mg.wav
into the mic talk proverbial
ctrl+c arecord terminal on end
then run ./ec
then the same but use the asound.conf defaults
arecord -fs16_le -r16000 ec.wav
in a seperate terminal
aplay  file_example_wav_10mg.wav
into the mic talk proverbial
ctrl+c arecord terminal on end
then compare if all ok then we will automate the startup, prob seems all complex but once you have done that step by step once it will all become fairly apparent and easy.

@stuartiannaylor
“i am pretty sure the echo mentions of respeaker are snake-oil as the silicon lacks dsp.”
i am beginning to think that also. it’s a pity, as i was under the impression, from looking at the seed website, that it was possible on the card itself, which is why i bought it. but to be fair, i don’t think they were being intentionally deceitful, i think they just have poor english. but this is useful information, so i can stop chasing down this rabbit hole.
i have tried setting up ec, but i could not get it working. i will try your instructions and see if i can make some progress.
by the way, i did try some of the seeed scripts, and they did seem to work ok. so i can look at integrating them into mycoft. but honestly, the mycroft wakeword works 95% of the time. i am not sure how much effort i am willing to invest to get it to 97/98%.
i’ll try your instructions tomorrow and i’ll let you know the results.
thanks for the help,
regards,
stephen

i sort of view it snakeoil as if you check the forums there is a long history of this and nothing has been done to change the sales pitch.
they do know, have known and still continue.
my first noob purchase of the 4mic was the same but found its common for them to make claims that are really not true or at least misleading.
the number of times that happens surely can not be by mistake but surely by choice.
ec is for when your playing media and you will not get 95% when it does as you might be getting lucky to get 10% whilst yelling closely to the mic.
depends on the volume and position of your speaker but often for the mic its louder than your voice and at that point ec is essential.
if your not doing ‘barge in’ then you don’t need ec, if you are playing media use ec or wait hopefully till it ends.

@stuartiannaylor
i tied again this morning, but no joy.
i just have a few questions:



 stuartiannaylor:

then copy https://github.com/voice-engine/ec/blob/master/asound.conf  to either /etc/asound.conf (system wide) or ~/.asoundrc (user)


do i completely overwrite the old file or just add the new values to the end of the file?



 stuartiannaylor:

./ec -i plughw:0 -o plughw:0 -d 20


i need to shutdown pulse audio, or else i get a message saying the resource is busy. is this normal? when the ec command is running, should i be seeing something? i see nothing change as i am recording.
the recording seems to be an empty file. it seems to record nothing.



 stuartiannaylor:

aplay file_example_wav_10mg.wav


when the ec command is running, i get no output through the speakers.
any further tips?
by the way, for the 6 mic array, the command should looks something like this:
./ec_hw -i plughw:0 -c 8 -l 7 -m 0,1,2,3

`
regards,
stephen

start with a fresh flash of raspbian just to prove your doing it right.
your probably using the picroft image which i think has a really bad sound setup or at least did the last time i looked at it.
that is how much i don’t like it as will not use it, but presuming so as you have pulseaudio installed.
don’t use ec_hw as all it does is use the hardware loopbacks of the card which has no advantage over kernel loopbacks really. the 4 mic is supposed to have loopbacks but its another seeed snakeoil so never used hw_ec but just read comments on the github they didn’t get it to work.
its zero advantage in terms of ec to have loopbacks, they just allow you to present an output as a standard source input. the kernel supports 128 of them or was it 256 i forget but one of the 2  via modprobe snd_aloop, so whoop if you have 1 on a card. thinking its 128 as 16 snd-aloop of 8 subdevices.
we can add to an existing asound.conf or use pulseaudio https://github.com/voice-engine/ec/blob/master/pulse.default.pa or even setup for pulseaudio and set alsa to use that pulseaudio.
for some unknown reason picroft doesnt use the default devices and sets hardware index parameters in various conf files.
usually in an app it uses default unless deliberately overwritten and for some reason picroft does it the other way round.
the utility on setup prob has set those on whatever you selected during initial setup, so we don’t get lost with picroft, docker or anything else just test on a fresh flash of raspbian so you know everything else is default.
you can run through that once on a fresh raspbian and it will all come apparent and then you can choose to install on image of choice.
also do a aplay -l and arecord -l and get the alsa index of the playback and capture card.
if an asound.conf exists we can adopt and add but it will prob need changes to acomodate what it already contains.
just post the aplay/arecord -l info and the contents of asound.conf do a sudo mv /etc/asound.conf /etc/asound.conf.old so you have a back up and i will post you an asound.conf that can just replace the current.
i could check whats on the respeaker github for 6mic but just send your asound.conf just to make sure nothing is overlooked and really start with a fresh raspbian install with drivers no pulseaudio or picroft so we don’t get sidetracked.
then i can leave you in the hands of mycroft and pulseaudio expert @j1nx
who does keep telling me he is going to test pulseaudio webrtc ec 
ps once more the respeaker asound.conf https://github.com/respeaker/seeed-voicecard/blob/master/asound_6mic.conf is confusing as why its mapping 8 channels like so dunno?

@stuartiannaylor
i did do a fresh install, while i was waiting for you to answer. no pulseaudio installed, and i was able to get it working.
to be able to hear the play back, i needed to run the ./ec -i plughw:0 -o plughw:0 -d 200 command, as it had been stopped as the recording was ended.
i can now see feedback in this terminal, i.e.:
default pipe size: 65536
new pipe size: 8192
skip frames 200
^[[aenable aec
playback filled 160 bytes zero
no playback, bypass aec
enable aec
playback filled 160 bytes zero
no playback, bypass aec
enable aec
playback filled 480 bytes zero
no playback, bypass aec
enable aec
playback filled 160 bytes zero
no playback, bypass aec
and what’s being played back appears to be at a lesser volume, but not totally removed.
something i did differently this time was, when compiling the speexdsp, i added ./configure --libdir=/usr/lib/gcc/arm-linux-gnueabihf/8. before i had left it out (just./configure before)  . is this the correct directory?
i guess pulseaudio could be a part of the problem. now that i “know” what i am doing, i’ll try again with a picroft image and follow the same procedure. but, i have been working on this for 5 hours now. i need a break. i’ll try it out later an get back to you.
thanks again for the help.
stephen




 stephen_o_sullivan:

arm-linux-gnueabihf


just /usr/lib/arm-linux-gnueabihf its just the arm-hf location for libs as arm is multi-arch in terms of 32/64 bit.
i don’t have a pi powered up and doing this all from memory which isn’t great.
yeah doesn’t git rid totally but attenuates echo so that your voice is the predominant sound source rather than the other way round.
its why i have asked @j1nx about webrtc ec as it totally gets rid of echo but totally garbles mic input.
to keep recording and stop ec from stopping we sudo modprobe snd-aloop.
if you aplay -l you will then see we have an aloop card.
from ec you just arecord -dec | aplay -dloopback0 and that keeps ec also live.
just pipes the ec into loopback and that stays recording and you just access the other side of the loopback.
a loopback just lets you play to a source so you can use that as a normal alsa source with your linux audio settings.
its actually aloop:card index, input/output, subdevice of 0-7 as 8 of them
forgot what side is input and output but if the card is index 1 you can just omit subdevice number and :1,1 would be one side and :1,0 would be the other.
also latency i found on a pi4 and pi3 latency was much less that 200 msec seemed approx distance of mic via speed of sound but set to 20.
alsabat --roundtriplatency if it will work with the respeaker drivers as it can be a bit choosey and doesn’t seem to like the ec.
test your device natural latency and then use that with ec.
you can just add snd-aloop to /etc/modules so it loads on boot.
forgot the options for multiple cards but we only need one but would be nice to set the index and can not quite remember the location of those options.
edit /lib/modprobe.d/aliases.conf add ``options snd-aloop index=-2` then it will always come after your respeaker card.
multiple aloop cards just add enable=1,1,1,1,1,1,1 before the index with a 1 for each card up to 16.
in the mycroft or any start-up script just have the ./ec cli command with and & on the end.
same with the redirect of ec to loopback and change asound.conf so the default capture is the loopback rather than eco.
i think this is what ec_hw is supposed to do so you just don’t have to do the aloop yourself.
but without a hardware loopback i never tried, but software its much the same.
confused at the reaspeaker asound.conf as you will see its mapping your both sides of the loopback to both playback and capture !? dunno.
prob with your 6 mic you might want to add something like.
pcm.cap {
type plug
slave {
pcm “my_card”
channels 6
}
route_policy sum
}

as that will sum channels into a mono stream.




 stuartiannaylor:

who does keep telling me he is going to test pulseaudio webrtc ec 


 almost there… 

@stuartiannaylor @j1nx
looks like i cracked it!!!
now i am able to see input coming into the ec program, when running pulse audio.
here’s the trick:
pacmd load-module module-pipe-sink sink_name=ec.sink format=s16 rate=16000 channels=1 file=/tmp/ec.input
pacmd load-module module-pipe-source source_name=ec.source format=s16 rate=16000 channels=2 file=/tmp/ec.output
pacmd set-default-sink ec.sink
pacmd set-default-source ec.source

and then start echo cancellation. it’s getting late where i am, so i can’t have the music too loud. but, i have placed the speakers right on the mic, and i can still trigger the wake word.  so, it looks like it’s working! let me see can i replicate the results tomorrow, and configure it so that this is enabled by default.
regards,
stephen

good, it works quite well for barge in where voice would be swamped by echo.
its pi3a+ and above only but your pi4 obviosly does the trick.
from your tests you will notice ec only kicks in when media is playing so it works really well as has quite high load but only runs during the generally low load of audio media playback.
what you can do with frameworks that mix and match alsa & pulseaudio is use pulseaudio_alsa or the alsa pulse plugin.
then even aplay and other alsa only will output through pulseaudio and use ec.
https://wiki.archlinux.org/index.php/pulseaudio#expose_pulseaudio_sources,_sinks_and_mixers_to_alsa
64bit raspbian lite should be out soon and will gain quite a bit of performance when compiles catch up.
i think ec produces slightly better results on the pi4 so the extra oomf must help and so will better cope with load or introduce latency.
you don’t need noise susspression as if it was implemented it could be done as part of audio>mfcc with no load induced other than mfcc creation.
same with vad but sonopy is actually pretty limited.
for opensource all the edge voiceai devices i know are heavily implemented on arm.



github



juliadsp/mfcc.jl
mel frequency cepstral coefficients calculation for julia - juliadsp/mfcc.jl






is a much more comprehensive mfcc lib code is julia and likely extremely fast but it would be very inteesting if a c guru took the julia code and implemented the fft routines with https://projectne10.github.io/ne10/
juliadsp.mfcc.jl its called sad (speech) not vad (same thing) and it is just sad we don’t have it employed.
linto.ai have a great hmg (hotword model generator) that allows you to pick mfcc settings and profiles and saves that as a json profile for use with thier apps.



github



linto-ai/linto-desktoptools-hmg
gui tool to create, manage and test keyword spotting models using tf 2.0 - linto-ai/linto-desktoptools-hmg






https://www.ee.columbia.edu/~dpwe/resources/matlab/rastamat/ is extremely cutting edge which is what juliadsp tries and does encapsulate.
ps if you are installing mycroft my preffered way is mycroft-core on vanilla raspberry as it makes a much cleaner audio implementation than picroft.

i keep asking about webrtc ec 
due to it being more complex it is more permissive of clock drift between input and out which means as a waveform subtraction tool its much more flexible that speexdsp.
i got the same from the puse dev community and just stopped harassing for a reply but lay a challenge if anyone can provide 2 pulse audio recordings of an echo situation with pa_ec off and on.
i have wondered due to it being a much more complex alg than speex that maybe the pi simply can not do the load in the time for the fft routines to return valid results.
it doesn’t matter as with alsa or pulse audio as @stephen_o_sullivan showed you can get the speex version to run and the most important show stopping dsp routine works at least with one version.
the permissive clock drift synchronisation that pa_ec supposedly can provide could have many benefits especially across different input/output hardware  and rtp.
@jarbasal i know this is a big big ask but your programming skills are extremely high that…



github



juliadsp/mfcc.jl
mel frequency cepstral coefficients calculation for julia - juliadsp/mfcc.jl





is just a very interesting mfcc lib that far outstretched sonopy in scope, some of the feat extraction, diarisation for asr model switching could be used to switch not just language, but regional, gender and age models that provide greater accuracy.
also for a while the huge load hit of vad & mfcc because of the similarity of fft routine on the same input stream i have had a hunch that rather than 2 separate processes one can be just a byproduct of the other and its possible to almost halve the load if the function is combined into a single lib.
you are our only hope obi @jarbasal 
also the linto-desktop-tools for kws is just a great tool for model creation.
if not adoption then maybe emulate its layout and finish some of its rough edges but the ability to evaluate, test your model on completion and test by playing the false positives & negative wavs is extremely important when assessing your dataset.
i made the presumption the google command set was actually validated and good and it couldn’t be further from the truth with up to almost 10% of the dataset being cut and padded totally wrong.
that massively skews model accuracy.
also linto seemed to of copied much of what mycroft have done and even employ sonopy for mfcc as they have dropped there arm only neon version for cross platform support.



medium – 3 oct 18



computing mfccs voice recognition features on arm systems
i worked as a trainee in linagora’s r&d departement on the linto project. do you know linto? if not, you should! linto is a smart…
reading time: 6 min read







they have adopted tensorflow 2 also via keras and they accuracy results are much higher than precise or at least seem to be.
maybe its time to update precise maybe even think about employing pytorch that follows a much more python logic orientated operation than the heavy reflection of c+ of keras.
also its worth thinking about alternatives to a gru as even for cortex_m pretty much paralel results can be expected on cortex_a.



github



arm-software/ml-kws-for-mcu
keyword spotting on arm cortex-m microcontrollers. contribute to arm-software/ml-kws-for-mcu development by creating an account on github.





gru was pretty much cutting edge but seems to be of surpassed by 2 options ds_cnn for greater accuracy or crnn performance as ops are more than halved.
as said even though based on mcu https://github.com/arm-software/ml-kws-for-mcu/blob/master/deployment/quant_guide.md has quite a bit of info on adopting the quantising and batch normalization layers of the more cutting edge models.
also even if only led bling the doa can be used as in.
https://github.com/voice-engine/voice-engine/tree/master/voice_engine as you can see with the various array types such as https://github.com/voice-engine/voice-engine/blob/master/voice_engine/doa_respeaker_2mic_hat.py
python versions of ec speexdsp also https://github.com/voice-engine/voice-engine/blob/master/voice_engine/ec.py
"
267,youtube audio skill testing and feedback,skill feedback,"
mycroft skill to play audio from youtube, using the common play framework.
this is heavily based on the excellent i heart radio and tunein skills by johnbartkiw.
it uses mycroft’s vlcservice for playback, and pafy / youtube-dl to fetch media details.
it’s still a bit of a mess, but the basics work.
how to install youtube audio skill
msm install https://gitlab.com/mcdruid/mycroft-youtube-audio

youtube audio skill connects to … youtube, without any credentials.
how to test youtube audio skill
no configuration needed. simply say something like

“play hendrix (on|with|using) youtube”
“play midnight in harlem”
“play clapton crossroads”
“stop”

where to direct feedback
happy to receive feedback here, or as issues / prs on gitlab.
","
that’s awesome 
great to use existing resources too, we’re all standing on the shoulders of giants!

working great here. love this skill.
i would like to give you some ideas:

pause/resume option
go to minute x (ff/rewind feature)
search for more titles (e.g: play chillout music always returns balearic chill out vibes compilation 2 + balearic summertime 2 if when finish or telling “another like this”, will play the next result

image956×742 181 kb


“play random xxxxx” would return any result not just the first one, so most probably won’t play the same song if you search by style, like above.
use the mark i mouth screen to show information like name song and duration/time remaining
use the markii screen to show information like name song, duration/time remaining and thumbnail/video
playlist support
(with playlist support): next/previous song


i did notice i cannot speak to mark 1 when playing, so i cannot say “hey mycroft, stop”

unfortunately the mark-1 does not support echo cancellation, so it will not work…
…unless you turn the mark-1’s volume low and shout loud enough (preferably directly in front of the faceplate where the microphone is) - then it will work… 

heheh, in that case i would rather prefer to push its “stop” button 
i will test the skill on my desktop computer, but i guess my cheap usb mic won’t support echo cancellation either…
edited: surprisingly, on my desktop works 

tested this skill the last few days, now my system is finally setup to work as supposed (volume control, aec, mute on wakeword, ducking of audio) (rpi3b running my own mycroftos)
what works / my findings

finds almost always what i want to play
sometimes take a bit to find it, but that is most likely because of the search itself, amount of returns.
hey mycroft during playback nicely mutes the playback (although slightly delayed)
stop command when the playback is muted stops the playback (although because of the small delay mentioned above and the rather short period of muted sound, doesn’t feel “snappy”. sometimes i am just to early or to late)
while playing something from youtube, i can give the command to play something else. playback continues and is nicely ducked while tts speaks the “one moment” feedback, then stops the playback and searches for what i justed asked.

feedback / bugs / issues (not necessary this skill, could well be the cps system or both)

it doesn’t always finish the tts feedback (example: “give me a moment to check for that”) giving the play command. however i believe this is a cps bug. depending on how long the cps query takes it can either finish that sentence or not. it appears to be that, as soon as cps figures out this skill can handle the command and forwards the command to this skill, playback of the tts sentence is cut off if it wasn’t finhed yet.
while playing something from youtube and i give another play command during playback, like said above, first it ducks the audio stream while speaking the tts “give me a moment to check for that”, then figures out i want something again from youtube, forwards the request to this same skill, stops playback, searches and playback the new request. all fine, that is what i want; stop the playback and play something else. however, if i play something from youtube and while playing back i say “stream radio station bla bla” using the tunein radio skill from @johnbart cps figures out that i want to play that radio channel, forwards the request to that skill and then it starts playback as well. meaning i have two things playing at the same time as the previous playback from this skill on youtube is not stopped, while the tunein playback starts. not sure if this is caused by this skill, cps or the tunein skill, however doing it the other way around it works. if i stream a radio station and while playing ask to play something from youtube, the radio get’s stopped before the youtube playback starts.

feature requests / nice to haves

while playing somethin from youtube, when i ask to play something else from youtube the previous stream get’s stopped at the point cps forwards the new request to this skill, leaving a (long) period of silence before the next playback starts. perhaps the stopping of the previous stream could be initiated, after the search just before it plays the next found request. this way my playback continues till the new youtube search is ready. (again, not sure if this is a skill or cps feature request)
had some other “nice to haves” but can’t remember them all anymore. will report them as soon as my brain finds them back…

perhaps a bit to much text for a feedback post, but took some time to properly test this skill because it works so nicely to play anything without the need of any credentials and stuff.
used this skill to test all the muting, ducking and aec stuff. and it worked great !! i really like this skill and to be honest; strongly believe this skill should be installed by default at any mycroft instance because, i think any voice assistant should be able to play some music for free out of the box. that is what this skill provided.
keep up the great work.
@forslund sorry, i am tagging you as you can say more on the internals of cps. perhaps, you can pick something out of this as well to improve upon cps.

thanks for the detailed feedback j1nx.
we will have to do some more testing with switching between music skills to see what might be happening there as it would be great to get this in the official marketplace!

great, just to confirm. i think it has to do about this skill, because if i play something via another skill and play another or this skill, two streams will run as this skill doesn’t stop the previous playback where others do.
might be something that slipped through playing stuf via alsa as that system hardly allows access to sound hardware at the same time. with pulseaudio that is more than possible.

i think i figured out why two streams at the same time get’s started. this skill explicitly uses vlc as it’s audio service while the other doesn’t and therefor just uses the default audio service.
because of that, we spawn two different audio playback programs, and that is no problem for pulseaudio. meaning two streams played at the same time.
so i believe this is a cps bug as the stop command given to cps should stop all audio backends instead of only the one being used or the default.
i think…

hmm, this skill imports the vlc service into the skill which means it’s not running through the audioservice and won’t react to the mycroft.audio.service.stop  message and won’t actually report that there’s anything playing. if instead it’d use the audioservice() interface i believe it would be stopped.
that said the cps_start sends a mycroft.stop message to stop anything playing audio in this skill should trigger when starting another playback…
gonna see if i can repeat the issue here

i can start a stream with the tunein skill.
then start a second stream with this skill.
both streams play at the same time.
(or the other way around, forgot a bit)

hmm, for me it always stops the previous stream so far…
could it be a pulseaudio related thing?
i recall when using the module-role-cork plugin to mute it actually pauses the player and then unpauses when the stream is unmuted…

yeah for sure it is pulseaudio. i have the exact same settings as you guys use for the mark 2 and forcing all other stuff that doesn’t obey my wishes to use pulseaudio.
will have acces to my laptop a bit later today and will post my configs.

thanks, i’ll see if i can set it up on my laptop as well… been a while since i used the pulse ducking on this device

ok, just confirmed it.
“hey mycroft, play rise against hero of war from youtube”
“>> just a second”
“>> now playing rise against - hero of war ( official video ) from youtube”
“hey mycroft, stream radionl”
“>> playing streamin station radionl”
after the “hey mycroft” while playing the youtube stream, the listener beep got played, the stream get’s muted, and then continued at a low volume for the tts output of “playing streaming station radionl”. then the volume of youtube get’s restored and the radio station also get’s played. two songs at the same time.
giving the stop command, stops them both.
here are the default configs for my system;
asound.conf
# use pulseaudio by default
pcm.!default {
  type pulse
  fallback ""sysdefault""
  hint {
    show on
    description ""default alsa output (currently pulseaudio sound server)""
  }
}

ctl.!default {
  type pulse
  fallback ""sysdefault""
}

deamon.conf
# this file is part of pulseaudio.
#
# pulseaudio is free software; you can redistribute it and/or modify
# it under the terms of the gnu lesser general public license as published by
# the free software foundation; either version 2 of the license, or
# (at your option) any later version.
#
# pulseaudio is distributed in the hope that it will be useful, but
# without any warranty; without even the implied warranty of
# merchantability or fitness for a particular purpose. see the gnu
# general public license for more details.
#
# you should have received a copy of the gnu lesser general public license
# along with pulseaudio; if not, see &lt;http://www.gnu.org/licenses/&gt;.

## configuration file for the pulseaudio daemon. see pulse-daemon.conf(5) for
## more information. default values are commented out. use either ; or # for
## commenting.

; daemonize = no
; fail = yes
; allow-module-loading = yes
; allow-exit = yes
; use-pid-file = yes
; system-instance = no
; local-server-type = user
; enable-shm = yes
; enable-memfd = yes
; shm-size-bytes = 0 # setting this 0 will use the system-default, usually 64 mib
; lock-memory = no
; cpu-limit = no

; high-priority = yes
; nice-level = -11

; realtime-scheduling = yes
; realtime-priority = 5

; exit-idle-time = 20
; scache-idle-time = 20

; dl-search-path = (depends on architecture)

; load-default-script-file = yes
; default-script-file = /etc/pulse/default.pa

; log-target = auto
; log-level = notice
; log-meta = no
; log-time = no
; log-backtrace = 0

; resample-method = speex-float-1
; enable-remixing = yes
; enable-lfe-remixing = no
; lfe-crossover-freq = 0

; flat-volumes = yes

; rlimit-fsize = -1
; rlimit-data = -1
; rlimit-stack = -1
; rlimit-core = -1
; rlimit-as = -1
; rlimit-rss = -1
; rlimit-nproc = -1
; rlimit-nofile = 256
; rlimit-memlock = -1
; rlimit-locks = -1
; rlimit-sigpending = -1
; rlimit-msgqueue = -1
; rlimit-nice = 31
; rlimit-rtprio = 9
; rlimit-rttime = 200000

; default-sample-format = s16le
; default-sample-rate = 96000
; alternate-sample-rate = 48000
; default-sample-channels = 4
; default-channel-map = front-left,front-right

; default-fragments = 4
; default-fragment-size-msec = 25

; enable-deferred-volume = yes
; deferred-volume-safety-margin-usec = 8000
; deferred-volume-extra-delay-usec = 0

# mycroftos audio settings
resample-method = ffmpeg
default-sample-format = s24le
default-sample-rate = 48000
alternate-sample-rate = 44100
default-sample-channels = 4

system.pa (i am running pulseaudio systemwide)
#!/usr/bin/pulseaudio -nf
#
# this file is part of pulseaudio.
#
# pulseaudio is free software; you can redistribute it and/or modify it
# under the terms of the gnu lesser general public license as published by
# the free software foundation; either version 2 of the license, or
# (at your option) any later version.
#
# pulseaudio is distributed in the hope that it will be useful, but
# without any warranty; without even the implied warranty of
# merchantability or fitness for a particular purpose. see the gnu
# general public license for more details.
#
# you should have received a copy of the gnu lesser general public license
# along with pulseaudio; if not, see &lt;http://www.gnu.org/licenses/&gt;.

# this startup script is used only if pulseaudio is started per-user
# (i.e. not in system mode)

.fail

### automatically restore the volume of streams and devices
load-module module-device-restore
load-module module-stream-restore
load-module module-card-restore

### automatically augment property information from .desktop files
### stored in /usr/share/application
load-module module-augment-properties

### should be after module-*-restore but before module-*-detect
load-module module-switch-on-port-available

### load audio drivers statically
### (it's probably better to not load these drivers manually, but instead
### use module-udev-detect -- see below -- for doing this automatically)
#load-module module-alsa-sink device=""hw:1,0"" channels=8 rate=48000 format=s32le
#load-module module-alsa-source device=""hw:1,0"" channels=8 rate=48000 format=s32le
#load-module module-oss device=""/dev/dsp"" sink_name=output source_name=input
#load-module module-oss-mmap device=""/dev/dsp"" sink_name=output source_name=input
#load-module module-null-sink
#load-module module-pipe-sink

### automatically load driver modules depending on the hardware available
.ifexists module-udev-detect.so
load-module module-udev-detect
#channels=8 rate=48000 format=s32le
.else
### use the static hardware detection module (for systems that lack udev support)
load-module module-detect
.endif

### automatically connect sink and source if jack server is present
.ifexists module-jackdbus-detect.so
.nofail
load-module module-jackdbus-detect channels=2
.fail
.endif

### automatically load driver modules for bluetooth hardware
.ifexists module-bluetooth-policy.so
load-module module-bluetooth-policy
.endif

.ifexists module-bluetooth-discover.so
load-module module-bluetooth-discover
.endif

### load several protocols
.ifexists module-esound-protocol-unix.so
load-module module-esound-protocol-unix
.endif
load-module module-native-protocol-unix auth-anonymous=1

### network access (may be configured with paprefs, so leave this commented
### here if you plan to use paprefs)
#load-module module-esound-protocol-tcp
load-module module-native-protocol-tcp auth-ip-acl=127.0.0.1;192.168.0.0/16;172.16.0.0/12;10.0.0.0/8 auth-anonymous=1
load-module module-zeroconf-publish

### load the rtp receiver module (also configured via paprefs, see above)
#load-module module-rtp-recv

### load the rtp sender module (also configured via paprefs, see above)
#load-module module-null-sink sink_name=rtp format=s16be channels=2 rate=44100 sink_properties=""device.description='rtp multicast sink'""
#load-module module-rtp-send source=rtp.monitor

### load additional modules from gconf settings. this can be configured with the paprefs tool.
### please keep in mind that the modules configured by paprefs might conflict with manually
### loaded modules.
.ifexists module-gconf.so
.nofail
load-module module-gconf
.fail
.endif

### automatically restore the default sink/source when changed by the user
### during runtime
### note: this should be loaded as early as possible so that subsequent modules
### that look up the default sink/source get the right value
load-module module-default-device-restore

### automatically move streams to the default sink if the sink they are
### connected to dies, similar for sources
load-module module-rescue-streams

### make sure we always have a sink around, even if it is a null sink.
load-module module-always-sink

### honour intended role device property
load-module module-intended-roles

### automatically suspend sinks/sources that become idle for too long
load-module module-suspend-on-idle

### if autoexit on idle is enabled we want to make sure we only quit
### when no local session needs us anymore.
.ifexists module-console-kit.so
load-module module-console-kit
.endif
.ifexists module-systemd-login.so
load-module module-systemd-login
.endif

### enable positioned event sounds
load-module module-position-event-sounds

### cork music/video streams when a phone stream is active
load-module module-role-cork

### modules to allow autoloading of filters (such as echo cancellation)
### on demand. module-filter-heuristics tries to determine what filters
### make sense, and module-filter-apply does the heavy-lifting of
### loading modules and rerouting streams.
load-module module-filter-heuristics
load-module module-filter-apply

### make some devices default
#set-default-sink output
#set-default-source input
#set-default-source alsa_input.platform-soc_sound.seeed-source
#set-default-sink alsa_output.platform-soc_sound.seeed-sink

### mycroftos audio settings
unload-module module-suspend-on-idle
unload-module module-role-cork
load-module module-role-ducking

### enable echo/noise-cancellation
load-module module-echo-cancel aec_method=webrtc source_name=echocancel_source sink_name=echocancel_sink
set-default-source echocancel_source
set-default-sink echocancel_sink

/etc/mycroft/mycroft.conf
{
  ""play_wav_cmdline"": ""paplay %1"",
  ""play_mp3_cmdline"": ""mpg123 %1"",
  ""ipc_path"": ""/ramdisk/mycroft/ipc/"",
  ""enclosure"": {
    ""platform"": ""mycroftos"",
    ""platform_build"": 1
  },
  ""listener"": {
    ""mute_during_output"": false
  },
  ""tts"": {
    ""module"": ""mimic2"",
    ""mimic2"": {
      ""lang"": ""en-us"",
      ""url"": ""https://mimic-api.mycroft.ai/synthesize?text="",
      ""preloaded_cache"": ""/opt/mycroft/preloaded_cache/mimic2""
    },
    ""pulse_duck"": true
  },
  ""skills"": {
    ""priority_skills"": [""mycroft-pairing"", ""mycroft-volume""]
  },
  ""log_level"": ""info""
}

/home/mycroft/.mycroft.conf
{
  ""max_allowed_core_version"": 19.8
}

forgot to mention that at this moment i patched the volume skill to add my “mycroftos” enclosure tag to the alsa_platforms as i have not yet started coding that part. but this should not matter because removing the enclosure tag from my config makes it an unknown tag doing the same.

ok that i can repeat.
it’s the “stream x” command in the tune in radio that don’t emit a mycroft.stop message like a normal common play skill would (ex “play jack radio”).

ok great! that’s why i said, not sure who to blame.
then i will move over that feedback to that skill thread.
thanks and sorry for bothering you😊

no worries, glad we could get to the bottom of this 
i do have some todos on the audioservice (especially vlc backend) so it’s good i got a kick in the right direction to get me going.
"
268,cannot register device,none,"
using picroft, version 2018-9-12. everythings seems ok, get mic, speaker working. can hear register code from speaker. however, visit home.mycroft.ai and do registration for device
finished all info input and just get finished result and did not get back registered device in the device list
","
hi alex,
am i understanding correctly that after entering the registration code in your browser, you get the success screen with example phrases to try, but then heading to https://account.mycroft.ai/devices doesn’t show anything?
after you have entered the code, does your picroft stop reading the code and instead deliver the welcome message?
are you looking at the cli screen, the visual output for picroft? just want to make sure the code is accurate too. it’s easy to mis-hear one, or enter them in the wrong order though this shouldn’t then succeed.
if you’re still having the problem, can i check if you’re using the same email address for home.mycroft.ai as here on the forums? i’ll get someone to check on the backend to make sure there’s nothing going wrong at the account level.

thank you for your reply.  i can access to home.mycroft.ai and login successfully. select device and input the pairing code which display from picroft , it will show a page with finished key, press the finished key will show back the device + page. no welcome message comes out. my account is ay2628@gmail.com. i am a member
the picroft continue to show and speak the pairing code. web browser are also running at the same picroft machine using chromium.
there is response when i speak hey mycroft.
cli screen can show out the pairing code.
try with several accounts and the problem is the same
br

on your picroft is there a file at:
~/.mycroft/identity/identity2.json
if so try removing that file, and remove the device from your account at home.mycroft.ai, then tell mycroft to “pair my device” and it should restart the pairing process.

there is no identity2.json files in /home/pi/.mycroft/identity
and in /root the directory , .mycroft did not exist. i have installed the package twice and also have this problem. i am using raspberry pi 3b+. thanks

hello,
i have come across exactly the same issue. mycroft is asking me to register the device but despite getting success page, nothing is changing.
i also don’t have the identity file.
i verified that i can ping home.mycroft.ai webpage.
i believe i accepted all relevant data agreements.

i am going to bump this thread again, by saying that for me the problem was duplicate entries in locations lists. this has been described here for another town. i raised a new issue for my own town.
"
269,mycroft mark 1 pi4,none,"
is their anyway that i can use a pi 4 with my mycroft mark 1?
","
probably problems with drivers - mark-i is still on debian jessie and as far as i know rpi4 requires debian buster.
rpi4 can get very hot which might lead to other problems.

do you know of a way that i can use debian buster with mycroft mark 1?

hi autoboy,
as dominik mentioned the pi4 requires a debian buster based image. there are significant differences between that and jessie which would require a whole lot of work. we haven’t even mapped out what would be required as we won’t be building any more mark 1’s and the number of people upgrading it to a pi4 would be very small.
if you wanted to try get it working, all the code is open source and on our github, but it’s not something we’ll be working on internally, we need to stay focused on the mark ii.
is there a particular reason for wanting the mark 1? for the aesthetics of the eyes and mouth i assume?

i want to be able to use a pi 4 as it is faster than a pi3 at least when i tested picroft.  can you tell me what modules mycroft mark 1 uses?




github



mycroftai/hardware-mycroft-mark-1
open-sourcing our mechanical, electrical and industrial designs - mycroftai/hardware-mycroft-mark-1





"
270,testing and feedback futurism cartoons,skill feedback,"
futurism cartoons

602×580

about
comics from futurism cartoons
can be used as idle screen for mark2

759×776


752×789

examples

“show me the latest futurism comic”,
“random futurism cartoon”,
“how many futurism comics”

github



github



jarbasskills/skill-futurism-cartoons
futurism cartoons for mycroft. contribute to jarbasskills/skill-futurism-cartoons development by creating an account on github.





category
entertainment
tags

comics
idle screen
mark2
bigscreen

","
love these!
just fyi - i had to install ffmpeg for the instagram scraper to work.
"
271,control vehicle functions with mycroft,general discussion,"
i started to think about an interface for car functions. if anyone is interested i will continue the work. here is my first work https://github.com/gras64/car-control-example-skill
","
now if there was only a docker container for the offline back-end i would put an rpi4 in my car and build an obdii skill.

that would be cool. but first i thought of a cell phone hotspot or lte usb. obd2 doesn’t seem to have all the functions or do you know a good tool. maybe you could connect mycroft to carplay or obd2 bluetooth on your mobile phone. i’m still looking for ideas. i will surely need help and ideas. i still have some time because my 20 year old car can’t do anything but engine temperature .
"
272,testing and feedback knight rider skin,skill feedback,"
kitt skin skill
a skin for the mark2, replaces mark2-skill default animations
about
turn your mark2 into kitt from knight rider

1082×793


listening animation
idle animation
speaking animation
thinking animation

github



github



jarbasskills/skill-kitt-skin
turn your mark2 into kitt from knight rider. contribute to jarbasskills/skill-kitt-skin development by creating an account on github.





category
entertainment
tags

entertainment
screensaver
skin

known issues

mark2 skill is needed for idle screen, but incompatible with this skill
todo move idle screen logic to mycroft-core
todo blacklist mark2-skill if this skill is present (?)

","
good job i love the idea.
"
273,respeaker 4 mic array hat mycroft a i skill,skill feedback,"

14645-respeaker_4-mic_array_for_raspberry_pi-05.jpg600×600
 respeaker 4-mic array hat mycroft a.i. skill.
this skill is to enable and control the seeed - respeaker 4-mic array for raspberry pi. respeaker 4-mic array for raspberry pi is a quad-microphone expansion board for raspberry pi designed for ai and voice applications and provides a super cool led ring, which contains 12 apa102 programable leds.
disclaimer: at the moment this is not tested by me (as of yet), but as multiple people asked for a respeaker 4-mic array hat skill in the past, already created this topic to open the dialogue and possible feedback / testing.
credits: all credits go to @dominik as he created the almost exact same skill for the respeaker core v2. i only changed some small code to flip the library used (pixel-ring) to use the 4-mic array instead.
how to install respeaker-4mic-hat-skill
install respeaker-4mic-hat-skill by …

ssh into your mycroft, then


mycroft-msm install https://github.com/j1nx/respeaker-4mic-hat-skill.git


in case the requirements are not installed automatically you must perform the following;

mycroft-pip install pixel-ring gpiozero

turn on spi on the raspberry pi
to make sure the rpi can control the leds, you need to enable spi;
for raspbian based systems (picroft)

sudo raspi-config
go to “interfacing options”
go to “spi”
enable spi
exit raspi-config
? reboot ?

manually

edit the file config.txt on the boot partition of your raspberry pi
make sure the following content is there;


dtparam=spi=on

ideas & additions for later
finish / start enabling and disabling the pixel ring

“hey mycroft”, “enable pixel ring”
“hey mycroft”, “disable pixel ring”

change led mode

“hey mycroft”, “set pixel ring to alexa”
“hey mycroft”, “set pixel ring to google”

add mycroft led mode

“hey mycroft”, “set pixel ring to mycroft”

doa (direction of arrival) of keyword
where feedback should be directed a
feedback on the skill should be provided through:

replying to this thread
issue on github 

mycroft chat  in channel ~skills

","
warning !!! just installed on raspbian and 18.8.12 but it hangs the rpi !!!
do not install (yet)

now only need to figure out how to debug this?
a) i don’t know python yet
b) loading the skill brings the rpi on it’s knees
running out of time for now. must be somewhere in spidev <-> gpiozero as only hardware thingies can bring computers on its knees i think.

i would try to debug it like following:
log.debug(""initialising"")
power = led(5)
log.debug(""power set to led(5)"")
power.on()
log.debug(""power on"")

maybe user mycroft is missing permissions:
sudo usermod -g gpio mycroft

yeah, will start playing around with the ezample code first;


github.com


respeaker/pixel_ring/blob/master/examples/respeaker_4mic_array.py
""""""
control pixel ring on respeaker 4 mic array

pip install pixel_ring gpiozero
""""""

import time

from pixel_ring import pixel_ring
from gpiozero import led

power = led(5)
power.on()

pixel_ring.set_brightness(10)

if __name__ == '__main__':
    while true:

        try:


  this file has been truncated. show original






highly doubt it is the mycroft part, or your code in that regard.
when i first run the skill on mycroftos i was missing lsusb libs, which brought up errors in the section that checks which hat/hardware is present via pyusb. now that error is gone and the pi was bought on it’s knees.
because of that searched that old raspbian based sdcard, updated everything and tested there as well. same thing, so suspect it hangs on either that checking hardware thing with lsusb/pyusb or one step further in actually controllinig the spi via spidev/gpiozero.
oh well, a good first project to start learning some python. did my fair share of coding with other languages in the past, so can easily read the code. will see how far i can get…

i am not able to bring it to run on my rpi-3b+ with picroft stable.
but i am unsure if it is because of my setup or your skill.
can i send you any informations/logs?
edit:
with the commands here i think i bring it to run. i am running that skill on a rpi3b+ with picroft stable.
but… the lights only show up once on the startup of mycroft.
here is a video of that: watch me

win_20190407_22_06_12_pro1301×870 153 kb


hi suisat,
are there errors showing up in your voice or audio logs? this is located at  /var/log/mycroft/voice.log.
if the cli is accepting commands you can tell it to “create a support ticket” and it will upload all your logs to termbin and email you a link. this can be much easier than fetching them manually.
in your other post you said “mycroft is hearing me but only when i do some inputs” do you mean, only after typing things into the cli?

nothin in the skill-logs about that.
after isntall the j1nx skill for respeaker 4mic hat i had some issues about some pip-requirements but after install that, there gone.
but yesterday if foudn out how to run the led’s.

install the git skill from j1nx (msm install git-url or within home.mycroft.ai)
start mycroft
perhaps you need to say once “enable pixel ring” or “disable pixel ring”
restart mycroft and while booting up, check the pixel-ring on your respeaker. it should pop up for a second.
press ctrl+alt+f2 and open another raspi-terminal
type in python and then:

from pixel_ring import pixel_ring
from gpiozero import led
power = led(5)
power.on()
pixel_ring.set_brightness(10)
pixel_ring.wakeup()


as long as your python shell is running, it should work like a charm

anything does not work in j1nx skill. perhaps it has something todo with “self.en.write(1)” in the “def shutdown” function but i am npt sure. i try to replace the line with:
def shutdown(self):
     log.debug(""shutdown"")
     power = led(5)
     power.off()

but without success.
@j1nx or @dominik did you have any idea why it does not run on my mycroft-stable instance on my raspberry-pi-3b+ . it seems that mycroft has any problems with running or keeping the gpio power.
i would be very happy about any hints.
@gez-mycroft
how can i enable full-detailed-logging of the skill? i read this https://mycroft.ai/documentation/logs/#errors-in-a-skill and yesterday i did add thisline to the init.py (https://github.com/j1nx/respeaker-4mic-hat-skill/blob/master/init.py):
logger = getlogger(__name__)

but that just enable the output like “initialising” or “speak”.
here is a new video how beatiful it could be:

https://mega.nz/embed#!60xbzyra!3hmbeokgac9jlziixmpjoczgcxyotz1m78pzuzhmrc8


hey, looking at the code j1nx has imported the logging utility as “log” so if you see on line 28 there is:
log.debug(""initialising"")
this is what’s causing the output. if you want to log something else, you need to add in some extra log.debug()
slightly unrelated but fun side fact: the logger is now included in the base mycroftskill class so we no longer need to import it at all. you can call this within the skill using self.log.debug()

i will add this to the init.py and write here something if i can see more in the skill.log.
thx

@suisat and @gez-mycroft first of all, my apologies for my late response / abscence. momentarily it is rather busy with my daytime job, so haven’t much played with mycroft, nor being active online.
first things first;
this plugin was quickly, let say “blind” coded without any python knowledge what so ever. i just saw @dominik his skill for the respeaker v2 box and looking at his code i discovered that the python libraries it used, already had everything in place for the “other” mic arrays. that code also was properly commented, so it looked like a quick revamp to get it working on the 4-mic array. apparently is isn’t 
i still don’t have much / any time to investigate or fix / adjust, but here are my thoughts on the above testing and information.

with the commands here  i think i bring it to run. i am running that skill on a rpi3b+ with picroft stable.

yes, this is what i already expected some what. these commands can be added to a “requirements.sh” next to the “requirements.txt” file. similar as this one from @dominik ;


github.com


domcross/respeaker-io-skill/blob/master/requirements.sh
#!/bin/bash

# mraa libs are now added in the local skill folder
#sudo apt-get -y install python3-mraa=1.9.0 libmraa1=1.9.0
sudo apt-get -y install python3-mraa libmraa1
sudo usermod -a -g input respeaker
exit 0






however the first command; “mycroft-pip install rpi.gpio” should be removed and added to the “requirements.txt” file instead;

pixel-ring
gpiozero
rpi.gpio

i don’t know if both; “gpiozero” and “rpi.gpio” are needed, or that i made a mistake and “gpiozero” needs to be changed to “rpi.gpio”
then on to the strangest thing of all, you needed to type this into another terminal / python cli;

from pixel_ring import pixel_ring
from gpiozero import led
power = led(5)
power.on()
pixel_ring.set_brightness(10)
pixel_ring.wakeup()

all this code is within the skill init.py file as shown here;
https://github.com/j1nx/respeaker-4mic-hat-skill/blob/master/init.py
so it looks like the code within the skill is not properly loaded or initialised. i have no clue why as i don’t know python yet.
what i can think of is that, perhaps the skill initialisation fails because i deleted the "" settingsmeta.json"" ?!?! the content of it looked like only placeholders, so deleted it. it does not do much yet, but considering that you apparantly also can enable/disable the pixelring by commands, hence toggling it is needed to get stuff working, perhaps this file need to be there as well ?
perhaps @forslund can have a look, if that could be the cause of it. or perhaps you could test it by creating the file with the following content, similar as @dominik ;


github.com


domcross/respeaker-io-skill/blob/master/settingsmeta.json
{
    ""name"": ""respeaker pixel ring"",
    ""skillmetadata"": {
        ""sections"": [
            {
                ""name"": ""options << name of section"",
                ""fields"": [
                    {
                        ""name"": ""internal_python_variable_name"",
                        ""type"": ""text"",
                        ""label"": ""setting friendly display name"",
                        ""value"": """",
                        ""placeholder"": ""demo prompt in the input box""
                    }
                ]
            },
            {
                ""name"": ""login << name of another section"",
                ""fields"": [
                    {


  this file has been truncated. show original





so far, my ideas and thoughts. hope to start cracking with mycroft again any time soon now…

perhaps, this;


github.com/mycroftai/mycroft-core






restore bus on_message


  by jarbasal
  on 10:31pm - 16 apr 19 utc


1 commits
  changed 1 files
  with 1 additions
  and 0 deletions.






is/ was also related with this;
https://github.com/j1nx/respeaker-4mic-hat-skill/blob/master/init.py#l19

hmm, that shouldn’t make a difference the skill doesn’t seem to wait for the “message” message that the pr re-enabled…
the missing settingsmeta.json shouldn’t™ be an issue either.
from a quick glance i’m afraid i don’t see anything obviously wrong 




 j1nx:

i don’t know if both; “gpiozero” and “rpi.gpio” are needed, or that i made a mistake and “gpiozero” needs to be changed to “rpi.gpio”


no, rpi.gpio and gpiozero are two different libraries for the same thing (accessing gpio), where the gpiozero is the more conveniant one to program with. you are using gpiozero, so you can safely remove the rpi.gpio requirement.
as already stated before: maybe user ‘mycroft’ is lacking permissions to access gpio?
try sudo usermod -g gpio mycroft

there is no user mycroft in the pycroft stable release.

the newer picroft images doesn’t use the separate mycroft user. try doing the suggested change for the pi user: sudo usermod -g gpio pi

yeah, i did that already. without success. 
perhaps it has smething todo with the the order of the skills that get loaded when mycroft starts.
is there a chance to force mycroft to load the skill at the end?

@j1nx sorry for my afk’nes 
did you think we can together bring this skill to a stable rls?

@suisat no worries, i was (still am a bit) busy aswell.
i am about to reflash my sd with the latest picroft image to start tackling this plugin again. so yes, let’s work together to figure out how to fix this. assume it is just a small thing we are overlooking and we should be able to get it fixed.
hope to get start cracking after the weekend.

any more updates on this?   i’d love to see this working so i can replace the microphone in mine.  i really love the microphones in the newer alexa echo (yes we play with them all) and would love my mycroft to not feel left out.
"
274,request for feedback iching,skill feedback,"


install skill name by …

git clone` instructions:  https://github.com/renayo/iching-skill.git

no dependencies
no oauth authentication is required



speak intent phrase  iching or eching (should sound like eatching)


mycroft should respond with a randomized answer from the book of i ching.


how feedback on the skill should be provided: through issues on github or via mycroft chat.
it has been successfully tested at home. all thoughts and ideas are welcome.
this was just a practice run for a more complex turn-based game of sorts.
edit: weird. test failed to commit. no idea why. can someone help explain it? thanks!
edit: upload has succeeded. evidently, it was something not within the skill that was at fault.
",
275,npr news always plays bbc news,none,"
hi,
despite have npr set in my settings in home. mycroft.ai. i still only get bbc.  what do i need to change?
thanks
guy
","
hi i think there’s an issue with overmatching in the news that @gez-mycroft is working on (see this thread). try “what is the news” and see if that plays your preferred station.

thanks, you solved my issue!

thank you. my mark i has been doing this for weeks, i just got around to checking in, and your answer works!

there is a proper fix pending for the issue so hopefully it will be back in order soon

now i have a new news aggravation. i ask mycroft to play the news, i get “here is the latest news from npr news now” and nothing plays. the same result if i ask “what is the news?”. i rebooted the unit, no change in behavior. any ideas?
thanks.

hey there, the npr news rss feed is having an issue on their side.
i’ve posted a work around, just waiting for a review so we can push it up to the marketplace and out to devices. should be there in ~24 hours.

@mwgardiner this should now be resolved 

it works now. thanks.
"
276,training new expressions,mycroft project,"
like an eliza brain  - isnt it possible to train a mycroft with a local version of google duplex
","
if you were to either generate an api to duplex or develop an open source version of duplex, then you could.
"
277,testing and feedback standbymonitor,skill feedback,"
how to install standbymonitor

install skill name by msm install https://github.com/gras64/standbymonitor-skill/

for cec you need libcec3 and cec-utils



configured in skill settings.

the skill supports the following methods to set:xset dpms force,7 zoll pi display,dlp2000,tvservice,vcgencmd,cec. you can set the time delay and deaktivate automode

how to test standbymonitor
specify the steps the user should take to test the skill, such as;

configure the skill settings in home.mycroft.ai
“monitor activate”
“monitor deaktivate”
“monitor automatic”

the skill turns on your monitor which turns off if there is no voice input for some time.
unfortunately i haven’t been able to test all of the methods so far because i don’t have the hardware. the skill is written very quickly and depending on the feedback i will perfect the skill.
where feedback on standbymonitor should be directed a
be clear about how feedback on the skill should be provided, such as through issues on github, via email or via mycroft chat.
","
hello i could someone test my mark 1 option in my skill. select mark 1 under home.mycroft in the skill properties. if i did everything correctly, the mark 1 should be very dark and wake up at wakeword. unfortunately i don’t have a mark 1 to test it myself

hey andreas,
you can detect known platforms for mycroft through the device configuration. you need to import this specially:
from mycroft.configuration import configuration

then in your skill do something like:
    def initialize(self):
        config = configuration.get()
        platform = config.get('enclosure').get('platform')
        self.log.info(platform) # will output ""mycroft_mark_1""

unfortunately the mycroft.eyes.* messages don’t seem to be registering, even when called outside of your skill (at least on the mark 1 i’m testing on).

i already thought of that too. i actually wanted to be able to make individual settings. i have now set up an automatic mode by default.
as far as mycroft.eyes is that my mistake or a core problem?

just had another look, and it seems that the eye message namespace in the enclosure differs from the mark 1 skill.
so mycroft.eyes.default is right but you need to change the other messages to those listed here:
'enclosure.eyes.on'
'enclosure.eyes.off'
'enclosure.eyes.blink'
'enclosure.eyes.narrow'
'enclosure.eyes.look'
'enclosure.eyes.color'
'enclosure.eyes.level'
'enclosure.eyes.volume'
'enclosure.eyes.spin'
'enclosure.eyes.timedspin'
'enclosure.eyes.reset'
'enclosure.eyes.setpixel'
'enclosure.eyes.fill'

should add these to the docs…
edit: for future readers, these will be getting extracted from mycroft-core when we get around to it. so if the link is broken they will be in a dedicated mark 1 enclosure repo.

in the skills there’s also the self.enclosure which is an interface to the mark-1 enclosure api.
self.enclosure.eyes_off()
self.enclosure.mouth_reset()
self.enclosure.eyes_reset()

thank you so i took over please test
"
278,pulseaudio network delays,mycroft project,"
hy
my setup is looking like this:
mycroft - runs on a linux computer (lattepanda alpha) and is connected to the lan and has a a usb microphone
homesound system - a soundbar is connected over hdmi to a rpi where pulseaudio is running as a server.
my htpc is directly connected to a beamer at the ceiling connected via lan and playing th audio connected over the lan to the rpi connected to the soundbar. (works perfect)
now all the sounds from mycroft are also played over the rpi connected to the soundbar (over the load-module module-native-protocol-tcp). however i experience a large delay of around 2 seconds.
so when i say hy mycroft it takes almost 3 seconds to hear the beep from the soundbar that mycroft is listening.
how can i reduce this audio delay? has someone a similar setup and can report how well its working?
andy
","
did you disable / unload the suspend on idle module?

you think of this part in the /etc/pulse/default.pa file:   if so yes i did.

automatically suspend sinks/sources that become idle for too long
#load-module module-suspend-on-idle


could you post the content of the default.pa on the client and the server? just wondering how you have it all setup.
sometimes use the tunnel-sink is easier / better. or perhaps send white noise continously might help as well.
willing to have a look, think about it with you.

thx
on the “server” side so the rpi that is directly connect to the sound bar the default.pa has this additional line:
load-module module-native-protocol-tcp auth-ip-acl=127.0.0.1;192.168.1.0/24;172.16.10.0/24 auth-anonymous=1
and each client that likes to play some audio on the server has this line in the client.pa:
default-server = 192.168.1.192
i thought about the white noise thing this might help a lot keeping the audio channel open and ready for new things
andy

perhaps on the client side you could use the “module-tunnel-sink” instead;
load-module module-tunnel-sink server=192.168.1.192 sink_name=remote

and set the “remote” sink as the default sink of the pulseaudio client setup. thenit might keep it alive, which might reduce the latency.
worth a shot.

ok according to

pacmd list-sinks

with the tunnel i have a latency of around 250ms.
however when i ask a question in the cli, for example “weather” i can see the answer in yellow but it further takes 3 seconds till i hear the answer through the speakers…
is this “normal”

i have noticed similar but its not constant.
i thought it was mimic that was slow pushing a wav to paplay rather than paplay.
i never did sort it because because the delay would come and go.

its most likely paplay that does delay the whole process. you can watch the audio.log file and there you can see that receiving the audio file from the server (tts) is rather quick but after that till it actually plays takes some time since paplay needs some time to start up and send the data through pulseaudio to your speakers
"
279,speed of reacting to the wake word,general discussion,"
hy
i use mycroft on a pi3.
when i call the wake word and i can see that the wake word is recognized in the cli it takes 1.3 seconds till i hear the sound over my 3.5mm headphones to tell me that mycroft is listening. in some video i can see that it can react faster.
can some one tell me if the pi3 is the bottleneck or what can i improve?
andy
","
if you run the command;
pactl list sinks

what is the latency saying of your audio device?

0.78seconds
state: idle
        name: alsa_output.platform-soc_audio.analog-mono
        description: built-in audio analog mono
        driver: module-alsa-card.c
        sample specification: s16le 1ch 48000hz
        channel map: mono
        owner module: 7
        mute: no
        volume: mono: 51773 /  79% / -6.14 db
                balance 0.00
        base volume: 56210 /  86% / -4.00 db
        monitor source: alsa_output.platform-soc_audio.analog-mono.monitor
        latency: 782805 usec, configured 1365333 usec
        flags: hardware hw_mute_ctrl hw_volume_ctrl decibel_volume latency
        properties:
                alsa.resolution_bits = ""16""
                device.api = ""alsa""
                device.class = ""sound""
                alsa.class = ""generic""
                alsa.subclass = ""generic-mix""
                alsa.name = ""bcm2835 alsa""
                alsa.id = ""bcm2835 alsa""
                alsa.subdevice = ""0""
                alsa.subdevice_name = ""subdevice #0""
                alsa.device = ""0""
                alsa.card = ""0""
                alsa.card_name = ""bcm2835 alsa""
                alsa.long_card_name = ""bcm2835 alsa""
                alsa.driver_name = ""snd_bcm2835""
                device.bus_path = ""platform-soc:audio""
                sysfs.path = ""/devices/platform/soc/soc:audio/sound/card0""
                device.form_factor = ""internal""
                device.string = ""hw:0""
                device.buffering.buffer_size = ""131072""
                device.buffering.fragment_size = ""131072""
                device.access_mode = ""mmap+timer""
                device.profile.name = ""analog-mono""
                device.profile.description = ""analog mono""
                device.description = ""built-in audio analog mono""
                alsa.mixer_name = ""broadcom mixer""
                module-udev-detect.discovered = ""1""
                device.icon_name = ""audio-card""
        ports:
                analog-output: analog output (priority: 9900)
        active port: analog-output
        formats:
                pcm

removing the timebased schedular might bring it down.
edit your /etc/pulse/default.pa  and find;
load-module module-udev-detect

replace it with this;
load-module module-udev-detect tsched=0

might need a reboot for easy to get these setting applied. that might bring the latency down.
there is more ttweaking you can do, but is a bit of try and error;


the blog of juho – 5 feb 17



pulseaudio and latency
pulseaudio and latency if you are a linux enthusiast and are playing fps games or record audio of some musical instrument then you have probably encountered issues with pulseaudio latency. this blog post describes how to live with and manage...







wohoo nice => 0.098s
latency: 98142 usec, configured 99954 usec
now it is much snappier. still the answers take some time but it feels much better now

the answers need to come from external server, so nothing you can do there.

yeah exactly maybe in the future mycroft can already start talking while the info from the internet is delivered so there is not that “huge” waiting time

for the mark-2 they make use of the streaming tts of google. you could do that yourself as well, but requires a api key.
haven’t looked into that yet myself, perhaps @forslund or @gez-mycroft can help you out with that.


github.com


mycroftai/enclosure-mark2/blob/master/etc/mycroft/mycroft.conf#l6

{
  ""enclosure"": {
    ""platform"": ""mycroft_mark_2pi"",
    ""platform_build"": 1
  },
  ""stt"": {
    ""module"": ""google_cloud_streaming"",
    ""google_cloud_streaming"": {
      ""credential"": {
        ""json"":
          # google service key 
      }
    }
  },
  ""tts"": {
    ""module"": ""mimic2"",







thx i’m currently installing

mycroft-pip install google-cloud-speech

but this takes ages (stillt installing)… have already a working api key since i use ti with my openhab system.
by the way is it possible to send the recognized text by mycroft to a url? i’m think of this scenario: mycroft passes the recognized text to the rest url of openhab this then decides via a “rules” whether it should play the text via tts (of the oopenhab) to the pulseaudio speakers or not.
(i know there is a openhab skill but i like to have the text recognized by mycroft as text in a opnehab rule)

euhm, i see i (once again) mixed up the stt and tts stuff…
please double check and see if it is still what you want 

this is a great tip on the timebased schedular. will add it to the docs.
yeah the mark ii prototypes are using the google cloud streaming stt. there is also a built in support for deepspeech streaming stt and the new ibm watson service provides streaming stt but mycroft core hasn’t yet been updated to support it.

yes, as the rpi does not have a hardware clock, timebased scheduling is really not optimal.

i know nothing about it, is this something we should just drop to 0 by default?

depends a bit, could be now we use pulseaudio by default.
but perhaps need some testing from different users. i use it for a while now and have not run into problems, but perhaps with high bitrates playback of music this might not be beneficial.

can you help me with how to change it to that value it equals 0 on my rasspberry pi

in /etc/pulse/default.pa add tsched=0 to the line load-module module-udev-detect so it looks like
load-module module-udev-detect tsched=0


when i did this , it replied faster put the first word of the respond seems to be cropped, somebody wrote that 0.098s is giving greate results but i don’t know how to change it’s value from 0 to 0.098

have you set some parameter is the nano /etc/pulse/daemon.conf file?

for every answer mycroft generates a new mp3 that is then played to the pulseaudio server.
now for example mplayer needs around 2 seconds to open the tunnel to the pulseaudio sever and then play the mp3. how can we improve this?
one way would be to have a stream open the whole time mycroft is running with an unhearable audio sound. and once there is a new answer to play through the speakers the connection is already up and running and the new “mp3” can be injected into the the same stream…
"
280,contributing to home assistant,mycroft project,"
hi all,
i’d like to contribute to the home assistant skill, specifically adding the ability to say things like “set the office lights to 50%” for dimmers.
however, i notice in the repo that the latest commits are pretty old, and there are a lot of open prs. then there also seems to be a common iot branch with some other features but not a lot of details.
if i plan to develop, i don’t want to go down a road and then have the direction change just because i wasn’t aware of it happening already 
so, that being said, what branch should i base my work off of? and what branch is the official marketplace skill running off of?
thanks!
","
hi there,
thanks for starting this thread! for some reason i haven’t been getting notifications on that repo so haven’t seen these open issues and pr’s until now.
the common iot work was done late last year but has some small snippets that require python 3.6+. we are still supporting devices running 3.5 so it couldn’t be merged without modification. this common iot branch of this skill was created as an example skill for the iot framework, however as that framework doesn’t yet exist in core, it can’t be used.
the current marketplace skill is based off the 20.02 branch. though there are a couple of commits not yet pushed to the marketplace there too.
@stratus has more recently started looking at this skill and the common iot framework to see if we can polish off the final pieces and get it working. so he might want to weigh in here on whether it’s worth holding out for the common iot piece, or to continue building on the 20.02 branch.

ahh, thank you for the insight. that all makes total sense. i will wait to hear from @stratus but begin getting familiar with the code base.

of course, as i start to go through the code base, i see this:


github.com


mycroftai/mycroft-homeassistant/blob/20.02/regex/en-us/dimming.rx
(set) (?:brightness of )?(?p<entity>.*) (to) (?p<brightnessvalue>\d*)(?: percent)?
(dim|brighten|increase|decrease) (?:brightness of )?(?p<entity>.*?) (?:by)? (?p<brightnessvalue>\d*)?(?: percent)?





it looks like the capability is there, it’s just not working for me. 

so as i go through this further, could someone explain why some vocabs are handled via padatious? for instance:
set.light.brightness.intent:
set brightness of {{entity}} to {{brightnessvalue}} percent
while others work via:
lightdimverb.voc:
dim|dimmer|dark|darken|lower|down
+
dimming.rx:
(set) (?:brightness of )?(?p<entity>.*) (to) (?p<brightnessvalue>\d*)(?: percent)?
(dim|brighten|increase|decrease) (?:brightness of )?(?p<entity>.*?) (?:by)? (?p<brightnessvalue>\d*)?(?: percent)?

i don’t yet understand why one would be preferred over another (or why both exist in concert at all). thanks!

went ahead and started with a bugfix pr before i dive in further. let me know if anything needs to be adjusted in how i’m contributing. thanks!


github.com/mycroftai/mycroft-homeassistant








allow for fuzzy top/bottom brightness and adjust speech percentage


mycroftai:20.02 ← fmstrat:brightness_fix



        opened 02:02pm - 30 jul 20 utc




          fmstrat
        



+7
-9











thanks fmstrat!
i’ve let stratus know about this thread in case he’s not getting notifications, but he might be snowed under with work so if we haven’t heard i’ll try take a look next week.

i’ll take a look at this today.
to answer the question about the intent parsers, without examining the code closely, you would use both in concert when you needed to be context aware such as with the adapt parser while on the other hand not needing the complexity of regex in other situations. regex in mycroft are known to be… particularly sensitive and so are not always the best choice.
shameless plug i just recently wrote about this in a series on opensource.com. the intent parser article is here https://opensource.com/article/20/6/mycroft-intent-parsers

i left a comment on the commit. i don’t think its worthy of a request for change but lets discuss here what you think about my suggestion

it’s taken me a bit of time to get back to the pr from christopher due to work things evolving underneath me. i hope to start tackling that today.




 stratus:

i left a comment on the commit. i don’t think its worthy of a request for change but lets discuss here what you think about my suggestion


appologies, i totally missed the comment here since my emails from this forum are still dropped due to the spam listing on the sending mail server’s ip. in any event, as you probably saw in the pr, i thought the suggestion you had was spot on and made the alteration in the commit. thanks!

hello fmstrat, check also my pr #38. with others, we made lot of changes. started with rewrite all intens to padatious and applied fixes to 4 issues from git. your change is also already included. more people test it, more changes to get it approved. thanks
"
281,does mycroft have any graphical user interface,general discussion,"
i am new to mycroft and i was able to install mycroft on arch linux, but i really hate the command line interface shipped, it is 2020 and i would have expected mycroft to at least support some decently advanced graphical interface like amazon alexa or cortana and be integrated completely with my desktop but it seems i am clearly missing something as i can only start voice from installing mycroft-core git hub repository and from the terminal, i really want mycroft to be able to display videos, desktop notifications or show me browsable web results like cortana for windows, does mycroft have any graphical user interface i can install for arch/manjaro ?
","
yes.



github



mycroftai/mycroft-gui
the graphical user interface used by the mycroft mark ii and more - mycroftai/mycroft-gui






there’s another version and maybe a third party one out there i don’t know off the top of my head.

hi jaz,
the gui baconator linked to is the way to go, but it’s not something we have available for general desktop usage yet. as mentioned on that repo readme, the best way to test it out is to run kde neon.

i have it working from the link baconator pointed at and it seems to be working nicely thank you.
i am a newbie getting started with programming in general but i have some interesting ideas are there any video tutorials or templates available to get started on writing skills with user interface for mycroft ?

there is a page in the docs but not yet tutorials or template skills.
however i’d also suggest checking out some example gui skills from some of our core community members:

https://github.com/aiix
https://github.com/jarbasskills

"
282,self hosting and architecture questions,general discussion,"
i’m very new to the mycroft project and haven’t actually run a functional install at this point - still considering how i want to set things up.
one issue is that i’m trying to figure out the overall architecture of the mycroft project. at first, i thought that mycroft core was the “brains” of the system. given the name, this made sense. however, it’s just the “listener” system that runs on your local hardware, right? you pair it with your home.mycraft.ai account, and the mycroft systems do the actual voice recognition? it’s more mycroft client than mycroft core as far as what it does.
selene and selene ui are the actual brains, correct? this is that code that is behind the systems at home.mycroft.ai, from what i understand. is that correct?
i’m really interested in running the mycroft android app because i refuse to rely on the virus that is google. i see that there is mycroft android and mycroft core android. from what i understand, the mycroft android app is just a very basic “listener” that relays voice to a mycroft core system, mycroft core sends that to home.mycroft.ai for recognition, and the results are sent back through to the device. mycroft core android is an alpha-stage project for running mycroft core directly on android instead of syncing with another system on your network. is this correct?
assuming i understand all of this correctly, here’s what i’d really like to do:

host my own mycroft core system in the cloud using something like google cloud run (while i hate google, this is a great service - serverless docker)
build the mycroft android app and point it to my mycroft core instance on the cloud
(optional) host my own selene backend

however, i think that the android app requires a core system on the local network. also, the configuration uses an ip address, not a hostname. that puts a damper on using a cloud-hosted core system.
do i understand everything correctly? is there a way to do what i’m trying to accomplish?
","
hey summersab, welcome to mycroft!
it’s a good question and i’ve been working on some documentation to better show the overview of all mycroft technologies.
briefly, mycroft-core sits in the middle and coordinates all the other components on your device. eg precise our main wake word listener is it’s own repository but gets run from mycroft core.
selene (home.mycroft.ai) at a high-level provides a gui for settings and access to anonymized cloud services. eg by default we use google stt proxied through our servers for greater anonymity (explanation why here), and tts is our own mimic2 service.
the actual skills and process all happens on device. so:

detecting wake word (on device)
recording utterances aka voice commands (on device)
transcribing those to text (cloud)
determine intent of the utterance (on device)
skills process and respond to the utterance (on device)
response text gets synthesized into audio (cloud by default, on device backup or if you choose british male)

there are a few community android projects around but predominantly you are correct, they do the stt on the android device, then send the text to mycroft-core for processing, and send the synthesized audio back to the android device.
the core doesn’t have to be on a local network but you the messagebus does not have security built in by default. on mycroft devices we handle this at the operating system layer. so putting an open instance of mycroft in the cloud that anyone can connect to is a security disaster waiting to happen.
it’s certainly possible to run mycroft in the cloud (and is done), but is beyond the support we can provide without a paid service level agreement.

thanks for the explanation! i look forward to your documentation - a nice diagram made using something like https://www.diagrams.net/ might be helpful. pictures span all languages, after all.
cheers!
"
283,dev sync 2020 07 30,general discussion,"
warning terrible audio:
 

the audio seems terrible and michael’s audio is missing completely… will see if we can upload a new version but that may be all that exists. will do some more testing before the next meeting
",
284,what do you call your mycroft wakeup word,none,"
just as a bit of fun, i wonder what custom wakeup words people use for their mycrofts…
whilst i have no problem with “mycroft”, for “family acceptance” reasons i felt a less “bookish” name would be better.
i changed mine a few times to find a word long enough and with enough inflection  to make it easier for the engine to not false trigger etc. etc. and i  ended up with…
andromeda
i use the google “voice” for the same reasons as above, which is female, so it seems to fit well 
what is your name for your device?
","
my daughter had an imaginary friend when she was a toddler. now that she is a teen, i have brought her imaginary friend “kelsey” to “life”. she thinks that is cool.

i don’t have a mycroft set up yet (maybe if i pick up a pi 3 sometime), but kind of considering renaming it “crossbar” if i set one up. (think the old mechanical crossbar exchange switches.) maybe set it to use a female voice if i find a compatible one that i like.

i would use “jarvis”… well you know why.

one of mine is called hal.

i had the idea of looking through shakespearean insults for a name, which probably doesn’t help you with being less bookish, but i didn’t find one i liked so i just use the name shakespeare

mine is called “vicky” since she is the bad robot in the “i,robot” movie ;-).

i call mine hobbes in honor of calvin and hobbes.

i call mine $very_bad_names when i’m debugging and not working as intended 

how, exactly, do you change the name it answers to besides the 4 names listed in the setup ?
i tried editing the < /var/tmp/mycroft_web_cache.json >  but that had no effect

hey spyder, any chances made to that file will be overwritten from your home.mycroft.ai settings.
there’s instructions to set this up at:
https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/customizations/wake-word
it starts with precise wake words, then goes onto pocketsphinx. if you want a custom precise model but not comfortable with the training aspect you can ask in this thread and a community member might help you out, you’d just need to provide the relevant audio samples: custom wakewords r us!
i recommend using the mycroft-config helper command to edit your config (in mycroft-core/bin if you didn’t add these to your $path during setup). this tool validates your configuration when you save the file. it’s so  easy to miss a comma or something and this saves a lot of frustrating debugging.
"
285,cannot find spotify devices,support,"
hi i have a fresh picroft set up and i have the spotify skill installed and configured but mycroft cannot find any spotify devices to play too.
i have raspotify installed on the mycroft pi and i can stream from my computer and phone app to the raspotify endpoint.
i tried removing the skill and re-adding, without putting my creds in the ui and it said it was not activated, i then put my creds in and it activated but could not find any endpoints.
has something changed on the skill recently? i had this working a month or two ago.
","
i was able to get it working, i had to go into the raspotify config and manually put in my username and password, it is now showing up as a device for mycroft.
the issue now is that mycroft is waiting for songs to end or spotify to stop to respond to queries.
has anyone seen this ?

hi redacted,
i have the same issue that i cannot stream from my pi on the mycroft speaker. how did you solve it in detail? sorry if this question is kind of stupid. i’m new with my- and picroft.
"
286,how to install mycroft when your cpu does not support avx,support,"
according to the instructions at get-mycroft/linux, if you do not have avx support: “technical users may be able to build an older version of tensorflow (1.13) from source using the instructions provided on their website.” however the dev_setup.sh does not install any tensorflow, so why tensorflow is needed, and by which module?
if i need tensorflow, is the version 1.13 that i need?
after i compiled the old version, how do i make mycroft use it?
there no help for people that do not have avx support and i do not want to use pocketsphinx.
thanks
","
tensorflow is used by precise, any somewhat recent cpu should have avx instructions


github.com/mycroftai/mycroft-precise








the tensorflow library was compiled to use avx instructions, but these aren't available on your machine.



        opened 02:48am - 06 may 19 utc




          technerder
        





is there a binary release file in this repository which uses a tensorflow installation which runs on cpu's without avx instructions?
cpu:...








either find a way to install precise/tensorflow, or you have to use pocketsphinx
in a old 32bit laptop i ran into same issue and had to use pocketsphinx

i do not understand, is mycroft-precise installed by mycroft setup?
how do i integrate the old version of tensorflow in mycroft-precise?

i do  not understand, does dev_setup install mycroft-precise?

precise is installed as part of mycroft-core.  as mentioned above, it requires tensorflow.  on x86 cpu machines, you may have to download and install a version compiled for the kind of cpu you have. if you can’t find one, then compiling yourself is an option.
lastly, you can switch to using pocketsphinx for your wakeword, though it is not nearly as accurate as precise.

yes, i know all that, but first of all i could not find any precise installation as part of mycroft-core, i could not find any installation of tensorflow as part of mycroft-core, so my question is: once you have compiled your version of tensorflow 1.3 what files are required for mycroft to work?
where those files go?
saying: “technical users may be able to build an older version of tensorflow (1.13) from source using the instructions provided on their website.”
is too generic and does not help at all.

if you want an out of the box solution there isnt any, most likely precise will eventually move to tensorflow2.0, and it doesnt make sense for mycroft (the company) to spend resources on supporting decades old cpus for a couple users
if you found that comment too generic id say you need to bite the bullet and use pocketsphinx, or run in a more recent machine, there really isnt anyone around for hand holding on this
everything is open source, start by trying to run from source code



github



mycroftai/mycroft-precise
a lightweight, simple-to-use, rnn wake word listener - mycroftai/mycroft-precise






mycroft downloads a binary for precise automatically, you will need to provide your own, default path for binary is ~/.mycroft/precise/precise-engine/precise-engine

thanks, that makes sense.

almost there. i finally managed to get it almost working.
"
287,dev sync 2020 07 27,general discussion,"

",
288,dev sync 2020 07 23,general discussion,"
 

a little late to the forums. i need to setup an automated posting for these…
",
289,mycroft doesnt respond in zorin os,none,"
i have installed mycroft and was waiting to receive the pairing code. but no response/sound comes when i open it.
","
what have you tried so far?  what were the results of that effort? what’s in the logs (/var/log/mycroft/, voice.log and audio.log to start with)

hi anjoe, welcome to mycroft!
can i also check how you installed it? was it cloning the mycroft-core repo from github, installing the snap alpha release, or something else?
"
290,skill implemeting multiple common interface,support,"
hi,
i started to develop my really first skill to revive the karotz.
reading the developer documentation, i found some common skills interface: commonplayskill, commonqueryskill, commoniotscript. as the karotz is both a sort of iot (we can control led color, ears position) and a player (it can play http stream), i’m looking for the best solution to implement both commonplayskill and commoniotscript.
should i create multiple skills?
should i create a single class with mutlple inheritance?
should i create multiple classes and load them via a single skill?
any help to design such case would be really appreciated.
regards.
","
humh… i think i misunderstood the commonplayskill. it seems to provide sources of content, rather than providing an audio emitter.

hi guyou,
the common play and iot skills are created to disambiguate common utterances. in the case of the common play skill it is intended for music services like spotify, emby, etc that can play a whole range of content. so when a user asks to “play metallica” it checks in with each of the common play skills to see which is the most confident about it’s ability to fulfill the request.
similarly for iot if a user says “turn off the kitchen light”, do they mean to use homeassistant, lifx, wink or something else?
so one of the main deciding factors on which type of skill you would create is what language your users would use to talk to the karotz.
"
291,first attempt at precise wakeword,machine learning,"
i’m trying to use the precise engine to cut down on false positives with our custom wakeword. i did the pre-requisite installs, managed to capture a few dozen utterances of the wakeword and downloaded the pdsounds for the random sounds as a start to the not-wake-word collection. i’ll eventually add much more data, but i just wanted to do a pass to make sure everything was running.
the precise-collect ran w/o any trouble, but the precise train failed with an exception that means nothing to me. i’ve copied the exception as well as the pip3 output showing the various packages and versions in the virtual environment. any help is much appreciated.
[jnickel:~/src/mycroft-precise] [.venv] dev* ± precise-train -e 60 emilia.net emilia/
using tensorflow backend.
/home/jnickel/src/mycroft-precise/.venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: futurewarning: passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/jnickel/src/mycroft-precise/.venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: futurewarning: passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/jnickel/src/mycroft-precise/.venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: futurewarning: passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/jnickel/src/mycroft-precise/.venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: futurewarning: passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/jnickel/src/mycroft-precise/.venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: futurewarning: passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/jnickel/src/mycroft-precise/.venv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: futurewarning: passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
warning:tensorflow:from /home/jnickel/src/mycroft-precise/.venv/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
instructions for updating:
colocations handled automatically by placer.
data: <traindata wake_words=36 not_wake_words=593 test_wake_words=0 test_not_wake_words=0>
loading wake-word...
loading not-wake-word...
loading wake-word...
loading not-wake-word...
inputs shape: (629, 29, 13)
outputs shape: (629, 1)
test inputs shape: (0, 29, 13)
test outputs shape: (0, 1)
model: ""sequential_1""
_________________________________________________________________
layer (type)                 output shape              param #   
=================================================================
net (gru)                    (none, 20)                2040      
_________________________________________________________________
dense_1 (dense)              (none, 1)                 21        
=================================================================
total params: 2,061
trainable params: 2,061
non-trainable params: 0
_________________________________________________________________
2019-10-09 16:49:51.251980: i tensorflow/core/platform/profile_utils/cpu_utils.cc:94] cpu frequency: 2593915000 hz
2019-10-09 16:49:51.252969: i tensorflow/compiler/xla/service/service.cc:150] xla service 0x4b51ec0 executing computations on platform host. devices:
2019-10-09 16:49:51.253008: i tensorflow/compiler/xla/service/service.cc:158]   streamexecutor device (0): <undefined>, <undefined>
train on 629 samples, validate on 0 samples
epoch 1/60
traceback (most recent call last):
  file ""/home/jnickel/src/mycroft-precise/.venv/bin/precise-train"", line 11, in <module>
    load_entry_point('mycroft-precise', 'console_scripts', 'precise-train')()
  file ""/home/jnickel/src/mycroft-precise/precise/scripts/train.py"", line 178, in main
    trainer().run()
  file ""/home/jnickel/src/mycroft-precise/precise/scripts/train.py"", line 171, in run
    initial_epoch=self.epoch, callbacks=self.callbacks
  file ""/home/jnickel/src/mycroft-precise/.venv/lib/python3.7/site-packages/keras/engine/training.py"", line 1239, in fit
    validation_freq=validation_freq)
  file ""/home/jnickel/src/mycroft-precise/.venv/lib/python3.7/site-packages/keras/engine/training_arrays.py"", line 210, in fit_loop
    verbose=0)
  file ""/home/jnickel/src/mycroft-precise/.venv/lib/python3.7/site-packages/keras/engine/training_arrays.py"", line 469, in test_loop
    outs[0] /= num_samples  # index 0 == `loss`
indexerror: list index out of range
θ60° [jnickel:~/src/mycroft-precise] [.venv] dev* 4s 1 ± pip list
package              version location                                
-------------------- ------- ----------------------------------------
absl-py              0.8.1   
astor                0.8.0   
attrs                19.2.0  
bbopt                1.1.6   
cycler               0.10.0  
decorator            4.4.0   
fitipy               0.1.2   
future               0.17.1  
gast                 0.3.2   
grpcio               1.24.1  
h5py                 2.10.0  
hyperopt             0.1.2   
joblib               0.14.0  
keras                2.3.1   
keras-applications   1.0.8   
keras-preprocessing  1.1.0   
kiwisolver           1.1.0   
markdown             3.1.1   
matplotlib           3.1.1   
mock                 3.0.5   
mycroft-precise      0.3.0   /home/jnickel/src/mycroft-precise       
networkx             1.11    
numpy                1.17.2  
pip                  19.2.3  
pocketsphinx         0.1.15  
portalocker          1.5.1   
precise-runner       0.3.1   /home/jnickel/src/mycroft-precise/runner
prettyparse          0.2.0   
protobuf             3.10.0  
pyache               0.1.0   
pyaudio              0.2.11  
pymongo              3.9.0   
pyparsing            2.4.2   
python-dateutil      2.8.0   
pyyaml               5.1.2   
scikit-learn         0.21.3  
scikit-optimize      0.5.2   
scipy                1.3.1   
setuptools           41.4.0  
six                  1.12.0  
sonopy               0.1.2   
speechpy-fast        2.4     
tensorboard          1.13.1  
tensorflow           1.13.2  
tensorflow-estimator 1.13.0  
termcolor            1.1.0   
tqdm                 4.36.1  
typing               3.7.4.1 
wavio                0.0.4   
werkzeug             0.16.0  
wheel                0.33.6
","
i found my problem after reading the source code at the bottom of the stack. i hadn’t provided any test wake words and test not-wake-words and the number of samples was 0 and that exception was thrown.
i generated test words for each category and the model is now being created.

thanks for the update jody, i’m sure others will run into the same problem in the future, so it’s very helpful to have a solution available!




 jodynickel:

tensorflow 1.13.2


stumbled across this. how do you have managed to get tensorflow 1.13.2 installed? (running mycroft in the kde bigscreen os) i’m struggling quite a bit to get this one installed. is python 3.7 the key?

pip3 install tensorflow==1.13.2
usually does the trick.  may depend on python version as well.

thanks, but if it would be that easy i wouldn’t bother you. even tried it from file and url. always the same error

error: could not find a version that satisfies the requirement tensorflow<1.14,>=1.13 (from mycroft-precise==0.30) (from versions: none)
no matching distribution found for tensorflow<1.14,>=1.13 (from mycroft-precise==0.30)

i’ve read that tensorflow has jumped a lot through the python versions during its development cycle, so…
and i mentioned kde (kde neon btw). picroft and i guess mark2 environments have to use another tensorflow version, picroft 1.10 for instance. so i figured tensorflow1.13.2 and python3.7 were a conscious choice.

hello, we are an italian development team and we would like to know some information about the custom wake word.
we did the train with precise 0.3.0 and it works quite well with a small number of people: 110 registrations.
we wanted to know how many recordings you used and if the wake word works well for you and is recognized in precise times and if you have the same result in mycroft or pycroft.
we with our wake word on precise always recognize it while on pycroft (importing precise 0.3.0) we have problems that often do not understand the wake word.
how many registrations should be made to have 1000 different people of the same nationality recognize the wake word?
do you have any useful suggestions for us?

what do you mean by registrations?  wake word wav files?
generally, the more data, the better.  also try and have 3+ times the number of not wake words vs. wake words.  it will also take some time and usage combined with saved wakewords to really improve the model in real world usage in an updated model.
i have maybe 400 wake words and about 2500 not wake words for my model, combined with the google voice commands and public noises datasets.  the majority of my not wake words come from false activations.  at this point it’s highly accurate for me.  i also still record the uttrances, and use the not-wake-words for re-modeling on occasion.
"
292,custom wake word sensitivity,support,"
i changed my wake word.  it now wakes when the printer starts.  the default sensitivity looks to be 1e-90.  if i make this 1e-20 would that make it less prone to false positives, or am i backward, and would i want to make it like 1e-160 (if that’s a real number).
","
i had to move mine to 1e-20.
if you’re adventurous and technically apt, making your own precise wakeword is not terribly difficult, and can help with that.

from the custom wake word documentation

threshold (scientific notation): the level of sensitivity at which the wake word should trigger mycroft to respond. to increase the sensitivity, reduce the threshold. the threshold is given in scientific notation.


ok, so in scientific notation 1e-90 is a lower number than 1e-20.  meaning, 1e-20 is less sensitive, meaning less prone to waking up due to improperly hearing the wake word, correct?

yes that’s correct. however, it is less sensitive to the wake word, meaning that it may also have more “false negatives” - where you say the wake word, but it doesn’t “hear” you.

got ya.  its a delicate balance.  there isn’t a way to “train” the custom wake word is there?
@baconator can you give me a bit more info about what you mean?

it’s kind of a roll of the dice.  i fiddled with it for a few days til it was where i felt happy with it.
you can train a custom wake word model, it’s not terribly difficult, but the more data you have the better…as in thousands of false words, and quite a few wake words.

i got ya.  is there instructions by chance?

good detail and steps here.



github



mycroftai/mycroft-precise
a lightweight, simple-to-use, rnn wake word listener - mycroftai/mycroft-precise






i have not done it but it is on my todo list.

hi there,
i am quite excited and impressed with the technology that mycroft team is bringing up.
in particular, precise is a pretty neat tool. is the statistics presented in progress mycroft home page any where going in a positive direction? the graphs are hard to read and it seems that just a couple dozen people use the tool each day.
i need help to setup the configuration of precise:
it gets a little confusing to configure the listener when it is not clearly indicated whether the precise or pocketsphinx.
it seems to me that the wake-word configuration in the home.mycroft.ai settings is only intended for pocketsphinx. since there is option to set the phonemes.
in my picroft install of mycroft the default/pocketsphinx wake-word was not working, it did not detect at all my calling. however the precise model detects quite well. but i still have some issue with a few false positives when i’m watching tv, it reacts every now and then when someone says: "" hey …"" and a name of someone (not similar to mycroft at all).
how can we set the word sensitivity when using the precise listener?
thanks.

unless you have trained your own custom model for precise, you are currently limited to “hey mycroft” and maybe “christopher”.  see 1 for more on configuring your own.  you can ask mycroft what listener it is using if you’re not sure.
the sensitivity of the default models is also helped by tagging user-submitted wake up calls (2).  for your own, always add more data.
1 https://github.com/mycroftai/mycroft-core/pull/1782/commits/c6f9513e9d5925ae8e575c71d4f6096c341bddae
2 https://home.mycroft.ai/#/precise

thanks @baconator, but those two links do not solve what i am looking for.
i am fine with the wake-work “hey mycroft” in precise, i just want to play with the sensitivity level (increase and decrease) when using the precise model.

k-9 is very close just a little def, have to yell hey k9. which seting should i adjust ? sensitivity or threshold. and which direction ?  would sensitivity to .6  or .7 do the trick?
                                                                                thanks as always for you help. geordy

/home/pi/.mycroft/mycroft.config
{
“hotwords”: {
“hey k9”: {
“phonemes”: “hh ey . k ey n ay n .”,
""sensitivity"": 0.5,
“module”: “precise”
}
},
“listener”: {
“wake_word”: “hey k9”,
""threshold"": 1e-18
},
“precise”: {
“dist_url”: “https://github.com/mycroftai/precise-data/raw/dist/{arch}/latest”,
“model_url”: “https://raw.githubusercontent.com/mycroftai/precise-data/models-dev/{wake_word}.tar.gz”
},
“max_allowed_core_version”: 19.8
}

hi geordy,
for pocketsphinx i’d focus on the threshold.
not sure if you read this in the docs, but the threshold is:

the level of sensitivity at which the wake word should trigger mycroft to respond. to increase the sensitivity, reduce the threshold. the threshold is given in scientific notation. use this handy converter to convert between decimal and scientific notation.

hope that helps

how does “sensitivity” and “threshold” add to/work alongside each other?
is “threshold” even used in a precise-setup?
and to add to the confusion, i got some warnings despite using a custom precise wake word
2020-07-25 17:34:01.052 | warning  |   706 | mycroft.client.speech.listener:create_wake_word_recognizer:348 | phonemes are missing falling back to listeners configuration
2020-07-25 17:34:01.054 | warning  |   706 | mycroft.client.speech.listener:create_wake_word_recognizer:352 | threshold is missing falling back to listeners configuration

the fallback for threshold is e-90. what’s the fallback for phonemes?

threshold is for pocketsphinx.

hey there,
as baconator said, threshold is for pocketsphinx. it’s because you can include pocketsphinx configuration attributes in a precise config and if precise fails for some reason it will fallback to pocketsphinx. the default config for “hey mycroft” is a good example:
""hotwords"": {
    ""hey mycroft"": {
      ""module"": ""precise"",
      ""phonemes"": ""hh ey . m ay k r ao f t"",
      ""threshold"": 1e-90,
      ""lang"": ""en-us""
    }
}

more detail on what those phonemes are here:
https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/customizations/wake-word#getting-phonemes
"
293,recorded utterance much quieter than wake word,support,"
hi,
i have installed mycroft on linux. i’ve configured it to save recognised wake words and recorded utterances. what i have noticed is that the utterances are much quieter than the wake words.
is there someway to configure this?
","
hello, what language do you use in mycroft? can you send your user configuration?
do you use a personal wake word? do you have trianed with precise? only with your voice?
you can use ffmpeg to increase the volume level of the recording, in second step.
thanks
"
294,trouble connecting mark 1 to wifi,support,"
hi this is a long belated cry for help. i have tried a wifi dongle and trying to set it up the way your suppose to i cant use the  dongle because i cant install necessary drivers and the mycroft.ai start page is taking to long to load can some one help me.
btw!!! i am using a year old mycroft mark 1 unit that i connected to the internet via ethernet but his placement was horrid.
","
mark-i wifi-setup use internal wifi and it will not recognize the dongle during setup-phase. regarding the drivers consider that mark-i is still on debian-jessie, drivers for stretch and buster will probably not work.

what about my wifi set up issue. it takes to long to load
but thankyou for explaning the dongole issue

hi there, i’ve bumped this to a new thread as it seems to be its own issue.
it sounds like you can successfully connect to the mycroft network, but start.mycroft.ai is not loading properly when you are connected is that right?
how far away is the device you are using to connect to the mycroft wifi hotspot?
if you have an ethernet connection available and are comfortable ssh’ing into your device, you could set the wifi credentials manually in your wpa_supplicant.conf. let me know if you need instructions for that.

i have a monitor hooked up to mycroft so i don’t need to use ssh and yes if there is a faster way of connecting him to the internet please give it to me
thanks gez

yes i would like the instructions

okay whilst i was trying and weighting for your instructions i  looked for a way to edit the conf  and i found it, editing the wpa_supplicant.conf(https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/get-mycroft/picroft#editing-the-wpa_supplicant-conf-file[note not using ethernet cable]) but when i did it i am getting
[ error writing /ect/wpa_supplicant.conf: no such file or directory ]
(note looks mine looks like )
gnu nano 2.2.6__________________________________________________________________________________modified
country=us
network=(
ssid=""#""
psk=""@""
}
(not like)
country=us
network={
ssid=“mynetworkssid”
psk=“mypassword”
now i am looking at a reddit post from 2 years ago (https://www.reddit.com/r/raspberry_pi/comments/70w04w/editing_wpa_supplicantconf_to_connect_to_network/dn7epsa/) that says i am looking for a completely different directory.
so what is my actual problem

dose my wpa_supplicant-conf not exist in my mycroft mark 1 unit
is the reddit post right because i was looking at picroft stuff
did i type it wrowng

take a pick

ok so i did type something wrong and now i do have a connection to wifi. i am a little put off because he said he said he could not launch or find something i am going to check the wifi again but he is still not working. is there any thing else than the wifi conf i need to change

btw i curently am in the client and it is saying
l~~~~ycroftai/precise-data/raw/dist/armv71/latest (caused by newconnectionerror (’ <urllib3.conection.verifiedhtipsconection object at 0x6c1d590>: failed to establish a new connection: [errno -2] name or service not known’))"")))




 hellwalker:

error writing /ect/wpa_supplicant.conf: no such file or directory


its etc not ect
i was not going to answer because capslock is annoying, please dont use it and be a good netizen. typing in all caps is internet code for shouting, and it is rude.

okay sorry man i just wanted to enficise some thing

hey, glad you worked it out!
is that working now with the path corrected from ect to etc?

yes but aparantly he says there is still more i also posted the error message i got. is it possible to connect mycroft to 5 ghz wifi.

raspberry pi3 based mycroft-devices (e.g. mark-i or picroft) do not support 5ghz wifi by default,  because pi3 does not have a 5ghz capable wifi-chip onboard. you may try a usb-wifi-dongle with 5ghz chipset…

on the raspberry pi 3b+ and raspberry pi 4b, you will also need to set the country code, so that the 5ghz networking can choose the correct frequency bands. you can do this using the  raspi-config  application: select the ‘localisation options’ menu, then ‘change wi-fi country’. alternatively, you can edit the  wpa_supplicant.conf  file and add the following. (note: you need to replace ‘gb’ with the 2 letter iso code of your country. see wikipedia for a list of 2 letter iso 3166-1 country codes.)
https://www.raspberrypi.org/documentation/configuration/wireless/wireless-cli.md

so i found a mistake when i was looking back on the .conf. and i fixed it and i have a new error message
l~~~~ycroftai/percise-data/raw/diat/armu71/latest (caused by newconnectionerror(’<urllib3.connection.verifiedhttpsconnection object at 0x6bd644f0>: falied to establish a new connection: [errno-2] name of sevice not known’))"")))

so what diose this error message mean and how do i fix it



 hellwalker:

l~~~~ycroftai/percise-data/raw/diat/armu71/latest (caused by newconnectionerror(’<urllib3.connection.verifiedhttpsconnection object at 0x6bd644f0>: falied to establish a new connection: [errno-2] name of sevice not known’))"")))


i miss gez

btw i am using an actual mark 1 not piecroft

just restarted mycroft and he says that he can not set up his net work

hey, on the surface it looks like a dns failure. can you post the config you are using, with your password replaced of course 
please format this as code by using three backticks (usually to the left of the 1 key) like so:
```
your configuration
```
or is it still:
country=us
network=(
    ssid=""fort knox""
    psk=""secret password""
}

actually just noticed that has mismatched parantheses, so make sure its network={
can you verify if the device is connecting to the router either from the dhcp registrations on your router, or run ifconfig on the device.
what sort of authentication are you using on your network?
do you have a hidden ssid? if so try adding scan_ssid=1 to the network
"
295,wifi mac address keeps changing,mycroft project,"
the mac address keeps automatically changing on every reboot.
raspberry pi 4b+ 64-bit
fresh picroft image:
** picroft enclosure platform version: buster keaton - pork pi
**                       mycroft-core: 20.2.4 ( master )
i found a similar issue happening with network-manager installed, but i don’t see that it is present on picroft:
(.venv) pi@picroft:~ $ nmcli -bash: nmcli: command not found
anyone having this issue?
","
hi there, welcome to mycroft!
i just checked the mac addresses of a picroft with ifconfig, rebooted, and re-ran ifconfig and got the same addresses again.
what method are you using to fetch your mac address? and is this for ethernet or wifi?

thanks for checking this.
i have multiple wifi aps and i’ve noticed it only happens on wifi and only when the pi connects to a different wifi ap.
i use my router dhcp table to read the mac though i tried using ifconfig as well

i suspect now some of my tp-link aps are modifying the mac address. thank you for your help. if i get to a conclusion it has something to do with picroft i’ll report back here
"
296,error handling utterance,support,"
só i’m building some testing skills in mycroft and i suddenly have this error appearing in the mycroft-cli no matter what i write:

13:41:51.193 | error    | 15500 | mycroft.skills.intent_service:handle_utterance:393 | ‘\~?(?’
traceback (most recent call last):
file “/home/dinhego/mycroft-core/mycroft/skills/intent_service.py”, line 348, in handle_utterance
_intent = padatiousservice.instance.calc_intent(utt)
file “/home/dinhego/mycroft-core/mycroft/skills/padatious_service.py”, line 213, in calc_intent
return self.container.calc_intent(utt)
    matches = self.calc_intents(query)
~~~~roft-core/.venv/lib/python3.6/site-packages/padatious/intent_container.py"", line 282, in calc_intents
    self.train()
~~~~ego/mycroft-core/.venv/lib/python3.6/site-packages/padatious/intent_container.py"", line 226, in train
    self.padaos.compile()
  file ""/home/dinhego/mycroft-core/.venv/lib/python3.6/site-packages/padaos.py"", line 123, in compile
    self._compile()
  file ""/home/dinhego/mycroft-core/.venv/lib/python3.6/site-packages/padaos.py"", line 134, in _compile
    for intent_name, lines in self.intent_lines.items()
  file ""/home/dinhego/mycroft-core/.venv/lib/python3.6/site-packages/padaos.py"", line 134, in <dictcomp>
    for intent_name, lines in self.intent_lines.items()
~~~~""/home/dinhego/mycroft-core/.venv/lib/python3.6/site-packages/padaos.py"", line 116, in create_regexes
    for line in sorted(lines, key=len, reverse=true)
  file ""/home/dinhego/mycroft-core/.venv/lib/python3.6/site-packages/padaos.py"", line 117, in <listcomp>
    if line.strip()]
~~~~ ""/home/dinhego/mycroft-core/.venv/lib/python3.6/site-packages/padaos.py"", line 107, in _create_regex
    return re.compile(self._create_intent_pattern(line, intent_name),
~~~~inhego/mycroft-core/.venv/lib/python3.6/site-packages/padaos.py"", line 100, in _create_intent_pattern
    line = line.replace('{' + key + '}', value.format(self.i), 1)
keyerror: '\\~?(?'


also had these(testes-skill is the skill i’m building):

error running install_or_update on testes-skill: skillmodified(uncommitted changes:
m init.py
d locale/en-us/testes.dialog
d locale/en-us/testes.intent
)

","
hi there, welcome to mycroft!
to start with the easy second one:

error running install_or_update on testes-skill: skillmodified(uncommitted changes:
m init.py

this is the mycroft skills manager saying it won’t try to automatically update the skill because there are local changes that are not merged in the git repo.
by default mycroft skills will update themselves automatically with the latest version from the marketplace. if you change anything in that skill, we stop auto-updates so that you don’t lose the work you were doing.
in terms of the padatious error.

do you have a link to the skill in it’s current state? or can you post the contents of your .intent file(s)
does it continue if you remove your testes skill?


so i removed testes skill and it worked perfectly! i don’t have a link for the skill, but i can send it through here, if it’s reasonable. in the mean time i’ll try to figure it out again what i’ve done wrong, try to do some tests again.
thanks for the support!
"
297,looking for help please,none,"
so i’m new to all of this and i want to use mycroft to control devices using mqtt server and a nude-mcu every thing is working fine but i couldn’t modify the python code within the skill to send commands to the server and i need help with that if anyone is interested in that.
","
what skill are you looking to modify?

i’m actually not trying to modify a skill am creating a new skill

feel free to look through my mqtt skill, it may have some helpful items.




testing and feedback for mesh skill (mycroft to mycroft communicator) skill feedback


    what do you call a group of mycrofts? 

a group of lions is a pride,
a group of birds is a flock,
a group of bees is a swarm, 
a group of mycrofts is a mesh 
introducing my mesh skill 



[image] mesh
send mqtt messages and commands between multiple mycroft.ai devices. 
about
this skill utilizes the lightweight mqtt messaging protocol to connect a group (“mesh”) of mycroft units together. the skill has the ability to send messages (intercom) and commands (messagebus) to one or …
  



thanks man i really appreciate your help i will give it a try and once again really thanks for your help
"
298,unable to add device,general discussion,"
when i go thru the add device process; at the end it says successfully done but device isn’t list and my pc keeps requesting registration.
registered fleet england uk
","
hi there, welcome to mycroft!
i think this is caused by a bug in our location data importing on the backend.  basically, there are two ""fleet""s in england and it creates a conflict of some kind.
we’ve got it on the list to investigate further but in the meantime, try selecting the other fleet in the drop down list, and if that doesn’t work you would need to choose a nearby town that has a unique name.
let us know how you go 

yep; that did it. used basingstoke instead
many thanks
"
299,dev sync 20 july 2020,general discussion,"
new format - the good, the bad, and the ugly…

",
300,dev sync meetings a new experiment in openness,general discussion,"
hi all,
you may have already seen, but we’ve started recording and publishing our twice-weekly (bi-weekly?) dev sync meetings. these are internal meetings, but we’ve heard from many people that they want to know more about our teams priorities and what we’re working on. so we thought, why not just let everyone see.
these will all be added to the development sync meetings playlist on our youtube channel, but @j1nx also suggested starting a forum thread for each meeting as a place where the whole community can comment on, discuss, and make suggestions around topics that come up in each video.
so we’ll consider this the thread for the first two videos, and we will start a new thread for each new video from here on out.
thanks
gez
",
301,mycroft keeps lowering volume unprompted,support,"
i just set up mycroft in a debian vm on my mac for testing (my company is hoping to utilize it in a future product), but for some reason it seems to keep turning down its volume, unprompted? i assumed it probably has to do with the “auto volume”, but asking the system  to “set auto volume to off”, or “disable auto volume” doesn’t seem to have any effect (it reports the current volume, but it continues dropping).
example of the “conversation” i keep experiencing:
set volume to 5
>>> volume changed to 5
what is your volume
>>> the volume is at 3
what is your volume
>>> the volume is at 1
how can i fix this?
also (seemingly at least mostly unrelated), it seems to intermittently go completely deaf and mute until restarted, or some random point a little later, when it’ll start working again, although the debug terminal commands still work throughout the issue.
","
a.i. : artificial insanity. 
guessing here, but perhaps some fading module - suspending module of pulseaudio?!?

if you have the auto-volume skill installed, can you try removing that and see if you get the same results?
it’s a community contributed skill so not tested as deeply as default skills.
"
302,how to recognize custom words in skill intent,general discussion,"
how do i get mycroft to recognize words and terms that mean nothing to anyone but me?  i alluded to this problem in another post.
i created a skill to map to network shares on my home network (using cifs-utils).  i want to say the name of the share to mycroft and have it connect to that particular share.  the share names are english words, but have no real-world context.
for example, i have a share called “everstor” (pronounced ‘ever store’).  it is a backup drive where i dump miscellaneous data, and because i keep it forever, i called it “everstor”.  when i say “map to everstor” it is recognized as “everest store” or some other inaccuracy.  in response, i limited the skill intent to respond to “map to network share” instead of “map to ” but now i can only map to the one network share that i associate with the code.
i read some documentation about phonemes in the custom wake word sections.  is there a way to integrate custom phonemes into skill intents?  alternatively, is there a way to create a custom global phoneme dictionary available to all (of my) skills (rather than addressing on a skill-by-skill basis)?  i have read the documentation on both the padatious and adapt intents but neither seem to address this directly.  for example, regarding using :0 from the padatious intent, even if the unknown word is the name of the share (e.g. “map to :0 network share”) within the intent utterance, i don’t know how to get mycroft to recognize the actual share name when spoken by the user.
thanks to all.
","
we have the same problem.  the word mycroft isn’t common in english.  as a result is often inaccurately transcribed.
modern speech-to-text algorithms make use of language models based on large volumes of written language.
they run audio through a two-step process.  first they try to transcribe the audio directly into text, then they evaluate that text against the language model to determine if they’ve transcribed something inaccurately.
most of the time the second step improves the transcription, but in cases where people are using words in ways that they wouldn’t be used in common speech the second step actually makes the transcription worse.
so nearly every time i need to use a voice algorithm to transcribe the word mycroft it instead transcribes it as microsoft or minecraft because those words are more commonly used.
this is actually a pretty significant problem for specialty applications.  in places like hospitals they use very specialized dialogue that includes words that are not in common use.  by using language models that are adapted to the language as a whole, the models make it very difficult to accurately transcribe speech.
the solution is to create data sets that are customized for the application or to train existing speech and language models on additional data to enhance their accuracy.
unfortunately the largest open source effort to create a modern speech-to-text engine ( deepspeech )  is not making progress as quickly as is necessary to keep open source relevant in this space.  if it was, we’d be able to customize a model for applications like yours.
as it stands now we’re at the mercy of commercial products that don’t have the features that we need and don’t provide the data and training architecture necessary to improve them.
i’d say your best bet for creating a speech-to-text engine that works for your chosen vocabulary is to either use pocketsphinx or sphinx.  alternatively, you could kludge it by creating a filter that substitutes your chosen word for the strings the stt engine is inaccurately returning.
here at mycroft, it would make sense for us to build a filter into our software that always substitutes mycroft for minecraft or microsoft.
if you did build some code that does this substitution and did it in a reasonably elegant way i suspect we would pull it in to master.  frankly i’ve wanted a piece of code to substitute mycroft for microsoft for some time now.

@aldeaman, i just a random thought on this. maybe the best approach is to create your intent with the following.
use the trigger words “map to” then parse the remainder of the utterance (in your example will return “everest store”). next apply fuzzy matching between the utterance remainder and the expected utterance. then pick some threshold for a successful match.
network_resource_confidence = fuzzy_match(phrase_remainder, ""everstor"")

i have not tried this but it might overcome the initial issues outlined by @j_montgomery_mycroft

that substitution will cause more trouble than help i think, mycroft is usually in the wakeword not in the actual queries, id be happy to be proven wrong and maybe suggest a better approach (selective substitution), what are your use cases?
“hey mycroft, what do you think of microsoft?”
“microsoft is awesome and respects your privacy!”
“hey mycroft, launch minecraft”
“im already running”
"
303,google coral edge tpu,machine learning,"
anyone tested how much a boost a google coral edge tpu gives a pi4 with picroft?
deepspeech would benefit but fann used in padatious doesn’t?!?
surprised a tensorflow-lite neural net hasn’t preference over fann.
precise is the same i guess.
i name drop the google product as likely it will have much support, just wondered if someone had tried.
i was just wondering with the diversification of load that generally runs on mycroft could each mic input trigger an instance.
that would really throw a cat amongst the alexa/google pigeons… 
","
no need for a tpu/gpu: “deepspeech v0.6 with tensorflow lite runs faster than real time on a single core of a raspberry pi 4”.
inference performance is not the problem here, but word error rate (wer).
even though current deepspeech model has wer of 7.5% in tests, this is still too much for real world application - unless you want to repeat your phrases multiple times until you trigger the correct intent…




 dominik:

deepspeech model has wer


is that with training or just the pre-trained models?
i should just use my usb headset and give it a go, but waiting for mic and speaker.
i have had a look at a few vids and often there is quite a delay maybe it was a pi3?
i was presuming it was wake -> stt -> intent parser -> tts that apart from network latency you could accelerate the whole process.
are there any vids showing it in action on a pi 4?

another quote from the article i have linked in my previous post: “it achieves a 7.5% word error rate on the librispeech test clean benchmark” - this relates to the pre-trained model for deepspeech 0.6. librispeech is not “real world talk” but based on public domain audio books.
the overall delay you observe on the pi3 has several factors, one being the network latency for stt (with cloud service). another factor is that currently this is not a streaming stt, so mycroft waits for silence to detect the end of the spoken phrase. in noisy environments this may be not possible so that it cuts off after the maximum recording time (10 seconds if i remember correctly). so in worst case it takes up to 10 seconds after wake-word before the actual stt is started.
for deepspeech there is a streaming api but this only shifts the problem of “phrase ending detection” to the deepspeech side…
and yes, a beefier cpu like the pi4 will speed up the intent parsing.

i was looking at the roadmap and just getting up to speed with mycroft.
being a noob my assumptions are relatively blind but it would seem generally there is a shift to neural network technologies for key mycroft components.
i was just interested if anyone had tried tensorflow with the pi now its has an alternative to gpu based offering.
"" identify hardware targets and create recommendations list
as rapidly evolving technologies, the requirements to run full stt and tts services is a moving target. but recognizing the demands helps shape the system, too, so is valuable to look at early. examine options such as:

hosted stt/tts hardware recommendations and limitations, e.g. “1 gtx 1080 can handle x mycroft units” or “1 fpga (brand and type need to be specced) can handle y mycroft units”
look at lower-power options such as stt/tts passthru with account randomization""

it was just curiosity but i did wonder “1 coral edge tpu can handle x mycroft units” with the pi4 being able to steam to multiple dumb wifi speaker/mics.
they are still pretty pricey in comparison to the cost of a pi4 but $75 gives 4 tops and being usb the pi4 might be able to handle x2.
but my thoughts where along the lines of could a pi4 & tpu handle 4 or more mycroft units as its likely they will drop in cost and not far out of line of multiple google/amazon offerings in overall price.
that so much is in the neural network domain that even parsing text semantics from webpages could also be accelerated greatly.
like this article as a semantic aware webcrawler would be pretty damn awesome  https://medium.com/teleporthq-io/understanding-the-web-parsing-web-pages-semantically-805ef857854d
with the roadmap and current neural net libs chosen as i noob i am head scratching to why not tensorcore-light for all?
also has anyone played with gpu/tpu acceleration? as just extremely curious how much load can the tpu handle and what resultant load is on the pi4.
you can still run cpu based tensorcore-light but at an instant add a tpu and gain much accelaration…
there are quite a few now on the market but the coral edge just seems to be getting the most pi focus.

from what i read here in the mozilla-forums currently deepspeech will not run on coral edgetpu as it does not support full tensorflowlite function set and deepspeechs tflite model requires some functions that are not supported.
regarding tts inference performance: mozilla tts tacotron + griffinlim vocoder, which has rather low quality, is 2-3x realtime on a gtx1080ti (6 seconds audio take 2-3 seconds for inference). tacotron2 and/or higher quality vocoders like wavernn or waveglow are slower…
another issue with the smaller edge computing devices like coral tpu or jetson nano is the rather small amount of available ram that limits size of model that can be loaded. therefore usually these devices can only run one model at a time, so there is no parallel stt and tts. (loading a deepspeech model on my jetson nano can take up to a minute).




 dominik:

coral edgetpu


dunno not sure about tflite as think they are awaiting replies.


github.com/mozilla/deepspeech








feature request: full integer quantization for tflite: coral edge tpu compatibility



        opened 06:27am - 01 sep 19 utc




          jacobjennings
        





for support and discussions, please use our discourse forums.
if you've found a bug, or have a feature request, then please create...









dunno haven’t a clue about ram as with 2 kb ram presumed the models where not tpu based.
we talking language model? as not sure how the 46mb or whatever it is resides.
[edit]
they seem to use discourse more frequently, so looking like your right.



mozilla discourse – 14 feb 20



edgetpu board support?
you should be able to comment out the audiospectrogram and mfcc ops from the inference graph and see if that works. i think others have tried and run into further problems but i don’t know for sure. if that works, please share the results! you could...







coral support is limited.  i would not pick one up at this point.  wait for a more generally usable version or updates to current software to take advantage of its capabilities.
as for the n gpus are able to x mycroft units, that’s not a very clear metric, either.  there’s significantly more elements to make that a coherent equation.

yeah it seems it works ok with the object identification demo.
its my noobness as i thought tensorcore-lite support would mean it would support tensor-lite?!?

e.g. “1 gtx 1080 can handle x mycroft units” or “1 fpga (brand and type need to be specced) can handle y mycroft units”

was from the mycroft roadmap and only quoted in a similar vain of curiosity that x or y of a brand and type could be tested.
i always struggle getting through any ml documentation as always seems extremely longwinded and painful.
i was hoping it was going to be purchase and install with tensorflow-lite which i keep calling tensorcore, but doesn’t matter its currently a cul-de-sac.
apols for all the questions but just getting my bearings and direction.

lots of marketing, lots of tech, shove into a casing and you have the sausage that is modern “ai”.  

not so sure about the sausages in the casing, from personal experience seems to be us outside the case trying to develop.
there is a disconnect in the knowledge hireachy of ml, but up at the top close to the source get results.
i played around with unity and their openml stuff and the samples worked perfect, but my usual approach to hacking (what i call programming)  was a pointless disaster.
the model collection and hyperparameters seems more like an arcane art rather than the simple brute force of it works, it doesn’t check error msg.
tfl and gpu seem to be currently x86_64 only where the arm/raspberry version is native client_client only
i read enough of lissyx posts that if they are failing to compile then i am not even going to bother,

hello can i get faster cnn training time by using google coral dev rather than pynq-z1? can i get faster cnn training time by google coral dev comparing to jetson nano? has anyone who use it, give an advice?

yes but many only offer a subset of tensorflow compatibillity.
the coral works with the google image project but haven’t seen another project for it.
they are coming down in price but how restrictive there subset is still means unless you know of a working project prob don’t bother.



seeedstudio.com



coral usb accelerator
a usb accessory that brings machine learning inferencing to existing systems. works with raspberry pi and other linux systems.
price: usd 59.99







“the coral works with the google image project but haven’t seen another project for it.”
what do you mean? if i buy it and run custom build tensorflow cnn code, won’t it work??

prob not as they offer a subset of tensorflow lite and have you seen another project for it?
or any mention anywhere of another project that supports it?
if you can rewrite using its specifics then yes, but no one seems to.
deepspeech might be now they are using 1.15 but prob not as likely.


github.com/mozilla/deepspeech








feature request: full integer quantization for tflite: coral edge tpu compatibility



        opened 06:27am - 01 sep 19 utc




          jacobjennings
        





for support and discussions, please use our discourse forums.
if you've found a bug, or have a feature request, then please create...










is this better than the google coral dev?


linuxgizmos.com – 28 jan 20



asus launches coral-based tinker edge t sbc for $168
asus has launched a $168 “tinker edge t” sbc that runs linux on the same coral som module found on google’s coral dev board, equipped with an i.mx8m and an edge tpu. the sbc advances to 3x usb 3.2 ports and offers a second csi port and a 12-19v...






what i mean, is: is it more flexible as far as the ml programs i can use, is concerned?

no as they are all very similar with different compatibility issues that you will have to research yourself.
the google coral is as good as any and you could take the image kit and feed with mel-frequency cepstral coefficients, or mfccs
basically voice images and the standard image classification with that input should work.
i think google where/are in the process of improving whats available not sure what the state of play is.
asus say they are going to ‘support’ but if its any better or worse than the pi offering via google image dunno.
just don’t expect to grab deepspeech compile and fly as deepspeech even runs a fork of tensorflow 1.5 that i have no idea what stage with accelerators such as that are.
you can try but think its best to say that actual compatibility and whats available might be big constraints.
i want one but think it prob might be a dissapointment in what i can run.
the best overall compat are the new nvidia rtx cards and after that its all down hill with earlier cards often needing earlier versions of tensorflow as performance is badly effected.
my graphics card is a mweh gtx780 pretty old now and don’t even bother trying to use it.
deepspeech prob would benefit from an accelerator if it would work as on a pi at least its single thread.

that’s literally a google coral tpu plus an sbc, so a direct competitor from a different vendor.
other than a jetson tx2 or xavier board from nvidia, there’s not much in the sbc space that’s viable for anything but customer or very specific ml work yet.  if you’re looking to train, get an add in board for a desktop or go the cloud route.  sbc’s are inference boards.

the npu that this https://tinker-board.asus.com/prod_tinker-edge-r.html contains, has anything to do with tpu? is it faster than google coral dev/asus tinker edge t ?

tpu = tensor processing unit - this can be seen as a gpu that is specialized/optimized for tensor operations (vector and matrix multiplication)
npu = neural processing unit (based on fpga) where you can load the model directly to the processing unit. this may give excellent performance but as a drawback programming is quite complicated. most available model/algorithms are for visual processing (object detection) so this would not be my choice when it comes to speech recognition.
in absolute numbers: rockchip rk3399pro+npu is up to 2.4 tops, google coral tpu is rated up to 4tops.
"
304,trying to get mycroft to speak arduino sent data,general discussion,"
i am new to mycroft.  i am running picroft on a raspberry pi 4 model b rev 1.2 with mycroft-core: 20.2.4 and using python 3.7.3.
i have connected an arduino uno to the raspi via usb cable and created a “stupid simple” device that has a single pot (short for potentiometer aka a dial).  the arduino code reads the analog input and sends over serial.  the raspi successfully reads the serial information (see screenshot and code).  that’s not what i need help with.  (note: the “p3” command is just a bash alias i setup for ‘python3’)

uno-pot-test835×382 64.8 kb

the code being run is:
    #!/usr/bin/python3

    import serial

    ser = serial.serial('/dev/ttyusb0', 115200)

    read_uno=(ser.readline()).decode('utf-8')
    print(read_uno)

here is where i am stuck: i want to be able to say “hey mycroft, what is the sensor reading?” and have it reply (i.e. verbalize) the current reading at that moment.  i have two problems here:


i have “what is the sensor reading?”, “read the sensor.” and “what is the sensor level?” in the /opt/mycroft/skills/read-arduino-pot-skill/locale/en-us/pot.arduino.read.intent file but mycroft does not associate the utterances with this skill.  i even tried mycroft-say-to “what is the sensor reading?” from the command line but it doesn’t know what i mean.  i have read up on the adapt intents (https://mycroft-ai.gitbook.io/docs/skill-development/user-interaction/intents/adapt-intents) but from the examples on the page, i am not certain how that would help here.  perhaps the noobie is wrong.  wouldn’t be the first time.


let’s assume that problem #1 is solved.  how do i code the skill’s __init__.py to actually speak the reading?  if i assign the current value from the serial stream to a variable, how do i get mycroft to say “the sensor reads $value” where $value is the variable storing the current reading?  right now, i have the following.  if only i could test it but i still have problem #1.


from mycroft import mycroftskill, intent_file_handler
import serial


class readarduinopot(mycroftskill):
    def __init__(self):
        mycroftskill.__init__(self)

    def initialize(self)
        ser = serial.serial('/dev/ttyusb0', 115200)
        read_uno=(ser.readline()).decode('utf-8')

    @intent_file_handler('pot.arduino.read.intent')
    def handle_pot_arduino_read(self, message):
        self.speak_dialog('pot.arduino.read' + read_uno)


def create_skill():
    return readarduinopot()

thanks to all for your help.

edit: i corrected the markdown above to use a code block instead of a blockquote so the indentation and formatting is corrected now.
","
first off, use an existing skill ( lots of folks use the weather skill ).  copy it, rename it and tweak it to use your intents.  respond with a static response “the reading is 42 dave. the answer is always 42”.
once you have that working, tweak the weather skill actions to query your local sensor rather than the external weather api.
docs here:. https://mycroft-ai.gitbook.io/docs/skill-development/introduction/your-first-skill
"
305,how to get gui working,none,"
i am looking for the definitive guide to get the mycroft gui working. i am currently running picroft on a rpi3b+ and would like to get a gui running, after scouring the forum i found many items referring to gui but there seems to be many routes to take to get this running and i am looking for the least pain / most reward so i can start to add gui support to some of my skills.
all suggestions pros/cons are welcome.
","
step1, use at least a pi4, a pi3b+ will work if you use mycroftos, but usually there isnt enough juice
if you are running latest ubuntu (20) the instructions in the repo will just work
other os are usually missing some packages and you get into dependencies issues

good news is i have an rpi4 doing nothing but i am starting to think there is something architecturally i don’t understand about the gui.
the first step on the “foolproof setup instructions™”  trip me up when it says to "" download and install virtualbox"".

does the gui run on the rpi or is it hosted elsewhere via virtualbox?
did you recommend ubuntu (20) to run on the rpi4?
i did play with mycroftos a bit with this rpi4, so maybe i should go that route?

actually it was all your skills you published in the last week that made me decide i need to get the gui running 

for install i just ran dev_setup.sh from



github



mycroftai/mycroft-gui
the graphical user interface used by the mycroft mark ii and more - mycroftai/mycroft-gui





the gui can be running anywhere, the only requirement is that it can connect to bus, if you feel like it you can have several guis connected to a single mycroft  (be aware of the security risk of exposing your messagebus, i do not recommend connecting externally, i recommend blocking external connections in the firewall!)
when i said ubuntu i meant desktop, just pointing out a distro i know works, in previous version (of ubuntu) i ran into dependency issues, depending on your distro it might be easy or tricky to get it going. if you are adventurous and build stuff from source or enable other sources in your package manager it should run mostly anywhere
for a pi i cant really help since i havent gone that route, but i can suggest you put that pi4 to good use with a bigscreen image
https://plasma-bigscreen.org/
mycroft os comes with gui support and everything installed, being a production ready, minimal and dedicated system saves enough juice to run a gui confortably, usually a pi3 just doesnt have enough power to run a full desktop stack + mycroft
the bottom line is that the only easy way to get this going is:

mycroftos
bigscreen
on desktop, your milleage may vary depending on os


i have mycroft-gui and the new mint 20. installation succes but i become this error.
qt5ct: using qt5ct plugin
attribute qt::aa_shareopenglcontexts must be set before qcoreapplication is created.
qrc:/main.qml:37:5: unable to assign [undefined] to int
qrc:/main.qml:36:5: unable to assign [undefined] to int
qrc:/main.qml:35:5: unable to assign [undefined] to int
qrc:/main.qml:34:5: unable to assign [undefined] to int
qrc:/main.qml:33:5: unable to assign [undefined] to int
qrc:/main.qml:32:5: unable to assign [undefined] to int
qrc:/main.qml:214:17: unable to assign null to qstringlist
qml: trying to connect to mycroft
bereits aktuell.
starting all mycroft-core services
initializing…
starting background service bus
caution: the mycroft bus is an open websocket with no built-in security
measures.  you are responsible for protecting the local port
8181 with a firewall as appropriate.
starting background service skills
starting background service audio
starting background service voice
starting background service enclosure
main socket connected, trying to connect gui
received port 18181 for gui “{42ca529b-e900-4bbd-beed-32c9b8977928}”
qrc:/main.qml:257: typeerror: cannot read property ‘longduration’ of undefined

perhaps i will look into howto add only the bare minimal gui stuff onto a “normal” headless os image and document the steps.
but would first want to finish my switch to networkmanager and push the @&$@ mycroftos inage first🤦‍♂️

hi pcwii,
you can see my guide at https://www.thinkthinkdo.com/trac/project1/wiki/mycroft-gui-mark-2_sys
or my forum posting mycroft-gui on my mark-ii - simplified app if you do not want to run the kde plasma nano.
have fun - guhl
"
306,uninstalled skills appear back after a while,support,"
i need to uninstall two skills i will never use: spotify (i don’t have a premium subscription) and pandora (i’m outside the us) and i want to test plexmusic-skill, which hotword is “play”, and mycroft insists in try pandora skill with that word, so i’ve uninstalled those skills, both with msm remove and directly with rm -rf /opt/mycroft/skills/...
the skills, after a while, appear back after several minutes or the mark 1 reboot.
i don’t know how to completely get rid of a skill i don’t want.
","
hi malevolent, i know i posted this in chat already but just in case others are looking for this in the future…
default mycroft skills will return unless you blacklist them in your mycroft-conf, the following should sort that out:
""skills"": {
    ""blacklisted_skills"": [""mycroft-pandora"", ""mycroft-spotify""]
}

more details on mycroft-conf in the documentation:



mycroft



the `mycroft.conf` file - mycroft
learn about your mycroft.conf mycroft configuration file - where it's held, order of precedence and the data that it contains about your mycroft conf.







sweet! thank you @gez-mycroft!
you saved my day 
p.s. that’s why i posted the question on the forum, besides it will be indexed by search engines, i think is a better place to search for recurrent questions

i am new to mycroft and i still cannot stop these skills from loading.  i am running picroft on a raspberry pi 4 model b rev 1.2 with mycroft-core: 20.2.4
in the mycroft.conf file, on the blacklist line, i have the following:
“blacklisted_skills”: [“skill-media”, “send_sms”, “mycroft-playback-control.mycroftai”,“skill-wolfram-alpha”, “pianobar-skill”, “fallback-wolfram-alpha.mycroftai”, “mycroft-alarm.mycroftai”, “mycroft-joke.mycroftai”,“mycroft-naptime.mycroftai”,“mycroft-npr-news.mycroftai”,“mycroft-personal.mycroftai”,“mycroft-playback-control.mycroftai”,“mycroft-reminder.mycroftai”,“mycroft-singing.mycroftai”,“mycroft-stock.mycroftai”,“mycroft-timer.mycroftai”,“mycroft-weather.mycroftai”,“mycroft-wiki.mycroftai”, “mycroft-fallback-duck-duck-go.mycroftai”],
the instructions in the .conf file say that i should include the .mycroftai if it exists in the skill name as listed in the /opt/mycroft/skills directory, but i have tried it both ways (with and without .mycroftai) and the skills reload anyway.
i have looked in the other mycroft.conf files and there is no “blacklisted_skills” line so i think i am in the right place (~/mycroft-core/mycroft/configuration/mycroft.conf)
what am i missing here?  i’m sure it is something simple.
alternatively, is there a way to disable all skill checking without having to specify individual skills in a list?  essentially, a skills code freeze.  just a thought.
thanks in advance.
"
307,mycroft not starting up,none,"
hi team,
i am getting the following lines in my bus.log file. could you please help me with it. mycroft is not able to connect to the mycroft servers as well.
file “/home/chickoo/downloads/mycroft/mycroft-core/mycroft/configuration/config.py”, line 151, in init
setting[“location”] = location
typeerror: ‘str’ object does not support item assignment
2020-07-18 13:18:22.917 | info     |  5080 | main:main:54 | message bus service started!
2020-07-18 13:18:23.037 | info     |  5080 | tornado.access | 101 get /core (127.0.0.1) 1.03ms
2020-07-18 13:18:23.067 | info     |  5080 | tornado.access | 101 get /core (127.0.0.1) 0.45ms
2020-07-18 13:18:23.404 | info     |  5080 | tornado.access | 101 get /core (127.0.0.1) 0.38ms
2020-07-18 13:18:26.874 | info     |  5080 | tornado.access | 101 get /core (127.0.0.1) 0.55ms
","
127.0.0.1 is localhost, the computer it’s running on.
something in your configuration is not correct, though. what have you changed recently?

i upgraded pip to the latest version as it was showing a warning
/home/chickoo/downloads/mycroft/mycroft-core/.venv/bin/python -m pip install --upgrade pip
this is what i used to upgrade pip and the version is 20.1.1
i am unable to pair with the mycroft servers
"
308,wifi setup connect to network mycroft,mycroft project,"
mycroft on initial setup is asking me to connect to the network mycroft. the problem is, i can’t find this network. then, i cant continue with the setup. please help! thanks.
","
the network shuts down after a few minutes, are you trying it right after?
also, are you running on a raspberry pi 3 or 2?
so far on the 2 it seems initial configuration needs to happen to get it to use the network adapters.
you can manually connect it by editing the wpa_supplicant file. it should be here:
sudo nano /etc/wpa_supplicant/wpa_supplicant.conf

more info on that should be here:



raspberry pi



raspberry pi - teach, learn, and make with raspberry pi
the raspberry pi is a tiny and affordable computer that you can use to learn programming through fun, practical projects. join the global raspberry pi community.






would it help if we put this info somewhere, like the wiki page for the picroft_environment github repo?

i am running on a pi 2.

hello @uvissa
in order to use the  wifi setup client, you will need to use a raspberry pi 3.

yeah, what aatchison said. if that’s the case, you can manually connect via this guide:



raspberry pi



raspberry pi - teach, learn, and make with raspberry pi
the raspberry pi is a tiny and affordable computer that you can use to learn programming through fun, practical projects. join the global raspberry pi community.







3 posts were split to a new topic: trouble connecting mark 1 to wifi
"
309,need some development help with customizing mycroft,general discussion,"
my company is looking for developer help with customizing mycroft.  we want to customize on linux.
you may reach me at pb@prophetlabs.com
pete
","
like changing the tts voice and wake word or like branching and making your own?

we are not branching out on our own.  we want to change the voice and wake word (“hey zoe”) and get some help and insights on skills from someone who knows more then we do.  i you could help please reach out to me at pb@prophetlabs.com.
thanks.

put your ideas for skills to the community and someone may take it up.
https://community.mycroft.ai/c/skill-suggestions
"
310,need understanding of skill order fallback,mycroft project,"
hi all,
i’m about to jump into some skill development, but i want to make sure i understand how the “order” or fallback works. this question stems from the fact that with wolfram alpha and duckduckgo enabled, i can’t tell which one it’s going to use.
for instance “who is george washington?” gets a response from both wolfram alpha and duckduckgo, but uses the duckduckgo response.
if i were to build a third fallback instance, how does mycroft prioritize?
also, can a fallback be “de-prioritized” to only run if no answer is found to the query?
thanks!
","
hi there,
it’s a good question, and there’s actually two pieces here - fallbacks, and the common query framework
with each of the common frameworks (common query, common play, and common iot) a skill using that system reports its confidence that it can answer/fulfill a particular query. this is what ddg and wolfram are doing. they both go look for an answer and report back through the query skill how confident they are that they’re correct. the most confident then gets to answer.
so you can create another skill and register it as a common query skill, then it will also be asked to report how confident it is that it can answer questions. we have some new documentation to get you started on these types of skills here:
https://mycroft-ai.gitbook.io/docs/skill-development/skill-types/common-query-framework
getting back to fallbacks - the common query system is a special type of fallback skill, however you can have a general fallback. all fallbacks are assigned a priority level (from 1 to 100+) that determines the order in which they’re checked. so a fallback with priority 8, will get to try and answer a question before a fallback with a priority of 28. if it believes it has successfully answered, it will return true and no other fallback skills are called upon. if it doesn’t have an answer the skill returns false and the next skill in the queue gets a turn.
there’s some more docs here to get you started if you think a general fallback is the way to go.
https://mycroft-ai.gitbook.io/docs/skill-development/skill-types/fallback-skill
hopefully that clears it up a little, but if not please let us know. interested to hear what you have in mind for your skill too 

that cleared things up very well, exactly what i needed. thank you!
"
311,suggestion grouping mycroft devices for audio playing,general discussion,"
we’re seeing lately a great community effort to provide music skills to mycroft. at my home, i already have a few mycroft instances on several devices, and my idea is to have one at each room of the house, so i can use them as intercom, or interact with my smarthome system from everywhere.
but, what about to play the very same music on all or some of my devices at the same time? it would be great to let mycroft devices to “group” for music playing, so i can say from any of them to play something in some room, a group of rooms or a complete home.
i know this is rather a smart speaker feature, but it would be cool to have it in mycroft as well. the idea is the commonplay framework should handle this and make sure all the devices are playing the same at the very exact time, so the music is playing around the house like a big stereo system, and skill developers can just call the commonplay framework as usual.
what do you think about?
","
nice idea.
as you said it’s a smart speaker thing primarily.
just as an idea on multiroom setup.
i made a multiroom speaker setup based on pulseaudio with multiple mpd instances.
in pulseaudio you can defined virtual sinks and group rooms in that way. maybe mycroft can read defined sinks for being dynamic.
pactl list sinks
greetings
thorsten




 thorsten:

i made a multiroom speaker setup based on pulseaudio with multiple mpd instances.


that would be more or less the idea, plus controlling the audio from any of the mycroft instances (i could start playing music on my office, listening in all the house, but then stopping the audio on the kitchen).

could this location independency be expanded to other skills, eg. being in the living room and saying : set a kitchen timer for 10 minutes and mycroft users kitchen pa sink for output.
but this goes to another direction as i think you planned it with this thread.

yes, that is outside the scope of the idea. and i think, but not pretty sure, that @jarbasal hive mind can do something like those you’re saying.

i am pretty sure that @j1nx has implemented snapcast into his mycroftos. i suspect snapcast is what @thorsten is using with mpd.

i’ve no snapcast installation running, but it looks interesting.

snapcast is doing exact that and more such as syncing the playback.
configure pulseaudio to use snapcast as default sink.
configure mycroft to use pulseaudio
you can then use the snapcast api for all the stuff such as grouping etc.




 j1nx:

snapcast is doing exact that and more such as syncing the playback.


wow, didn’t know that software! so, a snapcast-skill would be awesome then, as i see snapcast has group support!
the only thing i cannot see is controlling playing from any snapclient, but i guess this could be handled by something like the mesh-skill o hive mind, don’t it?

a snapcast skill would be very welcome.
i posted more info about it including a link to the api elsewhere on the forums. not sure where it was exactly.



github.com


badaix/snapcast/blob/master/doc/json_rpc_api/v2_0_0.md
snapcast json rpc control api
=============================

## raw tcp sockets

snapcast can be controlled with a [json-rpc 2.0](http://www.jsonrpc.org/specification)
api over a raw tcp-socket interface on port 1705.

single json messages are new line delimited ([ndjson](http://ndjson.org/)).

for simple tests you can fire json commands on a telnet connection and watch
notifications coming in:

```json
$ telnet localhost 1705
trying 127.0.0.1...
connected to localhost.
escape character is '^]'.
// call ""server.getrpcversion""
{""id"":8,""jsonrpc"":""2.0"",""method"":""server.getrpcversion""}


  this file has been truncated. show original






don’t know what is supposed to be this jsons, i guess is a way to control what’s playing from any snapcast client?
"
312,kde neon and precise skill wakeword,support,"
i’m running mycroft on a pi4b in the kde bigscreen os environment (kde neon) and want to tinker with the wake word (as a non native english speaker you’re best adviced to do so, especially if you set it up to serve a non english speaker) through precise and the skill “wakeword” from gras64 taking advantage of precise. (ultimately wanting to help gras64 pushing it further)
as it turns out this setup pushes me into the tensorflow 1.13.2 territory and thus python3.7 (a guess/deduction on my side). if that holds true i guess i could altinstall python3.7. (a pip/mycroft-pip install isn’t possible with this setup) but how is the venv pulling it in?
","
you can access mycrofts virtual environment on the image to run any pip commands
cd ~/mycroft-core
source .venv/bin/activate
on the image the install is the same as it would be on any desktop, a git pull from mycroft core dev branch.

hi aiix,



 aiix:

on the image the install is the same as it would be on any desktop, a git pull from mycroft core dev branch.


you specifically reffering to the usual install/update process stated here ? (btw: update_dev.sh is missing)
bashing the dev_setup.sh doesn’t resolve my problems with missing tensorflow. and the installation afterwards with mycroft-pip install tensorflow==1.13.2 is resulting in
error: could not find a version that satisfies the requirement  tensorflow<1.14,>=1.13 (from mycroft-precise==0.30) (from  versions: none)
no matching distribution found for tensorflow<1.14,>=1.13 (from mycroft-precise==0.30)
(the same if i bash ~/.mycroft/skills/wakeword/precise/setup.sh; this was the first time i realized i got a problem with tensorflow)
no matter the environment or the source (file/url/ppa)
or do you specifically mean python3.7? is python 3.6.9 (which i’m running now) the problem?

the current installed version of mycroft core is a few versions behind than the current dev branch, try the following in the mycroft-core folder:
git stash
git pull origin dev
this should bring you upto date with the latest core on image.
i don’t have much idea about the tensorflow / python 3.7 / precise issues, since the default install didn’t have any issues for mycroft running on the image, maybe someone with more knowledge on those can help with that.
for bigscreen if updating the current install still has a issue you can nuke the current mycroft-core install and reinstall mycroft with the following:
rm -rf ~/.mycroft
rm -rf ~/mycroft-core
rm -rf /opt/mycroft
cd ~
git clone https://github.com/mycroftai/mycroft-core
cd mycroft-core
./dev_setup.sh -sm
select non stable / development branch in setup
that should give you a fresh install, you will have to reinstall the voice apps from the mycroft skill installer on the image to get those working again.

thanks for the write up although i don’t have any problem with mycroft and its skills until it comes to precise, where the sh** goes south.
but i will definitely revert because i guess i might have missed the step mentioned here  (essentially to call “hey mycroft, set the listener to precise” -> download) before i installed the wakeword skill. this might have caused the skill pull some resources that are not pre-processed by mycroft and thus not compatible.
this is the most logical thing i can come up with atm.
i’d love to hear more takes on this.
"
313,manual installation on raspberry pi,mycroft project,"
i’m new in this forum and i recently discovered the mycroft project.
i would like to install picroft on a existing os like home assistant. i have two questions :
1 it’s possible to install manually the picroft on a existing raspbian image ?
2 the raspberry pi is capable to run simultaneously hass and picroft ?
thanks.
","
maybe do it the other way round as the docker hass is very easy



yes, follow instructions for linux installation


hass.io and mycroft in parallel on a rpi3/4 will probably work, but i would keep my home automation always seperated from other tasks. you don’t want to sit in a dark and cold house because a mycroft update broke a dependency from hass.io. stuart’s suggestion on using docker could help here though…



i just ran that on a pi4-2gb on the mycroft image



home assistant



installation on docker
instructions to install home assistant on a docker.






seem to work fine for load.
has anyone ever done a docker for mycroft?



the dev community



installing docker and docker compose on the raspberry pi in 5 simple steps
installing docker on the raspberry pi sometimes can get messy, let me help you with that!









 stuartiannaylor:

has anyone ever done a docker for mycroft?


never tried it myself, but there are some instruction for mycroft on docker available.

thanks for your answers, i will look to use docker

@quadeur74 were you able to get this working?
i am also attempting to run both home assistant and mycroft via docker on my raspberry pi  4b with standard raspbian.
i use docker compose to successfully launch other applications (home assistant, deconz, rhasspy), but somehow, the mycroft container does not seem to load properly and continuously fails and restarts.

screen shot 2020-07-12 at 11.20.471222×882 39.4 kb

i pasted my docker-compose.yml config below. anyone has any idea what is going wrong here?
version: '3'
services:
  deconz:
    container_name: deconz
    image: marthoc/deconz
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ~/.local/share/dresden-elektronik/deconz:/root/.local/share/dresden-elektronik/deconz
    devices:
      - /dev/ttyacm0:/dev/ttyacm0
    environment:
      - tz=europe/berlin
      - deconz_web_port=801
      - deconz_ws_port=445
    restart: always
    network_mode: host
  mycroft:
    image: ""mycroftai/docker-mycroft""
    container_name: mycroft
    volumes:
        - ""~/mycroft:/root/.mycroft""
        - ""${xdg_runtime_dir}/pulse/native:${xdg_runtime_dir}/pulse/native""
        - ""~/.config/pulse/cookie:/root/.config/pulse/cookie""
    ports:
        - ""8181:8181""
    devices:
        - ""/dev/snd:/dev/snd""
    restart: always
    network_mode: host
  homeassistant:
    container_name: home-assistant
    image: homeassistant/raspberrypi4-homeassistant:stable
    volumes:
      - /home/pi/homeassistant:/config
    devices:
      - /dev/ttyacm0:/dev/ttyacm0
    environment:
      - tz=europe/berlin
    restart: always
    network_mode: host
"
314,acknowledge and start listening files custom,general discussion,"
if anyone modded these sounds, please share your files !
","
the file resides in
 mycroft-core/tree/dev/mycroft/res/snd/
and you can swap it for a similarly formatted wav file of your choice.

yes exactly. i was curious to see what kind of files/sounds people are using and if they wanted to share them here.

i have .01s of silence for mine.

in chatterbox i allow setting a custom sentence/list, so i have a bunch of short sentences to give it some variety,“yes”, “i’m listening”, “ready to help” and so on. i have like 15 different ones
"
315,mycroft chat email confirmation,none,"
i tried to create an account to get to the chat platform but never receive the confirmation link. multiple resend was made. anyone can help with this ?
","
hey there, i’ll email you and we’ll get you set up 
let me know if you don’t get an email from me.

as of now, i did not receive anything. thanks

sorry if this is obvious, but you have checked your spam folder right?

yes i did but it is empty.

very strange. can you email me directly at kris.gesling@mycroft.ai
we can’t help you reset the password until we can verify you are the owner of the account.

sent you an email yesterday.
"
316,3 5mm audio stopped working after installing seeed respeaker driver,support,"
i am having an issue with mycroft, combined with the seeed respeaker 4mic array on my pi. i performed the following steps:

installed mycroft, selecting 4, or “other microphone” for the input, and the 3.5mm for the output.
tested that the mycroft sound output worked by typing prompts into the mycroft command line
installed the seeedr respeaker driver from here: https://github.com/respeaker/seeed-voicecard

rebooted.

after the final step, the microphone was working properly, but the speaker had stopped working.
pactl info output:
server string: /run/user/1000/pulse/native
library protocol version: 32
server protocol version: 32
is local: yes
client index: 77
tile size: 65496
user name: pi
host name: picroft
server name: pulseaudio
server version: 12.2
default sample specification: s16le 2ch 44100hz
default channel map: front-left,front-right
default sink: alsa_output.platform-soc_audio.analog-mono
default source: alsa_input.platform-soc_sound.multichannel-input
cookie: d005:a521

pactl list sinks short output
0       alsa_output.platform-soc_audio.analog-mono      module-alsa-card.c     s16le 1ch 44100hz        suspended
1       alsa_output.platform-soc_audio.analog-mono.2    module-alsa-card.c     s16le 1ch 44100hz        suspended

","
thought: if that additional driver resulted in non-zero device id, you might be hitting: https://github.com/mycroftai/enclosure-picroft/issues/132

my guess is that somewhere you switched to using the new(er) (f)kms driver with the included sound driver.
then looking at your pulse setttings you have the first analog as default, which get’s routed to the hdmi. making sink1 default instead of sink0 would most likely fix it again.
or check your config.txt tonor load the vc4 driver.

thanks, i fixed the issue by:

removing the following lines from /etc/mycroft/mycroft.conf
“play_wav_cmdline”: “aplay -dhw:0,0 %1”,
“play_mp3_cmdline”: “mpg123 -a hw:0,0 %1”,
setting the audio output 20 #2 via pactl set-default-sink alsa_output.platform-soc_audio.analog-mono.2

rebooting

i am not sure which one of those it was that fixed it since i ran them at the same time.
"
317,precise personal wake word for everyone,mycroft project,"
hi guys,
i need your support. i installed mycroft on my raspberry pi 3b. i want to change the wake word from “hey mycroft” to “jerry”. i’m using mycroft-precise for this.
in the last weeks, i trained mycroft with 50 recordings of my voice plus ten during the test. the wake word works.
my goal now is that everyone can activate the wake word “jerry” without recordings and training of their voice. do you believe this can be possible?
if yes, i think that mycroft needs to be trained with several recordings from different people and voices before. do you know how many recordings it could need? has someone already tried this?
thank you
","
you’d want a lot of recordings from as wide a range of users as possible, 50 is a good start.  it really depends on how many different users you have and having their vocal characteristics matching your model.

one approach that the rhasspy crew are trying out is to use the output of a broad range of tts voices as training data. i doubt it will be as good as collecting real samples from a diverse group, but it requires much less work to get setup.

jarbas and i used this as well, it’s not a bad idea if you can get the pronunciations correct.

add some background noise and pitch shifting and you can double your samples

hi guys, i made my train. for now i use only my voice for test, and i use this guide:
https://github.com/mycroftai/mycroft-precise/issues/94
after this i try to put all together in pycroft but seam not work. i found a solution in this link (i used precise 0.3.0 for make ):
https://community.mycroft.ai/t/precise-wakeword-not-working/8140
i need your support again for some troubles with precise on custom skill.
1- some times ago, i created on raspberry pi 3b my dataset with the custom wake word “hey jerry” and then i modified the configuration file with the following instruction:
$  mycroft-config edit user
{
“max_allowed_core_version”: 20.2,
“lang”: “it-it”,
“skills”: {
“auto_update”: false,
“blacklisted_skills”: [
“mycroft-wiki”,
“mycroft-alarm”,
“mycroft-audio-record”,
“mycroft-date-time”,
“mycroft-npr-news”,
“mycroft-singing”,
“mycroft-timer”,
“mycroft-hello-world”,
“mycroft-weather”,
“mycroft-personal” ],
“priority_skills”: [
“jerry-data-ora”,
“jerry-chi-sei”
]
},
“listener”: {
“wake_word”: “ehy_jerry”
},
“hotwords”: {
“ehy_jerry”: {
“module”: “precise”,
“local_model_file”: “/home/pi/mycroft-precise/ehy-jerry.pb”,
“phonemes”: “jh eh r iy .”,
“threshold”: 1e-18
}
}
}
2- after this, i restarted mycroft, but i read from the file voice.log: “a lot of error in alsa lib conf.c 4568 and conf.c 5047”:
2020-07-09 15:58:17.134 | info     |  9112 | mycroft.client.speech.listener:create_wake_word_recognizer:323 | creating wake word engine
2020-07-09 15:58:17.146 | info     |  9112 | mycroft.client.speech.listener:create_wake_word_recognizer:346 | using hotword entry for jerry
2020-07-09 15:58:17.152 | info     |  9112 | mycroft.client.speech.hotword_factory:load_module:403 | loading “jerry” wake word via precise
2020-07-09 15:58:19.202 | info     |  9112 | mycroft.client.speech.listener:create_wakeup_recognizer:360 | creating stand up word engine
2020-07-09 15:58:19.207 | info     |  9112 | mycroft.client.speech.hotword_factory:load_module:403 | loading “wake up” wake word via pocketsphinx
2020-07-09 15:58:19.375 | info     |  9112 | mycroft.messagebus.client.client:on_open:114 | connected
3- today, when i pronounce “hey jerry!”…nothing happens and i don’t receive feedback from the log file. if i restart it again (mycroft-stop all and mycroft-start all), mycroft catches my wake word, and in the log file i read this:
2020-07-09 16:03:06.027 | info     |  9672 | main:handle_wakeword:67 | wakeword detected: jerry
playing wave ‘/home/pi/mycroft-core/mycroft/res/snd/start_listening.wav’ : signed 16 bit little endian, rate 48000 hz, stereo
2020-07-09 16:03:06.474 | info     |  9672 | main:handle_record_begin:37 | begin recording…
2020-07-09 16:03:09.359 | info     |  9672 | main:handle_record_end:45 | end recording…
2020-07-09 16:03:10.716 | info     |  9672 | main:handle_utterance:72 | utterance: [‘chi ti ha creato’]
4- so, when i restart mycroft, the custom wake word works and then the skill too, but at the end of the session, the wake word seems “turning off” and, i have to restart all the process explained before to continue. so sometimes mycroft and precise catch the wake work, other times not.
5 - using the tool mycroft-cli-client, i notice that the volume rises when i pronounce “hey jerry”.
i also tried “alsamixer” to raise and lower my microphone volume (logitech usb with integrated camera).
6 here, another example for the log file, that i receive when the wake word is caught for a while and then nothing more:
2020-07-09 16:21:52.576 | info     | 10501 | mycroft.session:get:74 | new session start: 5663591a-4c5c-4053-9c89-501354c33f43
2020-07-09 16:21:52.586 | info     | 10501 | main:handle_wakeword:67 | wakeword detected: jerry
playing wave ‘/home/pi/mycroft-core/mycroft/res/snd/start_listening.wav’ : signed 16 bit little endian, rate 48000 hz, stereo
2020-07-09 16:21:53.613 | info     | 10501 | main:handle_record_begin:37 | begin recording…
2020-07-09 16:21:55.538 | info     | 10501 | main:handle_record_end:45 | end recording…
2020-07-09 16:21:56.896 | info     | 10501 | main:handle_utterance:72 | utterance: [“puoi darmi un po’ d’acqua”]
2020-07-09 16:22:29.611 | info     | 10501 | main:handle_wakeword:67 | wakeword detected: jerry
playing wave ‘/home/pi/mycroft-core/mycroft/res/snd/start_listening.wav’ : signed 16 bit little endian, rate 48000 hz, stereo
2020-07-09 16:22:30.062 | info     | 10501 | main:handle_record_begin:37 | begin recording…
2020-07-09 16:22:33.281 | info     | 10501 | main:handle_record_end:45 | end recording…
2020-07-09 16:22:34.719 | info     | 10501 | main:handle_utterance:72 | utterance: [“puoi darmi un po’ d’acqua”]
2020-07-09 16:23:06.927 | info     | 10501 | main:handle_wakeword:67 | wakeword detected: jerry
playing wave ‘/home/pi/mycroft-core/mycroft/res/snd/start_listening.wav’ : signed 16 bit little endian, rate 48000 hz, stereo
2020-07-09 16:23:07.383 | info     | 10501 | main:handle_record_begin:37 | begin recording…
2020-07-09 16:23:12.263 | info     | 10501 | main:handle_record_end:45 | end recording…
2020-07-09 16:23:13.714 | info     | 10501 | main:handle_utterance:72 | utterance: [“com’è più comodo muoversi a roma”]
2020-07-09 16:23:41.141 | info     | 10501 | main:handle_wakeword:67 | wakeword detected: jerry
playing wave ‘/home/pi/mycroft-core/mycroft/res/snd/start_listening.wav’ : signed 16 bit little endian, rate 48000 hz, stereo
2020-07-09 16:23:41.602 | info     | 10501 | main:handle_record_begin:37 | begin recording…
2020-07-09 16:23:43.726 | info     | 10501 | main:handle_record_end:45 | end recording…
2020-07-09 16:23:45.084 | info     | 10501 | main:handle_utterance:72 | utterance: [‘di cosa sei fatto’]
2020-07-09 16:24:10.693 | info     | 10501 | main:handle_wakeword:67 | wakeword detected: jerry
playing wave ‘/home/pi/mycroft-core/mycroft/res/snd/start_listening.wav’ : signed 16 bit little endian, rate 48000 hz, stereo
2020-07-09 16:24:11.163 | info     | 10501 | main:handle_record_begin:37 | begin recording…
2020-07-09 16:24:12.707 | info     | 10501 | main:handle_record_end:45 | end recording…
2020-07-09 16:24:13.922 | info     | 10501 | main:handle_utterance:72 | utterance: [‘chi sei’]
7 - i did another test too. i had recorded the tone of my voice that precise catches and i had played it on my computer. same results: sometimes it recognizes the wake word, other times not.
i can’t understand what happens and where is the error.
could you please help me with this? is there a way to do a mycroft-precise debug, so i could understand why it doesn’t’ catch the wake word?
thank you!

did you model under .2.0 of precise or .3?  have you tried using precise-listen to evaluate your model in a standalone capacity?  how large was your dataset?
if you’re using custom models you need to turn on wake word saving so you can track false activations and remodel with those to improve it.

i used precise .3 and put precise .3 under mycroft. in listner i have a perfect recognise, all the times.

how large was the dataset you used?
"
318,picroft jabra 510 mic speaker issues,support,"
hi all,
i was hit by the dreaded jabra issues that seems to affect a number of people, it would work fine initially but would eventually stop. i’ve managed to get to a point where both the speaker and the microphone are reliably working.
currently i’m running the 2020-01-10 stable image, fully updated on a raspberry pi 3b+
first i disabled the onboard sound by editing /boot/config.txt and changing the following line:
dtpatam=audio=on
to
dtpatam=audio=off

next i edited the /etc/mycroft/mycroft.conf file to look like this:
{
   ""play_wav_cmdline"": ""aplay %1"",
   ""play_mp3_cmdline"": ""mpg123 %1"",
   ""enclosure"": {
      ""platform"": ""picroft""
   },
   ""tts"": {
      ""mimic"": {
         ""path"": ""/home/pi/mycroft-core/mimic/bin/mimic""
      }
   },
   ""ipc_path"": ""/ramdisk/mycroft/ipc/""
}

next issue was the speaker showed a status of suspended when entering the command
pactl list sinks short
to resolve this i edited /etc/pulse/default.pa and commented out this line:
load-module module-suspend-on-idle
somewhere along the line the speaker became muted, there wasn’t an obvious reason i could determine nor an obvious indicator for this as everything looked fine in alsamixer but issuing the amixer command showed [off] beside the master output volumes.
i un-muted the speaker with
amixer master unmute
finally, the microphone does not work after a reboot, to fix this i have to run the following command:
mycroft-start all reboot
i haven’t found a permanent solution for this last microphone issue but as it doesn’t reboot often it’s not a big deal.
hopefully this might help someone else facing the same issues.
","
i noticed on my pi4 dtpatam=audio=off actually caused problems and think you also need to modprobe blacklist it.
open /etc/modprobe.d/raspi-blacklist.conf and add blacklist snd_bcm2835

but prob best way is just to create  a /etc/asound.conf
i usually just molest the respeaker asound.conf to my device.



github



respeaker/seeed-voicecard
2 mic hat, 4 mic array, 6-mic circular array kit, and 4-mic linear array kit for raspberry pi - respeaker/seeed-voicecard






# the ipc key of dmix or dsnoop plugin must be unique
# if 555555 or 666666 is used by other processes, use another one


# use samplerate to resample as speexdsp resample is bad
defaults.pcm.rate_converter ""samplerate""

pcm.!default {
    type asym
    playback.pcm ""playback""
    capture.pcm ""capture""
}

pcm.playback {
    type plug
    slave.pcm ""dmixed""
}

pcm.capture {
    type plug
    slave.pcm ""array""
}

pcm.dmixed {
    type dmix
    slave.pcm ""hw:seeed2micvoicec""
    ipc_key 555555 
}

pcm.array {
    type dsnoop
    slave {
        pcm ""hw:seeed2micvoicec""
        channels 2
    }
    ipc_key 666666
}

you can get the pcms of hardware through aplay -l or aplay -l and same with capture devices arecord -l or arecord -l
maybe just keep inbuilt but just set the defaults in a /etc/asound.conf by just editing slave.pcm ""hw:seeed2micvoicec"" and pcm ""hw:seeed2micvoicec"" to your hardware.
ps dtpatam=audio=off ? dtparam=audio=off but prob just create /etc/asound.conf
don’t know how the jabra lists maybe just usb sound card if usb dunno if bluetooth but that can be a whole other problem, but prob could with bluez-alsa
it might be where and how you start the mycroft service as the soundcard service as i noticed starts very late in the boot order.

thank you very much for this guide. i just changed the mycroft.conf and added the load module command in pulsaudio and my jabra 710 does now work perfectly! i am using the stable version
"
319,intranet voice calls babyphone function,skill suggestions,"
skill name: voice call skill / babyphone skill
user story:
as a lazy person, i’d like to make a voice call from the mycroft device in my room to another one in the living room for example. to respect privacy, this connection has to be confirmed by both people. maybe a permanent voice connection to a child’s room could be used as a babyphone.
the mark ii will have a screen, right? so this could even be extended to video chat.
what will the user speak to trigger the skill?
for example “call the living room” or “call my brother” (mycroft would have to know which device is in my brother’s room, of course). and then of course “end call” or similar phrases.
what phrases will mycroft speak?
for example, “{family member} wants to talk to you. should i establish the connection?” or “incoming call from {room}. do you accept it?”.
","
currently the mesh-skill will accomplish some of this. i think the idea of opening up a chat between two devices is an interesting idea that could be completed with the mesh skill, or possibly @jarbasal hive mind.

i’m using @pcwii mesh-skill to send short messages from room to room (e.g: “the dinner is ready”, “need some help at the kitchen”), but there are not voice calls, as we’re using mycroft tts engine.
i also would like to see that feature in action, but -being a complete ignorant on that subject- i don’t know if it is feasible, as per my understanding, you cannot let the microphone listening and recording during too many time (or that was the reason why a dictation skill wasn’t feasible)




github



jarbasskills/skill-voip
voip for mycroft. contribute to jarbasskills/skill-voip development by creating an account on github.






yes, i completely forgot your skill @jarbasal…  
i even forgot to install on my computer to test it out
"
320,mark ii update june 2020,general discussion,"
originally published at:			https://mycroft.ai/blog/mark-ii-update-june-2020/
being part of the journey is one of the reasons that angel investors put money into startups and crowdfunding backers support projects.  sometimes that journey is simple and straightforward, but when it comes to developing new technologies it is often a rollercoaster.
crowdfunding platforms are generally pretty clear about where they stand on this thesis.  the early team at kickstarter even published a page titled “kickstarter is not a store” where they explain to backers what projects are and are not.
on the spectrum of journeys from “sean’s trip to the market” at one end and “frodo’s trip to mordor” on the other, mycroft’s journey is definitely closer to middle earth than suburban london.
over the past couple years we’ve tried to communicate this.  we’ve had some false starts on the hardware, fundraising challenges and detours into enterprise services to make ends meet, but hopefully we’ve been able to communicate effectively along the way.
after all, backers bought a ticket on our rollercoaster, it wouldn’t be fair if we didn’t communicate that empty stomach feeling you get when things don’t go as planned.
new ceo, new roadmap
in case you missed it, mycroft hired a more experienced ceo early this year.  our new ceo shares our original vision, but has much more experience with delivering large complex software projects and complex hardware.
under our original roadmap and release cycle mycroft went to alpha on may 20, 2016 and beta on march 7, 2018, but it turns out our leadership team was a bit aggressive in defining what alpha and beta mean.
among other things the mycroft stack lacked easily installable packages, automated updates for non-mycroft enclosures and, importantly, a comprehensive testing framework.  the result has been a stack that is feature rich, but buggy, hard to install and difficult to maintain.
so we’re taking our change in leadership as an opportunity to rewind our messaging revising our roadmap to production.
hardware
michael has been very clear that he doesn’t see mycroft as a hardware company.  our smart speakers are reference devices for mycroft-core in the same way that the pixel line of smartphones are a reference device for android.  our plans are to eventually enable thousands of software developers, automakers, robotics companies, device manufacturers and even drone companies to deploy voice technology in their products.
many use cases for mycroft, however, do require a reference speaker.  organizations like hotels and hospitals are eager to explore voice assistants as supplementary technologies for concierge desks and rooms, nursing stations and patient bedsides, but to do so they need a physical device.
so we are working hard to make the mark ii that device.  a reference device for backers that can also be used by schools, hospitals and hotels to serve their customers.
so where are we in our quest to ship the mark ii?  the good news is that we appear to be past the big drops and loop-de-loops.  today we’ve got more experienced leadership and are moving down the path to delivering on our promises.
based on our new ceo’s experience, this is where the company is in the development process:

mark ii hardware stages658×771

we have progressed from using commercial off-the-shelf parts for the proof-of-concept, to developing a pcb for the manufacturable prototype.  this decision was made for a variety of reasons, but both performance and cost were significant factors.
we’re not ready to release a timeline for the first dfm prototype pcbs, but the design is well underway and under active development using a low-risk process of integrating the off-the-shelf components of the poc into a single pcb.

mark ii daughterboard render600×536

the hardware production process should move swiftly once we’ve verified the pcb functionality and integrity. in fact, we anticipate that the long-pole in shipping a stable product is going to be the software, which is where the majority of our development team is focused.
software
despite more than five years of effort, 4,400 github stars on our primary repository, contributions from hundreds of software developers and thousands of other contributors our software remains early.
this is understandable given the complexity of the problem we’re trying to solve. after evaluating our software stack, we’ve decided to rewind our messaging to be more in line with industry standards.  as a result mycroft is back in alpha where it will remain until we solve issues related to performance and content.
updates
we try to balance being transparent and forthcoming with being too spammy.  hopefully we get it right, but if you feel you’re not getting enough information, please take a moment to subscribe to our e-mail list or check in on our blog from time-to-time.
as we stated earlier this year, the expectation for mark ii delivery stands at 2021. when we can give you a firm timeline, we will. if we can make it happen sooner, we will.  progress is dependent on staffing and distractions like patent trolls, which in turn affect, and are affected by, fundraising. but for now we can commit to keep you updated on our actual progress, and setbacks, on the path to fulfilling this kickstarter.
building new technology is quite a ride.  thank you for joining us on the journey.
","
joshua, thanks for the update. can you elaborate a little bit more on the hardware part, please?
looking at the pcb drawings i conclude that the sbc will be raspberry pi - 3 or 4?
from the way the rpi connector is aligned on the pcb and the led pixel ring on the backside i guess there will be a new hardware enclosure?
the daughterboard shown is obviously for audio output only (dac and class-d amplifier ics). what about microphone input, will there be another custom pcb or will the final design still use the respeaker 4mic usb board?
thanks in advance.

everything needed will be on board ( dac, mics, leds, buttons ). it is indeed a pi4 daughter board.
you are correct that we’re refining the enclosure.  it will continue to have the same overall configuration ( stereo speakers, screen, mic, physical microphone toggle ), but will likely have a different shape to accommodate the daughterboard.
we haven’t made any decisions on the specific shape, but are leaning toward a cube configuration.  there are so many tradeoffs between form factor, manufacturability, features and performance that the form needs to be driven by the function.

thanks for the update, that was long waited, but let me be a bit critical here.
i’m a bit concerned about the departure of steve and ake lately, it looks to me that we’re leaving behind the tech professionals who could boost mark ii launch. it seems to me mycroft inc is leaving behind engineers and just keeping ceos and community managers, so i don’t see who’s really working in deliver a real product.
when i saw michael lewis taking the ceo’s hat, my first impression after reading his profile was become afraid of the mycroft’s future, as it seems michael is very good at selling whatever it touches to someone else. and an irrational fear arose: will be mycroft ai sell to a big player who still doesn’t have any assistant.
i know some of you have spent many years of your life in this project, it has been long, hard, and very difficult, and perhaps if someone can ease your burden with some millions and forget it all… well, everyone is human and can get burnt. i’m an investor myself, and i invested in mycroft because i believe in an open source ai, not to gain revenue.
i guess if your next movement is to introduce several new engineers instead of even more leaving the ship (if still remains any), will make change my mind and air out my fears.
as for now, i’m probably wrong and there are still a few engineers working on it, i hope so. but i don’t really know who is really working on the mark ii at the moment. in fact, i’ve lost sight of any “pure” tech reference of mycroft inc, and it feels as is for now just @gez-mycroft is doing what he can, but to be honest, i saw more improvements lately on the community development field than in the core, and i saw community mark ii assemblies done just by one member. sure, that community 3d printed mark ii is not a ready-to-sell product, but it has been done by just one person in a handful of weeks, so i don’t really get why mycroft inc, with years of delay and several electronics and software engineers, didn’t advance in that field, just delaying and changing of mind of every aspect every few months.
don’t get me wrong, i’m with you since the beginning of this journey, and i will probably stay with you if nothing changes because i share your vision, just becoming concerned because it seems to me that talented people are leaving and with them, a great hole that nobody is filling it, and michael’s experience can be awesome, but i don’t see him developing or assembling anything.

@malevolent - ake @forslund was with us for 5 years and spent the whole time working in his teeny tiny attic all by himself.  from what i understand he wanted to go to work with other people every day and i don’t blame him.  he’s now working on new challenges, but he’s continued to be involved as a volunteer and i hope that will continue.
i’m not going to comment on any of the other staffing changes other than to say that our team has always had dedicated engineers who are passionate, capable and hard working.  i wish them well.
michael is a much better engineering leader than i’ll ever be.  hell, the whole company now is managing projects via jira and i can barely work a single ticket into my workflow.  he’s lead large teams building really cutting edge stuff and he’s already stepped up our engineering game in significant ways.  as an example, the voigt-kampff framework was much needed and i’d have never dedicated the resources to it.
a majority of our team continues to be technical.  i’m an aerospace engineer, michael’s a computer scientist and engineer, @chrisveilleux chris veilleux is a computer scientist, ken smith is a very experienced programer and kris gesling is…apparently…a social worker ( i’ll admit that today is the first time i’ve ever looked at @gez-mycroft’s credentials ).  explains how he is so patient the the community.  seriously though, gez is a pretty talented developer in his own right and contributes a lot of code and expertise to the stack.  finally, there is derick schweppe who is an industrial designer by trade, but has stepped up to manage all of our branding, promotion, design for manufacturing, prototyping, acoustics and user experience work.  he probably does more, but that is just off the top of my head.  without @derick-mycroft’s  willingness to step up and become a solidworks guru we’d never have delivered the mark i let alone the mark ii.
the only non-technical people on staff are johnny demaddelna who basically runs all of the operations ( contracts, business development, government paperwork, shareholder paperwork, legal, etc. ), emily st. martin who aspires to be an attorney for some reason, but for now keeps michael operating efficiently and kris adair who is part time, a co-founder and runs both the accounting effort and all of our social media and marketing.
the reality is that we’re late because we fucked up.  the original vendor for the mark ii hardware couldn’t deliver on schedule and our original software development effort didn’t result in a sable software stack that could be demonstrated reliably, much less sold.  that led to a crisis of confidence in the investor who signed on to provide $5m in funding ( which would have delivered the mark ii in early 2019 ).
if our software game had been on point by november 2018 and our hardware vendor had been able to deliver a working pcb we’d have delivered the mark ii 18 months ago, grown our year-over-year sales in 2019 and would be shipping both retail and wholesale globally.
ultimately the responsibility for all of this is mine.  i didn’t implement a “tight” fundraising strategy for our series seed which left me distracted by constant fundraising rather than focused on creating a product that didn’t suck.  i closed a series a anyway, but we weren’t as ready for our partner as the technology team claimed and it blew up in the company’s face.
so i replaced myself with someone more capable and am working to support my replacement.
i do want to address the “michael builds and sells companies” perception though.  i’m not sure what propelled michael to start stellar and cryptic, but the way he explains it he was motivated by the problem he was solving and the people he was working with, not by the payoff.
he’s been crystal clear why he got involved with mycroft.  he was a backer of our kickstarter and ended up as an investor because he believes that voice technology should be available without the ubiquitous surveillance that infests the technologies that big tech is shipping.  he doesn’t need the money.  not only is he the lowest paid employee on staff, his foundation has provided more than $650,000 in funding over the past 6 months.  other than taking his responsibilities to our investors seriously, i’ve never gotten the impression that he’s motivated by a future payday.  he was happily ensconced in his family life, sponsoring interesting projects at the university, contributing to his local maker space and teaching art to kindergarteners at his kid’s school when i talked him into taking the reigns at mycroft.
and he is absolutely moving the ball forward with both software and hardware development.  michael’s been instrumental in implementing a software development process that will ( one day soon ) result in a stack that doesn’t suck and can be supported long term.  our current hardware vendor comes from michael’s personal network and is making rapid progress toward a pcb that we can use on the mark ii  ( and the mark iii when we’re ready ).
honestly i’m probably the most useless person at mycroft right now.  with no product on the market and fundraising passed to michael i’m in a bit of a holding pattern.  i can spend time landing new enterprise business or getting retail distribution, but it won’t matter until we have mark ii units in boxes and ready to ship.  i could help with software development i guess, but it takes so long to spin up on the process and i’m distracted so often that it is very difficult to contribute meaningfully.  i’m also extremely burned out after 5 years of startup.  it’s been years of long hours, stress, uncertainty, self doubt and taking big financial risks that may one day sink my family.
and the patent suit hasn’t helped.  it is sucking up a huge amount of everyone’s time.  it’s also very expensive.  we’ve now got four separate law firms working on the case.  being principled and fighting bogus claims is our responsibility and we take it seriously.  we’ll fight to the bitter end and beyond ( if the trolls think they’ll ever get a penny they are out of their freaking minds ), but if anything sinks the company i expect it will be the patent litigation.  we’ll probably spend more on the bogus patent suit than we pre-sold during crowdfunding.
that said there are a lot of good things going on.  our partners at texas a&m are about to deliver the final deliverable on our nasa sbir phase i grant which will free us up to pursue a phase ii award.  our upcoming fundraising effort is looking very promising.  michael’s becoming a solid pitch man and his passion for our community shines through.  we’re targeting enough money to operate for the next 2 years with a significantly larger staff and i think it will happen.
there is huge demand for what we are doing.  i suspect if i had 10,000 mark ii devices to sell i could do it in a month.  between individuals who want a private, open, customizable experience and companies that want a voice stack, but eschew big tech there is probably $3b in pent up demand.  we just need to get a quality product out there for them to purchase.
keep the faith.  startup is a marathon, not a sprint.  we’ll keep after it until we get there.

wow, thank you for your long and crystal clear answer, @j_montgomery_mycroft.



 j_montgomery_mycroft:

i do want to address the “michael builds and sells companies” perception though. i’m not sure what propelled michael to start stellar and cryptic, but the way he explains it he was motivated by the problem he was solving and the people he was working with, not by the payoff.
he’s been crystal clear why he got involved with mycroft. he was a backer of our kickstarter and ended up as an investor because he believes that voice technology should be available without the ubiquitous surveillance that infests the technologies that big tech is shipping. he doesn’t need the money. not only is he the lowest paid employee on staff, his foundation has provided more than $650,000 in funding over the past 6 months. other than taking his responsibilities to our investors seriously, i’ve never gotten the impression that he’s motivated by a future payday. he was happily ensconced in his family life, sponsoring interesting projects at the university, contributing to his local maker space and teaching art to kindergarteners at his kid’s school when i talked him into taking the reigns at mycroft.


i have no doubt michael’s expertise will improve and boost how mycroft inc manages all the production, besides he must be tough when negotiating, so it’s great to have it onboard. as i said, i felt an irrational fear, because when we met each other in barcelona, i asked the very same question to you, and you stated you are committed to the vision of an open source ai as i have. so i guess michael’s vision is the same as yours.
on the other hand, i didn’t even know the voigt-kampff framework has been released because of michael, so that’s why regular updates, for insignificant they can seem, are important to us.
on my first post, i was obviously speaking just about my own fears, but surely many other community members, backers and tiny investors shared my own concerns about this awesome entrepreneur. you already aired out my fears, i hope this will be left as clear as for any other who could think about that subject.



 j_montgomery_mycroft:

a majority of our team continues to be technical. i’m an aerospace engineer, michael’s a computer scientist and engineer, @chrisveilleux chris veilleux  is a computer scientist, ken smith  is a very experienced programer and kris gesling is…apparently…a social worker ( i’ll admit that today is the first time i’ve ever looked at @gez-mycroft’s credentials ). explains how he is so patient the the community. seriously though, gez is a pretty talented developer in his own right and contributes a lot of code and expertise to the stack. finally, there is derick schweppe who is an industrial designer by trade, but has stepped up to manage all of our branding, promotion, design for manufacturing, prototyping, acoustics and user experience work. he probably does more, but that is just off the top of my head. without @derick-mycroft’s willingness to step up and become a solidworks guru we’d never have delivered the mark i let alone the mark ii.


i didn’t know about ken smith, i did take a look on his linkedin profile, looks like he’s the latest incorporation to the team, i hope he can be engaged by this project as others did before him!
what @gez-mycroft is doing is incredible, it was presented as @kathyreid successor, but, with all my love to kathy, her padawan has surpassed the master in many fields: not only deals with all the community members, and created a comprehensible documentation, but it also links community efforts with core improvements (let’s rock lingua_franca a bit harder!) and finally he codes and improves some annoying skills (nprgrrr skill, for instance)
the other members are as old as the project itself, but they always remained in the dark, so they are “hidden” to community members. and it’s a pity. yes, we agree they must work on mycroft, not creating bonds with community, but mycroft rely heavily on its community, and would be great some “daily” feedback and responses only a developer can offer. those were done by ake and steve back in their days, but now we -the community- can feel somehow orphaned (i.e.: if we need an urgent pr, or we noticed skills translations were quite outdated, ake answered in a heartbeat and arranged it quite fast, now this will fall in gez’ shoulders, who probably will need to escalate to whom has to do it, slowing the community feedback)
i can see on the career page you’re looking for two senior engineers and another manager, so i can expect the core team will be expanded soon, although you’re limiting to usa residents, which can let out very talented and committed developers around the world.



 j_montgomery_mycroft:

and he is absolutely moving the ball forward with both software and hardware development. michael’s been instrumental in implementing a software development process that will ( one day soon ) result in a stack that doesn’t suck and can be supported long term. our current hardware vendor comes from michael’s personal network and is making rapid progress toward a pcb that we can use on the mark ii ( and the mark iii when we’re ready ).


that’s what most of us want to hear about the project. i won’t ask for an eta, but i hope this stack will be ready sometime later this year, and be ready on 2021 when the m2 will be launched at last.



 j_montgomery_mycroft:

keep the faith. startup is a marathon, not a sprint. we’ll keep after it until we get there.


i keep the faith, the message has been received crystal clear for me. in fact, i preferred to start myself this discussion, and i really appreciate your sincere and fast answer.
keep doing the good job!  

i am really busy with other things lately, but reading back here and there, but have to compliment @j_montgomery_mycroft for that blunt but very honest post.
link this thread on kickstarter and people will appreciate it.

i’ve long been a fan of publishing all of our updates across all of our channels ( forums, kickstarter, indiegogo, facebook, crunchbase, angellist, etc. ), but apparently their is a group of backers on kickstarter/indiegogo who consider this spam and get hostile about it.
there is also a group that gets angry no matter what.  they basically view kickstarter as a store and have gotten worked up because the “product” they bought wasn’t shipped on time.  lots of hostility and hate.
so we try to manage that channel ( and indiegogo ) with a little more sensitivity to that audience.
if you ask the team they’ll tell you that i’ve brought up the “lets just be blunt” communications strategy dozens of times and been ( subtly and with great sensitivity to my feelings ) overruled.
since i trust the team we’ve got working with the community, crowdfunding backers, and social media, i’ve deferred to their judgement on the matter.
i don’t even get carte blanche to share my opinion that the patent trolls are blood sucking, inbred, anti-social, wastes of space.  they whined to the judge that i was in some way harassing them ( i’m not ) and got my original post sensored.  have a peek at the way back machine or the press coverage to see what was removed.
i do have nearly complete freedom with investor updates and i try to make them comprehensive.

i completely appreciate full transparency, or being blunt. i know it is difficult, but ignore the loud few on kickstarter and the like please 

i generally try to discount the shriller of the kickstarter backers, but they are still customers.  companies ignore their customers at their peril.  the shrillest customers are often helpful indicators of what the company is doing wrong ( in this case - being very late with the product ).

congratulations on a fabulous piece of spin!
i have bitten my tongue many times when reading updates on the project, primarily because the goals of an independent non-network based voice system are so important for the future.  critically important.
and yet…
every time i have tried to make the deliverables (on a pi) work, they fail.  now, i’m no software wizard, but i do have a modicum of experience and a reasonable degree of determination.  i also own a hardware/software company that designs, manufactures and sells a technically complex product, so have a reasonably good understanding of most, if not quite all, the issues.
frankly, i would like to hear directly from the new ceo - a loooooong overdue change btw -and get a straight version of what the future holds.  even if it is “you won’t be getting a mkii” then we can move on with a different approach - perhaps jasper is the platform that we can (should?) put our efforts into.
your credibility, and authority, vanishes every time i hear something like “the april update was a bit late so decided to reset for june.” - “the june update was a bit late so decided to reset for october”.  i don’t know if you understand how risible a throwaway comment like that is.  and a glib throwaway of two years late. thirty years ago i moved on development managers who reported progress in such a way. with todays systems, processes and team management techniques, the days of lame excuses like that, late in the day, are long gone.
yes, projects are hard.  and this may be leading edge technology, but not bleeding edge.  at least not two years later.  and the last 10% is 90% of the struggle.  but we don’t appear to be even close to the stage of refining and polishing, probably not in 2020.
i would ask you to step completely out of the way and hope that in doing so, competent managers can step up and deliver on the laudable goals of the project.  i applaud your efforts in starting - and sticking with (it’s easy to give up) - the project.  but it’s time to let the juvenile fly the nest without parents.
cheers

hi graham, i wanted to clarify that it’s often me responding on kickstarter and i posted that we were resetting for june in terms of the updates. i wanted to let people know when to expect further communication rather than just ghost the community. the way the platform is setup you can’t really have multiple users so it all comes from joshua but they are updates from the company.
we try to be as open and honest with where we are at. if we were not going to deliver the mark ii, we would have said this a long time ago. there is absolutely zero benefit to us lying about this. as we’ve said, the mark ii won’t be delivered in 2020 but we’ll be keeping everyone updated on the progress and provide a clearer timeline for delivery when we can.

see, @gez-mycroft with such diplomacy. putting his social work skills to use.  he’s awesome, right?
i do want to clarify that, as harry truman said, “the buck stops here”.
ultimately i’m responsible for the late delivery and anything else the company has got wrong including late updates and any inconsistent messaging.
graham has some valid criticisms and i acknowledge his feelings on the matter.  like many backers he is expressing frustration.  we’re listening.  we’ll continue to work to do better with communication and progress.

steve and ake are both still involved, though the manner and depth of that has changed.

one can be involved by volunteering or getting paid, the former means they pop up when they want or have time to, just as me or you, while the later means they are there every day -and, in the case of mycroft- of the week at any time.
anyway i didn’t say they weren’t, or at least, that wasn’t what i wanted to express. i meant they left mycroft and nobody replaced their vacant (later i knew ken smith is probably replacing ake because he started 2 months ago and being a developer, and mycroft is hiring two engineers more).
"
321,mmap cannot allocate memory,none,"
hi everyone ! i’m here because i’m facing an issue. i’m actually developping a youtube skill and sometimes it’s  working, and other times it’s not.

sans titre1920×1080 154 kb

as you can see in the screenshot up here, first line show that youtube is loading fine and lines after tells me that mamp() cannot allocate memory. after some alsa bug, skill just stop because of errors.
anyone ever facing this issue with mmap ? i think this is why alsa bug and my skill stop. i’m new to skill development but i’m really enjoying this ! community is awesome btw
","
oddly enough, i ran into this exact error when i installed vlc as a requirement on a skill i am working on. not sure if this is an issue with vlc or something else.

it seem like when i update my skill and try to load youtube the error shows up. i need to reboot mycroft in order to get this working again. working with vlc too so ther issue could come from vlc yep
"
322,problem in setup,general discussion,"
i am getting problems after executing this -
./start-mycroft.sh debug
here i am attaching the related screenshot of the error i am getting. plz help.

screenshot from 2020-07-08 14-35-451366×768 143 kb

","
have you completed the setup?
what have you tried so far?  what was the result of that effort?
"
323,testing and feedback rest countries,skill feedback,"
rest countries skill

data about countries from http://restcountries.eu/
description
country population, area, language, borders, currency, timezone, capital, denonyms…

759×788


766×792


761×782


1050×785


1052×788

usage

“what is the size of portugal”
“what are the borders of portugal”
“what is the capital of portugal”
“what is the currency of portugal”
“what is the language of portugal”
“what is the population of portugal”
“what is the timezone of portugal”
“what are the people that live in portugal called”
“where is portuguese spoken”
“where is portugal located at”
“how many countries do you know about”
“what are the countries in europe”

todo

use units from self.config_core when speaking country area
improve timezone spellings to use natural language
better region matching (it’s ok, but plots dont always match perfectly the answer)
use .dialog files everywhere, currently country lists are lazy responses
websettings for mapstyle
improve requirements handling, cartopy is optional and only needed for gui

github



github



jarbasskills/skill-countries
http://restcountries.eu/  skill. contribute to jarbasskills/skill-countries development by creating an account on github.





",
324,skill versioning,general discussion,"
hi everyone,
i would like to ask a question about skill versioning. i know that there is a settings file and it can be used to write info about skill. is there any specific setting for skill version or would it be useful to write skill version in the settings file ?
thanks.
",
325,how many android devs are here,general discussion,"
i’m developing the newest android core, and was just wondering how many devs are hanging around, to get some insight and discuss the project.
","
i haven’t done much android dev in a while, but i am familiar if you just want to bounce ideas off someone.

i actually have most of the core app for android developed, but i am sure that it could use some error-checking, and since i’m a junior dev it could probably use a touch up on good coding conventions. mostly, i’m trying to get a feel for who is around once i submit the pull request for it




github



tadashi-hikari/mycroftcore-android
mycroftcore on android as a native app. contribute to tadashi-hikari/mycroftcore-android development by creating an account on github.





that’s the link to my github. mind the ‘lack of activity’ because all the work i’ve done (the whole thing) doesn’t show until i have an accepted pull request (i rebased their app, and deleted everything, so it shared a merge-able git base)

i’d drop by https://chat.mycroft.ai, then on ~android channel, there you should find those who are interested/working on the android version

i’m in there as well. there just aren’t many android devs around. i’ve posted there before i posted here.

oh, i’m not there, so i didn’t see you there, sorry.
as android development is not a priority at the moment, but build a good core base, perhaps there is not much movement on that channel right now.
but hey, if you’re an android developer, probably you are in a good position to attract more experienced developers to work in this awesome project! 

yeah! i’ve been talking with skr8yn1ck, and my app is lined up to take over the existing core app. the speech recognition is working pretty well (on device), and the intent parser has been ported from padatious to a tensorflow network that i also have already working on device. i’m just refining the networks, and implementing the skill execution. it’s not that far off from being usable (but still very alpha), which is why i was trying to see who was around and interested.

cool! i cannot help at dev, but i would like to try out the android app when you want for betatesters!

for sure. i’ll be sure to let people know when it’s usable

@prof_birch i’ve got it starred and will take a look at some point. i am just now diving into mycroft and am focusing on skill development and room distribution first (as those are on my personal hotlist  )

looks like a cool project!   i have some android dev experience.

awesome. please, check it out. it’s still a work in progress, but i’m proud
"
326,beginner questions,general discussion,"
i have just finished installing mycroft on my raspberry pi 4. while i was unable to run the picroft image (it seems not supported on pi 4 yet), i successfully installed mycroft by cloning the core repo and setting stuff up by hand. i have then successfully paired the device, and while running the services in debug mode, i was also able to ask mycroft for basic stuff, e.g. the current time.
so while this basically seems to work, i have some beginner questions i could not figure out:


when doing the manual setup, there was no audio setup as compared to the picroft approach, and when asking for the time, the time was printed only in the cli, but not spoken via speakers. how do i tell mycroft which devices to use?


after the setup mycroft installed quite a few skills, most of which i uninstalled using the msm, however when starting mycroft later again, it seems to reinstall those. how can i get rid of skills i don’t want?


following the guide here, i have switched the language to german (“de”), set the tts to google with language “de-de”, and set stt to use a local deepspeech server (which i have running on port 8080). however, when i start mycroft again, the cli will print several attempts to install various skills, stating that they are not available in german, but then somehow seems to get stuck. there is no voice recognition, e.g. no reaction on the wake word. what am i missing here?


this is my config:
{
  ""skills"": {
    ""blacklisted_skills"": [
      ""mycroft-configuration.mycroftai"",
      ""mycroft-pairing.mycroftai""
    ]
  },
  ""stt"": {
    ""deepspeech_server"": {
      ""uri"": ""http://127.0.0.1:8080/stt""
    },
    ""module"": ""deepspeech_server""
  },
  ""tts"": {
    ""module"": ""google"",
    ""google"": {
      ""lang"": ""de-de""
    }
  },
  ""max_allowed_core_version"": 20.2,
  ""lang"": ""de""
}

for 3), this is the logs in the cli:
 13:47:07.287 | info     |  1117 | mycroft.skills.skill_loader:load:114 | attempting to load skill: fallback-wolfram-alpha.mycroftai
 13:47:07.364 | info     |  1117 | mycroft.skills.settings:get_local_settings:78 | /home/pi/.config/mycroft/skills/fallback-wolfram-alpha.mycroftai/settings.json
 13:47:07.393 | info     |  1117 | mycroft.skills.skill_loader:_communicate_load_status:280 | skill fallback-wolfram-alpha.mycroftai loaded successfully
 13:47:07.395 | info     |  1117 | mycroft.skills.skill_manager:put:80 | updating settings meta during runtime...
 13:47:07.398 | info     |  1117 | msm.mycroft_skills_manager | invalidating skills cache
 13:47:07.399 | info     |  1117 | msm.mycroft_skills_manager | building skillentry objects for all skills
 13:47:11.552 | info     |  1117 | mycroft.skills.skill_manager:send:64 | new settings meta to upload.
removing event mycroft-timer.mycroftai:showtimer
removing event mycroft-timer.mycroftai:showtimer
removing event mycroft-timer.mycroftai:showtimer
removing event mycroft-timer.mycroftai:showtimer
removing event mycroft-timer.mycroftai:showtimer
removing event mycroft-timer.mycroftai:showtimer
removing event mycroft-timer.mycroftai:showtimer
removing event mycroft-timer.mycroftai:showtimer
 13:47:20.743 | info     |  1117 | mycroft.skills.padatious_service:train:100 | training... (single_thread=false)
 13:47:21.186 | info     |  1117 | mycroft.skills.padatious_service:train:102 | training complete.
  ^--- newest ---^
history =======================================================================================    log output legend =============================================================== mic level ===
                                                                                                   debug output
                                                                                                   skills.log, other
                                                                                                   voice.log




input (':' for command, ctrl+c to quit) =========================================================================================================================================================
>

","
hey there,
the picroft image is based on raspbian buster, and does support pi 4. so wondering what happened when you tried to boot that image?
the picroft install script has a number of extra features for handling common hardware that’s not available in the bare bones mycroft-core install.


by default mycroft will use the default pulseaudio device, but this doesn’t always work flawlessly. there’s a general audio troubleshooting guide in our docs, but what sort of mic and speakers are you using?


so i’m assuming even the “blacklisted_skills” are reinstalling? is this the user level ~/.mycroft/mycroft.conf file? does this config file persist after a reboot? if the config itself reverts it means there’s a syntax error in there somewhere.


the lack of spoken audio may also exist if it can’t get a proper response from your tts service.
in terms of skills not working in german, the best place to ask about this would be our german language channel in mycroft chat.
i can’t see where the logs say the skill isn’t available in german, but you can find the complete logs at /var/log/mycroft/skills.log

thanks for your feedback.
regarding the picroft image, the pi would just not be reachable via wired lan. i did not have a proper hdmi cable so i was doing this headless, and could not find out what went wrong. i just read on the forums here that probably the pi 4 is still somewhat unsupported.
regarding the other points:

i found the audio troubleshooting guide also, and tried to adjust devices using the pactl tool. also i had to manually launch the pulseaudio daemon, but all that did not help.
nope, i did not have any blacklisted skills except the two mentioned above (which was my ~/.mycroft/mycroft.conf). the file persists between runs, so there is no syntax issue.

so the reinstallation of skills upon startup is the normal behaviour? should i just uninstall plus blacklist the skills i dont want? this is a bit confusing, especially since blacklisted skills seem to cause red error messages in the log (“failed to load”).
regarding german language support in skills, this is actually not an issue for me, i was just mentioning what i was seeing in the logs. i am a developer coming from snips/rhasspy, and i would try to implement my own skills anyway. i would not use any of the existing skills most likely.
so since really nothing helped at all, and i was unable to get mycroft back into a state where it would capture voice again, i reflashed the buster lite image and reinstalled mycroft. now i am back into the state where voice is detected, but no tts output can be heard.
steps i did:

clone git repo & run setup
launch mycroft/debug, microphone-meter is active
no audio is captured / nothing in the logs
run the audio test - it tells me pa_context_connect() failed: connection refused

run pulseaudio --start

run the audio test - it works, however no sound is played
adjust the audio sink using pactl

run the audio test - i can hear no playback
run paplay ./test/unittests/client/data/weather_mycroft.wav - i can hear the output
run mycroft/debug, say “hey mycroft, what time is it”. in the cli i see that the command is understood, and the cli prints the current time. no tts sound is played. i can hear the confirmation beep when the wake word is detected.

so there seems to be an issue with the tts. there is no errors in the logs, so i don’t know how to resolve this. in the mycroft.ai account settings when pairing the device i selected “british male” as voice. on my first installation i was using google.
 08:36:09.713 | info     |  6274 | __main__:handle_wakeword:67 | wakeword detected: hey mycroft
 08:36:11.500 | info     |  6274 | __main__:handle_record_begin:37 | begin recording...
 08:36:13.190 | info     |  6274 | __main__:handle_record_end:45 | end recording...
 08:36:14.573 | info     |  6274 | __main__:handle_utterance:72 | utterance: ['what time is it']
 08:36:19.613 | info     |  6268 | configurationskill | remote configuration updated
  ^--- newest ---^
history =======================================================================================    log output legend =============================================================== mic level ===
 what time is it                                                                                   debug output
 >> it's zero nine thirty six                                                                      skills.log, other
                                                                                                   voice.log

so, to summarize:
voice is captured successfully, but audio playback is only partially working / no tts is working.
regarding the installation, at least the step pulseaudio --start seems to be missing in the setup process / documentation, otherwise voice would not get captured.
how should i proceed?
"
327,programming mark1 directly from your computer no github needed,general discussion,"
hi folks,
after a year or so i have unpacked my mark1 again in an effort to automate my new house and learn some extra programming in the process  my goal is to set up the mark1 and be able to program it quickly without using github, since i don’t want to keep pushing and pulling stuff in order to test the functionality of it (i intend to use a lot of the hardware features of the mark1 so testing on my laptop is in many cases not an option). i am not a programming wiz by any means so right from the start i found myself running into some problems.
the mark1 itself is running fine, i installed the latest image for it and updated the software without any problems. to reach the files on the mark1 i tried using the ssh-extension of vscode (which i’m using as my default programming program) to connect to it, but it seems like jessie is to old to support it (i can ssh into it using the command line though). as an alternative i managed to mount the raspberry pi in my linux wsl that i run on windows so i can see and edit the code directly from there. the problem i’m currently facing with this approach is that vscode does not seem to recognize the mycroft libraries and i haven’t been able to find a lot of documentation about it online, so i’m kind of in the dark about all the functionalities the mycroft library provides.
to solve my problem i would like to tap into your knowledge and ask you the following questions:

do you think this is the right approach for me, or is there another approach that i didn’t think of to achieve the quick programming/debugging/testing i would like to have?
if so, is there anyone out there who might be able to help me with correctly using the mycroft libraries in vscode (or point me to a place where i can find the complete documentation)?

thanks in advance, looking forward to starting my project!
","
as an alternative to vscode you can try the theia-ide skill

thank you for your reply, that looks great! unfortunately the page states that mark1 is no longer supported in that skill 

sorry, its been a while that i used the skill on my mark-i and didn’t notice this change.
don’t know what resources you have available, but for mycroft development i currently use a ubuntu-18.04 vm running in virtualbox (on macos) - maybe that is an option for you?

i’m currently also using ubuntu 18.04 on a vm. my main problem is that i’m having trouble accessing the code on my mark1 for code-completion. this is not necessary for the execution itself since that still happens on the mark1 but it does allow me to check how i should program it. an alternative would be to have proper documentation somewhere online but i wasn’t able to find that as well.  an example of what i’m trying to do is to change the eye-color of my mark1 when it goes into sleep-mode. for this i need to know how to set it (by using some functionality of mycroft.enclosure), however i can’t seem to find which function/property to use.
how do you do your development exactly? do you have a program (like vscode) that uses autocompletion or do you have another way to check how to call upon mycroft’s functionalities?

in my ubuntu-vm i use vscode with a local mycroft instance (installed from the github repo). this setup won’t work when developing specifically for the mark-i enclosure of course.
as the theia-ide worked on mark-i in a previous version you might want to check out an older commit of the repository. maybe the author @andlo can help you out identifying the exact commit that will work?

if it just some simple coding that you want to edit on your windows pc but execute on the mark1, this is what i use;
https://blog.sleeplessbeastie.eu/2015/07/27/how-to-edit-files-using-notepad-plus-plus-over-ssh-file-transfer-protocol/

regarding your quest for documentation, here are two links that should be helpful, both found on the mycroft.ai website with some digging:
https://mycroft-ai.gitbook.io/docs/skill-development/displaying-information/mark-1-display
https://mycroft-core.readthedocs.io/en/master/
but i hope mr. @gez-mycroft will comment and correct or supplement my suggestions.
regarding your development set up, i think i share your desire for a very direct and streamlined experience. i have made certain concessions in my approach, but i really enjoy it. here is a quick description:
enable ssh on a mark1. then use whatever native convenient ssh client you have to get a shell session as user “pi” (and if your os supports mdns, you can just use “mark1.local” as hostname). change user to mycroft (perhaps sudo su - mycroft) and now you can quickly and easily cd /opt/mycroft/skills and user msk to create a new skill and just edit your skills hot and live right there! mycroft will automatically reload the skill every time you save, and you can almost immediately test it. so fast and efficient. although, be cautious, you could also accidentally damage a core skill and make it hard to recover. in my case i edit with vim, there are other console/tty based editors but i don’t know if you could replace all the features of vscode. git is immediately available if and when you want it, but it is unnecessary for the edit-test-repeat work cycle.
now launch a second ssh client session logging in with pi again, but immediately launch mycroft-cli-client where you will see debug and logs scrolling by so you can quickly and easily get more details on result of your skill code edits.
this works great for me, but there may be a deal breaker in there somewhere for you.

i’ve never done much tinkering on the mark 1 enclosure code but the easiest way to change the eye color would be to use this function in the mark 1 skill
we’re still supporting the mark 1 and updating it with the latest from mycroft but to be honest it’s not going to get a lot of additional dev time. we are a small team and need to stay focused on the mark ii.
"
328,testing and feedback wikihow,skill feedback,"
wikihow skill

1999×816

how to do nearly everything.
about
ever wondered about how to boil an egg, or the best way to brush your teeths. this skill enables
mycroft to anser a lot of “how to” questions with step by step guide. more detailed
information can be asked afterwods.
information is comming from wikihow.com

695×784


695×784

examples

“how to boil an egg”
“give me a random how to”
“explain better”

github



github



jarbasskills/skill-wikihow
wikihow for mycroft. contribute to jarbasskills/skill-wikihow development by creating an account on github.





","
great! wikihow support multiple languages, does your skill let to set one of the compatible languages on the wikihow site?


yes, supports all languages, if language is missing from wikihow it will use english and google translate

installed on picroft, works. using voice.

tested the “how to boil an egg”. ok
tested the “give me a random how to”. error


is this with voice only or using gui?
id very much like this skill to be useful also without a gui, and intents are designed that way, so im looking for feedback mostly in how to improve this front
actually the original skill didnt even use the gui, i updated the wikihow package for this 

random how to intent fixed, please test again

osom! i would desire deepl has a free api…
"
329,picroft not working with mic array hat,mycroft project,"
i tried picroft with an usb mic and it didn’t work so i ordered a makerhawk respeaker 4-mic array and it still isn’t working. i went through the setup wizard, it installed a bunch of stuff but problems started when i had to test the mic. the default mic test when setting up gets stuck while recording and so does it with
arecord -d 10 test.wav
it just says it’s starting to record and never stops and i have to ctrl+c out of it. (this seems like a major issue and probably the reason it isn’t working?)
it doesn’t show up in mic level when i go to mycroft-cli-client.
pactl info shows me that the right one is selected (i think) and it even shows up as “running”:
 



alsamixer looks like this, i don’t really know what to make of this:
 



can someone please help me with this or point in the right direction? the audio-troubleshooting site didn’t help.
","
you’d be right it’s a major issue.  please post the text itself and not a screen shot.
what have you tried so far? what was the result of that effort?  what’s in the logs (/var/log/mycroft/*)?
"
330,mycroft wont say anything,support,"
when i say “hey, mycroft” i will hear a noise and then it mutes my audio then i say what i want to say which right now is “pair my device” it unmutes the audio and then nothing, it doesn’t say anything
","
what have you tried so far?  what was the result of that effort?  what’s in the logs (/var/log/mycroft/*)?  have you been through the audio troubleshooting guide?
https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/troubleshooting/audio-troubleshooting
"
331,testing and feedback parrot skill,skill feedback,"
turn mycroft into a echoing parrot!
make mycroft repeat whatever you want
repeats recent audio transriptions and text to speech outputs
this skill is useful to debug stt when mycroft does not understand you, and tts when you dont understand mycroft. it also demonstrates the most basic usage of converse() method
about
turn mycroft into a parrot. speak a phrase and listen to it repeated in mycroft’s selected voice.
""hey mycroft, start parrot""
""hello""
hello
""what""
what
""who are you""
who are you
""stop parrot""

also provides an idle screen with parrot images and a random previous stt transcription

908×783


905×785

notes:

this will blacklist and replace the functionality of mycroftai/skill-speak, see issue#24

this will blacklist and replace the functionality of matthewscholefield/skill-repeat-recent

when asking to repeat what was previously said source is taken into consideration, if you ask in cli, gui, hivemind or stt response will vary accordingly, ie. using voice satellite wont respond with stt from device, only same source is taken into consideration
previous transcriptions are not persisted to disk

examples

“say goodnight, gracie”
“repeat once upon a midnight dreary, while i pondered, weak and weary, over many a quaint and curious volume of forgotten lore”
“speak i can say anything you’d like!”
“repeat what you just said”
“repeat that”
“can you repeat that?”
“what did i just say?”
“tell me what i just said.”
“start parrot”
“stop parrot”

credits

jarbasal
matthewscholefield/skill-repeat-recent

github



github



jarbasskills/skill-parrot
taks back to user like a parrot. contribute to jarbasskills/skill-parrot development by creating an account on github.





",
332,delay before ding or speak,support,"
i was wondering if it is possible to modify a file to be able to delay the ding sound and the response from mycroft by a few seconds. i noticed that when i use my bluetooth speaker, i miss the start of the message, probably due to the fact that it needs to reconnect bluetooth which cause this issue.
",
333,activation emails blocked due to inclusion in 8 blacklists,site feedback,"
hi all,
i just tried to activate and had to switch over to a gmail email because spamcop is blocking mycroft activation emails:
jul  4 12:41:11 mail postfix/smtpd[13043]: anonymous tls connection established from o1.3nn.shared.sendgrid.net[167.89.100.129]: tlsv1.2 with cipher ecdhe-rsa-aes256-gcm-sha384 (256/256 bits)
jul  4 12:41:11 mail policyd-spf[13055]: prepend received-spf: pass (mailfrom) identity=mailfrom; client-ip=167.89.100.129; helo=o1.3nn.shared.sendgrid.net; envelope-from=bounces+2781299-2af5-mycroft=<redacted>.com@sendgrid.net; receiver=<unknown> 
jul  4 12:41:11 mail postfix/smtpd[13043]: noqueue: reject: rcpt from o1.3nn.shared.sendgrid.net[167.89.100.129]: 554 5.7.1 service unavailable; client host [167.89.100.129] blocked using bl.spamcop.net; blocked - see https://www.spamcop.net/bl.shtml?167.89.100.129; from=<bounces+2781299-2af5-mycroft=<redacted>@sendgrid.net> to=<<redacted>>> proto=esmtp helo=<o1.3nn.shared.sendgrid.net>
jul  4 12:41:12 mail postfix/smtpd[13043]: disconnect from o1.3nn.shared.sendgrid.net[167.89.100.129] ehlo=2 starttls=1 mail=1 rcpt=0/1 quit=1 commands=5/6

it appears the ip address being used by the forum for email delivery has been labeled as a spam device by users and/or spamcop spam traps: https://www.spamcop.net/w3m?action=blcheck&ip=167.89.100.129
it also appears to be listed in 7 other blacklists: https://mxtoolbox.com/supertool.aspx?action=blacklist%3a167.89.100.129&run=toolpage
thanks.
","
doesn’t your isp offer a “whitelist” option to allow certain email through? you don’t say which provider is using spamcop, although att is apparently a big user.
spamcop themselves say: “the scbl is aggressive and often errs on the side of blocking mail. when implementing the scbl, provide users with the information about how the scbl and your mail system filter their email. ideally, they should have a choice of filtering options.”

that is not the first time this happens. spamcop lists and delists this ip continuously.
as this ip belongs to sendgrid, i would configure your filter to whitelist that ip.

ahh, assuming you’ve tried the delisting procedure of contacting via support before, and no luck or it keeps coming back? https://sendgrid.com/docs/ui/sending-email/deny-lists/
this is exactly why i switched over to running https://github.com/tomav/docker-mailserver on a private ip (linode $5 a month, handles plenty of volume). if the team was interested i’d be happy to help set it up on a server you already have existing. very low cpu/memory utilization, and has been highly reliable with very low maintenance over the past number of years.

no, i did nothing, that information is given by spamcop itself. i guess some sendgrid admin ask spamcop to delist now and then.
but again, the problem is not sendgrid but spamcop returning false positives. so, your server is not as reliable as you think.
"
334,testing and feedback tts control,skill feedback,"
tts control skill
allows to change or retrieve info about text to speech by voice
usage

“what is the current voice”
“available voice engines”
“text to speech demo”
“change voice to alan pope”
“change voice to kusal”
“change voice to google”
“change voice to alan black”
“change voice to female”
“change voice to richard”
“change voice to amazon”
“change voice to whisper”

engines confirmed working: espeak, polly, google, mimic, mimic2
todo

gui integration
“use that voice” follow up intent (voice demo)
improve selection by voice name for untested engines (yandex, bing, ibm)
pr fix for responsive voice in mycroft-core
consider removing requirement of hellochatterbox/text2speech for voice validation/detection  (ideally whole package would be pred to/adopted by mycroft-core)

if anyone is using a untested paid tts and wants to share a key for dev purposes i will improve support
github



github



jarbasskills/skill-tts-control
control and configure text2speech by voice. contribute to jarbasskills/skill-tts-control development by creating an account on github.





",
335,more microphones speakers setup,none,"
hi, i have an idea to build mycroft which will eventualy cooperate with home assistant. i would like to build it in the whole apartment so i expect to connect more microphones and speakers to raspberry. any idea how to build it in the most effective way? is it possible to make mycroft respond only to the closest speaker (not to the whole apartment)?
thanks for response,
martin m.
","
hi martin, welcome to the forums!
this sounds like a job for hivemind satellites  
we did recently add a feature for projects like hivemind so that mycroft can know where an utterance came from and respond back to that instance rather than to every connected endpoint.
just a disclaimer that none of this is used in a “standard” mycroft install so it will take some playing around, and learning about the mycroft system.

great, thanks for reply  i will try it out.
"
336,skills privacy policy and type of data taken from users,general discussion,"
hi all,
do you know how can i see the privacy policy for each skill( if any), and what type of data each skill collects, and whether there is permission taken from users on that? either on the website or on ubuntu through a virtual box would work.
thanks
","
the idea behind an open source assistant is precisely not to collect anything from anyone. some skills do need some information from you, like location, credentials, and others. if the skill interacts with a third party service, you’ll need to see that 3rd party privacy policy (e.g: spotify skill will need spotify credentials just to connect to spotify, but will be spotify who will track what are you listening, not mycroft)
this information can be configured at home.mycroft.ai for the convenience to configure it with a gui, but then you’ll be storing that on mycroft inc servers. if you don’t trust mycroft inc to store your data, you can configure locally all those in each mycroft instance through the mycroft.conf file.
that being said. if you still want to see what a skill is doing with your data, you can see the code by yourself, is not a black box like commercial assistants, so there is nothing to hide.

@malevolent explained it very well
i just want to add that if you install a skill from the marketplace it has been reviewed by humans and is safe to install. so even if you dont look at source code you can be assured someone did
when installing skills from random github repos use your judgment,  if you can look at the code do, if not check if its a known comunity member etc… there have been 0 known cases of spying or malware skills so far

thank you @jarbasal and @malevolent. you both explained very well. to clarify, i am a student doing a project on the privacy of the skills in mycroft. i am trying to answer some questions that are going to be of interest to the users, developers, and owners of mycroft.   i am sure this is going to reassure the users about the high privacy of mycroft.
if you think there are further relevant sources on privacy of mycroft, please do not hesitate to share.
thank you so much

this is not related to skills, but mycroft in general: i guess you already had a read to the documentation; there explains how mycroft has opt-in privacy, meaning mycroft doesn’t store your voice by default, you must manually opt-in for that, so you can contribute with your voice to improve community voice recognition, as those datasets are donated to mozilla deepspeech, who also has a strong privacy policy, and deletes all datasets from time to time.
you can run mycroft entirely offline, by installing and hosting the selene backend by yourself or the personal backend, and use mozilla deepspeech as your stt, so no information needs to leave your own network, yet you’ll need to have some horse power to run all those engines.
"
337,picroft multiple rooms,mycroft project,"
hi everyone,
i’m planning to combine my house automation with picroft. is it possible to use one raspberry with multiple microphones (one microphone for each room) or do i need to use von raspberry with picroft in each room?
thanks in advance.
eyk
","
welcome @eyk107,
currently you need a rpi with picroft in every room. you are not the first to ask if mycroft supports “satellite microphones” like some other voice assistants do. my prediction is that this will be available somewhere in the future (but i can’t see if it is near or distant future…).

thank you for your fast reply. i’ve searched the forum, but couldnt find any information

welcome @eyk107
even @dominik answered your question, here are existing threads on this topic.




mycrofts in every room general discussion


    how would one setup multi mycrofts? i want one in every room. 
is it configuring each one, installing skills etc, or are there a way to centralize a setup - one server and rest clients ? 
for the moment i am using picroft.
  





mycroft questions from a snips user general discussion


    hello from a snips user. if you haven’t heard already snips has been bought by sonos and is shutting down user access in a month. so there’s a good chance i’m not the first and certainly won’t be the last to check out mycroft. 
coming from snips i do have some questions. 
does mycroft have the ability to run satellite devices? i currently have a pi0 w acting as a remote device. it’s pretty awesome. 
if there is no satellite option for mycroft will it run on a pi0 w? cause i have one of those :sl…
  


i am working on an mqtt skill that will publish and subscribe to commands and messages. this will permit multiple mycrofts (and home assistant) to communicate with each other. each mycroft will have a location topic and the skill will publish the location that appears in the skill command. all commands and responses will be published to its own topic.

i’ve had this for ages https://github.com/jarbasal/hive_mind
ill try to find some time to write docs and make docker images, check “terminals” to see how to make satellites
this is not for everyday users, you kinda need to be a dev and read trough code to understand how to set things up until i write docs
note: this pr is recommended https://github.com/jarbasal/jarbas-core/tree/feat/message_targetting

would love to see documentation for hivemind.  i’m pretty weak on coding but would love to setup a half dozen picrofts around my house… plus on my phone whenever a legit, standalone android apk is released.

pulseaudio does work over the network so guess you could use a pi zero as a microphone sender.
https://wiki.archlinux.org/index.php/pulseaudio/examples#pulseaudio_over_network
usb microphone, still uses a pi but a much cheaper one.
you can set pulseaudio to summing mode with arrays and assign that to a stream.
or hardwired multiple usb soundcards and sum those via alsa.
https://alsa.opensrc.org/twocardsasone
never tried, but prob possible.
anyone ever tried?


amazon.co.uk



techrise external sound card, usb audio adapter external stereo sound splitter...
buy techrise external sound card, usb audio adapter external stereo sound splitter converter with 3.5mm stereo speaker/headphone and microphone jacks for windows, laptop, ps4, notebook, plug & play at amazon uk. free delivery and return on eligible...


£5.99







also guess there is pulseaudio - bluetooth and suppose even a mobile could act as a mic / speaker. they would all be on the same channel but they are really cheap now.



aliexpress.com



us $7.99 30% off|fashion  bluetooth speaker wireless portable mini with...
smarter shopping, better living!  aliexpress.com






needed pr has been merged into core, check the voice_satellite repo



github



openjarbas/hivemind-core
join the mycroft collective, utils for mycroft-core mesh networking - openjarbas/hivemind-core






@jarbasal does asyncio  solve the qos problems of pulseudio with wifi and you would get timing problems with pulseudio over vanilla 802.11 wifi connections?

the current voice satellite version handles the audio on device, the utterances is processed remotely
in a future version i will add a way to send the audio for transcription, i can’t really compare qos at this point
voice satellite is basically a standalone version of the speech client from mycroft-core, just running a different machine

i know this is an old thread, but as i’m diving into things this occured to me. with a linux server already running docker, could i run multiple mycroft instances as containers, each using a different bluetooth speaker+mic combo?

if you can get bluetooth running in anything but narrowband audio then prob yeah.
you could just route sum your mics into a combined channel and same with ouput audio would come out on all as they share the same alsa pcm.
but otherwise there is nothing in code to index and thread operation which really should be quite easy to do that way but isn’t.

ahh yes i had not considered narrow vs wideband. that being said, i would love to have mycroft output voice through the bt device, but always play music on the stereo anyway.

my personal fave is snapcast as its a superb rtp audio app that is latency compensated.
still no interface to mycroft for multiple instances as relative dumb satelites to a central mycroft would be really cool and also far more cost effective.



github



badaix/snapcast
synchronous multiroom audio player. contribute to badaix/snapcast development by creating an account on github.






i wonder how intensive snapcast is… an interesting use case would be snapcast on a pi0w with a powered mic/speaker like this one: https://www.amazon.com/tonor-conference-microphone-omnidirectional-compatible/dp/b07gvgmw59
that brings the cost of the “client” way down. you could have a setup like:

server running an instance of mycroft per-room (this would work for now)
set mycroft output to the snapcast device (additional dev)
set mycroft input to… ???

that last bullet is the hardest part. somehow we would need a way to combine all audio inputs, and identify which room was active.

i love the pi0 because of the form factor but with the p3a+ being 10x more powerfull for just £10 more it not as good value as the pi3a+.
snapcast is really lite but say if you want to encrypt through say stunnel then like anything you quickly start to max load on a pi0 whilst plenty to spare on the 3a+
but yeah should run fine on a pi0 just haven’t tried, pi3a+ you have a lot of room even for a full mycroft itself as it will squeeze it in.
shame its not more of a modular system that can thread input as audio wise it would be relatively easy and as said very cost effective.
"
338,how to get better smarter fuzzy matches,support,"
i’m writing a skill that digs through my browser bookmarks and pulls out relevant matches to a spoken phrase.  so for example if i say “search my bookmarks for chicken” i want it to find all the recipes i have stored for chicken.
i thought that the smart thing to use here was mycroft.util.parse.fuzzy_match(), but it’s not doing what i expected.
in a list of bookmarks with titles like chicken kiev , chicken soup with garlic and sour cream , and chicken parm lasagna, the most relevant according to this function is arch linux. (score: 0.5)  that soup recipe has a score of 0.26!
now i know that i could just do a search for the keyword, but that’d have its own problems like “chicken parm” wouldn’t match “chicken parmesean” for example.
what’s the “right” way to do this?
","
nevermind, i think i’ve found what i need.  the fuzzywuzzy package seems to do what i need:
choices = (
  ""arch linux"",
  ""chicken parmesean"",
  ""gradma's chicken soup with garlic and potato - somerecipesite.com"",
  ""this is almost chiken soup""
)
process.extract(""chicken"", choices=choices)

result:
[('chicken parmesean', 90),
 ('this is almost chiken soup', 77),
 (""gradma's chicken soup with garlic and potato - somerecipesite.com"",
  60),
 ('arch linux', 47)]


yeah fuzzywuzzy is a great choice for fuzzy matching. the implementation in mycroft is a poor-man’s version. i think the main reason it’s not used in mycroft-core were licensing issues.

fuzzywuzzy is gpl-2.  what’s mycroft using that won’t play nice with that?

it’s under apache v2.
i’m not a lawyer so i don’t know how / if it would work but that’s the reason i remember from way back.

you may try rapidfuzz which produces similar matching results like fuzzywuzzy but comes with mit licence…

we did run into this license issue with rhasspy aswell (and it was notoriously slow). which is why we created rapidfuzz 
"
339,where to get cli log paste,none,"
hey everybody,
i’m new to mycroft and i’d like to know how coul i have a copy of my cli debug console. i’m using mycroft on raspberry not linked to any computer so maybe it’s saved in the sdcard ?
thank anyone,
","
logs are stored in  /var/log/mycroft/.
you’d need to connect to the command line (ssh or hook up a monitor/keyboard) to get to them.

the other option if you can’t ssh in for any reason is to say:

hey mycroft, create a support ticket

this packages up your logs, uploads them to 0x0.st and emails you a link.

hey guys thank you for your answers !
i didn’t thought about ssh because i’m new to this. and using a keyboard / screen to the pi would’nt let me copy and paste them if i need.
i’ll surely try ssh and i thing it will do the job !
thank’s all !
"
340,connectionrefusederror kills mycrofts start process,general discussion,"
is anyone else seeing error messages like this on (re)start?  i’m running into this intermittently, but generally if i stop the docker container (not remove the container, just stop it) and then start it again, this is more likely to happen.
i have a mark 1 on my desk that dies on the regular.  could this be what’s causing it?
mycroft_1  | 2020-05-23 19:27:53.553 | error    |    63 | mycroft.messagebus.client.client:on_error:81 | === connectionrefusederror(111, 'connection refused') ===
mycroft_1  | traceback (most recent call last):
mycroft_1  |   file ""/opt/mycroft/.venv/lib/python3.6/site-packages/websocket/_app.py"", line 254, in run_forever
mycroft_1  |     proxy_type=proxy_type)
mycroft_1  |   file ""/opt/mycroft/.venv/lib/python3.6/site-packages/websocket/_core.py"", line 220, in connect
mycroft_1  |     options.pop('socket', none))
mycroft_1  |   file ""/opt/mycroft/.venv/lib/python3.6/site-packages/websocket/_http.py"", line 120, in connect
mycroft_1  |     sock = _open_socket(addrinfo_list, options.sockopt, options.timeout)
mycroft_1  |   file ""/opt/mycroft/.venv/lib/python3.6/site-packages/websocket/_http.py"", line 190, in _open_socket
mycroft_1  |     raise err
mycroft_1  |   file ""/opt/mycroft/.venv/lib/python3.6/site-packages/websocket/_http.py"", line 170, in _open_socket
mycroft_1  |     sock.connect(address)
mycroft_1  | connectionrefusederror: [errno 111] connection refused
mycroft_1  | 2020-05-23 19:27:53.556 | error    |    63 | mycroft.messagebus.client.client:on_error:88 | exception closing websocket: connectionrefusederror(111, 'connection refused')
mycroft_1  | 2020-05-23 19:27:53.557 | warning  |    63 | mycroft.messagebus.client.client:on_error:91 | message bus client will reconnect in 5 seconds.

","
i have the same error:

traceback (most recent call last):
file “/opt/mycroft/mycroft/messagebus/client/client.py”, line 84, in on_error
self.emitter.emit(‘error’, error)
file “/opt/mycroft/.venv/lib/python3.6/site-packages/pyee/init.py”, line 178, in emit
raise args[0]
file “/opt/mycroft/.venv/lib/python3.6/site-packages/websocket/_app.py”, line 254, in run_forever
proxy_type=proxy_type)
file “/opt/mycroft/.venv/lib/python3.6/site-packages/websocket/_core.py”, line 220, in connect
options.pop(‘socket’, none))
file “/opt/mycroft/.venv/lib/python3.6/site-packages/websocket/_http.py”, line 120, in connect
sock = _open_socket(addrinfo_list, options.sockopt, options.timeout)
file “/opt/mycroft/.venv/lib/python3.6/site-packages/websocket/_http.py”, line 190, in _open_socket
raise err
file “/opt/mycroft/.venv/lib/python3.6/site-packages/websocket/_http.py”, line 170, in _open_socket
sock.connect(address)
connectionrefusederror: [errno 111] connection refused

during handling of the above exception, another exception occurred:
i setup the mycroft docker on debian 9. i’m trying  to find some kind of solution, but i don’t have any luck for now.

well at least i’m not alone.  i’m afraid i don’t yet know much about the mycroft internals, but i have a feeling this might have something to do with the message bus framework as it uses a websocket to do the work.

i  make a little progress. i installed `pulseaudio:
https://www.freedesktop.org/wiki/software/pulseaudio/ and the error was fixed. but still,i can’t figurate out why mycroft doesn’t response. if you don’t have pulseaudio in your system, installed and try again.
"
341,reloading submodules,support,"
i’m developing a skill and i’ve noticed that it does a handy reload whenever i edit and save my __init__.py file.  however for the files that i’m importing in that file, edits don’t seem to take effect.  for example:
# __init__.py
from mycroft.skills import mycroftskill

from .thing import mything


class myskill(mycroftskill):
    def do_the_thing(self):
        ...

if i make an edit to the myskill class, the file is reloaded, and the new code is executed when my skill is triggered.  however, if i edit thing.py, wait a minute or so, and trigger my skill, the old code is triggered as if it hasn’t changed.
is this a known/expected behaviour?  is there a work-around that i should be following?  is it just common practise to put all of your logic into __init__.py?
","
hi,
it has been common to put all the code in the __init__.py despite it not being ideal, due to this limitation.
there is an old wip fix, but it may reload the submodules in the incorrect order if there are nested module imports. i was looking into this some more last week to see if i could get it simplified and updated.
a quick workaround may be to manually enforce the reload
from importlib import reload

from . import my_submodule
reload(my_submodule)

not ideal but i think it should work.
i’m looking to either simplify the old pr using find module or checking if modules such as lazy_reload works for the mycroft case.

that is really unfortunate, as it throws a wrench into continued development of any skill of substantial size.  the one i’m working on currently has 4 external imports along with the main __init__.py, and managing all of this in one great big file would be brutal.
i’ll try putting your work-around into my skill’s __init__() method and see if that does the job.  thanks.

timing is everything, i was going to post this exact question yesterday. i am having this exact issue during my development.
currently what i am doing is deleting the submodule compile directory before i msm update my latest build.
/opt/mycroft/skills/cpkodi-skill.pcwii/__pycache__


hey my skill is talking to kodi too!  well, kodi and a few other things.  i know it’s off-topic, but i gotta know: what’s yours do?
here’s mine (work in progress)

@danielquinn, i already have a kodi skilll that works very well. i am in the process of updating it to support the common play architecture. i am definitly interested in what you might be developing.
currently mine supports, music / youtube(music/videos) / movies, these are already working, my next release will include tv-shows and i am also planning the ability to cast any items from the kodi library to a chromecast enabled device.
my original work is here




testing and feedback for kodi skill skill feedback


    kodi-skill

how to install kodi skill

install kodi skill by …

msm install https://github.com/pcwii/kodi-skill.git


requirements.txt should install kodipydent and requests, alternatively you can pip install. 
pip install kodipydent 
pip install requests


prepare kodi by …

configure kodi to “allow remote control via http”, under the kodi settings:services
configure kodi to “allow remote control from applications on other systems”, under the kodi settings:services
under kodi settings:services …
  


non-functioning new work is here…



github



pcwii/cpkodi-skill
updated mycroft.ai kodi skill using the common play framework - pcwii/cpkodi-skill







well there’s certainly a lot of overlap between the two projects.  have a look at kodi.py for the most relevant stuff, but note also that my “majel” skill is also using the common play framework, so you can reference that if you get stuck (though it’s pretty straightforward, they’ve done a good job there).
my project only does movies & tv, and doesn’t use kodi directly, but only as a indexer/search engine for locally-stored stuff.  in that way, the lookup for episodes etc. may be handy (i’m using kodijson unless you’ve got a better idea?).

i am just using requests for all my json. i had been using kodipydent in the past but found it didn’t have all the api functions available so i began doing my own. example…
def get_requested_movies(kodi_path, search_words):
    """"""
        searches the kodi library for movies that contain all the words in movie_name
        first we build a filter that contains each word in the requested phrase
    """"""
    filter_key = []
    for each_word in search_words:
        search_key = {
            ""field"": ""title"",
            ""operator"": ""contains"",
            ""value"": each_word.strip()
        }
        filter_key.append(search_key)
    # make the request
    json_header = {'content-type': 'application/json'}
    method = ""videolibrary.getmovies""
    kodi_payload = {
        ""jsonrpc"": ""2.0"",
        ""method"": method,
        ""id"": 1,
        ""params"": {
            ""properties"": [
                ""file"",
                ""thumbnail"",
                ""fanart""
            ],
            ""filter"": {
                ""and"": filter_key
            }
        }
    }
    try:
        kodi_response = requests.post(kodi_path, data=json.dumps(kodi_payload), headers=json_header)
        log.info(kodi_response.text)
        movie_list = json.loads(kodi_response.text)[""result""][""movies""]
        log.info('getreqeustedmovies found: ' + str(movie_list))
        # remove duplicates
        clean_list = []  # this is a dict
        for each_movie in movie_list:
            movie_title = str(each_movie['label'])
            info = {
                ""label"": each_movie['label'],
                ""movieid"": each_movie['movieid'],
                ""fanart"": each_movie['fanart'],
                ""thumbnail"": each_movie['thumbnail'],
                ""filename"": each_movie['file']
            }
            if movie_title.lower() not in str(clean_list).lower():
                clean_list.append(info)
            else:
                if len(each_movie['label']) == len(movie_title):
                    print('found duplicate')
                else:
                    clean_list.append(info)
        return clean_list  # returns a dictionary of matched movies
    except exception as e:
        print(e)
        return none


i’ll redouble my efforts to get the submodule reloading working without hitches. (i’m moving house so my spare time is sadly limited)
bigger skills really need to be able to use multiple files.

thanks for your efforts @forslund, both in the code itself and in posting here!




 danielquinn:

i’ll try putting your work-around into my skill’s __init__() method and see if that does the job. thanks.


if you figure this out and it works for you, please let me know. in the meantime i will continue to delete my cache directory.

it does!  here’s the top of my file to see how i’ve implemented it:
import concurrent.futures
import os
import sys

from importlib import reload
from typing import union

from mycroft import intent_handler
from mycroft.skills.common_play_skill import cpsmatchlevel, commonplayskill

from .bookmarker import bookmarker
from .exceptions import bookmarknotfounderror, medianotfoundexception
from .kodi import kodi
from .streaming_services import streamingservices
from .youtube import youtube


# hack to work around the auto-reloader not recognising files other than
# __init__.py: https://community.mycroft.ai/t/8879

reload(sys.modules[""majel-skill.bookmarker""])
reload(sys.modules[""majel-skill.kodi""])
reload(sys.modules[""majel-skill.streaming_services""])
reload(sys.modules[""majel-skill.youtube""])

# /hack


note that reload() is looking for a module, rather than a string, and if you’re importing the modules with relative paths as i’ve done here, i found the cleanest way to do this was to import sys and use the name of the module to lookup the actual module in sys.modules.
also note that i decided to put this at the top of the file (rather than inside __init__()) as this will ensure that the modules will be reloaded whenever the file is reloaded (ie. when it’s edited and the auto-reloader does its thing) rather than reloading whenever the class is instantiated.

this seems to work for me. thanks
for each_module in sys.modules:
    if ""kodi_tools"" in each_module:
        log.info(""attempting to reload kodi_tools module: "" + str(each_module))
        reload(sys.modules[each_module])


i’ve updated the pr with something that seem to work (both according to theory and practice from what i’ve seen). if you want to try it out check the the feature/importlib branch.
let me know if there are other issues that i’ve missed.

this is now in dev and will be included in the next major release, thanks for flagging it daniel, and of course to ake for the fix 

wow, that’s really great guys, thanks for the hard work!
"
342,finished booting skill right after gets input wake up and says i dont know what that means,support,"
so i am not sure if this is from the finished booting skill or something else, but every time i reboot  it will say “hi this is mycroft and i am ready to work” . in the mycroft-cli i will see that it gets the input ‘wake up’ from somewhere and responds by saying “i dont know what that means”.
i am on picroft and i looked through the logs but am not finding where that input is coming from, but i am not sure where but i started after i installed this skill. also i feel like that phrase should be understand anyways it says something similar when i say that with voice. is this something else i might not have configured properly? also which logs would help with this?
thank you,
david/snukkums
","
the message bus and voice logs would be where to start.

ok thanks i will look into those when i get time.

the telegram skill does this for me as well. i use picroft too. i believe it is because mycroft gets a voice command (somethinghhe has to say) too early, before he can talk.
"
343,no updates about mark ii since the last 12 weeks,mark ii,"
hey there.
i know it’s hard to do business while covid is spreading all over the world, but i really need some news. i mean, i can’t imagine there’s been 0% progress on the project to bring mark-ii to the people.
we have trust into the project mycroft and some of us have spent a lot of money to show you this. this was the biggest amount for me i ever pushed into an opensource project. and yes, it was a lot of money for me but i sell all my google devices for this great idea to reduce this amount and i wait. i’d like to get my 3 mark ii devices tomorrow, but i understand it took his time.
your blog entries on patent abuse are interesting but it is rather a marginal topic for me.
i just want to get some news about the progress of mark ii.
please dont understand me wrong. i honor your job at mycroft and i love the idea of a free and open source voice-assistant.
greetings by suisat
","
the mark2 updates are usually posted over at kickstarter: https://www.kickstarter.com/projects/aiforeveryone/mycroft-mark-ii-the-open-voice-assistant/posts


markii786×172 9.98 kb

dunno but from the comments many have just lost faith.

the most recent post on kickstarter was last april! when the mark ii was delayed we were promised transparency and regular updates. even if no progress is being made in a given time period, that should be communicated along with an explanation. come on, guys! don’t leave your long-suffering supporters hanging.

on june 6th there was an ama.

hey all, just to make sure you see it, the latest mark ii update just went up.
i hear the frustrations around communication and i know we didn’t put out a may update. we are trying to keep the updates coming regularly alongside making progress on the mark ii, dealing with patent aggressors, and the general state of the world right now…
the good news is that we are making progress, so please stick with us 

thx. this is what i need…
"
344,testing and feedback new better calculator skill,skill feedback,"
link to github: https://github.com/gras64/calculator.git 
hi i have a great pocket calculator. with this skill you can do a lot of arithmetic operations. you can also use formulas like ohm’s law. the skill supports the conversion of units, formulas brackets, net and gross as well as sales price and calculations of any length “theoretically”.
how to install calculator-skill

install calculator by …
install https://github.com/gras64/calculator.git 


how to test calculator
""what is 12 and 2,4 and 6,2 and 12,5""
""addiere 4 and 5 from that gross""
""divide 600 by 2 from that net""
""divide bracket on 9 and 3 bracket off and 2""
""what is 9 times 72 from that sale""
""multiply 2 with 3""
""what is 5 minus 4""
""subtract 7 with 6""
""what is ohm from 40 ampere and 60 volt""
""what is tension from 40 milliampere and 60 millivolt""
""what is the breaking point at 130 kmh""
.......

you are welcome to suggest formulas that i can then integrate.
this skill is very complex and contains a lot of triggers, making it as easy as possible to use. should someone find a wrong trigger then do not hesitate to tell me.
for development and see how to add formulas go to https://github.com/gras64/calculator#development
have fun
where feedback on calculator-skill should be directed a
be clear about how feedback on the skill should be provided, such as through issues on github, via email or via mycroft chat.
",
345,testing and feedback sunspots,skill feedback,"
about
daily sunspot data from sunspot index and long-term solar observations and pictures from nasa’s solar and heliospheric observatory
can be used as idle screen for the mark2

620×790


649×793


717×781


712×791


709×782


712×785


712×790


615×786

examples

“live picture of the sun”
“whats the number of sunspots”
“how many sunspots yesterday”
“how many sunspots 3 days ago”
“show me a picture of the solar corona”
“ultraviolet sun picture for 2 million degrees”
“ultraviolet picture of the heliosphere”
“show me magnetic field of the sun”
“animate that”
“next picture”
“previous picture”

todo

quick facts

github



github



jarbasskills/skill-sunspots
solar and heliospheric observatory. contribute to jarbasskills/skill-sunspots development by creating an account on github.





","
more intents added, first post updated with pictures and info
i think i got all the soho instruments covered now
"
346,ps3 eye not working,support,"
i have a fresh install of picroft on a pi3b and a brand new ps3 eye. it works perfect with my windows 10 pc, but when i try to go through the quick setup it doesn’t work. going to alsamixer and selecting the device from sound cards gives me this error:
“cannot load mixer controls: invalid argument”
i know i’m selecting the correct device because it’s the same it says on my windows pc. is there another way to make this work? i’m on version 7.86 core 19.8.1
","
hi there,
sorry to hear the ps3 is having trouble.
i’ve reproduced the  error when attempting to select the sound card in alsamixer. a quick search online reveals lots of people with the same problem, but haven’t yet found a solution.
the (hopefully) good news is that this shouldn’t be required. can i check which picroft image you are using? what’s the date in the filename?
when you say you’re on version 7.86 - what does this relate to?
what sort of power supply are you using for the pi? insufficient or fluctuating power could be a cause. if you don’t have another power supply, i’d at least unplug anything non-essential like the hdmi display, and ssh in.

i have the same problem. i use ssh to get in, only thing connected are my speaker via 3.5mm jack and ps3eye. it is recognized but it doesn’t capture my voice at all.

hi beastflow,
is this using the latest picroft stable image dated 2019-11-10?
did the pseye work in the mic test during the setup process? or not at all?

a post was split to a new topic: wake word not working with ps3 eye
"
347,testing and feedback hubble telescope,skill feedback,"
about
pictures from hubble space telescope
can be used as idle screen for the mark2

512×785


504×792


502×779


922×787


918×783


508×791


503×786

examples

“picture from hubble site”
“explain”
“tell me about the hubble telescope”
“who was hubble”
“what is hubble space telescope mission”
“why was hubble created”
“what are hubble’s instruments”
“how does hubble work”
“did hubble find any planets”
“can hubble take pictures of earth”
“can hubble see things on the moon”
“is hubble data public”
“can we see live photos from hubble”
“where is the hubble telescope”
“where is hubble looking at”  -
“are the colors in hubble images real”

github



github



jarbasskills/skill-hubble-telescope
all about the hubble space telescope. contribute to jarbasskills/skill-hubble-telescope development by creating an account on github.





",
348,testing and feedback rocket launches,skill feedback,"
about
this skill interacts with https://launchlibrary.net/ api to return the latest space launch.

571×785

this is an updated version of an old skill, the original was made 4 years ago, one of the very first 3rd party skills for mycroft
examples

“when is the next rocket launch”
“tell me more”

todo

next/previous intent

github



github



jarbasskills/skill-rocket-launch
a mycroft skill to get the latest space launch. contribute to jarbasskills/skill-rocket-launch development by creating an account on github.





",
349,cant get mycroft set up,support,"
i am running ubuntu mate. aznd i cant get mycroft set up at all. it all worked fine for a few hours but it was touch and go or a few hours and then all go. i cant use mycroft at all unless i type the question and then even it has to be basic.like what time is it. anything needing a internet for the answer is no go. what do i do?
","
what have you tried so far?
what’s in the logs? (/var/log/mycroft/*)

kind of a redundant question, but it often works: have you restarted / reinstalled mycroft yet or tried un-linking and then linking it to your acct yet?
"
350,testing and feedback national geographic picture of the day,skill feedback,"
about
national geographic picture of the day
can be used as idle screen for the mark2

988×783

examples

“national geographic picture of the day”
“explain”

github



github



jarbasskills/skill-natgeo-pod
national geographic picture of the day. contribute to jarbasskills/skill-natgeo-pod development by creating an account on github.





where feedback should be directed at
any feedback is welcome, please let me know of any issues or suggestions for extra functionality
leave any feedback here, as an issue on github or in mycroft chat
",
351,testing and feedback iss tracker skill,skill feedback,"
i made a skill to track the location of the iss
any feedback is welcome, please let me know of any issues or suggestions for extra functionality
can be set as idle screen
examples

“where is the iss”
“who is on board of the space station”
“when is the iss passing over”
“tell me about the iss”
“how many persons on board of the space station”


898×694


1038×623


1042×696

todo

websettings

github



github



jarbasskills/skill-iss-location
track the iss. contribute to jarbasskills/skill-iss-location development by creating an account on github.





where feedback should be directed at
any feedback here, as an issue on github or in mycroft chat will be highly appreciated.
",
352,testing and feedback algorithmic art,skill feedback,"

about
creating art from math
see example generated pictures in samples folder

759×791

can be used as idle screen
examples

“draw a picture”

github



github



jarbasskills/skill-algorithmic-art
idle screen skill for mycroft mark2. contribute to jarbasskills/skill-algorithmic-art development by creating an account on github.





where feedback on skill name should be directed a
be clear about how feedback on the skill should be provided, such as through issues on github, via email or via mycroft chat.
credits

inspired by random (psychedelic) art


",
353,testing and feedback nasa epic,skill feedback,"

about
earth polychromatic imaging camera provides near real time pictures of earth from orbit
can be used as idle screen for mark2

506×781


1052×819

examples

“show me earth from space”
“tell me about earth polychromatic imaging camera”
“open the epic website from nasa”

todo / known issues

next/previous intents
tell me about conflicts with wikipedia skill

github



github



jarbasskills/skill-nasa-epic
epic provides near real time pictures of earth from orbit    - jarbasskills/skill-nasa-epic





where feedback on skill name should be directed a
be clear about how feedback on the skill should be provided, such as through issues on github, via email or via mycroft chat.
",
354,testing and feedback chandra x ray observatory,skill feedback,"
about
pictures from nasa’s chandra x-ray observatory

784×788


1103×786


1146×775


721×785

can be set as idle screen
examples

“picture from chandra x ray observatory”
“latest nasa x ray observatory picture”
“explain”
“tell me about chandra x ray observatory”

todo / known issues

tell me about conflicts with wikipedia skill

github



github



jarbasskills/skill-chandra-xray
pictures from nasa's chandra x-ray observatory. contribute to jarbasskills/skill-chandra-xray development by creating an account on github.





where feedback on skill name should be directed a
be clear about how feedback on the skill should be provided, such as through issues on github, via email or via mycroft chat.
",
355,testing and feedback astronomy picture of the day,skill feedback,"
about
nasa’s astronomy picture of the day

825×789

can be set as idle screen
examples

“astronomy picture of the day”
“explain”

todo / known issues

sometimes image is a youtube video

github



github



jarbasskills/skill-apod
astronomy picture of the day. contribute to jarbasskills/skill-apod development by creating an account on github.





where feedback on skill name should be directed a
be clear about how feedback on the skill should be provided, such as through issues on github, via email or via mycroft chat.
",
356,testing and feedback satellite imagery skill,skill feedback,"
about
near real time satellite imagery from nasa’s visible infrared imaging radiometer suite
can be used as idle screen for the mark2

902×783


901×791


878×784


896×787

examples

“show my location from space”
“show me london from space”
“next picture”
“previous picture”
“show my house from space last month”
“increase zoom”
“zoom out”
“set zoom to maximum”
“why so many clouds”
“explain the holes in the equator”
“talk about visible infrared imaging radiometer suite”

todo

websettings

github



github



jarbasskills/skill-viirs
near real time satellite imagery from nasa's visible infrared imaging radiometer suite - jarbasskills/skill-viirs





where feedback should be directed at
any feedback is welcome, please let me know of any issues or suggestions for extra functionality
leave any feedback here, as an issue on github or in mycroft chat
",
357,audio jack not working in picroft,support,"
hi all,
i’m new to the mycroft community, but i’ve been playing with mycroft for a while, so one day i tried to install magic mirror in the picroft image, bought a two-way mirror and a driver for my display, and since i connected the display via hdmi, audio just stopped being outputted from the jack. i’ve tried changing the output in raspiconfig, with amixer cset, and booting without the hdmi cable, and nothing seems to work. the error is system-wide. (probably this is just some simple noob error.)
","
hi sam, can you check the current pulseaudio settings?
https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/troubleshooting/audio-troubleshooting#pulseaudio-settings
pactl info
should give you the current default sink, then
pactl list sinks short
to see what devices are available and their associated number, finally
pactl set-default-sink 1
to set a new default
let us know if that works

i already tried this before posting and it didn’t worked. anyway here is the information
server string: /run/user/1000/pulse/native
library protocol version: 32
server protocol version: 32
is local: yes
client index: 44
tile size: 65496
user name: pi
host name: samcroft
server name: pulseaudio
server version: 12.2
default sample specification: s16le 2ch 44100hz
default channel map: front-left,front-right
default sink: auto_null
default source: alsa_input.usb-c-media_electronics_inc._usb_pnp_sound_device-00.analog-mono
cookie: a9a7:2436

pactl list sinks short

0       auto_null       module-null-sink.c      s16le 2ch 44100hz       suspended



 gez-mycroft:

pactl set-default-sink 1


failure: no such entity

pactl set-default-sink 0

doesn’t give an error but audio still not outputting.

pactl list modules

module #0
name: module-device-restore
argument:
usage counter: n/a
properties:
module.author = “lennart poettering”
module.description = “automatically restore the volume/mute state of devices”
module.version = “12.2”
module #1
name: module-stream-restore
argument:
usage counter: n/a
properties:
module.author = “lennart poettering”
module.description = “automatically restore the volume/mute/device state of streams”
module.version = “12.2”
module #2
name: module-card-restore
argument:
usage counter: n/a
properties:
module.author = “lennart poettering”
module.description = “automatically restore profile of cards”
module.version = “12.2”
module #3
name: module-augment-properties
argument:
usage counter: n/a
properties:
module.author = “lennart poettering”
module.description = “augment the property sets of streams with additional static information”
module.version = “12.2”
module #4
name: module-switch-on-port-available
argument:
usage counter: n/a
properties:
module.author = “david henningsson”
module.description = “switches ports and profiles when devices are plugged/unplugged”
module.version = “12.2”
module #5
name: module-udev-detect
argument:
usage counter: n/a
properties:
module.author = “lennart poettering”
module.description = “detect available audio hardware and load matching drivers”
module.version = “12.2”
module #6
name: module-alsa-card
argument: device_id=“1” name=“usb-c-media_electronics_inc._usb_pnp_sound_device-00” card_name=“alsa_card.usb-c-media_electronics_inc._usb_pnp_sound_device-00” namereg_fail=false tsched=yes fixed_latency_range=no ignore_db=no deferred_volume=yes use_ucm=yes card_properties=“module-udev-detect.discovered=1”
usage counter: 1
properties:
module.author = “lennart poettering”
module.description = “alsa card”
module.version = “12.2”
module #7
name: module-bluetooth-policy
argument:
usage counter: n/a
properties:
module.author = “frédéric dalleau, pali rohár”
module.description = “policy module to make using bluetooth devices out-of-the-box easier”
module.version = “12.2”
module #8
name: module-bluetooth-discover
argument:
usage counter: n/a
properties:
module.author = “joão paulo rechi vita”
module.description = “detect available bluetooth daemon and load the corresponding discovery module”
module.version = “12.2”
module #9
name: module-bluez5-discover
argument:
usage counter: n/a
properties:
module.author = “joão paulo rechi vita”
module.description = “detect available bluez 5 bluetooth audio devices and load bluez 5 bluetooth audio drivers”
module.version = “12.2”
module #10
name: module-native-protocol-unix
argument:
usage counter: n/a
properties:
module.author = “lennart poettering”
module.description = “native protocol (unix sockets)”
module.version = “12.2”
module #11
name: module-default-device-restore
argument:
usage counter: n/a
properties:
module.author = “lennart poettering”
module.description = “automatically restore the default sink and source”
module.version = “12.2”
module #12
name: module-rescue-streams
argument:
usage counter: n/a
properties:
module.author = “lennart poettering”
module.description = “when a sink/source is removed, try to move its streams to the default sink/source”
module.version = “12.2”
module #13
name: module-always-sink
argument:
usage counter: n/a
properties:
module.author = “colin guthrie”
module.description = “always keeps at least one sink loaded even if it’s a null one”
module.version = “12.2”
module #14
name: module-null-sink
argument: sink_name=auto_null sink_properties=‘device.description=“dummy output”’
usage counter: 0
properties:
module.author = “lennart poettering”
module.description = “clocked null sink”
module.version = “12.2”
module #15
name: module-intended-roles
argument:
usage counter: n/a
properties:
module.author = “lennart poettering”
module.description = “automatically set device of streams based on intended roles of devices”
module.version = “12.2”
module #16
name: module-suspend-on-idle
argument:
usage counter: n/a
properties:
module.author = “lennart poettering”
module.description = “when a sink/source is idle for too long, suspend it”
module.version = “12.2”
module #17
name: module-console-kit
argument:
usage counter: n/a
properties:
module.author = “lennart poettering”
module.description = “create a client for each consolekit session of this user”
module.version = “12.2”
module #18
name: module-systemd-login
argument:
usage counter: n/a
properties:
module.author = “lennart poettering”
module.description = “create a client for each login session of this user”
module.version = “12.2”
module #19
name: module-position-event-sounds
argument:
usage counter: n/a
properties:
module.author = “lennart poettering”
module.description = “position event sounds between l and r depending on the position on screen of the widget triggering them.”
module.version = “12.2”
module #20
name: module-role-cork
argument:
usage counter: n/a
properties:
module.author = “lennart poettering”
module.description = “mute & cork streams with certain roles while others exist”
module.version = “12.2”
module #21
name: module-filter-heuristics
argument:
usage counter: n/a
properties:
module.author = “colin guthrie”
module.description = “detect when various filters are desirable”
module.version = “12.2”
module #22
name: module-filter-apply
argument:
usage counter: n/a
properties:
module.author = “colin guthrie”
module.description = “load filter sinks automatically when needed”
module.version = “12.2”
module #23
name: module-x11-publish
argument: display=:0
usage counter: n/a
properties:
module.author = “lennart poettering”
module.description = “x11 credential publisher”
module.version = “12.2”
module #24
name: module-x11-cork-request
argument: display=:0
usage counter: n/a
properties:
module.author = “lennart poettering”
module.description = “synthesize x11 media key events when cork/uncork is requested”
module.version = “12.2”
"
358,news plays instead of music,none,"
hi all,
lately we’ve had reports of the news skill jumping in when people were actually asking for some music to play.
i wanted to get a nice list of examples so we can #addatest and make sure that we not only fix it now, but that any future updates don’t cause it to re-occur. so if you ask for some music and a news station plays instead, please let us know what you said to mycroft so we can stop it happening.
thanks
gez
","
i have been testing my new cpkodi-skill and i am having the news skill respond to any and all requests that my cpkodi-skill does not respond to. i think the fuzzy matching is far to loose on this skill. i should be able to come up with a few examples, but basically i have never had the news skill not start when my cpkodi-skill can’t find an item.
how about these…




i don’t know what this says about the ap news 
but thanks 

i see same behaviour when using the tune-in skill, there is also an issue for that.
examples that to not work for me “play xyz on tunein” with xyz “1live”, “sunshine”, “bigfm”

i’ve been trying to use rythmbox skill and also youtube skill, and news play instead of music.

imagen1410×590 18.6 kb

probably mycroft thinks that the news are more didactic.

plex skill when saying “play random music” also triggers news skill, and with youtube skill also triggered news skill, as youtube skill stopped working for me and plex skill works quite better, i cannot test yourube skill right now.
i need to blacklist news skill, as i found it pretty useless, because there is just one spanish channel, and is a local channel from valencia, so most of the news are irrelevant for global spanish-speakers… most are irrelevant for me, which i’m from catalonia (the province just besides valencia)
i think news skill should be invoked with “tell me the news”, instead of “play the news”, as i guess the word “play” is creating the conflict here.

alright i’ve added a bunch of these suggestions as tests to the skill and updated the matching functions, particularly for common play queries.
if anyone is free to give it a test run, it’s on this branch:



github



mycroftai/skill-npr-news
mycroft ai official news skill, providing the latest news report from your favorite broadcast.  - mycroftai/skill-npr-news







hey the update is now live so should be updating on all devices automatically. would love to hear how you find it 
"
359,server backend bootstrap script,none,"

screen shot 2020-06-19 at 8.43.31 pm739×331 15.8 kb

following the github readme, i am at the “run the bootstrap script” step. i ran into an issue missing two md files, i created both files with “—\n—”. it now gets stuck here. i tried adding .encode(“utf-8”)  but this didn’t fix the issue.
","
hey there, instead of creating semi blank files, you can also comment out the lines where they’re imported, 266&267 i think.


github.com


mycroftai/selene-backend/blob/6d87705a6603fc67cdcb5dd5807c28f95a34e63a/db/scripts/bootstrap_mycroft_db.py#l266

    environ.get('mycroft_doc_dir', '/opt/selene/documentation'),
    '_pages',
    'embed-privacy-policy.md'
)
terms_of_use_path = path.join(
    environ.get('mycroft_doc_dir', '/opt/selene/documentation'),
    '_pages',
    'embed-terms-of-use.md'
)
docs = {
    'privacy policy': privacy_policy_path,
    'terms of use': terms_of_use_path
}
try:
    for agrmt_type, doc_path in docs.items():
        lobj = mycroft_db.db.lobject(0, 'b')
        with open(doc_path) as doc:
            header_delimiter_count = 0
            while true:
                rec = doc.readline()
                if rec == '---\n':







"
360,playback control timeout,none,"
is there a way to change the playback control timeout?
i am having issues with my skill whereas the playback control times out before my skill has an opportunity to parse a library to ensure that it can play what is requested.
what is the timeout period for a skill to complete the
cps_match_query_phrase

image1872×282 13.2 kb

","
i think the delay is hard coded to 5 seconds (see here)
for testing you can change this to 10.
i think you can reset it manually by sending an additional play:query.response message with a searching field set to true in the data
        self.bus.emit(message('play:query.response', {""phrase"": search_phrase,
                                                     ""skill_id"": self.skill_id,
                                                     ""searching"": true}))

set a timer for 4 seconds triggering the above should give you 9 seconds, this is something i haven’t tried though.
perhaps a parameter can be added to allow your skill to request a longer period?




 forslund:

self.bus.emit(message(‘play:query.response’({“phrase”: search_phrase, “skill_id”: self.skill_id, “searching”: true}))


i get a str object is not callable error with this.

sorry bad copy-paste-edit, should be
        self.bus.emit(message('play:query.response', {""phrase"": search_phrase,
                                                     ""skill_id"": self.skill_id,
                                                     ""searching"": true}))

(i think)

thank you!
found this for others searching…




is the request timeout message actually """"?


    hey @fruitywelsh  first up, thanks for all your efforts recently on the message bus stuff. i’m gonna make some time to go through your suggestions. i think there are certainly some improvements we can make to ensure they are more consistent and better documented. 
this one shows two different possibilities for play:query.response which as you’ve pointed out is not clear at all in the current docs. 
@forslund correct me if i’m wrong, but the play:query can actually receive three different respons…
  


"
361,really need help on how to change wake word,general discussion,"
hey everyone i’ve been trying for three days to figure it out but can’t seem to find it, so i installed everything for precise then used the precise-collect command to record 12 times of me saying computer, then (while still being in the mycroft-precise folder created a folder named test-1 (the name of my wake word) and put everything inside where it belonged… but when i used precise-train it gave an error message, i think it has to do with the location i put the recordings (they were .wav) and that that’s why it couldn’t find it. but where should i put them?
(github page, i was following source install)



github



mycroftai/mycroft-precise
a lightweight, simple-to-use, rnn wake word listener - mycroftai/mycroft-precise






","
can you post the error? what directory structure have you laid out for your training data?
you’ll also want to collect more samples, 12 is pretty low.
"
362,regarding polly tts support,general discussion,"
hello,
i see the latest release notes say - amazon polly support is now enabled. but i tried setting it up and i get the below error in audio log.

2020-05-11 21:30:30.784 | error    | 61668 | mycroft.tts.tts:create:519 | the selected tts backend couldn’t be loaded. falling back to mimic

what am i missing?
thanks!
","
can you share the “tts” section of your mycroft.conf (don’t forget to remove you credentials/api-key before sharing?)

below is my tts config. thanks for your help…

$ mycroft-config get tts
{
“pulse_duck”: false,
“module”: “polly”,
“mimic”: {
“voice”: “ap”
},
“mimic2”: {
“lang”: “en-us”,
“url”: “https://mimic-api.mycroft.ai/synthesize?text=”,
“preloaded_cache”: “/opt/mycroft/preloaded_cache/mimic2”,
“voice”: “kusal”
},
“espeak”: {
“lang”: “english-us”,
“voice”: “m1”
},
“polly”: {
“voice”: “matthew”,
“region”: “us-east-1”,
“access_key_id”: “”,
“secret_access_key”: “”
},
“bing”: {
“api_key”: “”,
“format”: “riff-16khz-16bit-mono-pcm”,
“gender”: “zirarus”,
“lang”: “en-us”
}
}


hey there, just wanted to make sure you have an access_key_id and secret_access_key?
i’m sure you don’t want to post them publicly but if they’re not in your config that will definitely be why it’s not working.
also did you install the required boto3 package?
more details on both at:
https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/customizations/tts-engine#amazon-polly

getting the same error. and yes, i have my aws credentials and boto3 intstalled. here’s my mycroft.conf from /etc/mycroft:
{
“play_wav_cmdline”: “aplay %1”,
“play_mp3_cmdline”: “mpg123 %1”,
“enclosure”: {
“platform”: “picroft”
},
“tts”: {
“module”: “polly”,
“polly”: {
“voice”: “matthew”,
“region”: “eu-central-1”,
“access_key_id”: “xxxxxxxxxxxxxxxxxxxxxx”,
“secret_access_key”: “xxxxxxxxxxxxxxxxxx”
}
},
“ipc_path”: “/ramdisk/mycroft/ipc/”
}

hey, i’m still kind of new to contributing to open source projects, but @gez-mycroft it looks like the polly_tts.py file that jarbasai wrote was never actually merged to the dev or master branches and the jarbasai/polly_tts branch was deleted. here’s the pull request where support was planned to be added (and sorry if i’m using the wrong terminology, i’m still new to git): https://github.com/mycroftai/mycroft-core/pull/1262

@misterbristles - you are right. i looked at the current dev branch (mycroft/tts/tts.py). there is no actual code to  handle polly.

you were absolutely correct, we had a mix up, thought the pr got merged but it hadn’t. it’s now in the dev branch

thanks @gez-mycroft… pulled the latest code and its working without any problems. cheers.

attempting to setup mycroft using the docker image and ran into this same problem with my mycroft.conf.
the docker image hasn’t been updated since before this thread was opened.
is there a schedule for when this will likely be merged into stable? i’m not aware of a way to easily specify running the dev branch within the docker image.
and some additional requests:
support use of “standard” or “neural” for the “engine” key. this will support use of more natural sounding voices. nueral voices are only available for a sub-selection of us and uk voices, and only when connecting  to specific regions.
https://docs.aws.amazon.com/polly/latest/dg/ntts-main.html#ntts-engine
and support use of the <amazon:domain name=“conversational”> ssml tag. this would be a bit trickier as it’s not a key value, but an additional tag that has to be inserted inside the  tag but encompassing the body of the tts content.
i think the key in the settings file would have to be parsed as a true/false and if true the tag is inserted.
“conversational” speaking style takes tts to an even more natural sounding level than even neural.
"
363,why should i choose picroft but not amazons echo dot,general discussion,"
i am shopping for an ai solution for my home. since i totally new to this and assuming others have more experience in this, i want to know what are the advantages of choosing picroft over amazon’s echo dot?
cost wise, i think the dot is a winner. at $50, it makes no sense why should i spend on raspberry pi 3, sd card, power supply, casing and a comparable mic that will end up more costly. on top of that, i have to go through the trouble of assembling everything, install the software and troubleshooting the picroft in oppose to plug and play on the dot.
skills wise, i think the dot is also a winner. from the list, i can see thousands of skills are there to install with a click of a button. as for mycroft, the only place i can find the list of skills is at https://github.com/mycroftai/mycroft-skills
hardware wise, the dot wins hand down. it is available in black or white and i believe the mic is far superior than any usb mic in the market unless you go for those enterprise level speakerphone use for tel-conference which is also costly.
dependency on third party wise, amazon is definitely more reputable and stronger than any startups.
the only advantages i can think of for mycroft are…

customization - i can change the wake word and the voice
availability - unlike echo which is only available officially in us, anyone can build picroft at anywhere.

please correct me if i am wrong. i am still sitting on the fence on this. is it worth to diy my own ai in long run? let me know your opinion. thanks.
","
open source and privacy concerns make mycroft an obvious choice for me, plus you get to learn something and understand your product to the guts

from my understanding, both products are sending my voice command to some servers in the internet for processing. how is that going to help you in privacy concern? please enlighten me.


mycroft intends to launch openstt
and you can use a local kaldi server already for offline voice recog (haven tried it but push request is in github)
and there is several options to choose from where the voice is processed, you can always add more as they appear (choose who to trust, revoke trust without changing product)

you can just proxy your mycroft connect to the server , make a new pariign code that didnt ever run on original connection, and you will be using a shared api even on googlestt, they have your voiceprint sure, but less data to process it since its mixed with lots of user’s requests
and amazon is big, for profit, they will try and harvest more data for sure,
making money out of you and listening to your voice requests is a recipe for …
also, do you know when it is recording? is it open source? or does it just promisse to not record on demand by amazon?
mycroft cant spy on you, you know the code, it listens when you order it only, guaranteed!
privacy by design, not by policy, thats how it helps me in my privacy concerns, hope this answer helped you making a choice 
edit: i actually want to discuss this subject better, so i created a topic privacy concerns and discussion

interesting discussion
i dont think mycroft vs echo as a consumer device is a fair comparison at this point in mycroft’s development life cycle / startup phase
for me in the south pacfic region here my opinion - a few points well

mycroft is open source and can be customized to local regions skills with out waiting for amazon to open up alexa skill developers ecosystem
can run on commodity rpi hw which is cheap to experiment with  and has a strong developer eco-system
can use a variety of the third party apis for weather , news etc . eg. watson news, weather.io



thanks for everyone’s input.
i can see the appeal of mycroft and i always support the open source movement as an alternative to commercial products.
however, i feel that mycroft still have rooms to grow if it wants to give real competition to the commercial counterparts.
for example, i found out that the delay in responding to my questions in raspberry pi 3 is too long to for my patience. maybe it is hardware related and can be overcome by using more powerful devices.
this is an example of little things that makes a huge difference in user experiences.
so the real question is, is it worth it to sacrifice user experience in expense of privacy?

my biggest issue so far, having built multiple instances of mycroft on different devices… has been the microphone
i can’t seem to find a good solution to having a compact, high quality microphone. there are large usb “studio” microphones (too big and expensive) small (rubbish) ones and a variety of random ones in between that are not good
for any type of iot / connected-home use mycroft will need a microphone compact enough to be mounted on a small device, yet good / powerful enough to work well when someone in the room speaks to it
the echo has a great microphone and i’m sure it’s not a standard usb mic off ebay.
anyone have a solution for a good quality, compact microphone that can be used with the pi? i’m eyeballing my old iphone at present

i’m using jabra speak410. it works pretty good. but the problem is it costs more than an echo dot. it really makes no sense to use this cost wise.

i have tried ps3 eye camera. the mic works pretty well too. you can consider that.
anyway, back to the topic.
i got my first echo dot a couple of day ago. after playing with picroft first, i must say i m impress with alexa’s performance. it is years ahead of mycroft. the hardware is beautiful and solid, the response is fast and the voice sounds so natural.
i believe mycroft is heading towards the right direction but until it is fully grown, alexa will temporary be my household ai. i will continue to tinker with mycroft as a hobby on my free time and hopefully do my bits to push mycroft as real competitor to commercial ai such as alexa and google home.

reading up on respeaker https://www.seeedstudio.com/respeaker-4-mic-array-for-raspberry-pi-p-2941.html
has 4 array mics and programmable leds haven’t tried it myself, they also have one in a pi zero factor but once again haven’t tried one myself so have no idea how easy it is to setup or it’s quality.

better mic, lower price, screen, camera.

you want to support mycroft because this technology is too important to the future of computing to remain the property of a few huge companies.  you also want to support us because we are not building a smart speaker.  we are building an ai that runs anywhere and interacts exactly like a person ( a strong ai ).
if you want a simple smart speaker and you are happy being the product, buy amazon or google.  if you want to help advance the state of the art in open source ai and support an effort to open this technology to the world, use and support mycroft.

mark 2 seems interesting. is it still based on raspberry pi?

we’re looking at that.  we want to add some new features to the next version ( array mic with beam forming, fpga based noise cancellation, touch screen, camera module ) and we’re working on the bill of materials now.
the version shown has a pi3 in it.  it is our second draft.  here is a pic of the first three drafts.  the one on the right is the latest.  is is cloth covered ( leaving more options for driver placement ) the plastics on the top leave room for a camera module/rotary encoder.

drafts-1-2-31631×1047 625 kb


is it available for pre-order?

pre-orders open january 22, 2018.
until then we are heads down trying to improve the user experience and deliver the skills we promised in our kickstarter.

let me try to answer this in 3 steps.
1.mycroft uses offline speech engines like precise, pocketsphinx.same is not true for alexa vs google.
2. command based interactions with mycroft is completely local. eg create timer, switch on light etc. same is not true for alexa/google/apple
3. for queries which need internet access (book a uber, play music from spotify) no history is maintained, but same is not true for other tech giants…
checkout this link for how mycroft ai works

also note, you can run mycroft without backend



github



jarbasskills/skill-mock-backend
disable mycroft from phoning home. contribute to jarbasskills/skill-mock-backend development by creating an account on github.





"
364,classification how to find relative coordinates,machine learning,"
hi,
i am running my object classification using raspberry pi 4, model b, coral edge tpu. i am using this command to classify the image. ‘model.classify_with_image(frame, threshold=args[“confidence”])’. it works perfectly but it does not give me coordinates like ‘model.detect_with_image()’. is there any way i can get the coordinates?
","
what’s in the logs (/var/log/mycroft/*)? what have you tried so far?
"
365,shop payment options,site feedback,"
hello,
in the shop i see only credit cards as payment options. are there no alternatives like paypal?
","
hi deutran, welcome to the forums.
we’ve discussed adding other payment options, it’s just a lot of work and we’re a pretty small team so it hasn’t been a priority yet.
it’s useful for us to hear from the community like this though, so if others are coming to ask similar things, please comment or at least like deutrans post. the more demand there is for it, the more likely we are to bump it up the backlog.

thank you, i’ll keep an eye on this.
"
366,the mycroft personal server starting the conversation,general discussion,"
originally published at:			http://mycroft.ai/blog/mycroft-personal-server-conversation/
in my july post where i introduced the mycroft roadmaps, i laid out plans for a mycroft personal server. i’ve had conversations with many about the concept, but exact goals and designs haven’t been established yet. i see this as a highly community-centric project, so i’d like to start a conversation so we can all get on the same page.
what is it?
mycroft is inherently modular, allowing pieces of the system to be moved around as appropriate for the specific implementation. up to this point, the typical implementation runs the majority of mycroft on a device such as a mark 1, raspberry pi, or laptop. this includes wake word processing, intent parsing, and text to speech(tts) (more on mimic2 tts below).
for normal operation, there is one critical piece isn’t included in that list – speech to text (stt). the typical mycroft device today uses mycroft’s home to perform the stt operation. this is automatic and invisible to most users.
the server also offers several other services that are important:

mimic2 for those using this new voice technology

general user settings


web interface to specific skill settings


the marketplace to find and install new skills


in my view, the personal server would provide some version of all of these services. it should allow a household to run all of their mycroft equipment without any network necessary until a skill needs to access the internet to retrieve information.
this means the personal server would at minimum need to run speech to text (deepspeech), text to speech (mimic), and provide a configuration web interface.
why would anyone need this?
there are several very good reasons for implementing this capability.


slow, unreliable internet - i’m personally spoiled by google fiber here in kansas city and forget that not everyone in the world has gigabit connection speeds.


limited or expensive internet - similar to the above, but slightly different motivation


no internet - yes, this exists. imagine locations in the mountains, on boats, or the far side of the moon.


privacy concerns - every time data leaves your control, there is a possibility of others misusing it, not safeguarding it adequately, or it being intercepted.


for those willing to accept the responsibility, keeping all operations within a home provides the ultimate in reliability and security.
what a personal server isn’t
the personal server is intended to be personal -- not enterprise grade. the main reason for this is simplicity. for example, if you don’t have to perform speech to text requests for thousands of users, the odds of collision are very low. that means stt requests can be run sequentially instead of requiring a bank of stt servers that can handle a high load.
a personal server also isn’t for everyone. you don’t have to be a high geek but it will require some significant computational resources, like an always-on pc with a high-quality gpu.
does this mean home is no longer needed?
no, for several reasons.
firstly, many people will still want the convenience of just plugging in their device and running it. no worries about setting up a server, no challenges accessing the web ui from their phone without firewall magic, etc. it just works.
second, there is still value in having a central collaboration hub. mycroft has always been about communal efforts, and community requires gathering places. home provides a place to:

share and assist in tagging data to advance the technology
discover new information
download voices, and skills from others
provide a gateway to access other mycroft devices and mycroft-hosted services

your thoughts?
all of the above are my thoughts. but as i said at the beginning, i want this to be a conversation. what do you want and see for the mycroft personal server? are there concerns i’m overlooking? would you like to be involved in the building of this, taking control of your own fate?
please let us know your thoughts below!
","
hi steve,
this sounds very interesting.  agreed enterprise grade machines are not for the home, however as they say, there is always one.  me!  i happen to have a reasonably pokey hp proliant dl380 generation 7.  it is sporting dual hexcore xeon processors, hyperthreaded, hence 24 cores, clocking at 2.93ghz and 32 gb of ram.  it also has about 500gb of disk space on a low latency raid5 array.  however, it lacks the gpu part.
it is currently uncommitted and has a straight install of ubuntu 16.04 server (no hypervisor).
the question is, is this platform worthy of a trial?
i am reasoning that when a packet of work comes in from a mycroft front end, then the power is drawn, following which it backs off.  on that basis, setting the machine to run distributed computing tasks, such as folding@home, makes use of the wasted idle energy.
that is the down side, it is thirsty and chews 300 watts regardless.  plus, if you go to youtube and find a video of a b52 bomber idling, yeap, that’s what it sounds like.
out of interest, what does your cloud based platform spec look like?
many thanks,
dave

if you want to run stt or tts in anywhere near real-time, you need a gpu.  even on an i7 you can’t come close to the performance of a gpu for things like this.
at the same time, the underlying technology is changing very rapidly too.  with deepspeech, the training sessions this spring took 2 weeks on 2 multi-gpu machines.  after some major rearchitecture this summer, it now can run a training session against even more data on 1 of those machines in 3 days.  so… specing exact hardware is probably not the way to start.  i think we should begin building the technology and see what is needed to support it once we have all the basic pieces in place.

hello steve,
i am quite interested in a mycroft server. i tend to run many local network services vs using the equivalent cloud services.
a few things i’d like to see in a mycroft server.

docker/docker compose as a deployment option

i recently moved from a kvm based environment to docker and i’m running more services on the same hardware. begin able to deploy/update this server using docker would be great.


incubate features.

users of the local server could opt in to beta releases/features and do an initial round of testing before that feature is deployed to the public server.


optional paid support.

while the intent clearly states personal and not for business. a server will be deployed in/on various network/hardware configurations and there will be a support cost. this could be structured in a way where all the information to troubleshoot the server would be available to those who look and completely optional.


client to server version support/compatibility.

what is the contract for backwards compatibility. will the server always support the oldest to the newest client?
will the server be able to support clients with varying versions?


ability to update similar clients with a single image.

would be nice for the server to download a single update image per client type. that way if you have multiple mark 1’s the server downloads the update/image once and the clients update via the server.



i agree with all of your points and am looking forward to a local mycroft server… though it sounds like i’ll be needing a dedicated gpu to run it 

hi,
due to mozillas blog post, it should be possible to run a 3s file in 1.5s on a “laptop cpu” with deepspeech 0.2:
https://hacks.mozilla.org/2018/09/speech-recognition-deepspeech/
so no need for a gpu anymore 

ah, i hadn’t see that blog by reuben, just heard him saying it was faster – that is closer to acceptable!  as i said, this is a moving target and it might be possible to operate without one.  some components, like the mimic2 voice generation, still perform much better on a gpu.  regardless, i think it should be part of this projects goal to not only allow a user to run a basic mycroft experience but also run mycroft as optimally as possible.  that would certainly include whatever is necessary to support gpu utilization.

super glad to hear about the home server. i like to host (almost) everything by my own, and let geek users to host themselves the services can be a good point. happy to read a gpu is not needed anymore, because many microservers has powerful xeons processors with sad integrated gpus
besides the obvious benefits you told before (connectivity and security), can be performance another benefit, if finally fast gpus aren’t needed as requirement? i mean, mimic2 is quite slow here (on a 300mb/300mb fiber connection) home server would improve this as is entirely dedicated to my few mycroft devices?
i would like also contribute with some ideas (and testing when you finally decide to release it):


docker containers: as @sampsonight suggested. the docker approach is better than git pulling as we do with mycroft-core: no matter what distro are you using, what libraries do you have, it will install with your mycroft image. on the other hand, as mycroft is not io-intensive, it doesn’t need too much kernel context changes, so containerization will provide full hardware access and easiness for deploy.


federation: as a community, we can federate ourselves, and… i don’t know, whatever we want: create a mesh network, provide some cpu power to those less capable, sharing something useful. surely you can think about something useful if you can count with some hundreds more servers. obviously, federation must be optional, and it should be profitable for the users (premium voices and so, perhaps?) and for you to invest time in developing it (cpu time and horsepower to do some secondary tasks like training voices or the like). federation algorithm should be fair enough to use the federated cpus when idle (or low usage) and have less priority and niceness to let usable the home user server.



i am glad to hear about the possibility of running without the gpu.  assuming (as i haven’t checked recently) that the effect that cryptocurrency mining had on the price a decent graphics cards is still in effect.  a quick look would say “yes it is” then really it is hard to justify the cost of building the hardware platform to host the server, if a gpu is mandatory.
as i said earlier if it can serve a purpose then i’ll happily have a go with my dl380.

i don’t want to totally promise “no gpu” – neural networks are really well suited to run on the massively parallel architecture of a gpu.  if you don’t know why, here is a little info on it.
however, even a cheapo gpu can be valuable for this.  for example, even the cheezy gpu on a raspberry pi can run some ml type tasks 3x faster than the cpu.
as for the cost of gpus, they’ve dropped significantly since the ridiculous spike this spring.  only time will tell where this goes in the long run, but i’m assuming moore’s law will make the mid- to low- power gpus cheaper, as the crypto folks really don’t get any advantage running one of them as a miner.

very valid point @steve.penrod. it does strike me though that there will be an exponential relationship where the number of parallel high performance cpu cores crosses the point where the real time advantage of a gpu becomes minimal. do you have any idea how many parallel threads are running in the deep speech platform?

i tend to use intel + integrated cpu nowadays on my linux computers because anything else gives problems at one time or another…
current devices have a somewhat improved gpu, but i had the impression that when you say gpu you really mean nvidia. is this not the case anymore? from what i read on tensorflow has a cuda backend, or a cpu backend, and cuda is nvidia specific.

as far as i see, nvidia is the king: https://www.tensorflow.org/performance/benchmarks
on the other hand i was under the impression while nvidia cards were far more powerful, ati had more processors, so why they shouldn’t be better for parallel processing?
(i need to see inside my hp microserver g8, to see if there is room for a gpu  )

this is actually one of the reasons i am following mycroft and testing it. independent infrastructure.
if you look at trends, there is probably the biggest potential for voice control/recognition within the home area in controlling things (car, home automation, etc.). all very private spaces.
you do not want to have someone listening all the time in your private space (no matter what their privacy policy says). right now there are little alternatives, but with development over time a fully independent e.g. home automation would be quite a good selling point.
this translates to business area as well. think of a voice controlled meeting room for presentations, etc. again a company would want to have control about microphones in that room.
hence a server would exactly deliver this. at first for nerds, with maturity of the technology for mass market.
yes, i am looking on this very much on the privacy side of this. however you could also make technical cases for availability and reliability of internet access.
features i would love to see:

docker, makes deployment so much easier
should be able to site serve, e.g. multiple devices in a home network
clustered training data in public and private. e.g. i can gather and use training data locally combined with a central repository
exchange training data, i can download an updated version regularly and if i want to, send my local data to the central repository.
regarding use of gpu vs cpu, i would keep it open to both.
central administration of clients via the server would be nice for larger deployments.


intel dropped the phi experiment and has started plans to make discrete gpu’s themselves.  that should tell you how they think it’s going in the cpu/gpu space.
if you have plans to run a multi-user setup, you will almost certainly want a gpu for deepspeech.  for mimic2, you definitely will.  a gpu doesn’t necessarily mean a $400 nvidia 1070, though.  i can run dozens of sentences through deepspeech a minute on a 1030 ($80ish new). mimic2 using the demo server isn’t speedy with a 1030, but it can work.
it’s more a matter of how much latency you’re comfortable with in your interactions.

i’m just getting started in the mycroft world so please pardon my ignorance but as i understand the way things are currently, everything listed under “why would anyone need this?” is already being addressed by the status quo. am i wrong?
to me, the appeal of a separate server would be:

the ability to use a more powerful machine, thereby implementing functionality that would require more horsepower than a raspi is capable of.
a distributed architecture that allows for “thin(ner) clients” around the home (i.e. pi zero w or particle photon) that can do audio-only processing for an integrated whole-home system.

i have several ideas about what item 2 might look like. integration with an affordable, easily implemented, allways-on whole-house audio system would be pretty important.

@donnybahama.  essentially three parts to the process of interacting with mycroft.
first your speech is converted to text (stt).
second. the text is analysed and applied to a “skill” that can handled your request.  the skill returns a text response.
third the response text is fed into a text to speech converter (tts) and played to you.
the second and third parts are conducted on your local machine (pc, rpi, or mark 1), however the first part requires a neural network that is very processor intensive, beyond that of household computers if you want a near real time experience (as in the answer doesn’t take 15 minutes to come back).  at the moment the heavy work is done in the cloud, using dedicated, high performance enterprise servers.  however this does mean your spoken words are sent over the internet.  if this bothers you then the object of this exercise is to bring that server power local to you and keep all your interactions with mycroft on your private network.

thanks for the clarification. i wasn’t aware that the first part wasn’t done locally. now that i know, i’m doubly supportive of a local server!

this is no different to alexa.  however in this case it is amazon who have your speech.  be aware that only the speech recorded following you saying “hey mycroft” is transmitted.  its not sending everything it hears.  can we say the same for alexa?  who knows!

hi @steve.penrod,  so what would you be thinking about, along the lines of a hardware platform for a server?  based on what i am reading here i’m getting a feel for a reasonable machine with an half decent gpu, perhaps not a $1000 beast but something reasonable.  any recommendations on the graphics card type?
then for the os, linux server of some description?
i have pci slots in the proliant so adding a gpu isn’t out of the question.
many thanks,
dave

check if your server supports power lines to the pci-e slots.  most don’t by default.  if that’s the case then you’re stuck with an nvidia 1030 or similar.  if it does, then you can probably get up to a  1050ti…beyond that requires additional power connectors that you’d have to hack into place.  doing that you could get a 1070, which is a superb card for most of these things.
i have a desktop (i7 4770) with two 1030’s in it that handles deepspeech well, and mimic2 somewhat slowly.
"
367,ec respeaker echo cancellation,mycroft project,"
i managed to get ec to work as in run but actually wondering if it works.
anyone else had any success with ec from.



github



voice-engine/ec
echo canceller, part of voice engine project. contribute to voice-engine/ec development by creating an account on github.






my main problem was when i stopped recording ec would stop.
i got past that by using a loopback device sudo modprobe snd-aloop
so firstly my /etc/asound.conf
 pcm.!default {
    type asym
    playback.pcm ""eci""
    capture.pcm ""plughw:card=loopback,dev=1""
}


pcm.eci {
    type plug
    slave {
        format s16_le
        rate 16000
        channels 1
        pcm {
            type file
            slave.pcm null
            file ""/tmp/ec.input""
            format ""raw""
        }
    }
}

pcm.eco {
    type plug
    slave.pcm {
        type fifo
        infile ""/tmp/ec.output""
        rate 16000
        format s16_le
        channels 2
    }
}

pcm.cap {
 type plug
 slave {
   pcm ""plughw:card=camerab409241,dev=0""
   channels 4
   }
 route_policy sum
}

start ec with
./ec -i 'cap' -o 'plughw:card=alsa,dev=0' &
then redirect the mic to the loopback
arecord -d eco -q -r 16000 -f s16_le -c 2 | aplay -d plughw:card=loopback,dev=0 &
the big question is does it actually work as yeah in the cli i can see aec switching on & off on play.
but does it make a difference?
","
perhaps you play play some music over the speakers while you record. then if you play back your recoding, the music should be removed isn’t it?

yeah that would seem wise 
must be because the mic volume is low, but does a pretty good job.
wake word doesn’t seem to like and its been confusing me.
that will be pretty ace if i can get the mic volume louder but it my ps3eye  and amixer or alsamixer just fail if i try and access.
its got quite low load but the ps3eye is driving me mad, i can not even get anything in alsamixer for it and was sure it used to show.
maybe on top of that the one i have is on its way out, just going to wait and wait for the respeaker.

i did some experiments with “ec” some time ago. you need an audio device that has hardware loopback channel otherwise results will be disappointing.

@stuartiannaylor you can try to let alsa ignore the pcm errors by;


github.com/j1nx/mycroftos








mycroftos: make alsa a bit more forgiving for usb cards.



        committed 09:31am - 06 apr 20 utc




          j1nx
        



+1
-0










not sure if that works for the ps3, but give it go. alsamixer doesn’t crash/exit anymore, but don’t know if it brings you anything.

that is a kernel compile option @j1nx and prob not a good idea.
ps3eye cam is a bad hack from a ps3 and to be honest now i know more its not even that cheap.
i am not going to hack a kernel just so a relatively buggy piece of equipment can work as there working usb mics such as the respeaker usb 4 mic.
prob should take a look at what they did with the driver on that.
its something to do with channel maps is it iec958? the format that sets out all the surround sound mapping as at times the ps3 has worked and not worked.
i think its been changes to the iec or maybe raspbian have been adding and removing your line at points in time, but doubt that.
@dominik i don’t know what are good results with ec are as never really listened to the output just switched it on and used.
i just used a loopback device  sudo modprobe snd-aloop and with my 50watt boom s-pipe about .7m away from the mic is an extremely loud sound source.
its sort of wierd as the echo source is rendered totally to this really quiet muffled background and my voice is at the normal recording volume of the ps3eye and seems to sound the same.
but wake word detection seems to go to hell?!
thing is the normal volume is pretty crappy with the ps3eye as always been bemused how well mycroft does when i actually record and playback myself with it.
ps3eye got me going but its destiny is the bin.
@dominik i was wondering as the instructions are pretty sparse for ec  if i was mixing ec & ec_hw as was looking at the input and and output and wondering if that made sense, as sort of doesn’t.
i will wait for the 4 mic pi respeaker so i have a clean datum to play with as the minimal load it produced is worth giving it further play.
i can not work out how the ps3eye seems to of gone worse was it my move from pi3 to pi4 or does pycroft contain that kernel option? as the thing does seem to be worse and the volume on recording is bad.
its just not a good datum for any testing and is said is destined for the bin.
ps3eye was a good recomendation when we where lacking options but there are new 2/4mic versions that are just a couple of $ more and maybe not have the possibility of being an unknown used cam that could be 15 years old.

@stuartiannaylor i know! it is just a “quick fix” to make sure stuff get’s loadeddespite alsa kernel freaking out.
did a quick test yesterday and indeed, if you have alsa ignore the ctl error, alsamixer doesn’t error out / quits selecting the ps3 card. but as i though, it doesn’t bring you anything as then of course alsamixer says there are no controls 
no clue about setting things with amixer on the cli, but as you already threw your ps3 out of the window it doesn’t matter anymore.
btw: on mycroftos with my pulseaudio setup, i was pleasantly suprised about the ps3. picked up my normal speaking voice from acroos the room. no clue how it holds up with heavy background noices though. will have a look at it later on…

that is the wierd thing @j1nx it does pick up in a quiet room across the room but if i record generally the volume is always low even close up.
sort of bemused with it  and its heading for the box of bits and pieces which is often a precusor to the bin but i am terrible for hoarding electric bits & pieces, but doubt honestly i will use it.
was wondering if others had played with ec as the config setup and instructions are not simple or great.
the load on a pi4 was nothing and didn’t try but instantly thought if that worked ok and not sure about settings or the mic i picked that would also be ok on a pi3a+ for load.
“quick fix” is great you provide i am very tempted to purchase a 4 mic clone just to say if the work without probs.


ebay



ac108 smart voice recognizer audio decoder apa102 rgb shield for raspberry pi ...
ac108 low power stereo code chip. it is designed for raspberry pi audio sound card expansion board, low power consumption, stereo code, support for high quality voice capture, also can used for diy smart sound.







i have the official respeaker 4-mic hat and it works perfectly, but to be honest you are better of buying either the 4-mic linear;
https://respeaker.io/linear_4_mic_array/
or the 6-mic;
https://respeaker.io/6_mic_array/
because of the hardware loopback channels on them. with those ec works so much better. plus you can use the audio out on those devices which are way better that the onboard jack of the rpi.
they are a bit more expensive, but give you more freedom to tinker with.

confused as the loopbacks are the echo channels for on board ec?
shouldn’t really make much difference to quality with the right algorythm?
its just the ec processing is done on the card so no load?

ps do you guys know if alsabat -pplughw:card=alsa,dev=0 -cplughw:card=device,dev=0 --roundtriplatency -b1024 actually works?
seems to come up with the same figures whatever
with ec you can set a delay which is fine but how do you know what the latency is.
thought there would be tools unless the above is correct and is 53ms but that is very good for a pi but it is a pi4 so maybe?

sorry, no clue. never looked at ec.
perhaps @dominik as he played with ec before (and discarded it, when there is no hardware loopback channel)

sorry, i don’ remember any configuration specifics - my experiments date back to august 2018 when working on the bounty source issue.

i think that is what you will find the hardware loopbacks are for built in ec.
you dont really need ec if you have hardware ec, but would have to read up with my memory.
the pulseaudio aec works well but for some is a bit load heavy i was just on a mission to find an alsa one and see what it does for load.
the other cards starting climbing out of the price bracket i was looking at   to be honest hence why was having a look at software ec.
i am not particularlly bothered about beamforming, but ec is pretty important and maybe forced to use the pulseaudio module but webtrc are no longer supporting and apparently it will go from pulse also.
decideded to read.

respeaker 4-mic linear array kit for raspberry pi support 8 input & 8 output channels in raspbian system. the first 6 input channel for microphone recording(only first 4 input channels are valid capture data), rest of 2 input channel are echo channel of playback. the first 2 output channel for playing, rest of 6 output channel are dummy.

not sure but when i read it echo channel is the audio to consider echo i thought they where looped back as that should correlate with system latency… or anything else played that you have a signal.
its one i am not going to buy so can’t tinker.
with ./ec -i plughw:1 -c 8 -l 7 -m 0,1,2,3 use loopback channel 7 but why / what is beyond me.

now then after reading the speexdsp manual which ec is based on its says you can not do echo cancelation on different sound cards as they have seperate clocks that drift and will stop the ec adapting.
i haven’t got the foggiest what that means really apart from it aint going to work well.
https://github.com/xiph/speexdsp/blob/master/doc/manual.pdf has the details @ 6.2.1
at least it gives me something to play with as now the 2 mic respeaker is looking back in the frame.
spent some time googling about hardware loopbacks and can not find a reason apart from on board ec which is great if you have it.
you can do ec without a hardware loopback but clock drift and seperate cards at least stops the algs i can get hold of.
so i guess you can use any microphone / soundcard combo if you are prepared for a vocal forrest gump otherwise it has to be same card for capture/playback or really no ec.
so have one of these on the way as much prefer the layout of the 4 mic but looking like for ec that might be a prob.
https://www.ebay.co.uk/itm/respeaker-2-mic-pi-hat-v1-0-expansion-board-for-raspberry-pi-zero-w-b-2b-3b/312242135809?
wish when they designed that they though about maybe having connectors on one side and mics, button leds on the other, but hey ho.

keep us posted! that 2-mic with it pricepoint can never be a bad decision to buy.

yeah it sort of fits my approx £10 per module price range and also means no dac is needed.
£9.57 aint bad i paid £7.99 for a ps3 eye but didn’t shop much.
the layout of the 4 mic is much better but will have to see how ec compares but prob think that will set my budget picks.
the other drawback about ec is audio quality as in all i have seen its set to 16k sampling and i am pretty sure if clock drift matters so much as does sampling rate that playback and capture must match?

ps @j1nx @dominik
there is also a python script for that ec which with a bit of molesting i guess you could add portaudio streams to the script and negate the malarky for the fifo and ec usuage.


github.com


voice-engine/voice-engine.github.io/blob/mkdocs/docs/audio_processing/aec.md
# acoustic echo cancellation

in a smart speaker, the algorithm acoustic echo cancellation (aec)  is used to cancel music, which is played by itself, from the audio captured by its microphones, so it can hear your voice clearly when it is playing music.

![](/assets/images/aec.png)

the open source library `speexdsp` has a aec algorithm. there are two examples to use it in python and c.

## using aec in python
1.  `pip3 install speexdsp`
2.  create a python script named `ec.py`

    ```
    """"""acoustic echo cancellation for wav files.""""""

    import wave
    import sys
    from speexdsp import echocanceller




  this file has been truncated. show original





""""""acoustic echo cancellation for wav files.""""""

import wave
import sys
from speexdsp import echocanceller


if len(sys.argv) < 4:
    print('usage: {} near.wav far.wav out.wav'.format(sys.argv[0]))
    sys.exit(1)

frame_size = 256

near = wave.open(sys.argv[1], 'rb')
far = wave.open(sys.argv[2], 'rb')

if near.getnchannels() > 1 or far.getnchannels() > 1:
    print('only support mono channel')
    sys.exit(2)

out = wave.open(sys.argv[3], 'wb')
out.setnchannels(near.getnchannels())
out.setsampwidth(near.getsampwidth())
out.setframerate(near.getframerate())


print('near - rate: {}, channels: {}, length: {}'.format(
        near.getframerate(),
        near.getnchannels(),
        near.getnframes() / near.getframerate()))
print('far - rate: {}, channels: {}'.format(far.getframerate(), far.getnchannels()))
echo_canceller = echocanceller.create(frame_size, 2048, near.getframerate())

in_data_len = frame_size
in_data_bytes = frame_size * 2
out_data_len = frame_size
out_data_bytes = frame_size * 2

while true:
    in_data = near.readframes(in_data_len)
    out_data = far.readframes(out_data_len)
    if len(in_data) != in_data_bytes or len(out_data) != out_data_bytes:
        break

    in_data = echo_canceller.process(in_data, out_data)

    out.writeframes(in_data)

near.close()
far.close()
out.close()

with the https://github.com/xiph/speexdsp/blob/master/doc/manual.pdf and the speexdsp source https://git.xiph.org/?p=speexdsp.git;a=summary
there is an absoute plethora of new toys to play with.
vad, agc and all sorts that will prob wait until i have a all-in-one soundcard to play with.
the tail length of the ec is confusing as there is an optimal small length for ec but also smaller means more load, so might take some playing with to get it optimal.
ec also has a delay that gives an example approximation of 200ms which when i ran alsabat -pplughw:card=alsa,dev=0 -cplughw:card=device,dev=0 --roundtriplatency -b1024 on a pi4 its a long way out as was an approx average of 50ms but don’t know how good that test is.
audacity and the other main audio app on linux that i can not remember (http://ardour.org/) have some latency tools that could prob verify alsa latency.
if you only have a tail length of 100ms then that is a country mile out.
it will be weeks before the 2 mic turns up pointless me testing on something sub optimal of 2 sound cards.
i did notice int frame_size = config.rate * 10 / 1000; // 10 ms whilst speex suggest 20ms (stereo?). noticed also the default filter length is 2048 but if 100ms is a ‘good choice’ at 16khz then shouldn’t that be 1600?
also its not an order (multiple) of the frame size.
also the buffer which is 128x the filter length ?
maybe the manual & latest release are a bit out of sync or some mistakes have been made.
been sat waiting for ages for the 4 mic and might have to wait longer as should of got the 2 mic at least for initial setup.




 stuartiannaylor:

with the https://github.com/xiph/speexdsp/blob/master/doc/manual.pdf and the speexdsp source https://git.xiph.org/?p=speexdsp.git;a=summary
there is an absoute plethora of new toys to play with.


been there, done that. i even built a ladspa plugin from speexdsp-lib myself. i never got satisfying results.



 stuartiannaylor:

now then after reading the speexdsp manual which ec is based on its says you can not do echo cancelation on different sound cards as they have seperate clocks that drift and will stop the ec adapting.
i haven’t got the foggiest what that means really apart from it aint going to work well.
https://github.com/xiph/speexdsp/blob/master/doc/manual.pdf  has the details @ 6.2.1


try to understand that and you will see that it is a waste of time to get this working on rpis for soundcards with hw-loopback. there is a reason why the better respeaker devices have a dedicated dsp onboard…

no you tried with a pi3 but from windows to pulseaudio there have been excelent software ec.
the ec part unlike beamforming isn’t really rocket science or that heavy and doesn’t nessitate a hw-loopback or dedicated dsp.
it even has working software on mobile phones, the speex implementation might not be great but from what i can see ec not has implemented it great either.
shame webrtc doesn’t have something with the aec via python but what you are saying doesn’t make sense as ec has had many software implementations on many platforms the only thing is load on a pi and with the pi4 its prob likely.
the point is the ‘better’ respeaker products create an opensource product that is out of whack with commercial reality.
maybe if you have commercial uses for them but your paying the price for a mic that is the price of a google home.
that was my main gripe with the initial designs as unless cheaper modules can be found its not about expense its cost disparity to what is already available.
so yeah i am going to give it a go. 
if its been working on android for as long as it has i am sure it can work on a pi4.



bits & qubits



acoustic echo cancellation in android using webrtc
echo cancellation is method in telephony/voip to improve voice quality by preventing echo from being created or removing it after it is already present.

what is acoustic echo?

echo is a sound or sounds caused by the reflection of sound waves from a...






if speex just doesn’t manage it then it will be a matter of pulseaudio but prob is next release its going to be dropped as its been dropped from webtrc.
so again doesn’t matter if pulseaudio or asla but an alternative to webrtc or rehash the existing code as webrtc has supposedly a great implementation of ec.
"
368,utility baby care control,skill suggestions,"
skill name: baby-care-control-skill
user story:
as a father i want this skill to write into somewhere like google spreadsheets -so i can share easily with my wife, so a nextcloud onlyoffice or collabora office spreadsheet could do the job as well- when i feed the baby and how for long, when i changed the diaper and if there was a pee, a poo, or both, or when the baby felt asleep.
that’s pretty important in the first few weeks or months of a newborn’s life, especially if he or she has been premature.
so the idea would be fill a spreadsheet with this information.

imagen954×153 11.3 kb

for those (un)fortunate who has more than one newborn child -twins, triplets, etc- it would be great to let specify the child’s name in order to fill the corresponding spreadsheet (or add an extra column with the child’s name)
nb: functionality could be expanded with supplement milk when the baby has six months, or with solid foodstuffs when is even older, so this skill could be a great companion for parents during all the baby growth.
what third party services, data sets or platforms will the skill interact with?
google spreadsheets
are there similar mycroft skills already?
no, as far as i know
what will the user speak to trigger the skill?
whenever the user says any of the following triggers, the skill has to see in first place what day is currently and fill the corresponding row.  poops and pees, navel cleanings and vitamin d supplements are auto increments, and all feeds periods should lie on the same row.


“just started breeding my son/daughter/name” and “just stopped breeding my son/daughter/name” : this will add the starting time and ending time of a breading, if it is breastfeeding, we could also specify which breast was used (left or right) so the mother can see what was the last breast used

“i’ve changed the diaper for a poo|pee|perfect to my son/daughter/name” :  this will increment the poo, pee or both columns.

“just cleaned the navel of my son/daughter/name” : this will increment the navel cleaning column.

“just gave vitamin d” to my son/daughter/name : this will increment the vitamin d column.

“when was the last breeding”: this is unnecessary, but cool, to know if it’s necessary to wake up the baby

“what was the latest breast used”: useful for those sleepy and disoriented moms who have forgotten which breast they need to use next.

what phrases will mycroft speak?
with all the “inserts” on the google spreadshet, mycroft should say something fast, just letting know the user it has recorded successfully the item on the spreadsheet, like

“done”
“pee|poo|feed|diaper|cleaning|etc recorded”

when answering to mother’s questions, like the time she feed the baby or what was the breast she used:

“last feed was at xx:xx and took y minutes”
“last breast used was left|right”

what skill settings will this skill need to store?

oauth’s credentials or another google authentication mechanism
spreadsheet name

other comments?
i find no useful software to keep track of that scenario. something that the mother could use easily at any time when she is feeding the child, and keep a detailed log somewhere where she could see the progress of her effort. besides, having it on a shared spreadsheet will help a lot to keep a detailed track and take a look anywhere -like in the pediatrician and midwife’s visit- just with her smartphone. surely there are plenty of smartphone apps doing this and a lot more, but they are not so convenient if the smartphone isn’t on the reach of the mom when she is feeding the baby, and probably that record will be missed or imprecise.
","
yeah this would have been so useful in the first few weeks when you have no idea what day it is let alone how many times the “input/output cycle” has completed!
i can see the underlying functionality being really helpful for tracking pretty much anything too - exercise, house jobs, when you actually stood up from your desk, etc.

he he he, that’s what i’m living right now… 
yes, this could be extended|forked to whatever action need to be logged and reviewed, like remote working logging, aliment ingest in a diet, water drinking, etc
i know there are plenty of backends developers would prefer to store information rather than a spreadsheet, the idea in using that is make easier for the user to see the raw data and easily share with other people if they want. i currently have created a google spreadsheet with my wife with those baby-control stuff, and created a shortcut which opens directly that spreadsheet on her phone, i’m currently using knocki to fill it, but it doesn’t works very well and it eats four aaa batteries in three days, besides she needs to remember the “knocks” for each action, and at 4 am that’s quite impossible for her, that’s why i though mycroft would do the job perfect there.
"
369,mycroft and google assistant,none,"
is not possible to connect mycroft to a google action?
","
nothing’s impossible, somethings are just improbable.
if you can program a connector, then you could connect a mycroft to your endpoint of choice.

yes! i want to program a connector. but i dont know how to do it. do you know where can i find documentation or any tutorial?

https://mycroft-ai.gitbook.io/docs/skill-development/introduction
"
370,precise download error,support,"
i am getting the following error error: could not find a version that satisfies the requirement tensorflow<1.14,>=1.13 (from mycroft-precise==0.3.0) (from versions: 2.2.0rc3, 2.2.0rc4, 2.2.0)
error: no matching distribution found for tensorflow<1.14,>=1.13 (from mycroft-precise==0.3.0)
i am running ./setup.sh file for precise download and i am on python 3.6. would anyone know how to fix it or why i am getting this? i have tried the recommendation in this forum https://github.com/mycroftai/mycroft-precise/issues/109 but still does not work.
thanks
","
what versions did it show you were available on your system when you tried the pip command?
"
371,using mycroft precise,general discussion,"
hi,
i would like to use mycroft technology in my own app. i want it it continuously listen and provide a notification when the user says a word. if i want to add mycroft precise to my code, will i need to run it in linux ubuntu or can i download and start coding in my mac using a python ide terminal.
thanks
","
try downloading and running and see how it works for you first perhaps?  you can probably make it work on a mac, though.

i tried downloading by following the git hub guide, however i kept getting an error. this is why i am not sure if its because i should be on linux or it is a different error

if you’re having an error, might be useful to look up what that is on a search engine, then if you can’t find anything that fixes it, post here with verbose text of what you’ve tried, what happened for those attempts, what the logs say, your environment you’re running in, etc.
"
372,why python is needed in data science,none,"
python is one of the top trending technology of the era and one of the most preferred programming language by software developers. it has stretched its roots deep down in the corporate industry. due to this vast inference, new age learners and working professionals are keen to learn this programming language. to curb this, various online python tutorial are available through which one can master python and
begin career as a python developer.
python is widely used in core domains like:

web development
data science
cyber security
machine learning
artificial intelligence
so, a person who is expert in python can easily build career in the above mentioned domains.

","
so…yeah, cool?

you should get a job in jeopardy as there are loads of jobs in jeopardy.




 kdeven:

so, a person who is expert in python can easily build career in the above mentioned domains.


or they can make skills for mycroft.
"
373,making api calls,none,"
hi! how can i make an api call ?
as an example, i want to do a get request to this api: http://worldtimeapi.org/
","
welcome to the community, in python try this…
import json
import requests
url = ""http://worldtimeapi.org/api/timezone/america/toronto""
headers = {'content-type': 'application/json'}
r = requests.get(url, headers=headers)
details = json.loads(r.text)
print(details)
print(details['timezone'])
print(details['abbreviation'])
print(details['utc_datetime'])
print(details['utc_offset'])



did you try requests python library yet?
here are three examples of skills from the mycroft market place that use requests:



github



forslund/skill-cocktail
mycroft skill for making drinks. contribute to forslund/skill-cocktail development by creating an account on github.









github



richhowley/national-parks-skill
us national parks skill for mycroft. contribute to richhowley/national-parks-skill development by creating an account on github.









github



andlo/iss-tracker-skill
this skill allows mycroft to tell you where the international space station(iss) in orbit realtive to the earth in latitude and longitude. it uses reverse ge - andlo/iss-tracker-skill










 pcwii:

details[‘timezone’]


thanks! i get it!
however, i will show you what i was doing:
url = “http://worldtimeapi.org/api/timezone/america/toronto”
response = requests.get(url)
self.speak_dialog(response) – here i had an error -> response is not json serializable
i try with a lot of things. the reasonable solution was doing: jsonify(response)
but, nothing. always the same error.

thank you! i will take a look!

self.speak(details[‘timezone’])
self.speak_dialog would imply your are using a .dialog file to get the response from.
"
374,respeaker 2 mic hat led and gpio skills,mycroft project,"
i just create skill for using led and gpio on respeaker 2 mic hat board
according to manual, there are 2 gpios on grove digital port,
connected to gpio12 & gpio13
i connect gpio12 to blue led and gpio13 to green led
then the skill will turn on/off if led color name being called
here is my github



github



dony71/respeaker-2mic-led-skill
mycroft a.i. unofficial - respeaker 2mic led skill.  - dony71/respeaker-2mic-led-skill






","
hey dony, great to hear of your new skill!
looks like a great little mic array pi hat too.
"
375,pip install error,general discussion,"
hey all! new to mycroft and recently picking up programming again so i can start my goal of being a lifelong learner in field i love. so this is that one project to keep referring to when my will falters. also forgive my ignorance i’m relearning python and os’s other than windows lol. i’m just trying to get this set up initially so i can start contributing lol. anywho.
ubuntu v20.02
i’ve installed the most recent version of pip (20.1.1) and verified with pip --version. when i then dev_setup.sh it get’s a decent way before uninstalling the most current version of pip for the older version (20.0.2) the giving me an error that the most reason version is up to date and i should update, then cancels the mirror install. thoughts? thanks in advance 
","
hi kayboofy,
we lock most of our packages to specific versions to be safe. it’s easy for a small update in one of our many dependencies to break everything and it has happened before.
the way it’s setup, mycroft first creates it’s own python virtual environment and then installs all the dependencies in this. so it shouldn’t be removing the more recent version of pip from your main python install, only for the virtual environment (or venv).
so you probably have your regular version of pip at:
~/.local/lib/python3.8/site-packages/pip
then if you enter the mycroft-core venv and check your pip version:
cd ~/mycroft-core
source .venv/bin/activate
pip --version

you should find that you have a completely different version of pip at:
~/mycroft-core/.venv/lib/python3.6/site-packages/pip
because it’s not the absolute latest version it will flash an error at you, but it should still work perfectly fine.
can you give some more detail on the mirror install? is that referring to apt not being able to fetch from local mirrors? if you can include any error messages you’re seeing that will be very helpful.
"
376,testing and feedback for kodi skill,skill feedback,"
kodi-skill

how to install kodi skill

install kodi skill by …

msm install https://github.com/pcwii/kodi-skill.git


requirements.txt should install kodipydent and requests, alternatively you can pip install.
pip install kodipydent
pip install requests


prepare kodi by …

configure kodi to “allow remote control via http”, under the kodi settings:services
configure kodi to “allow remote control from applications on other systems”, under the kodi settings:services
under kodi settings:services note the port number (8080)



how to test kodi skill

kodi skill connects to a single instance of kodi on your network …

after installation your mycroft.ai home page should have default settings for your kodi instance.

you will need to update these to reflect your kodi instance.
username and password are a wip.



example intents

“turn kodi notifications on”
“turn kodi notifications off”
“move the cursor up / down / left / right / back / select / cancel” (conversational context)
“play film {film name}”
“play the movie {movie name}”
“show the movie information”
“hide the movie information”
“pause the movie”
“re-start the movie”
“stop the movie”

conversational context

if mycroft.ai locates more than one movie that matches your request it will permit you to itterate through your requests using conversational context.
eg. “hey mycroft:”
request: “play the move iron man”
response: “i have located 3 movies with the name iron man, would you like me to list them?”
request: “yes” / “no”
response: “iron man, to skip, say next, say play, to play, or cancel, to stop”
request: “next” / “skip”
response: “iron man 2”
request: “play” / “select”
response: “o-k, attempting to play, iron man 2”

feedback can be via this topic or via issues on github.
enjoy - pcwii
","
i installed it but i’m not sure if the skill is working.
here’s a transcript:
play film bullit


i’m not sure i understood you.
resume kodi
kodi find bullet


you might have to say that a different way.
kodi connect to 192.168.1.199


sorry, i didn’t catch that.


based on your update i’m not sure if these commands are working. can you list which words should work or work best so i can test?

@sanderant,
a few questions for you.

did the skill install correctly?
were you able to configure the skill from the microft.ai home page?
is your kodi configured to “allow remote control via http”, under the kodi settings: services?
is your kodi set to “allow remote control from applications on other systems”, again under settings: services?
make sure that you correctly specify the port number in the mycroft home page. some systems are 80 and others are 8080.
lastly, try some of the commands from the “example intents” above.

good luck.
p.s. what platform are you running mycroft from?



did the skill install correctly?
i ran the msm install command it seemed to work.


were you able to configure the skill from the microft.ai home page?
i was able to see and set the ip and port from the home page.


is your kodi configured to “allow remote control via http”, under the kodi settings: services?
yes.


is your kodi set to “allow remote control from applications on other systems”, again under settings: services?
i now have it have enabled.


make sure that you correctly specify the port number in the mycroft home page. some systems are 80 and others are 8080.
mine is 8080 running the chorus interface i can access the web interface this way.


lastly, try some of the commands from the “example intents” above.
it doesn’t seem to recognize the commands. should it say can’t reach kodi or something?



here’s the output from a cli session:
play film bullet


sorry, i don’t understand.


turn kodi notifications on


sorry, entity “notifications” wasn’t found.


connect to kodi on 192.168.1.199


sorry, i didn’t catch that.



@sanderant,
i appreciate you testing this. “play film bullet” and “turn kodi notifications on” should work if the skill is running as expected. the last command “connect to kodi on 192.168.1.199” would not be a valid command.
are you comfortable looking at the log file?
what platform are you running on?
thanks again.

@pcwii
sure which log file should i check?
i’m running 18.2.11b on a mark i

i am not 100% certain of the location on the mark 1 but on the rpi it is located in
/var/log/mycroft-skills.log
if you search the log for kodi-skill you should see if there is an error. feel free to send me the relevant part of the log to investigate.

i think the skill isn’t being recognized or something, i’m assuming the intent_failure means it’s not finding “play film”, but just a guess. it looks like it tries the home assistant and wolframalpha skilll after failing to figure it out.
22:54:43.789 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
22:54:43.792 - skills - debug - {“data”: {“utterance”: “play film bullet”, “lang”: “en-us”}, “context”: {“ident”: “1532127282.41860961738497185”, “client_name”: “mycroft_listener”}, “type”: “intent_failure”}
22:54:43.822 - skills - debug - {“data”: {“handler”: “fallback”}, “context”: {“ident”: “1532127282.41860961738497185”, “client_name”: “mycroft_listener”}, “type”: “mycroft.skill.handler.start”}
22:54:43.832 - urllib3.connectionpool - debug - starting new http connection (1): ha
22:54:43.841 - urllib3.connectionpool - debug - ha:8123 “post /api/conversation/process http/1.1” 200 85
22:54:43.849 - mycroft.skills.padatious_service:handle_fallback:111 - debug - padatious fallback attempt: play film bullet
22:54:43.857 - wolframalphaskill - debug - wolframalpha fallback attempt: play film bullet
22:54:43.858 - wolframalphaskill - debug - non-question, ignoring: play film bullet
22:54:43.864 - fallback-aiml:load_brain:51 - info - loading brain
loading brain from /home/mycroft/.mycroft/skills/aimlfallback/bot_brain.brn…done (0 categories in 0.00 seconds)
kernel bootstrap completed in 0.00 seconds
22:54:43.873 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
22:54:43.972 - urllib3.connectionpool - debug - https://api.mycroft.ai:443 “post /v1/device/2b78a955-ef3f-4c21-804f-99437685c6d2/metric/timing http/1.1” 200 280
22:54:44.030 - urllib3.connectionpool - debug - https://api.mycroft.ai:443 “get /v1/device/2b78a955-ef3f-4c21-804f-99437685c6d2 http/1.1” 304 0
22:54:44.044 - mycroft.api:send:111 - debug - etag matched. nothing changed for: device/2b78a955-ef3f-4c21-804f-99437685c6d2
22:54:44.070 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
22:54:44.233 - urllib3.connectionpool - debug - api.mycroft.ai:443 “post /v1/device/2b78a955-ef3f-4c21-804f-99437685c6d2/metric/fallback-unknown:failed-intent http/1.1” 200 149
22:54:44.261 - urllib3.connectionpool - debug - starting new https connection (1): api.mycroft.ai:443
22:54:44.269 - skills - debug - {“data”: {“utterance”: “i don’t know what that means.”, “expect_response”: false}, “context”: {“ident”: “1532127282.41860961738497185”, “client_name”: “mycroft_listener”}, “type”: “speak”}
22:54:44.281 - skills - debug - {“data”: {“fallback_handler”: “unknownskill.handle_fallback”, “handler”: “fallback”}, “context”: {“ident”: “1532127282.41860961738497185”, “client_name”: “mycroft_listener”}, “type”: “mycroft.skill.handler.complete”}
22:54:44.303 - skills - debug - {“type”: “recognizer_loop:audio_output_start”, “context”: null, “data”: {}}
22:54:44.367 - skills - debug - {“type”: “command: mouth.talk”, “data”: {}, “context”: null}
22:54:44.431 - urllib3.connectionpool - debug - api.mycroft.ai:443 “post /v1/device/2b78a955-ef3f-4c21-804f-99437685c6d2/metric/timing http/1.1” 200 289
22:54:46.220 - skills - debug - {“type”: “recognizer_loop:audio_output_end”, “context”: null, “data”: {}}
22:54:46.309 - skills - debug - {“type”: “command: mouth.reset”, “data”: {}, “context”: null}
22:54:50.502 - skills - debug - {“data”: {}, “context”: null, “type”: “skill-date-time:timeskillupdate_display”}
22:54:50.521 - mycroft.client.enclosure.display_manager:remove_active:140 - debug - removing active skill…

are you able to find any reference to kodi-skill in the log?

this is the only thing i see, which is happening fairly frequently.
22:08:30.411 - mycroft.skills.settings:_request_my_settings:440 - debug - getting skill settings from server for kodiskill
22:09:30.598 - mycroft.skills.settings:_request_my_settings:440 - debug - getting skill settings from server for kodiskill
22:10:30.786 - mycroft.skills.settings:_request_my_settings:440 - debug - getting skill settings from server for kodiskill
22:11:30.988 - mycroft.skills.settings:_request_my_settings:440 - debug - getting skill settings from server for kodiskill
22:12:31.168 - mycroft.skills.settings:_request_my_settings:440 - debug - getting skill settings from server for kodiskill
22:13:31.353 - mycroft.skills.settings:_request_my_settings:440 - debug - getting skill settings from server for kodiskill

this is strange, it looks to me that the skill is having trouble accessing the home.mycroft.ai settings page. i could be wrong on this but i think that is the error you are getting. @kathyreid do you know what the 440 error is above?

@sanderant
are you able to check the skills directory for the name of the kodi skill folder and it’s permission? there is an issue with msm that appends the github hub username to the skill directory name and incorrectly sets the folder permission to root instead of mycroft.

@pcwii sorry that doesn’t seem to be the case:
pi@mark_1:~/skills/kodi-skill.pcwii $ ls -l
total 116
-rw-r--r-- 1 mycroft mycroft 20587 jul 20 19:55 api.txt
drwxr-xr-x 3 mycroft mycroft  4096 jul 20 19:55 dialog
-rw-r--r-- 1 mycroft mycroft 12993 jul 20 19:55 __init__.py
-rw-r--r-- 1 mycroft mycroft 35147 jul 20 19:55 license.md
-rw-r--r-- 1 mycroft mycroft   175 jul 20 19:55 links.txt
drwxr-xr-x 2 mycroft mycroft  4096 jul 20 19:56 __pycache__
-rw-r--r-- 1 mycroft mycroft  1360 jul 20 19:55 readme.md
-rw-r--r-- 1 mycroft mycroft    20 jul 20 19:55 requirements.txt
-rw-r--r-- 1 mycroft mycroft   106 jul 20 22:39 settings.json
-rw-r--r-- 1 mycroft mycroft  1347 jul 20 19:55 settingsmeta.json
drwxr-xr-x 3 mycroft mycroft  4096 jul 20 19:55 test
-rw-r--r-- 1 mycroft mycroft  3835 jul 20 19:55 tests.py
drwxr-xr-x 3 mycroft mycroft  4096 jul 20 19:55 vocab
pi@mark_1:~/skills/kodi-skill.pcwii $ ls -ld
drwxr-xr-x 7 mycroft mycroft 4096 jul 20 19:58 .

rename kodi-skill.pcwii to just kodi-skill then reboot
the .pcwii should not be there and is an issue with msm.

yay! that did it. i was able to play a movie from command (awesome!) and cycle through a list of movies and get one to play (really awesome!).
i can test specific commands now if you’d like.
this is great!

excellent news! i will try to update the installation instructions for future testing. if you have any recommendations feel free to pass them along.

sorry for taking a while to come back to you on this one.
the 440 error is not  common one; i had to go digging into the http documentation to figure out what it was for. it looks like it’s some sort of authentication timeout error
http://getstatuscode.com/440
is it still happening?

i don’t think so. i believe the issue had something to do with the fact that the msm install appended my github username .pcwii to the installation that the member performed. this was an issue with msm that we discussed under another topic a while ago. once the skill directory was renamed without the .pcwii the skill began to function as expected.

ok that’s interesting. the name of the skill directory itself doesn’t have any direct dependency on the skill functioning unless there’s some sort of hard-coded file path that the skill uses - for instance to save or access files?
"
377,easiest way to use mycroft completely offline,support,"
i have backed on the new mycroft and want to use it ideally completely offline.
i have a central home automation server, that fetches all important data from sensors or apis that should be controled by mycroft.
but i want to avoid that my speech is sent to a server outside my controlled home-subnet.
so the question is, what is the easiest way to make mycroft work offline (without an extra server or with a self-hosted one.)
","
hi there @skeltob, this is one of our most requested features - completely offline use. unfortunately we don’t have good documentation on how to do this at the moment. i do know that some of our community members have done this in the past, including @jarbas_ai and may have some guidance.




 skeltob:

i have backed on the new mycroft and want to use it ideally completely offline.
i have a central home automation server, that fetches all important data from sensors or apis that should be controled by mycroft.
but i want to avoid that my speech is sent to a server outside my controlled home-subnet.
so the question is, what is the easiest way to make mycroft work offline (without an extra server or with a self-hosted one.)


i’ve been waiting for a year on this and it still has not happened which is a complete shame considering the post about google/alexa and siri recently:
alexa_siri_google hidden command attacks




 kathyreid:

hi there @skeltob, this is one of our most requested features - completely offline use. unfortunately we don’t have good documentation on how to do this at the moment.


are there plans to implement such functionality in the future (even if it was an optional setting), @kathyreid?

it’s definitely something we want to do @gregory.opera, however it requires a bit of work on our side. our existing home.mycroft.ai platform is scaled to support tens of thousands of users, and runs across several virtual hosts - probably not all that usable as a local / personal backend. so we need to work on scaling that down.
the other layers to this problem are;


speech to text - really this is the biggest blocker at the moment. until we can get deepspeech to a point where it can run (or at least a vocabulary subset can run) on an embedded device, then we’re going to be stuck with cloud-based stt, irrespective of which cloud that runs on. there have been some substantive efforts by the deepspeech community toward this objective.


skill support - most skills need some form of internet connectivity as they’re connecting to third party apis.


configuration settings - at the moment, configuration of devices is done via skill settings at home.mycroft.ai so we would need to find a way to do configuration locally.




i’ve been waiting for a year on this and it still has not happened which is a complete shame considering the post about google/alexa and siri recently:
alexa_siri_google hidden command attacks 18

if i understand it correctly, that attack could still work on a completely offline solution.

it is possible to run it offline, but there is no out of the box solution…
if you want to get your hands dirty

remove all metrics
disable pairing
disable remote config
find an offline stt (pocketsphinx is not good…)
many skills need internet and won’t work

compare changes from my fork (slightly outdated, no py3)



github



mycroftai/mycroft-core
mycroft-core - mycroft core, the mycroft artificial intelligence platform.






i have been running deepspeech locally with the ‘pretrained’ model on a separate computer in my house recently.
it was fairly easy to set up and to point mycroft at it. the server does not have a gpu, so it’s not as fast as it could be, but i think the gain in local network speed makes it not that different from the cloud service, which is kind of slow too, in my opinion. i will probably get a gpu based server at some point, but don’t expect a huge improvement in speed, because non-gpu is already usable for the short commands i use.
the big hit i’m taking is with accuracy. i have to speak slowly, right in front of the mycroft, and leave gaps of silence between words.
i’m currently starting to research ways to better train the local service. i have not gotten very far
my pipe dream would be for the mycroft community to be able to share and asimmilate incremental training gains without sharing any audio. that’s way over my head at this point, though

you know about the deepspeech trainer, to improve accuracy:
https://home.mycroft.ai/#/deepspeech

i do now haha - thanks
will that training somehow be accessible for local deepspeech services to get? or would i be able to install the trainer software itself local?
i’ll definitely take some time and listen/train whether or not it’s doable locally - great project

if you manage to release something on windows any time soon, i’m fairly certain that it’d be really easy to do something with c# or vb.net that uses the system.speech.recognition namespace in the common language runtime.  its accuracy isn’t the best, but it is definitely a functional baseline.  and it doesn’t seem to need pauses between each word like deepspeech apparently does.

i think well trained deepspeech doesn’t require spaces between words. just the combination of my voice and the ‘pretrained’ model that mozilla distributes seems to result in that

you can also contribute to project common voice. this is the data that deepspeech is using in the end. in this way it will get at least used to your accent and tone of voice.

yeah, an offline version would be great.  my thought is, even if its a “server” program that runs on a gpu server in my house, then i link my imbeded devices to that server, and skills that need to reach out to the internet would, but most of the stuff would happen within my network would be great!
it running on the device itself would be cool, but at least for me, running a central server that those devices connect to within my house instead of the cloud would be awesome!

an offline version is on our roadmap 

just curious, https://home.mycroft.ai/#/deepspeech is just english, isn’t it? because i’ve found some sentences in spanish and in german… and they were recognized not only correctly, but written in a perfect spanish (e.g: cómo estás, even with the proper accents!)

buenos dias @malevolent! great question. deepspeech is starting to provide translations for both german and spanish. if you’re confident that the transcription is correct then you’re welcome to tag it 

wow, that’s an awesome new!  
it would be interesting to tag or filter somehow those new languages, so people who can understand them can flag them out as correct and don’t create false positives… i mean, english people who doesn’t speak any other language, most probably will flag any other language as “no” because they will think is not english. even me, who speaks spanish, doubted if mark it out or not, because  i didn’t see any language filter on the site. it would be a shame to have so many few sentences on those new languages marked as non-valid when the are really good, don’t you think?

any updates on offline/private/firewalled use?

as written in the other topic - you might want to have a look into this: https://github.com/mycroftai/personal-backend
"
378,mycroft security,general discussion,"
hi all, i’m new to mycroft and so far loving the project. i’m running mycroft on ubuntu and setting up a picroft.
when i start up mycroft i get this warning:
caution: the mycroft bus is an open websocket with no built-in security
measures.  you are responsible for protecting the local port
8181 with a firewall as appropriate.
what does this really mean? how do i protect local port 8181? i’m also curious about future security implications. if i had mycroft connected to all my iot devices in my home what security precautions would i need to take?
i ask these questions more out of curiosity than real concern. i’m curious about security with ai especially in domestic situations.
thanks, henry.
","
you can try doing:
sudo /sbin/iptables -a input -p tcp -s localhost --dport 8181 -j accept
sudo /sbin/iptables iptables -a input -p tcp --dport 8181 -j drop
perhaps add that to your start-mycroft.sh script if you want (or to your boot scripts–see https://major.io/2009/11/16/automatically-loading-iptables-on-debianubuntu/)
that would block traffic that’s not from localhost.
the other stuff…
you would want to secure your home network as much as you can, ie, wpa2 (or wpa3 when that gets here) with strong passwords, regular firmware updates, limit access to it as much as possible via mac/dhcp restrictions, monitor your network regularly, shut it down when you’re not home if you can, etc.  i go one step further and run an isolated (ie, not connected to the internet) network for the iot bits.
mycroft is regularly updated, so update your instance when you see new releases, update the underlying os and packages as well.

as a noob concerned with security, i’m trying to understand this warning. it appears port 8181 is open to internal lan networks. it looks like the port to https://api.mycroft.ai (remote server set in the ~/mycroft-core/mycroft/configuration/mycroft.conf) is communicating over secure web port 443 along with other common secure traffic such as the browser to https sites.
this other post mentions using ufw on linux to set the firewall config, yet doesnt describe what those would be. assuming the iptable setting above are answering the same or similar question, ufw can be set on kubuntu with gufw to deny both incoming and outgoing traffice on port 8181, but that only secures the intranet local network. what does this break or disable? talking to mycroft remotely when setup, obviously(?), but also the kubuntu plasmoid? what other services would it break from functioning. will try again now that better setup…
the other post (what is port 8181 for?!) also does mention an attack that looks worrisome, but ‘only’ seems to apply to data used by mycroft. i suppose if someone has access to your internal network you may have bigger problems to worry about, depending on what data you store inside mycroft.
what are the best methods to secure the traffic going to and from https://api.mycroft.ai? i did see this topic here and replied with the method to disable initiation of all connection to the main website from the readme: easiest way to use mycroft completely offline. is it just as secure as other data travelling over secure port 443?
locations of the config files:  https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/customizations/mycroft-conf
* default - mycroft-core/mycroft/configuration/mycroft.conf
* remote (from home.mycroft.ai) - /var/tmp/mycroft_web_cache.json
* system - /etc/mycroft/mycroft.conf
* user - $home/.mycroft/mycroft.conf

traffic between your mycroft instance and the backend at api.mycroft.ai is sent using the industry standard tls (aka “ssl”) encryption protocol. so that’s the same type of connection you use to connect with your email provider or bank.
locking the 8181 port down to localhost means that nothing outside your machine can communicate with it. so a plasmoid, if it’s running on the same device should be fine. a web gui running on a separate computer or server will be blocked.

thanks again @gez-mycroft. thats what i was assuming. the error message looks like it is aimed more for folks using the mycroft mark devices on their internal network. especially in response to the privacy and security concerns in comparison to google home and alexa, which are opaque and could be compromised like the chromecasts were.
also, circling back to the iptables, it looks like this accepts the source localhost and drops all others, both on tcp. a similar rule can be created in ufw or gufw if that is more readable or more persistent for others.
sudo /sbin/iptables -s localhost -a input -p tcp --dport 8181 -j accept
sudo /sbin/iptables iptables -a input -p tcp --dport 8181 -j drop

@baconator  something didnt look right about those commands, but i usually use gufw instead of commands to set firewall rules… i did put those two lines in my start-mycroft.sh script, but it looks like the second line is ‘malformed’ and doesnt work, and the first line repeatedly creates a rule in sudo iptables --list every time the startup script is run. as for permanence, i think there is an /etc file to put these commands in for system startup… or people can use gufw.
i believe its just a typo that the word iptables occurs twice, but im not sure if the source localhost -s localhost was intended to be included in the drop command or if it defaults to all sources to be dropped without specifying.
this command did work:
sudo /sbin/iptables -a input -p tcp --dport 8181 -j drop
use the -d flag to delete rules multiple times. dont know if there is a ‘delete all’ command.
sudo /sbin/iptables -d input -p tcp -s localhost --dport 8181 -j accept
then check with:
sudo iptables --list | grep 8181

to understand why that port needs to be closed in the firewall see



github



nhoya/mycroftai-rce
""zero click"" remote code execution in mycroft ai vocal assistant - nhoya/mycroftai-rce






this may be of interest:
`context sensitive access control in smart home environments
in this paper, we propose the creation of the pals system, that builds upon existing work in an attribute-based access control model, captures physical context collected from sensed data (attributes), and performs dynamic reasoning over these attributes and context-driven policies using semantic web technologies to execute access control decisions. reasoning over user context, details of the information collected by the cloud service provider, and device type our mechanism generates as a consequent access control decisions. our system’s access control decisions are supplemented by another sub-system that detects intrusions into smart home systems based on both network and behavioral data. the combined approach serves to determine indicators that a smart home system is under attack, as well as limit what data breach such attacks can achieve.

maybe someone can double check these, but to clarify i believe the correct firewall rules to block port 8181 except from localhost 127.0.0.1 are either:
using iptables:
sudo /sbin/iptables -a input -p tcp -s localhost --dport 8181 -j accept
sudo /sbin/iptables -a input -p tcp --dport 8181 -j drop

using ufw:
sudo ufw allow in from 127.0.0.1 to any port 8181
sudo ufw deny in from any to any port 8181  

using gufw (tested):
add         a 'simple' rule with rule name, allow, in, both, 8181
add an 'advanced' rule wth rule name, insert position 1, allow, in, all interfaces, log or not, both, from 127.0.0.1 port 8181, to 0.0.0.0/0, port 8181 (or leave blank).

im not positive on the destination ip, whether it should be to all or not…
"
379,no sound on wakeword recognition,mycroft project,"
mycroft (picroft) detects wake word but doesn’t make a confirmation noise. i’ll say, “hey mycroft” and will hear total silence, but if i go on to give a command, it’s responsive.
how do i set the wake word sound?
derek
","
hi there @derekcaelin, i’m ruling out a volume issue here because the volume is ok when text to speech activates.
the wake word file itself is stored at;



github



mycroftai/mycroft-core
mycroft core, the mycroft artificial intelligence platform. - mycroftai/mycroft-core






ie - on your picroft you’ll find the wav file for the wake word “ding” at this file path.
has this been modified at all?
your logs may also have some clues as to why the wake word sound isn’t “dinging” - if the raspberry pi can’t play the wav file for example then it may not play - for example if a library or a package is missing.

i had this problem and did a little digging…
for some reason the ~/.mycroft/mycroft.conf had been changed to confirm_listening false.
change it to true and the problem is solved.

hi folks,
thanks to you both. i attempted to solve via @cbxbiker61’s solution but my mycroft.conf file doesn’t reference “confirm_listening”.

{
“play_wav_cmdline”: “aplay -dhw:0,0 %1”,
“play_mp3_cmdline”: “mpg123 -a hw:0,0 %1”,
“enclosure”: {
“platform”: “picroft”
},
“tts”: {
“mimic”: {
“path”: “/usr/local/bin/mimic”
}
},
“ipc_path”: “/ramdisk/mycroft/ipc/”,
“hotwords”: {
“hey mycroft”: {
“module”: “precise”
}
},
“skillinstallerskill”: {
“path”: “/opt/venvs/mycroft-core/bin/msm”
},
}

i haven’t touched the “wake word” file, either.
will keep poking around…

hey @derekcaelin is this still occurring for you?

i ended up reinstalling picroft. at this point, my mic is no longer detected, so technically this issue is resolved! 

oh, technically resolved, but not exactly a great outcome!
what sort of microphone is it? anything in the error logs?

i am having a similar issue with mycroft on ubuntu 18.4
when i downloaded and ran, it worked great! when i restarted the service however; it no longer gave the ding after the wake word. everything works and follows command just no ding. i confirmed i have the .wav file and i did check the config file as noted above and it does say “confirm_listening true”. where else could i look, or what log would be helpful to look at?

i can confirm this behaviour in picroft unstable 2019-09-26 image
after rebooting it does recognize the wake word, but prompt it not anymore.

isn’t there a solution for the problem yet?
maybe there’s a connection: in the voice.log i got an error every time when i speak the wakeword:
2020-03-13 12:47:53.994 | info     |   778 | main:handle_wakeword:69 | wakeword detected: hey mycroft
too many arguments.
(the main has two typical python _ _ (without space) at the beginning and at the end. editor killed them)
what does this error message mean?

2020-03-13 12:47:53.994 | info | 778 | __main__ :handle_wakeword:69 | wakeword detected: hey mycroft
basically mean that the wake word was detected. the second line “too many arguments” i don’t know where it could come from. if the “ding” isn’t playing then it could be the playback command that’s failing and printing that.
what system are you running?

raspberry pi 4 with 4 gb memory, picroft 20.2.1 (master), sound managed via pulseaudio to a usb-soundcard. i’m working with a headset, mic and speakers are ok. the audio system in general is working fine. i’m only missing the sound after hey mycroft and after end of speaking. as long as i can see the logs at the mycroft-cli-client i know, if picroft has recognized my speech. but in future the pi should work “headless”. so i need acoustic feedback.

i had the same problem, the line in ~/.mycroft/mycroft.conf had been changed to
confirm_listening false.
i don’t know why it occured, but here’s how it happened:
yesterday, i told mycroft to set an alarm for today, at 08:00.
the alarm went off today at 08:00, as it should.
afterwards, there was no notification beep when speaking the wakeword.
the timestamp on the mycroft.conf file indicates it was changed at 08:00 today.
chaning the line in mycroft.conf to confirm_listening true and rebooting solved the issue.

yeah i had the same thing happening today!
mycroft woke me up for the first time. and then the whole day the wake-up-sound did not “ding”. but the .wav-file is there and the mycroft.conf file is also fine (confirm_listening true)
what can i do??

try this:



newbie problems? no audio output on usb-card, no change to german language


update! it’s more simple without killing the audio drivers for bcm2835. just edit two lines in the /etc/mycroft/mycroft.conf file. if the usb -card ist registrated as card 1 (aplay -l) then write this:
“play_wav_cmdline”: “aplay -d hw:1,0 %1”,
“play_mp3_cmdline”: “mpg123 -a hw:1,0 %1”,
and all sounds are routed to the usb sound card.


both file types .wav and .mp3 must be routed to the correct audio hardware.
"
380,speech rec for mycroftpi using kinect,general discussion,"
i was thinking of downloading the raspberry pi mycroft image.  however i was not interested in using any ol’ microphone for speech rec.  i wanted to use a 1st gen microsoft kinect for the mic.  the kinect has a microphone array so it is excellent microphone for speech recognition.
my problem is this.  i do not know how to install the kinect to be able to do this.  i have looked around and most people who talk about pi and kinect are only concerned with the video portion of this.
would one of you kindly walk me thru how to install the kinect?
much appreciated.
","
does anyone know how to do this?  havent had any answers yet.

hey dominique,
it’s something i’m wanting to do. i have a 1st gen kinect, but running into issues getting it working on the pi. it’s definitely not the easiest option, but i’ll post once i find out more.

thanks.  i would like to get this working as well.  i think the kinect using the microphone array it has onboard would make an excellent speech rec microphone.
someone turned me onto this but i do not know how to implement it.



adam's code blog – 2 jan 16



kinect support for raspberry pi using libfreenect
developing apps for the raspberry pi that utilize the xbox 360 kinect is best accomplished with libfreenect. libfreenect is an open-source library that provides access to the sensors and motors on …






hey guys, i have just bought a kinect and i am trying to set it up to use as a microphone for picroft however, i am not having much luck with it. did either of you ever get it working?

hi there, i’m pretty much doing the same thing - i have kinect (v1414) working under both raspbian & ubuntu mate (16.04) and you need to do a few things to get it working :-
raspbian - will work for cam but not mic
ubuntu mate - will work for cam & mic - preferred solution


install libfreenect & deps following this (do lsusb to check os can see kinect)
https://sites.google.com/site/hntuan94/home/kinect/-2-connect-kinect-to-raspberry-pi-2-using-libfreenect


i then test in x to see cam, etc,
test in x-windows using “python demo_cv2_async.py”   from libfreenect/wrappers/python/


get the microsoft sdk & firmware
apt-get install kinect-audio-setup


check you can see everything - note the hw: info for each
aplay -l
arecord -l


i added    ~/.asoundrc   file following instructions here
https://developers.google.com/assistant/sdk/guides/library/python/embed/audio


probably reboot now is a good time


set volumes & check devices are seen  (if level 0, press m to unmute and < > to increase vol)
alsamixer
alsactl store


check with a test wav file
aplay  filename.wav


can you record too ?
arecord --duration=5 -t wav testfile.wav


all seems to work ok for me on rpi3 with ubuntu mate, however, i can’t get it to work in mycroft !!!
i only installed last night (took all night to compile) - so if i sort this today i’ll let you know.
if anyones mic or output in mycroft with kinect is working let me know !!!
this works a dream for google assistant - but i want a custom wakeword (and snowboy doesn’t work well and i rather support mycroft).
good luck !

ok, found a typo in my conf,  i had “play_mp3_cmdline” instead of  wav  - so make sure it looks like this in the mycroft.conf file :-
// mechanism used to play wav audio files
// override: system
//“play_wav_cmdline”: “paplay %1 --stream-name=mycroft-voice”,
“play_wav_cmdline”: “aplay -dhw:0,0 %1”,
then both mic & output work (speakers from 3.5mm jack, kinect mic).
however mic input seems to freeze after a few mins ?
edit:
all working fine now, fresh ubuntu 16.04 install, kinect or usb headset working for mycroft (run in gui console !), libfreenect & servo controls all working, kinect depth & video work too 

very very late reply, but i’m trying to make the kinect work, but i’m having trouble… the above method works, but the recorded file are very very low in volume, and the part with alsamixer wont work for me …
anybody made progress or could give some advices ?
thanks a lot 

does the alsamixer volume change not work at all, or only work until you restart the device?
i’ve just heard from some people who found alsactl store didn’t always work for them, and they instead set a volume update command in their .bashrc to modify it on each boot.

hi, thanks for the reply.
there’s no control over the kinect mic in alsamixer. i’m gonna try the volume update at boot and see if there’s any change…
"
381,location causing issue when adding device,support,"
hi there,
;tldr
can only add device by using ‘wrong’ city.
when trying to add a new device at home.mycroft.ai/devices:
if i select chippenham (where i live) as city, when i click next i get to step two where i’m told the device was added and gives some commands to try. however my mycroft doesn’t acknowledge this in the cli and my list of devices is empty.
if i change the city to swindon (nearest proper city) the same thing happens.
if i change city to city of london it seems to work, mycroft acknowledges, continues setting up and appears in my devices list.
if i edit the device and try to change the city to chippenham or swindon i receive error updating device
i noticed that when selecting city there are two entries for chippenham (there are two places called chippenham) and also two entries for swindon (there’s only one swindon as far as i know).
so i have mycroft up and running, fantastic! but my city is not where i am.
","
there’s an accepted list of cities, and if yours isn’t in the list it won’t accept it.  i couldn’t figure out why it wouldn’t accept mine once, the difference was capitalizing the first letter to match their list. 

i think there’s a separate issue with some cities conflicting with others that have the same name and are in the same country. england seems particularly good at this…
unfortunately i don’t have a solution for you yet, but i’ve got it on the bug board to investigate and will add these cities as examples.
thanks for reporting them 
"
382,hardware recomendations,mycroft project,"
i have use for another mycroft unit in my home, but they’re a little hard to get a hold of these days. mark i is out of stock, and mark ii isn’t expected this year. i want an all in one unit that encloses the speaker and microphone (no usb bits). i have a 3d printer, so i’m happy to print an enclosure. i don’t want to buy a google aiy for googly reasons. does anybody have a list of hardware that just works with minimum interaction that fulfills those requirements? i’ve already checked the list found here, but nothing really jumps out as something that i can use to do this. advice would be greatly appreciated. thanks!
","
have you considered the respeaker core v2?
during the mark ii protyping phase this was a candidate as sbc and a enclosure was designed for it: https://mycroft.ai/blog/mark-ii-update-revised-architecture/#phase-ii-optimized-audio-prototype
mark ii dev team promised to published the stl for this - but didn’t happen until now. maybe they do if i am not the only one asking for it 

i suppose the tidy solution is the respeaker core2 which is a rk3288 which at a guestimate is somewhere between the pi3 & 4 in terms of performance.
you will prob just be running debian rather than raspbian.
but until the mycroft ii arives presume there is nothing that looks as finnished as the available cases and hex board arrangement that fit.



unmanned tech uk fpv shop – 7 dec 18



respeaker pro case
this little elegant case is officially produced by seeedstudio and it is probobaly the most suitable case which you can get for respeaker. the design is compact while at the same time provides…






bit pricey though but you get the snazzy far field mic array and the alango closed source audio processing.
strangely or at least i think so they didn’t give room for a speaker but you can bt5 that.
i am a big fan of the pi4 as the 2gb has more oomf and also a great price but from googling myself the enclosure is diy.
the respeaker as @dominik said is quite handy though as they include mycroft in there wiki and also you can use it as a fancy cutting edge far-field bt5 mic and mycroft could reside anywhere.
mycroft-cardboard 


hackaday.io



prototype #1 based on raspberry pi + respeaker 4 mic linear array | details |...
this is a paper case smart speaker made with raspberry pi and respeaker 4 mic linear array. see also on github 

 hardware 

  



 the hardware includes: 



  raspberry pi 3b (or 3b+)  respeaker 4 mic linear array  45mm speaker  laser-cut paper...






or



github



mycroftai/hardware-mycroft-mark-ii-rpi
mycroft's mark ii rpi mechanical, electrical and industrial designs  - mycroftai/hardware-mycroft-mark-ii-rpi







the corev2 does not fit into the “respeaker pro case” - i learnt it the hard way…

so its version 1 only then ?
that pretty crazy innit 

get the google aiy kit v1.  no google involved if you flash with picroft.




 baconator:

google aiy


i seem to be of a similar opinion of the op @linuxrants that its actually quite desirable to have something that resembles a ‘finished’ product that is not just about asthetics but a robust working unit.
it really bodes well for mycroft that respeaker core 2 doesn’t have an enclosure off the shelf.
its also why i think the mycroft ii prototype  is far off the mark as there is a huge array of kit(s) crying out for a simple enclosure system.



github



mycroftai/hardware-mycroft-mark-ii-rpi
mycroft's mark ii rpi mechanical, electrical and industrial designs  - mycroftai/hardware-mycroft-mark-ii-rpi





i am not at all sure why its has such a specific bom and design rather than a modular system that can be an open extensible system of many options and for many reason shouldn’t be a specifc model.
that it could be a respeaker core2, raspberry pi to jetson nano and really suggest thinking about some simple building block solutions that can complete an array of options to provide finished products that are fit for purpose.
i think you can have a series of stackable ‘disk’ compartment  modules that sit on top of each other and are interchangeable to give a huge array of options.
if you design it well you give even set a typle of ai format that aix could be the atx of ai.
likely needs to try and gain a low center of gravity and get weight into bottom compartments and stack up with lighter units. but also sound advantage of density.
mic array should have choice so it suits a wide array of taste, pocket and application.
starting with the excellent value 2 mic https://www.seeedstudio.com/respeaker-2-mics-pi-hat.html
its $9.90 and its amazing how hard it is to get a multichannel mics as all equivalent priced usb audio adapters seem to be mono.
i have been slowly chipping away at pulseaudio webrtc beamforming module and yesturday found a doa (direction of arrival) lib as its been puzzling me how does it beamforming with the doa that seems missing from the code.
https://github.com/voice-engine/voice-engine has a doa wake-word routine that finally allows me to add the target_direction to the beamforming routine of the webrtc aec module.
its rather inelegant as i am thinking i will have to pactl unload/load the module to update the target_direction co-ordinates but hey this is a base cheap and cheerful option.
also can be fitted with a adafruit pixel ring but again its extensible from a very basic base.



seeedstudio.com



respeaker 4-mic array for raspberry pi
alexa
price: usd 24.9







the 4mic array is excellent value has doa and all the goodies with an inbuilt pixel ring.
hard to beat @ $24.90
then the step up to the https://www.seeedstudio.com/respeaker-mic-array-v2-0.html
$64.00! but that xmos audio processor is cutting edge.
if the top of the stack (which it can be or not, but will get to that later) is a single compartment that stacks on the audio / psu / hub compartment with and an array of fixing holes that fits all or they are simply interchangable compartments of a common format.
then the next in the stack is the amp / buck / hub compartment.
next is the sbc compartment as with heatsinks they can form much of the low center of gravity.
bottom compartment is for speaker(s) and may not even have one as bt5 and true satelite seperation might also be a choice option.
each compartment is a series of open frame stacks that fit into a pvc or acrylic tube that can be vinyl or cloth wrapped to finish.
that further options should be lcd displays but here again there should be a huge array of choice.
from the adafruit ‘eyes’ module



the pi hut



adafruit animated eyes bonnet for raspberry pi pack
give your next animatronic or robotic project the gift of eyes with this snake eyes bonnet pack for raspberry pi. the snake eyes bonnet is an accessory for driving tft lcd displays, and it also provides four analog inputs for...
price: gbp 46.00







to flexible e ink or oled
https://www.eink.com/flexible-technology.html
and even standard lcd touch screens  if you wish.
i dunno what worldwide standard tubes are but 110mm as an example could squeeze but whatever is can be standardised uo to 160mm as at that size its aix format but things definately fit.
the stackable compartments don’t need to be much more than fixing rings that will help with aperture stacks if touch is going to be included but i have to say touch and far-field seem totally different applications to me.
there is loads to think about much thought needs to be about material density or mycroft will sound like a plastic hollow box.
to crown the stack it could well be possible to provide a pt (pan/tilt) camera as all the above arrays have direction of arrival technology.
that this could be for out-of-ocupancy security, baby-monitor or inter-room to door video conferencing.
its a niche market and making a specific niche product in a niche market narrows numbers massively and why it doesn’t make sense to create a specific bom product when you could satify an array of options, budget and applications.
mycroft could have instant possibilities in being the adafruit of ai as its extremely frustrating for many that there is a lack of some simple enclosures, never mind an extensible ai format that offers an aray of choice quite simply.




 stuartiannaylor:

tl;dr


if finished = cheap hardware that is known to work well + 3d printed case, that allows for a great range of answers that qualify, so lmgtfy…



thingiverse.com



things tagged with ""google aiy"" - thingiverse
thingiverse is a universe of things. download our files and build them with your lasercutter, 3d printer, or cnc.






https://www.yeggi.com/q/google+aiy/
turns out there’s quite a few interesting options already available.  i like this one: https://www.thingiverse.com/thing:2847975

haven’t used google aiy but to be honest not a fan.
from playing with pi3/4 load of wakeword/beamforming, stt , tts of any quality its not going to work on a pi zero.
even magicmirror which is liitle more than a very simple local webserver/webapp recommend something more than a pi zero.
google aiy bemuses me as the fun is likely the build but definately not the use.

when people here mention google-aiy they most likely think of the v1-kit that runs with a rpi3. don’t know if that is still available though.




 dominik:

google-aiy


dunno as its not really what it runs on, its what the sdk can do. have never bothered with it and it might be a presumption its extremely stripped down with no echo-cancellation, beamforming…
just presumed so as it can run on a pi-zero and my logic is it much can not  exist as there isn’t a hope on the zero.
prob assumption but it looks like a basic tutorial kit and introduction than anything of any use?

well i can’t wait to see the writeup of your ideas.

already have its in the above hardware recomendations
its purely a matter of choice as there is some great kit available, but just requires a simple enclosure system that can stack.
its that simple it doesn’t really warrant a writeup.
also commenting on https://github.com/mycroftai/hardware-mycroft-mark-ii-rpi/issues/2
purely feedback but thinking actually mycroft could fill some gaps in the market that have much wider turnover.
the mycroft-mark-ii is cool but its very specific in what you can use with it and just wondering why?

noob question: is a usb speaker/microphone possible?
i’m ramping up to implement mycroft.  was conceptualizing using a linux box i have available paired with something like a ue boom speaker/microphone.
thanks.

yeah but like the lower end respeaker stuff have a look at what they do with vad/aec and stuff as only the higher end have all those goodies and libs thrown in.
so you can do what they are doinf with the lower end stuff.
you should be able to use bluetooth as long as you stay away from hsp and do it a2dp.

yes, most people use usb mics and speakers.




 amiko:

ue boom speaker/microphone


i couldn’t find much about the ue boom in terms of microphone apart from its dual.
much about the ue boom is that its a high quality bluetooth device where you can  ‘double up’  and play a pair as stereo.



best buy blog – 23 jul 18



how to use multiple bluetooth speakers for amazing sound | best buy blog
need more power than just one bluetooth speaker can give you? these speakers are made to pair two... or more! check out our picks.






much about those speakers is the buetooth wideband audio and the ability to link 2 in stereo and they will latency sync.
bt5 has wireless beamforming built in and i think they use this so actually its inbuilt bt5 functionality but not absolute sure how these devices sync.
not exactly sure as maybe it does use its stereo microphone in the same way mycroft uses multi mic far field arrays.
i am actually awaiting delivery of a bt5 speaker mic and if that goes well will be buying another as i am not sure how well all this goes together in reality.
in bt5.1 they introduced  angle of arrival (aoa) and angle of departure (aod) which are used for location and tracking of devices
i am presuming this is what is used in the high end bt stereo syncing speakers rather than mic arrays.
in bt5 they doubled the bandwidth and quadrupled transmission difference so now you can run 2 wideband high quality a2dp audio streams concurrently.
it will also auto switch between to hosts and all whole load more features.
but before you go out and splash £100+ and more on a blue 5 speaker mic just bear in mind there are under £10 dongles that do the same.



tomtop



flang gs1 wireless bluetooth 5.1 receiver bt 5.1 audio adapter sales online...
only us$8.39 with fast free shipping. shop best black flang gs1 wireless bluetooth 5.1 receiver bt 5.1 audio adapter for sale, there are a wide variety of discounts waiting for you at tomtop.com
price: us$8.39







or



tomtop



bt 5.0 wirelessly receiver audio adapter transmitter 3.5mm music aux car...
only us$5.49 with fast free shipping. shop best bt 5.0 wirelessly receiver audio adapter transmitter 3.5mm music aux car for sale, there are a wide variety of discounts waiting for you at tomtop.com
price: us$5.49







you could just stick a dongle in a dumb powered satelite speaker.
but also theoretically and what i am working on is that with bt 5.0 you can have myford and a single dumb bt5.0 speaker work as a stereo pair.
there is so much with bt5.0 that my head is still spinning and will not have a full picture until i actually test.



bluetooth® technology website



core specifications | bluetooth® technology website
at the core of everything bluetooth the bluetooth® core specification defines the technology building blocks that developers use to create the interoperable devices that make up the thriving bluetooth…









bluetooth® technology website – 10 feb 20



an introduction to le audio | bluetooth® technology website
the key features of the next generation of bluetooth audio are explained, including the lc3 codec, multi-stream audio, and added support for hearing aids, plus a look at the new use case…





but generally bt5.0 is a game changer for audio distribution and home automation, but where bluez, codec support and function comes in is confusing.
if you do a trawl of youtube there are some really good videos and its surprising for $ what you can get.
i have a tpa3116 board which matches well with a 3.3"" 30watt rms speaker and a 24v supply.
the original ue boom was only 9 watt and they do so really odd s*** of supposed stereo that isn’t just audio snobery its just basic audio physics.



darko.audio – 11 jun 17



why your bluetooth speaker sucks
because stereo sound matters.






its actually quite interesting what you can do, but beware there is an absolute plethora of false claims and overpriced trash available.
its very dependent on what you want and what you have got but its likely a bt5 audio reciever attached to your existing hifi will blow the proverbials off what is currently a marketing festival of bs in personal audio.
i will be doing a write up of my findings of a 1+1 bluetooth system of a myford & single satelite with matched cones and amps in stereo.
just wish i has scopes and studio mics so i could get really geek audio engineer and present facts rather than opinion.
dunno what the dacs are like in those el cheapo dongles but at that price pron worth a go, do a google for a few reviews.
also when in stereo mode if the mics are available or switched as stereo would be 2x a2dp wideband streams and the max? as it is still slightly confusing.
but again bluetooth with its frequency hopping can run multiples together with little interference.




 amiko:

noob question: is a usb speaker/microphone possible?


and yes, as some of you properly interpreted, i had meant to type bluetooth speaker/microphone, not usb.  doh!

its ok but as i often do the same 
bluetooth is a bit more complex than you might think.
needs to be a2dp for both mic & speaker as hsp/hfp is a nightmare.
i purchased a flang as 5.1 devices cheap are pretty new and the cheap imports are often a lotto.
if you already have a speaker then give it a whirl, but a cheaper option to try thing out might be something like that flang.



habr.com



audio over bluetooth: most detailed information about profiles, codecs, and...
this article is also available in russian / эта статья также доступна на русском языке  the mass market of smartphones without the 3.5 mm audio jack changed he...






is the best info i have found and even after reading it i am not sure, so hence i am doing a ‘suck it see’ trial.
i still don’t if you sync 2 speakers does that steal the mic bandwidth and it presumes its just stereo mode then.
then there are the different versions and codecs…
hence why i am going for a 1+1 that is my own construction so the cones are matched as an amp input of a bt reciever like the flang is fairly easy for me.
i also came across some new technology with speakers that will work much better and maybe those devices conatin similar.
wave bending speakers is a new one to me and not that expensive.


parts-express.com



""tectonic tebm46c20n-4b bmr 3"" full-range speaker 4 ohm"" from...
tectonic tebm46c20n-4b bmr 3"" full-range speaker 4 ohm






the google units use smaller ones of those but you can read up about wave bending 
https://www.tectonicaudiolabs.com/
in bluetooth 5.2 they released  le audio as one of the reasons is to give a default format because currently there are a lot of proprietary adaptions.

there are lots of good ideas here, @stuartiannaylor, i’ve seen your recommendations on github and in other parts of the forum. there are some really good suggestions i especially like the idea of making a modular system. i can expand a little on why the raspberry pi based prototype is the way it is now, and why it has a specific bom (instead of something more modular). the biggest reason is that it wasn’t designed from the beginning to be a diy or maker project. the design was modified from a more production-ready design to accommodate some off the shelf components. the quick goal was to give our internal developers a dedicated piece of hardware, and also have something we could show potential investors as buyers. although it wasn’t perfect for diy we decided to release the design anyway. however, i would like to go back to the drawing board and work on a design that is meant to be a lot more accessible.
"
383,aiy voice v2 bonnet with a pi 3,mycroft project,"
hi,
i have v2 of an aiy voice kit and a couple of pi 3’s sitting around.  i know the zero won’t work with picroft but will it work with the v2 bonnet paired with a pi 3?
thanks in advance!
","
i dont have a v2, but i think that the hat can go on a pi 3 or 4 as i think the pins are the same as on the zero. and if so - i am sure you can get it to work on raspbian, and if so you can get it to work on picroft or on raspbian with a git install of mycroft.
but it is work you have to do, as picroft dosnt support the v2 aiy kit.
if you get something to work - then please share your experiance, so it can get included in the picroft as an option.

i’ll give that a try.  i did try to plug the bonnet into a pi 3 using the picroft image but then it wouldn’t even give me an ethernet connection.  should i start without plugging the bonnet in, boot up raspbian, install mycroft and then plug in the bonnet?

i would love to hear your progress on this!
i also ordered the v2 aiy kit because i wanted to build a mycroft myself but i didnt’t knew the v2 won’t work…




 zoof:

should i start without plugging the bonnet in, boot up raspbian, install mycroft and then plug in the bonnet?


i dont know - but i would try gogling around as someone else must have sme question “raspberrypi3 and google aiy voicekit v2” or try in the google aiy forums.
when it works in standard raspbian - then next step is to get mycroft to use it (and that part will be much easier)

all right guys, i have tested this several times from scratch and i think that i have everything worked out now.  if you have a aiyv2, please check it out and see if it works for you.
thanks andreas, i copied most of your code, with slight changes, credit given!



github



chiisaa/picroft-google-aiy2-voicebonnet-skill
enables google aiy2 voicebonnet button. contribute to chiisaa/picroft-google-aiy2-voicebonnet-skill development by creating an account on github.





"
384,picroft gpio skill error,general discussion,"
i install this skill below



github



mycroftai/picroft_example_skill_gpio
mycroft ai demonstration skill for using gpio pins with the rpi 3 - mycroftai/picroft_example_skill_gpio






however installation unsuccessful due to error below
any idea why?
file “/opt/mycroft/skills/picroft_example_skill_gpio/init.py”, line 56, in 
import gpio
file “/opt/mycroft/skills/picroft_example_skill_gpio/gpio.py”, line 59, in 
gpio.add_event_detect(17,gpio.both,buttonhandeler)
runtimeerror: failed to add edge detection
2020-06-11 02:26:28.962 | error    |    24 | mycroft.skills.skill_loader:_communicate_load_status:286 | skill picroft_example_skill_gpio failed to load
","
i google and find this
https://www.raspberrypi.org/forums/viewtopic.php?t=205327
but i’m running as root, and change gpio still issue
i found also need to add 99-com.rules but still issue


raspberrypi.stackexchange.com






what to do with 99-com.rules?


pi-2, update


  asked by
  
  
    bcan
  
  on 08:47am - 02 mar 17 utc








try to update the gpio user
if you are running picroft venv.
in the virtual environmnet sudo adduser mycroft gpio
sudo adduser mycroft gpio


after add this, i have different error now
2020-06-12 17:47:52.744 | info     |    61 | mycroft.skills.skill_loader:load:114 | attempting to load skill: picroft_example_skill_gpio
2020-06-12 17:47:52.754 | error    |    61 | mycroft.skills.skill_loader:_load_skill_source:215 | failed to load skill: picroft_example_skill_gpio (runtimeerror(‘conflicting edge detection already enabled for this gpio channel’,))
traceback (most recent call last):
file “/opt/mycroft/mycroft/skills/skill_loader.py”, line 208, in _load_skill_source
(’.py’, ‘rb’, imp.py_source)
file “/usr/lib/python3.6/imp.py”, line 235, in load_module
return load_source(name, filename, file)
file “/usr/lib/python3.6/imp.py”, line 172, in load_source
module = _load(spec)
file “”, line 684, in _load
file “”, line 665, in _load_unlocked
file “”, line 678, in exec_module
file “”, line 219, in _call_with_frames_removed
file “/opt/mycroft/skills/picroft_example_skill_gpio/init.py”, line 56, in 
import gpio
file “/opt/mycroft/skills/picroft_example_skill_gpio/gpio.py”, line 59, in 
gpio.add_event_detect(11,gpio.both,buttonhandeler)
runtimeerror: conflicting edge detection already enabled for this gpio channel
2020-06-12 17:47:52.759 | error    |    61 | mycroft.skills.skill_loader:_communicate_load_status:286 | skill picroft_example_skill_gpio failed to load
"
385,simplified mark ii body for fdm printing,none,"
hi i started working on a simplified body for the mark ii that can be easier on an fdm 3d printer as i had some very frustrating hours trying to print the original stl files.
this is a work in progress, the current status is 
mark2727×645 16.4 kb

the sources (freecad cad files and the stl files) can be found on my github fork https://github.com/guhl/hardware-mycroft-mark-ii-rpi
i hope to have this finished within the next week as i have all the parts for the mark ii on my desk and like to have a case for it.
have fun - guhl
","
hi,
i finished the first prototype (besides the 2 side panels  as they are not so important).
the current status is:

mark2507×501 6.35 kb

it has got all the important parts and all connectors.
i did update the github. the freecad file can be found in cad/freecad and i did export the stl files in cad/stl_simple.
i am currently printing the first prototype starting with the base so there might be some design changes based on the print experience.
all parts but the front should be printable without support material.
i will probably never become a product designer but it was an interesting experience to design this and learn freecad.
have fun - guhl

@guhl i don’t like the bom because for some reason its so dictated to specifics.
i think its quite possible to create an extensible design that allows an array of equipment and budget all from the same base.
that you can start simple by firstly getting just a simple enclosure and build up in steps  or if you want you can purchase complete all bells and whilstles, ready made.
my first issue with i posted on github wasn’t very focussed but took some time to make a clearer proposal.


github.com/mycroftai/hardware-mycroft-mark-ii-rpi








slightly bemused but prob just me



        opened 12:31pm - 14 mar 20 utc




          stuartiannaylor
        





the pi4 2gb recently dropped in price as the 1gb model is no longer produced.
https://cpc.farnell.com/raspberry-pi/rpi4-modbp-2gb-bulk/raspberry-pi-4-model-b-2gb-bulk/dp/sc15186
https://cpc.farnell.com/raspberry-pi/rpi3-modbp/raspberry-pi-3-model-b/dp/sc14882
strangely at farnell the practically same price...










opensource design isn’t easy because the materials and purchase qty you have often means everything is a compromise.

i am bemused at very specific solutions that seem at odds with a opensource collaborative project that is extremely modular with a plethora of options as that would seem to be a polar oppiste of what the project is.
it also narrows interest rather than increasing it, as either you want that very specific bom or its of no interest at all.
i think its possible to create a 4.5"" puck that is extensible, modular with a range of function and much choice of budget.
that apart from some of the enclsoure parts there isn’t really a bom as its about choice.
that you have a choice from a $7 ps3 eye usb cam, $10 respeaker 2-mics pi hat, $25    respeaker 4-mic array for raspberry pi or $99.00 respeaker core v2.0 with the last one also being your sbc.
which even though they say the are pi hats actually its any sbc that is the original ‘credit card’ profile or smaller.
pi/respeaker core v2.0 is prob best option but some may already have a tinker or rock64 or even a pi zero.
that this can start as a speakerless bluetooth ai, be just a bluetooth speaker or be ai, pan/tilt, cam display, speaker mic deluxe, personal assistant robot.
all from the same collection of extensible and modular community designs.
that as a community people can say hey anyone make ‘spi cables’, ‘cuts vinyls’ to ‘print backplates’ or even does anyone provide finnished loaded and ready to go ‘recipe’.
so i am hoping i can twist your arm and wet your appetite to start the ball rolling with an aix format specification that like atx did for pc we have a basic format that can provide a plethora of options.
also that as a community we have a look at some common engineering materials on maybe how we can molest them for others uses.
https://www.amazon.com/sch-40-pvc-end-cap/dp/b0193yh9gg

hi,
well this is a very far reaching topic that you are laying out here.
first i will definitely give this a thorough thought an see if i can and want to work on a modular chasing platform if i have time (and thanks to a certain virus this might well be the case).
i really like the idea of modular designs and a variety of options but i also would like to tell you that i am not so convinced any more that these are the keys to success.
there is always the risk of ending up in a gnu hurd situation if modular design is taken to far.
in the last 20 years i have been involved in too many community and startup projects that spread out too far until there was no focus and no coherent strategy left. in german we would call this trying to create the “eier legende woll-milch-sau”.
and regarding a restrictive bom i would like to add something.
when the 3d-printing thing started and i got involved in the reprap project there where plenty of options and modular designs and it was all just done by hard core tinkerers and diy specialists. but the first really successful model was the availability of a complete rather restrictive bom and in addition some, then small, companies that picked up the idea and basically sold the complete bom as a diy kit.
not everybody wants and has the ability to source the different parts separately or even think about individual setups.
and as the mark ii in principle is a commercial product (at least in the end i really hope that it still will be one) this community driven version could also be something like the reprap mendel prusa.
don’t take this as a turn down of your strive for a highly modular and flexible platform, i just wanted to point out a different view.
what i think would be necessary to make a mark ii community version successful would be the availability of a diy kit that you can purchase from one source. maybe even including the a 3d-printed case (probably not my version) so that people interested in building and tinkering with a voice control device can have an easy starting point.
thanks again for you feedback!
have fun - guhl

progress 
works ok, up to now.
i messed up the size of the carrier of the power supply, it is to small but works for me. fixed that in cad and will update the git soon.
but otherwise the bottom and the audio chamber look good.

have fun - guhl

i think you are right gruhl “successful would be the availability of a diy kit”.
but part of the modular thing is to be that carrot to get you started and once hooked you end up buying the lot anyway 
to be honest its not that modular as seed have by far the best array of mic kits for price.
there are a few options with sbc but max pi3 format size and min perf level gives a few choices.
its more you just don’t have to buy them all in one go or at all modular, rather than trying to adopt all.
i understand what you mean about have your eyes set so far you lose focus.
there is a huge difference with the specific job a 3d printer does than some ad hoc components for a great array modules/software provided by mycroft.ai and community.
mycroft doesn’t do a specific job like a 3d printer does.
that is why it needs to be far less specific and more general but this is at such a low level.
there isn’t even something as simple as a decent pi enclosure that can house a speaker and mic array.
even respeaker core2 don’t have an enclosure and the core 1 doesn’t fit a speaker.
i really agree with the idea of a diy kit and and slightly bemused when the absolute basic blocks are absent and bemused that mycroft are not rubbing there hands at the possibility of some  simple solutions and the traffic that will garner as elsewhere it doesn’t exist, so it would come.
so much so that i am one of those without a 3d printer and poor old myford is going to reside in a 4"" shit pipe as its called here in the uk.
so yeah i am the same and have a strong wish they would do a diy kit, but if you think shit pipe will provide vastly better audio quality and generally overall solution, like i do.
your not going to buy but just wish someone somewhere would create a kit and set a format in a similar way they have a framework rather than just a binary blob.

i don’t at all have your cad or printing talents so you will have to forgive the quality of the next image but helps describe what i mean.

speaker-roof960×540 6.09 kb

in my s-pipe version i wish to adapt what the google engineers did with a cone facing a single speaker, its a slight curved cone also but on the right i just drew that awfull triangle to represent the side view of a cone  .
my drawing skills are so bad that the pointless effort of centering the left v shape means its too low, the bottom of the v should be the center of a circle.
you will have to forgive the drawing as said and forgot the solidworks term which is oppisite to extrude but if you started with that standard cone and shaped it into that v that is something like what i am expecting.
the badly worded oppiste extrude creates a chamfer so its a wider and more directional, also harder to explain but basically that and blended to create a final form.
the myford puck i am playing with uses schedule 40 4"" pipe with a 3.3"" full range driver.
so the imaginery speaker cutout is 75mm in an inside diameter sched 40 4"" tube of 102.2604 mm.
the cutout is hidden which is good for the likes of me as we can be quite brutal with a jigsaw or dremmel.
the actual shape is far from formalised but its an impossible shape and profile to describe or at least it was for me.
it just makes more sense to describe how i am going to assemble.
on top of the tube i have a 6mm thick circular acylic disk as i can get those cut pretty cheap.
its 114.3mm diameter and sits on 4 hex spacers that go to another acrylic disk that is the internal diameter of 102.2604 mm.
the spacers raise the top disk so there is 10mm gap from tube to the lid forming top disc that does a similar thing as the google design but rather than omnidirectional it creates an unidirectional sound pattern.
its actually a really complex shape that 3d tools would have no problem making.
as well as creating the dispersion pattern of sound it acts as an acoutic cover to the microphone array.
also the v shape is to house the possibility of a tilt mechanism and provide a path for a pi cam style ribbon cable to pass down to the lower non acoustic compartment of housed electronics.
@guhl if you ever have any spare time then please give it some thought as you seem to be really talented that way.
i am a terrible hack with code and also the same when it comes design assembly but part from that one 3d printed part i can accomplish much with just a drill and some simple tools.
the top disk as said fits on of 4 spacers that on the underside hold mic array and this wierd and wonderful sound dispersion cover.
they go to another acrylic disk that i forgot about which is the speaker cut-out (75mm) but underneath the spacer pillars continue to the bottom acrylic disc that continue again to a frb board that makes the base of the sbc/amp/buck housing.
those 2 disk should really be sealed with resin, hot glue maybe silcone or something but they make quite an adequate acoustic cabinet once sealed.
if you are going to go all audiophile you might want to line with some rubber sheeting.
what i am proposing is super low tech that i am wondering how you can produce such a great and complex design and then have an opinion and belief people might get lost in modularity complexity.
the modularity is so simple that the choice and position of the drill holes allows you to mount and array of mic boards, sbc and amps that can be printed out on a paper guide.
the only thing that is complex is the shape form of “dispersion cone” and is perfect for 3d printing as apart from bespoke molded parts there isn’t really a way to do it with conventual engineering or materials.
like i say if you ever do have any spare time or anyone else in the community who is a solidworks or whatever and 3d printing guy then please do.
i have ordered a few bits of plastic just to build one up minus those essential bits of finnese.
i will post some pics when its assembled as you never know it might perk some interest.
so the s-pipe is on its way.

don’t know if this meets the basic idea of what you are proposing - for the respeaker core v2 there is a case assembly which consists of “discs and spacers”: https://github.com/seeeddocument/respeaker_v2/raw/master/res/respeaker_core_v2.0_case_assembly.pdf
somewhere in the wild (a.k.a. internet) there were actual photos of this but i can’t find them anymore…
stls and dxf for this: https://github.com/seeeddocument/respeaker_v2/raw/master/res/respeaker_core_v2_box.zip
https://github.com/respeaker/get_started_with_respeaker/raw/8111196e821fec10c65b00d96cf011dc90111546/files/respeaker_core_v2_case.dxf

yeah that brill @dominik to be honest what i am doing is just an example and even hobnailed myself by picking us 4"" schedule pipe puely for example, as with the respeaker core2 it might be a tight squeeze on what i am doing.
110mm standard eu tube might of been a better option.
but that is exactly the sort of thing where you can make modular systems and its not much more than hex pillars in some sort of enclosure.
its what  @guhl said about an offcial kit and what i m trying to demonstrate that if one was available all that is needed is a couple or parts from simple precut tube, some discs and you have that modularity.
the respeaker core2 is pretty cool as its would allow the lowest profile design, but there is definately a common set of a few parts that would allow some simple modular choices.
i am trying to work out what they are doing in that pdf is it something similar to a pibow case?
but yeah with a common tube size you can make an array of modular pucks.
someone could make decent $ as the engineering materials are really cheap so they could be offered at quite a decent price and still make good $
with a vinyl kit to finish.
the main drawback of the respeaker core2 is the closed source alango routines that for opensource is slightly taboo but as a bit of kit its a great bit of kit.
the onboard amp can only drive 8 watt, but it would also fit the exact same orangement of the pi version but just have a but more space.
you could add an amp then, but its just a stack on some common kit materials which currently are lacking for all.
s-pipe is extremely cheap for a very dense and ridgid external tube.
https://www.plasticdrainage.co.uk/plain-ended-pipe-3-mtr-4-8297.html
3 meters costs £7.50 in the uk and could be cut into 18 - 20 sections that cost price is £0.40
the material costs of cut discs if sourced is equally cheap.
you could easily create a £10 - 20 kit with quite a whopping markup that incudes some printed parts and vinyls.
or as seperate items.

and this is what it looks like.

i had some delay because my 3d-printer broke down (it is a very old reprap mendel prusa) and i had to rebuild some parts of it. (but i managed to print the necessary spare parts before it broke totally).
unfortunately the threaded inserts were not delivered yet (and who knows when they will be) so i can not finally assemble the body.
but so far everything looks fine.
have fun - guhl

hi,
from my perspective this is complete. i did some fixes to the cad and added the sides.
all stls are complete and the repository is up to date.
have fun - guhl

hi,
this is the complete assembled body including all the installation.

during the final assembly i detected some things that should be improved. i will do that and update the git afterwards.
now for the software, …
have fun - guhl

hi,
i did some changes to the freecad file and the stls and updated the git repository.
changes

increased the height by 4 mm to get some more space for the mic array
reduced the size of the holes for the threaded inserts

have fun guhl
"
386,mycroft not responding ubuntu 20 04,support,"
mycroft is running and the cli shows that my voice is being recorded and mycroft sets the requested timer -
history ==============================    log output legend ====== mic level ===
set a timer                              debug output


how long of a timer?                  skills.log, other
how long of a timer?                  voice.log
set a timer for 2 minutes


i’m starting a timer for two
minutes


however, mycroft cannot seem to start the audio service.
audio logs:
2020-06-10 23:09:39.502 | info     | 25393 | mycroft.messagebus.load_config:load_message_bus_config:33 | loading message bus configs
2020-06-10 23:09:39.960 | error    | 25393 | mycroft.tts.tts:create:527 | the tts could not be loaded.
traceback (most recent call last):
file “/home/justin/mycroft-core/mycroft/tts/mimic_tts.py”, line 187, in validate_connection
subprocess.call([bin, ‘–version’])
file “/usr/lib/python3.8/subprocess.py”, line 340, in call
with popen(*popenargs, **kwargs) as p:
file “/usr/lib/python3.8/subprocess.py”, line 854, in init
self._execute_child(args, executable, preexec_fn, close_fds,
file “/usr/lib/python3.8/subprocess.py”, line 1583, in _execute_child
and os.path.dirname(executable)
file “/usr/lib/python3.8/posixpath.py”, line 152, in dirname
p = os.fspath§
typeerror: expected str, bytes or os.pathlike object, not nonetype
during handling of the above exception, another exception occurred:
traceback (most recent call last):
file “/home/justin/mycroft-core/mycroft/tts/tts.py”, line 517, in create
tts.validator.validate()
file “/home/justin/mycroft-core/mycroft/tts/tts.py”, line 435, in validate
self.validate_connection()
file “/home/justin/mycroft-core/mycroft/tts/mimic_tts.py”, line 189, in validate_connection
log.info(""failed to find mimic at: "" + bin)
typeerror: can only concatenate str (not “nonetype”) to str
traceback (most recent call last):
file “/home/justin/mycroft-core/mycroft/tts/mimic_tts.py”, line 187, in validate_connection
subprocess.call([bin, ‘–version’])
file “/usr/lib/python3.8/subprocess.py”, line 340, in call
with popen(*popenargs, **kwargs) as p:
file “/usr/lib/python3.8/subprocess.py”, line 854, in init
self._execute_child(args, executable, preexec_fn, close_fds,
file “/usr/lib/python3.8/subprocess.py”, line 1583, in _execute_child
and os.path.dirname(executable)
file “/usr/lib/python3.8/posixpath.py”, line 152, in dirname
p = os.fspath§
typeerror: expected str, bytes or os.pathlike object, not nonetype
during handling of the above exception, another exception occurred:
traceback (most recent call last):
file “/usr/lib/python3.8/runpy.py”, line 193, in _run_module_as_main
return _run_code(code, main_globals, none,
file “/usr/lib/python3.8/runpy.py”, line 86, in _run_code
exec(code, run_globals)
file “/home/justin/mycroft-core/mycroft/audio/main.py”, line 49, in 
main()
file “/home/justin/mycroft-core/mycroft/audio/main.py”, line 36, in main
speech.init(bus)
file “/home/justin/mycroft-core/mycroft/audio/speech.py”, line 182, in init
tts = ttsfactory.create()
file “/home/justin/mycroft-core/mycroft/tts/tts.py”, line 517, in create
tts.validator.validate()
file “/home/justin/mycroft-core/mycroft/tts/tts.py”, line 435, in validate
self.validate_connection()
file “/home/justin/mycroft-core/mycroft/tts/mimic_tts.py”, line 189, in validate_connection
log.info(""failed to find mimic at: "" + bin)
typeerror: can only concatenate str (not “nonetype”) to str
exception ignored in: <module ‘threading’ from ‘/usr/lib/python3.8/threading.py’>
please let me know what i have done wrong 
thank you!
(edited) just noticed the following, as well.
2020-06-10 23:09:39.433 | info     | 25421 | mycroft.messagebus.load_config:load_message_bus_config:33 | loading message bus configs
alsa lib pcm_dsnoop.c:641:(snd_pcm_dsnoop_open) unable to open slave
alsa lib pcm_dmix.c:1089:(snd_pcm_dmix_open) unable to open slave
alsa lib pcm.c:2642:(snd_pcm_open_noupdate) unknown pcm cards.pcm.rear
alsa lib pcm.c:2642:(snd_pcm_open_noupdate) unknown pcm cards.pcm.center_lfe
alsa lib pcm.c:2642:(snd_pcm_open_noupdate) unknown pcm cards.pcm.side
alsa lib pcm_oss.c:377:(_snd_pcm_oss_open) unknown field port
alsa lib pcm_oss.c:377:(_snd_pcm_oss_open) unknown field port
alsa lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) invalid type for card
alsa lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) invalid type for card
alsa lib pcm_dmix.c:1089:(snd_pcm_dmix_open) unable to open slave
2020-06-10 23:09:39.624 | info     | 25421 | mycroft.client.speech.listener:create_wake_word_recognizer:323 | creating wake word engine
2020-06-10 23:09:39.624 | info     | 25421 | mycroft.client.speech.listener:create_wake_word_recognizer:346 | using hotword entry for hey mycroft
","
reinstalled (including mimic this time) and it now works as expected.
"
387,conversational context timeout,general discussion,"
when performing conversational context in a skill is it possible to know if the “expect_response=true” for speak_dialog has timed out? i ask as i would like to use this to clear my context?
",
388,custom wake word precise installation problems,support,"
hey,
so i installed picroft and everything is running. now i want to use a custom wake word and followed the documentation form here: https://github.com/mycroftai/mycroft-precise#source-install
so far the installation of the dependencies worked out, but i don't understand the last chapter which comes after the **./setup.sh**

it says that i can write my programm, but i don't really get what to put in it and how to store/use it.

when i tried to: **precise-listen my_model_file.pb**
i get these errors:
´´
warning:tensorflow:from /home/pi/precise-data/mycroft-precise/.venv/lib/python3.7/site-packages/tensorflow/__init__.py:98: the name tf.auto_reuse is deprecated. please use tf.compat.v1.auto_reuse instead.

warning:tensorflow:from /home/pi/precise-data/mycroft-precise/.venv/lib/python3.7/site-packages/tensorflow/__init__.py:98: the name tf.attrvalue is deprecated. please use tf.compat.v1.attrvalue instead.

warning:tensorflow:from /home/pi/precise-data/mycroft-precise/.venv/lib/python3.7/site-packages/tensorflow/__init__.py:98: the name tf.compiler_version is deprecated. please use tf.version.compiler_version instead.

warning:tensorflow:from /home/pi/precise-data/mycroft-precise/.venv/lib/python3.7/site-packages/tensorflow/__init__.py:98: the name tf.cxx11_abi_flag is deprecated. please use tf.sysconfig.cxx11_abi_flag instead.

warning:tensorflow:from /home/pi/precise-data/mycroft-precise/.venv/lib/python3.7/site-packages/tensorflow/__init__.py:98: the name tf.conditionalaccumulator is deprecated. please use tf.compat.v1.conditionalaccumulator instead.

traceback (most recent call last):
  file ""/home/pi/precise-data/mycroft-precise/.venv/bin/precise-listen"", line 11, in <module>
    load_entry_point('mycroft-precise', 'console_scripts', 'precise-listen')()
  file ""/home/pi/precise-data/mycroft-precise/precise/scripts/base_script.py"", line 43, in run_main
    script = cls(args)
  file ""/home/pi/precise-data/mycroft-precise/precise/scripts/listen.py"", line 58, in __init__
    self.listener = listener(args.model, args.chunk_size)
  file ""/home/pi/precise-data/mycroft-precise/precise/network_runner.py"", line 107, in __init__
    self.runner = runner_cls(model_name)
  file ""/home/pi/precise-data/mycroft-precise/precise/network_runner.py"", line 51, in __init__
    self.graph = self.load_graph(model_name)
  file ""/home/pi/precise-data/mycroft-precise/precise/network_runner.py"", line 62, in load_graph
    with open(model_file, ""rb"") as f:
filenotfounderror: [errno 2] no such file or directory: 'my_model_file.pb'

can someone help me out?
best regards
vadi
","
it’s not able to see the “my_model_file.pb”.  where is that file and does it have read permissions for the current user?

ok i got that the sample programm should be called my_model_file.pb so i put in the code which is in the documentation.
i think that my_model_file.pb is the model which you train later, but i still get errors
warning: failed to load parameters from my_model_file.pb.params
but i don’t know what code to write into the file. my_model_file.pb.params

which steps in the docs have you completed so far?  what were the results of those steps?

hey vadi,
the .pb and .pb.params files are your wake word model. if you haven’t yet trained one that’s what you’re missing.
you can also grab existing v0.2.0 models from this repo.

ok yesterday i got a bit confused with the documentation and tried it again today and got a bit furhter untill i tried to train precise with this:
precise-train -e 60 felix-filgis.net felix-filgis/
first i got some warnings about tensorflow but it seemed to have worked.
then when i tried precise-listen felix-filgis.net
i got this load of errors:
use tf.where in 2.0, which has the same broadcast rule as np.where
expression 'alsa_snd_pcm_hw_params_set_period_size_near( pcm, hwparams, &alsaperiodframes, &dir )' failed in 'src/hostapi                                                                                                                    /alsa/pa_linux_alsa.c', line: 924
expression 'alsa_snd_pcm_hw_params_set_period_size_near( pcm, hwparams, &alsaperiodframes, &dir )' failed in 'src/hostapi                                                                                                                    /alsa/pa_linux_alsa.c', line: 924
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.front.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm front
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.rear
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.center_lfe
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.side
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.surround51.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround21
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.surround51.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround21
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.surround40.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround40
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.surround51.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround41
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.surround51.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround50
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.surround51.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround51
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.surround71.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround71
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.iec958.0:card=0,aes0=4,aes1=1                                                                                                                    30,aes2=0,aes3=2'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm iec958
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.iec958.0:card=0,aes0=4,aes1=1                                                                                                                    30,aes2=0,aes3=2'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm spdif
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.iec958.0:card=0,aes0=4,aes1=1                                                                                                                    30,aes2=0,aes3=2'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm spdif
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.hdmi
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.hdmi
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.modem
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.modem
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.phoneline
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.phoneline
expression 'alsa_snd_pcm_hw_params_set_period_size_near( pcm, hwparams, &alsaperiodframes, &dir )' failed in 'src/hostapi                                                                                                                    /alsa/pa_linux_alsa.c', line: 934
expression 'alsa_snd_pcm_hw_params_set_period_size_near( pcm, hwparams, &alsaperiodframes, &dir )' failed in 'src/hostapi                                                                                                                    /alsa/pa_linux_alsa.c', line: 934
expression 'alsa_snd_pcm_hw_params_set_period_size_near( pcm, hwparams, &alsaperiodframes, &dir )' failed in 'src/hostapi                                                                                                                    /alsa/pa_linux_alsa.c', line: 934
alsa lib pcm_oss.c:377:(_snd_pcm_oss_open) unknown field port
alsa lib pcm_oss.c:377:(_snd_pcm_oss_open) unknown field port
alsa lib pcm_a52.c:823:(_snd_pcm_a52_open) a52 is only for playback
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.iec958.0:card=0,aes0=6,aes1=1                                                                                                                    30,aes2=0,aes3=2'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm iec958:{aes0 0x6 aes1 0x82 aes2 0x0 aes3 0x2  card 0}
alsa lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) invalid type for card
alsa lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) invalid type for card
xxxxxxxxxxxxxxx-----------------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx----------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx----------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx----------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx----------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx----------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------
^c


looks like it needs more data, particularly not-wake-words.

were you worried about the alsa errors? looks like it’s managed to pick up your device anyway given the output below.

hello again and thank you for the last tipps.
i recorded more data and trained the model. like @gez-mycroft said the errors were to be ignored.
so everything looked good until i tried to do the precise-test where the script crashed.

image699×391 5.93 kb

since this is only to test the wake word i am not shure if  i can just ignore it.
i tried it with the precise-listen command and it looks like i need more recordings of the wake word.
the output looked like this. am i correct that when a x is followed by a x it has detected the wake word?
(.venv) pi@picroft:~/mycroft-precise $ precise-listen felix-filgis.net
warning:tensorflow:from /home/pi/mycroft-precise/.venv/lib/python3.7/site-packages/tensorflow/__init__.py:98: the name tf.auto_reuse is deprecated. please use tf.compat.v1.auto_reuse instead.

warning:tensorflow:from /home/pi/mycroft-precise/.venv/lib/python3.7/site-packages/tensorflow/__init__.py:98: the name tf.attrvalue is deprecated. please use tf.compat.v1.attrvalue instead.

warning:tensorflow:from /home/pi/mycroft-precise/.venv/lib/python3.7/site-packages/tensorflow/__init__.py:98: the name tf.compiler_version is deprecated. please use tf.version.compiler_version instead.

warning:tensorflow:from /home/pi/mycroft-precise/.venv/lib/python3.7/site-packages/tensorflow/__init__.py:98: the name tf.cxx11_abi_flag is deprecated. please use tf.sysconfig.cxx11_abi_flag instead.

warning:tensorflow:from /home/pi/mycroft-precise/.venv/lib/python3.7/site-packages/tensorflow/__init__.py:98: the name tf.conditionalaccumulator is deprecated. please use tf.compat.v1.conditionalaccumulator instead.

using tensorflow backend.
warning:tensorflow:from /home/pi/mycroft-precise/.venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3138: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
instructions for updating:
please use `rate` instead of `keep_prob`. rate should be set to `rate = 1 - keep_prob`.
warning:tensorflow:from /home/pi/mycroft-precise/.venv/lib/python3.7/site-packages/keras/optimizers.py:757: the name tf.train.optimizer is deprecated. please use tf.compat.v1.train.optimizer instead.

warning:tensorflow:from /home/pi/mycroft-precise/.venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1251: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
instructions for updating:
use tf.where in 2.0, which has the same broadcast rule as np.where
expression 'alsa_snd_pcm_hw_params_set_period_size_near( pcm, hwparams, &alsaperiodframes, &dir )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 924
expression 'alsa_snd_pcm_hw_params_set_period_size_near( pcm, hwparams, &alsaperiodframes, &dir )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 924
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.front.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm front
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.rear
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.center_lfe
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.side
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.surround51.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround21
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.surround51.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround21
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.surround40.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround40
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.surround51.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround41
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.surround51.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround50
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.surround51.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround51
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.surround71.0:card=0'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm surround71
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.iec958.0:card=0,aes0=4,aes1=130,aes2=0,aes3=2'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm iec958
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.iec958.0:card=0,aes0=4,aes1=130,aes2=0,aes3=2'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm spdif
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.iec958.0:card=0,aes0=4,aes1=130,aes2=0,aes3=2'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm spdif
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.hdmi
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.hdmi
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.modem
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.modem
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.phoneline
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm cards.pcm.phoneline
expression 'alsa_snd_pcm_hw_params_set_period_size_near( pcm, hwparams, &alsaperiodframes, &dir )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 934
expression 'alsa_snd_pcm_hw_params_set_period_size_near( pcm, hwparams, &alsaperiodframes, &dir )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 934
expression 'alsa_snd_pcm_hw_params_set_period_size_near( pcm, hwparams, &alsaperiodframes, &dir )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 934
alsa lib pcm_oss.c:377:(_snd_pcm_oss_open) unknown field port
alsa lib pcm_oss.c:377:(_snd_pcm_oss_open) unknown field port
alsa lib pcm_a52.c:823:(_snd_pcm_a52_open) a52 is only for playback
alsa lib confmisc.c:1281:(snd_func_refer) unable to find definition 'cards.bcm2835_alsa.pcm.iec958.0:card=0,aes0=6,aes1=130,aes2=0,aes3=2'
alsa lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: no such file or directory
alsa lib conf.c:5047:(snd_config_expand) evaluate error: no such file or directory
alsa lib pcm.c:2565:(snd_pcm_open_noupdate) unknown pcm iec958:{aes0 0x6 aes1 0x82 aes2 0x0 aes3 0x2  card 0}
alsa lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) invalid type for card
alsa lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) invalid type for card
xxxxxxxxxxxxxx------------------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxx-----------------------------------------------------------
xxxxxxxxxxxxxxxxxxxx------------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxx----------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxx----------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxx-----------------------------------------------------------
xxxxxxxxxxxxxxxxxxx-------------------------------------------------------------
xxxxxxxxxxxxxxxxxxx-------------------------------------------------------------
xxxxxxxxxxxxxxxxxxx-------------------------------------------------------------
xxxxxxxxxxxxxxxxxxx-------------------------------------------------------------
xxxxxxxxxxxxxxxxxxx-------------------------------------------------------------
xxxxxxxxxxxxxxxx----------------------------------------------------------------
xxxxxxxxxxxxxxxxx---------------------------------------------------------------
xxxxxxxxxxxxxxxxx---------------------------------------------------------------
xxxxxxxxxxxxxxxxxxxx------------------------------------------------------------
xxxxxxxxxxxxxxxxx---------------------------------------------------------------
xxxxxxxxxxxxxxxx----------------------------------------------------------------
xxxxxxxxxxxxxxxx----------------------------------------------------------------
xxxxxxxxxxxxxxxx----------------------------------------------------------------
xxxxxxxxxxxxxxxxx---------------------------------------------------------------
xxxxxxxxxxxxxxxx----------------------------------------------------------------
xxxxxxxxxxxxxxx-----------------------------------------------------------------
xxxxxxxxxxxxxx------------------------------------------------------------------
xxxxxxxxxxxxxx------------------------------------------------------------------
xxxxxxxxxxxxxxx-----------------------------------------------------------------
xxxxxxxxxxxxxxx-----------------------------------------------------------------
xxxxxxxxxxxx--------------------------------------------------------------------
xxxxxxxxxxxxxx------------------------------------------------------------------
xxxxxxxxxxxxxxxx----------------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx----------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx--------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxx---------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxx-----------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx--------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx----------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxx--------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxx----------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxx----------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxx-----------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxx--------------------------------------------------------
xxxxxxxxxxxxxxxxxxxx------------------------------------------------------------
xxxxxxxxxxxxxxxxxxxx------------------------------------------------------------
xxxxxxxxxxxxxxxxxxx-------------------------------------------------------------
xxxxxxxxxxxxxxxxxxx-------------------------------------------------------------
xxxxxxxxxxxxxxxxx---------------------------------------------------------------
xxxxxxxxxxxxxxx-----------------------------------------------------------------
xxxxxxxxxxxxxxxxx---------------------------------------------------------------
xxxxxxxxxxxxxx------------------------------------------------------------------
xxxxxxxxxxxxxxxxx---------------------------------------------------------------
xxxxxxxxxxxxxxxxx---------------------------------------------------------------
xxxxxxxxxxxxxxx-----------------------------------------------------------------
xxxxxxxxxxxxxxx-----------------------------------------------------------------
xxxxxxxxxxxxxxxx----------------------------------------------------------------
xxxxxxxxxxxxxxx-----------------------------------------------------------------
xxxxxxxxxxxxxx------------------------------------------------------------------
xxxxxxxxxxxx--------------------------------------------------------------------
xxxxxxxxxxx---------------------------------------------------------------------
xxxxxxxxxxxxxx------------------------------------------------------------------
xxxxxxxxxxxxx-------------------------------------------------------------------
xxxxxxxxxxxx--------------------------------------------------------------------
xxxxxxxxxxxx--------------------------------------------------------------------
xxxxxxxxxxx---------------------------------------------------------------------
xxxxxxxxxxxxx-------------------------------------------------------------------
xxxxxxxxxxxxx-------------------------------------------------------------------
xxxxxxxxxxxxx-------------------------------------------------------------------
xxxxxxxxxxxx--------------------------------------------------------------------
xxxxxxxxxxxxxx------------------------------------------------------------------
xxxxxxxxxxxxxx------------------------------------------------------------------
xxxxxxxxxxxxxxxx----------------------------------------------------------------
xxxxxxxxxxxxxxxxxx--------------------------------------------------------------
xxxxxxxxxxxxxxxxx---------------------------------------------------------------
xxxxxxxxxxxxxxx-----------------------------------------------------------------
xxxxxxxxxxxxxxx-----------------------------------------------------------------
xxxxxxxxxxxxxx------------------------------------------------------------------
xxxxxxxxxxxxxx------------------------------------------------------------------
xxxxxxxxxxxxxxx-----------------------------------------------------------------
xxxxxxxxxxxxxx------------------------------------------------------------------
xxxxxxxxxxxxxxx-----------------------------------------------------------------
xxxxxxxxxxxxx-------------------------------------------------------------------
xxxxxxxxxxx---------------------------------------------------------------------
xxxxxxxxxxxxxx------------------------------------------------------------------
xxxxxxxxxxxxxxxx----------------------------------------------------------------
xxxxxxxxxxxxxxxxxx--------------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxx--------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------------------------
xxxxxxxxxx----------------------------------------------------------------------
xxxxxxxxxxxxx-------------------------------------------------------------------
xxxxxxxxxxxxxx------------------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx----------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------------------------
xxxxxxxxxxxxxxxxxxxxx-----------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxx--------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx----------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx----------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx--------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxxxx---------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxx------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxx--------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxx-----------------------------------------------------------
xxxxxxxxxxxxxxxxxxxx------------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------------------------------
xxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------------------------------
xxxxxxxxxxxxxxxxxxxx------------------------------------------------------------
xxxxxxxxxxxxxxxxxxx-------------------------------------------------------------
xxxxxxxxxxxxxxxxxx--------------------------------------------------------------
xxxxxxxxxxxxxxxxx---------------------------------------------------------------
xxxxxxxxxxxxxxxxx---------------------------------------------------------------
xxxxxxxxxxxxxxx-----------------------------------------------------------------
xxxxxxxxxxxxxxxxx---------------------------------------------------------------
xxxxxxxxxxxxxxxx----------------------------------------------------------------
xxxxxxxxxxxxxxxxx---------------------------------------------------------------
xxxxxxxxxxxxxxxxx---------------------------------------------------------------
xxxxxxxxxxxxxx------------------------------------------------------------------
xxxxxxxxxxxxx-------------------------------------------------------------------
xxxxxxxxxxxxxx------------------------------------------------------------------
^c^
xxxxxxxxxxxxxxx-----------------------------------------------------------------

the x’s indicate recognition.
"
389,alternative tts engine,mimic,"
hi, totally new to mycroft here. i’m wondering if apis exist to add on a new tts engine… i would love to see if it would be possible to integrate wellsaid labs… the absolute best tts i’ve ever heard. ever. here’s a sample of what you all are missing  https://drive.google.com/file/d/1cjyrru05bigvrc8p3e6-pf-ck2cnhukc/view?usp=drivesdk
","
kinda pricey:



wellsaid labs



wellsaid labs – pricing
beautiful voices at your fingertips with text-to-speech that captivates. reduce costs and streamline the voice production process.






…and everything that should lead to useful info goes directly to “contact sales”.  they’re very corporate-focused.  their blog seems to indicate they’re mainly about larger latency products than pure tts engine.
interesting that none of the samples they provide are on quiet backgrounds, makes it much harder to properly eval how good they sound.
can’t find anything on an api call for them, or latency.  there’s nothing on github for api usage currently, which is rather worrisome.
to answer your main query, anything is possible if you can program it to interface with the messagebus.  look at the mimic2 tts piece, it can basically handle sending text to an endpoint and expecting a wav file back, or the google tts one, which does auth as well.

looks like they do some kind of tacotron2 + wavenet stuff. cto petrochuk: “we took research like tacotron and pushed it even further…”  his github has a bunch of tts projects starred.

hey @baconator, i’m definitely curious about what’s under the hood. i’ve got a subscription and i’ve been using them to generate pbx voice prompts for our customers. you really can’t tell that it’s not a human, especially on the other end of an 8khz call. as for the api portion, you need to request access, but they do have it. they want to basically determine that you’re not reselling their service as something identical. i did notice some interesting things as i’ve worked with the files generated by them - identical phrases, for example “press 1 for so and so, press 2 for so and so” are generated with slight differences in intonation and pace (i’m no speech expert here so forgive my basic terminology).  this really makes it sound very human-like. it’s the best i’ve found as of yet. i looked into tacotron/wavenet, and yep, some of those examples are almost on the level of wsltts, but not quite. i would say it seems like wellsaid has polished that technology. i can get you any samples you’d like from wellsaid (i have access to 4 voices with my low-end plan, but they have many more you can unlock if you pay more).
i’m curious as to why mainstream tts such as google, amazon etc still does not compete with the quality of wsltts. here are some great side-by-sides:



@baconator i’ve thrown your initial reply into wsltts and generated output without any fixes to the pronunciation. a few things need to be tweaked, but this is easy to do. i wanted you to hear the un-touched output though: https://drive.google.com/file/d/1btgdkgeshbjcpqxmpisnqu3nmfvcponl/view?usp=sharing

sounds very much like a tacotron backed engine and some vocoder.  note the skips in acronyms, for instance.
how’s latency on the api end?
check out google duplex and you’ll see how close they are.

their voice actor demo is actually just ljspeech: https://keithito.com/lj-speech-dataset/

oh, they’ve also done one of those marketing promos on that first video.
https://cloud.google.com/text-to-speech/docs/wavenet

i don’t think google wave net sounds anywhere near as lifelike as wsltts. i’ve actually got a google assistant device in the kitchen (don’t shoot me - i haven’t had a chance to dabble with mycroft yet) and the rest of my family uses it regularly. i can say without a doubt that although very good, it’s voice isn’t as life like as wsltts. now duplex - i saw that demo and it slipped my mind! yeah that is just unbelievable. having that with a voice assistant like mycroft would be just nuts! any idea how they got to that point? i appreciate your insight. i have an interest in deep learning but just don’t have the time right now to immerse myself in it. when i eventually do have some time, i’d like to have a lay of the land already. i also like to know what the current state of the art (or at least most popular at the time) is in each of the common areas… such as yolo for object recognition. by the way, pyimagesearch.com is an awesome resource for hands-on learning of computer vision and deep learning topics.

as for latency on the wsltts api, i can’t speak to it yet as i haven’t obtained access yet. i can say that using the web interface to generate files takes a bit of time. the snippet i last posted for you probably took around 15 seconds or so to prepare the file download. but i’m not sure how much of that can be chalked up to whatever queuing system they have in place to generate files. i’m going to request api access and i’ll let you know how that goes.




 icanhazpython:

i’m curious as to why mainstream tts such as google, amazon etc still does not compete with the quality of wsltts. here are some great side-by-sides:


on my computer speakers it sounds like the better demos of nancy and ljspeech you find when looking for mozilla-tts, tacotron2, wavenet, waveglow. maybe wsltts does sound a bit better when listening on really good loudspeakers or headphones - but that would not be mycrofts use case scenario…



 icanhazpython:

i did notice some interesting things as i’ve worked with the files generated by them - identical phrases, for example “press 1 for so and so, press 2 for so and so” are generated with slight differences in intonation and pace (i’m no speech expert here so forgive my basic terminology). this really makes it sound very human-like.


when you like that you might want to look into nvidia’s flowtron - they have some very interesting parameters for variation and style.
"
390,google cloud stt transcribing problems,support,"
i have trained my own precise wake word and using google stt, however, it is just not recognizing my audio inputs.  i have generated a new access token and applied it in mycroft config user. any help would be much appreciated!
thanks.
06:39:08.400 | info     | 21331 | main:handle_wakeword:67 | wakeword detected: ok poli
06:39:08.780 | info     | 21331 | main:handle_record_begin:37 | begin recording…
06:39:11.796 | info     | 21331 | main:handle_record_end:45 | end recording…
06:39:11.806 | info     | 21331 | googleapiclient.discovery | url being requested: get https://www.googleapis.com/discovery/v1/apis/speech/v1/rest
06:39:12.161 | info     | 21331 | googleapiclient.discovery | url being requested: post https://speech.googleapis.com/v1/speech:recognize?alt=json
06:39:12.161 | info     | 21331 | oauth2client.transport | attempting refresh to obtain initial access_token
06:39:12.162 | info     | 21331 | oauth2client.client | refreshing access_token
06:39:13.013 | error    | 21331 | mycroft.client.speech.listener:transcribe:239 |
06:39:13.013 | error    | 21331 | mycroft.client.speech.listener:transcribe:240 | speech recognition could not understand audio
","
hi deepak,
can you post what you have for your user config? without the access token of course.

hi gez,
thanks for helping! i also checked on google cloud, the queries i think are going through to the cloud…but here is what i currently have on the user config:
{
“max_allowed_core_version”: 20.2,
“lang”: “en-us”,
“stt”: {
“google_cloud”: {
“lang”: “en-us”,
“credential”: {
“json”: {
“type”: “service_account”,
“project_id”: “mycroft-cs”,
“private_key_id”: “”,
“private_key”: “-----begin private key-----”
“client_email”: “xxx@mycroft-cs.iam.gserviceaccount.com”,
“client_id”: “xxxxcc”,
“auth_uri”: “https://accounts.google.com/o/oauth2/auth”,
“token_uri”: “https://oauth2.googleapis.com/token”,
“auth_provider_x509_cert_url”: “https://www.googleapis.com/oauth2/v1/certs”,
“client_x509_cert_url”: “https://www.googleapis.com/robot/v1/metadata/x509/pollyva%40mycroft-cs.iam.gserviceaccount.com”
}
}
},
“module”: “google_cloud”
},
“listener”: {
“wake_word”: “ok poli”
},
“hotwords”: {
“ok poli”: {
“module”: “precise”,
“local_model_file”: “/home/parallels/.mycroft/precise/ok-polly.pb”
}
}
}

hmmm, the only things that seem wrong are the quotation marks being non-standard, but think that’s most likely discourse changing them.
and there’s a comma missing at the end of private_key but i assume that’s just from deleting the majority of the line?
can you try updating the google-api-python-client  pip package?
if you have the helper commands it’s just:
mycroft-pip install google-api-python-client==1.9.1

otherwise:
source ~/mycroft-core/.venv/bin/activate
pip3 install google-api-python-client==1.9.1
deactivate





 gez-mycroft:

google-api-python-client==1.9.1


yes the quoetes are single in the user configs, and there a comma after the private key.
i updated the google api client, it is till not working, and it is still not working…
i also noticed something strange with the mic levels in the cli…the levels run perfectly in general, and pick up my custom wake word. however the level goes very low almost to 3-10 when recording my utterance after the wakeword. i tried reinsatalling pulse audio, i checked the levels with the alsa mixer, everything seems to be working, but this seems to be the only glitch i have noticed.
"
391,command 2 script,support,"
hey,
i have already picroft installed, and have it working with respeaker mic arrray v2.0.
the hotword is detected and the basic things are working.
now, i looking arround, how i can achive my next step.
i need, to say the wake word, and want to have a “command” afterwards.
if the command is detected, i want execute a bash-script.
is there for such kind of request already a skill or a prozess ?
to be honest, i was already looking in the skill-manager, and on github.
but was not able to find such “skill” 
regards, and thanks
peter
",
392,mycroft with bluetooth,none,"
hi.
i am trying to setup mycroft with bluetooth. i know it is not supported, but i got it working on an x86 machine pretty easily.
when moving to an arm machine, i had some issues with pulseaudio but they were fixed after following this:


hackaday.io



setting up the bluetooth | details | hackaday.io
bluetooth allows to communicate between different types of devices and with different purposes. therefore, several profiles exist for each communication type. the classical and best known ones are: 

  a2dp  (advanced audio distribution...






now my linux machine can play sound just fine from the bluetooth speaker even when paired through a hsp profile.
however now i am in a weird state:

microphone works fine, picks up wake work and understands commands
when mycroft hears the wake word, a ‘ping’ is heard though the hdmi connection (even though the bluetooth speaker is the default speaker and all other system sounds go there)
when mycroft wants to say something, i don’t hear anything. not from the bluetooth speaker, nor from the hdmi connected tv.

 22:15:25.840 | info     |  2122 | __main__:handle_wakeword:67 | wakeword detected: hey jarvis
 22:15:26.965 | info     |  2122 | __main__:handle_record_begin:37 | begin recording...
 22:15:26.982 | error    |  2116 | volumeskill | couldn't allocate mixer, alsaaudioerror('unable to find mixer control master,0 [default]')
 22:15:28.964 | error    |  2116 | volumeskill | couldn't allocate mixer, alsaaudioerror('unable to find mixer control master,0 [default]')
 22:15:28.944 | info     |  2122 | __main__:handle_record_end:45 | end recording...
 22:15:30.011 | error    |  2116 | volumeskill | couldn't allocate mixer, alsaaudioerror('unable to find mixer control master,0 [default]')
 22:15:30.237 | info     |  2122 | __main__:handle_utterance:72 | utterance: [""what's the time""]

that is what mycroft prints out when detecting the wake word.
does anyone know what might be wrong, or how i can force mycroft to use a specific output?
lateredit: running this on a pi 4 with raspberry pi os
","
hey there,
i’d first try setting the default pulseaudio sink (aka output).
if that doesn’t work you could set your play commands as described here and use the -d flag to specify the output device to be used.

thanks!
initially setting the default pulseaudio sink didnt work, but after a few restarts and some messing around, it did. having the aplay and arecord comands (from the links you sent) helped me speed up debugging by a lot :d.
it works now sort of. only problem i now have is that the bluetooth microphone is very noisy when connected to the pi… looks like headset bluetooth devices are kinda broken on arm 
"
393,converse isnt running,support,"
i’m starting down the path of learning my first skill via the documentation and at this stage it’s very simplistic:
import logging

from mycroft import mycroftskill, intent_file_handler

logger = logging.getlogger(__name__)


class myskill(mycroftskill):
    def __init__(self):
        logger.info(""__init__()"")
        mycroftskill.__init__(self)

    def converse(self, utterances, lang=none):
        logger.info(""converse"")
        return false

    @intent_file_handler(""myskill.intent"")
    def handle_myskill(self, message):
        logger.info(""intent"")
        self.speak_dialog(""myskill"")


def create_skill():
    return myskill()

the problem i’m having is that the converse() method doesn’t appear to trigger… ever.  the skill works just fine: i say “hey mycroft”, he chirps at me, i say the keyword i’m looking for and he responds with one of the responses in the “myskill.dialog” file.  but the converse() code never runs.  i even tried putting “assert false” in there to see if i could blow everything up, and it still acted the same.
what am i missing here?  the documentation tells you what to do, but doesn’t do a very good job of telling you why things aren’t happening as it claims things should.
","
there definitely is something wonky here…it works for me but not quite right…
so the converse method can only be hit after a skill’s intent handler has been selected (and then there is a 5 minute timeout). for me the skill’s converse method doesn’t get called on the first user utterance after the intent handler is triggered, but the second, third, etc is caught correctly.
and the get_response() method works as expected all the time which is weird. it should basically fail if converse was broken like this…
will dig into the code more, there’s a pr simplifying the converse handling gonna test that as well and see if it behaves better or worse.

well that’s refreshing to know that i’m not going crazy.  please report back if you find anything i guess.  i’m a pretty accomplished python guy, but this is my first serious attempt at writing a mycroft skill so if there’s something i’m missing that wasn’t immediatley obvious from the documentation, then this could at the very least be an opportunity to clear something up.

i’ve found at least one problem (or two depending on how you look at it). when using a padatious intent an extra “empty” active skill gets added, this is a problem but shouldn’t really do anything.
when looping through the active skills the “empty” skill however seem to break the loop so the real skill isn’t checked at all.
i’ve got a work-around going but will dig into the actual cause, the loop should continue on bad skill id’s.

think i found the root issue…removing a skill from a list while iterating over said list…

posted a pull request for a fix (or three separate fixes for the single issue  ) for the issue i could see. if you have the ability, i’d be glad to know if it works for you with this addition


github.com/mycroftai/mycroft-core








bugfix converse error


mycroftai:dev ← forslund:bugfix/converse-error



        opened 06:47pm - 15 mar 20 utc




          forslund
        



+70
-20











i’m still having this problem where the converse method doesn’t get called until after the skill implementing it has been called once.

hey thomas, that’s the expected behaviour. the skill must have been active within the last 5 minutes for converse to trigger.
as a work around, you could add something like the following to your initialize() method:
self.schedule_repeating_event(self.make_active, none, 270, name=""keep active"")

this would call self.make_active every 270 seconds.
"
394,custom wake word for noobs,none,"
i want to add a custom wake word for my picroft. i have no experience whatsoever in programming. i don’t want to do it the training method and want to do it with pocketsphinx. can i receive step by step instructions on how to do so for a custom wake word for eg- say… hey nora or something
","
easy way. look at this not perfect but work now wake word skill trainer if you have problems please post it. you can also use precise community data https://github.com/mycroftai/precise-community-data unzip in .mycroft/precise.

hi incendio,
if you want to add a custom wake word with pocketsphinx there are instructions here:



mycroft



the `mycroft.conf` file - mycroft
learn about your mycroft.conf mycroft configuration file - where it's held, order of precedence and the data that it contains about your mycroft conf.






basically:

pick your word and translate it using the cmu dictionary
paste the phoneme string into the mycroft.conf example and move it to the correct location on your computer.
if things stop working it’s almost guaranteed that there’s a typo in your json. so use a validator to make sure it’s correct.

it’s worth noting however that pocketsphinx will not perform anywhere near as well as a trained wake word model using precise. so they’re good for personal use and playing around but if it’s for a business purpose or you want other people to use it then it’s worth investing the time.

i have already gone through the following link suggested to me by many. but i have this weird issue where i am not able to edit the mycroft.conf file. whenever i try to access the file it says access denied. thank you both for taking your time to reply though.

are you using sudo before you try and edit the file?

well um no because i don’t exactly know how to use it. 
as i mentioned above i have  little to no experience in programming

hey, we also have this brand new configuration manager utility that now helps you edit configuration values. try running the following command:
mycroft-config edit user

just to clarify the reply from gez-mycroft sep 22 '19…
the docs on the pocketsphinx wake word are here:
https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/customizations/wake-word#pocketsphinx
and the cmu dictionary linked there is here: http://www.speech.cs.cmu.edu/cgi-bin/cmudict
an example for ‘yo mike’ is on that first page. here’s a couple examples below. the shorter the phrase, the higher the threshold number needs to be to prevent false triggering, which means reducing the number of the negative exponent. ‘yo’ is inconsistent, ‘homie’ is not bad.
you can use either mycroft-config reload or the phrase ‘hey mycroft, update configuration’. you might need to set the listener ‘hey mycroft, set the listener to pocketsphinx’.
editing ~/.mycroft/mycroft.conf is the same as using mycroft-config edit user.
at the moment, there is a pull request that needs to be finished to use multiple wake words with out a workaround https://github.com/mycroftai/mycroft-core/pull/1233. below, only the last one is set.
{
  ""max_allowed_core_version"": 20.2,
  ""listener"": {
    ""wake_word"": ""yo mike"",
    ""wake_word"": ""hey homie"",
    ""wake_word"": ""yo"",
    ""wake_word"": ""homie""
  },
  ""hotwords"": {
    ""yo mike"": {
      ""module"": ""pocketsphinx"",
      ""phonemes"": ""y ow . m ay k ."",
      ""threshold"": 1e-18
    },
    ""hey homie"": {
      ""module"": ""pocketsphinx"",
      ""phonemes"": ""hh ey . hh ow m iy ."",
      ""threshold"": 1e-18
    },
    ""homie"": {
      ""module"": ""pocketsphinx"",
      ""phonemes"": ""hh ow m iy"",
      ""threshold"": 1e-18
    },
    ""yo"": {
      ""module"": ""pocketsphinx"",
      ""phonemes"": ""y ow ."",
      ""threshold"": 1e-10
    }
  }
}




nytimes.com



‘homie,’ a book of poems that produces shocking new vibrations
danez smith’s innovative new collection exalts friendship, while also exploring its darker corners.






by the way, the name ‘homie’ here borrows from mycroft’s last name, as well as me hearing an interview with the above author danez smith on the radio while i was considering it… im currently enjoying the abbreviation to mike. so with that, here’s two more wakewords:
{
  ""max_allowed_core_version"": 20.2,
  ""listener"": {
    ""wake_word"": ""hey holmes"",
    ""wake_word"": ""ay mike""
  },
  ""hotwords"": {
    ""hey holmes"": {
      ""module"": ""pocketsphinx"",
      ""phonemes"": ""hh ey . hh ow m z ."",
      ""threshold"": 1e-18
    },
    ""ay mike"": {
      ""module"": ""pocketsphinx"",
      ""phonemes"": ""ey . m ay k"",
      ""threshold"": 1e-18
      }
    }
}
"
395,mycroft skills not working in other language,languages,"
hi!
i managed tu run mycroft with a german deepspeech instance and german espeak. unfortunately, our conversations are very one-sided, because mycroft cannot execute any skill, but instead tells me that he did not understand me.
example conversation:

suche nikon
>  ich bin nicht sicher, ob ich dich verstanden habe.

skills.log:

16:20:29.905 - mycroft.skills.core:load_skill:117 - info - attempting to load skill: mycroft-wiki.mycroftai with id mycroft-wiki.mycroftai
listening for mycroft-wiki.mycroftai.set
[…]
16:20:29.982 - skills - debug - {“type”: “register_vocab”, “data”: {“start”: “suche”, “end”: “mycroft_wiki_mycroftaiwikipedia”}, “context”: null}
[…]
16:26:57.936 - skills - debug - {“type”: “recognizer_loop:utterance”, “data”: {“utterances”: [“suche nikon”], “lang”: “de-de”}, “context”: null}
16:26:57.941 - skills - debug - {“type”: “intent_failure”, “data”: {“utterance”: “suche nikon”, “lang”: “de-de”}, “context”: {}}
16:26:57.943 - mycroft.skills.padatious_service:handle_fallback:133 - debug - padatious fallback attempt: suche nikon
16:26:57.945 - skills - debug - {“type”: “mycroft.skill.handler.start”, “data”: {“handler”: “fallback”}, “context”: {}}
16:26:57.945 - skills - debug - {“type”: “enclosure.mouth.think”, “data”: {}, “context”: null}
16:26:57.946 - questionsanswersskill - info - searching for suche nikon
16:26:57.948 - skills - debug - {“type”: “question:query”, “data”: {“phrase”: “suche nikon”}, “context”: null}
16:26:57.949 - skills - debug - {“type”: “mycroft.scheduler.schedule_event”, “data”: {“time”: 1549294018.0, “event”: “fallback-query.mycroftai:questionquerytimeout”, “repeat”: null, “data”: {“phrase”: “suche nikon”}}, “context”: null}
16:26:58.160 - skills - debug - {“type”: “fallback-query.mycroftai:questionquerytimeout”, “data”: {“phrase”: “suche nikon”}, “context”: null}
16:26:58.160 - questionsanswersskill - info - timeout occured check responses
16:26:58.162 - skills - debug - {“type”: “enclosure.mouth.reset”, “data”: {}, “context”: null}
16:26:58.950 - wolframalphaskill - debug - wolframalpha fallback attempt: suche nikon
16:26:58.950 - wolframalphaskill - debug - non-question, ignoring: suche nikon
16:26:58.953 - skills - debug - {“type”: “speak”, “data”: {“utterance”: “ich bin nicht sicher, ob ich dich verstanden habe.”, “expect_response”: false}, “context”: {}}

from my understanding “suche nikon” should be handled by the wikipedia skill. but i can not get any skill to work.
any advice on this?
thanks,
marc
","
try “wikipedia nikon” instead - works for me with lang=“de”

that does work for the wikipedia skill but other commands don’t work either:
homeassistant skill (some parts are working):

schalte licht ein
>> bitte formuliere deinen befehl um.
knippse licht an
>> es tut mir leid, ich verstehe nicht.
knippse an licht
>> ich weiß nicht, was das bedeutet.
was ist der wert vom thermostat wohnzimmer
>> temperatur_wohnzimmer ist 21,76 °c.

what are you:

was bist du
>> es tut mir leid, ich verstehe nicht.

my language setting is “de-de”, because i get a lot of missing intent files with “de”

i stand corrected - my configuration for lang is “de-de” as well.
looking at the vocab folder of the homeassistant skill i see two subfolders “de” and “de-de” which is already confusing. but looking into the .voc files for “de-de” confuses me even more - some vocab-files were updated some days ago and the translations are not good in my opinion.
if i understand the switch intent and vocab/regex files correctly following should turn on a light:
“drehe licht an” or “drehe  an” - which isn’t common german (maybe my grandparents would have said that as at that time the light switches were actionally knobs that had to be turned…)
@msn: feel free to join channel language-de on mycroft chat to have a discussion in german…
@gez-mycroft - this is an example where translations without deeper understanding of the intent and context can go wrong.

yeah, it is a big limitation of the current translation platform, and i know it’s hard for people translating to visit each skill and try to understand how the vocab is being used.
unfortunately it’s a significant piece of work for us to build in that contextual ability. it’s definitely on our radar but we have some other projects that need to be prioritised at the moment.
if you do have an alternative translation that you think would work better in this context it would be great to add it into the translate platform. if we can keep making these small iterative improvements, then all the languages will slowly get better over time, particularly as more users are actually using the skill and run into small issues like this.

yeah i saw the “drehe an” thing as well but even that is not working, which is very confusing. i tried creating an automation in homeassistant which can succesfully run with the phrase in regex/de-de/automation.rx so the homeassistant problem seems to be something else.
turning the lights on worked while mycroft was set to english btw
i am typing the commands to eleminate stt errors.
can you recommend other skills, that work in german @dominik? currently the only things i can do are: read sensor values from homeassistant, run automations in homeassistant and query wikipedia…

weather skill works for me (actually one of my most frequent used skills): “wie ist das wetter?”

hi everybody, i am too trying to use mycroft with home assistant in german and was stumbling over this thread. is there any progress, last post was almost 1 year ago… i still cannot switch on any of my lamps or switches, any hints?
what’s with other skills in german, any progress there?
thanks!

hi @iceman1000,
can you provide some more details, please?
what is your mycroft.conf configuration?
which skill are you using for switching lights and which phrases are you using?

hi dominik,
thanks for your quick reply. my setup: raspberry pi 4 with picroft, home assistant on a seperate raspberry pi 3 (hassio).
with picroft as a standard setup (english) and the skill “home assistant” from the marketplace the voice commands for home assistant were fine so i had no problem turning on my lights (phrase: “turn on the couch light”).
but as i changed the language to german with “tts: google… language: de” in the etc/mycroft/mycroft.conf and “lang: de-de” in ~/.mycroft/mycroft.conf i could not control home assistant anymore. the weather still works. even setting a timer and telling a joke doesn’t work.
german commands i tried: “schalte couch an”, “schalte couch licht an” and some more, answer is “ich verstehe nicht” or something similar.
“couch” is a philips hue bulb - the friendly name in home assistant - i also tried the the other name and other commands like “mache … an” and “drehe … an”.
oddly checking my temperature sensor with “wie ist der wert von aqara temp” works even with a not 100% correct entity name: “aqara 2 sensor temp hat den status 22,89 °c”.
asking for a joke the answer is “bei der verarbeitung der anfrage ist ein fehler im skill joking skill aufgetreten”.
phrase: “wer ist boris becker”, answer “ich weiß nicht wer das ist” - in english mycroft would remember the wimbledon champion…
any help is so appreciated!
thanks in advance!

hi everybody, i am trying to use mycroft in german but i am struggeling a bit.
i have problems with the timer skill.
when i aks mycroft to set a timer for 10s

hey mycroft, stelle einen timer für 10s

i get the answer: it will be zero o´clock

es wird null uhr sein

can someone help me?

currently there is an issue in the functionality that parses german numbers (lingua-franca) that leads to this problem.
"
396,no speech tts on raspberry pi 4 but everything seems to work including audiotest,mimic,"
hardware: raspberry pi4
os: raspbian buster
i used this install script, but have tried installing normally using linux instructions too with the same situation.
things working:

running ./start_mycroft.sh debug  works as expected, no errors, and the voice logs “talk” back to me as if nothing is wrong.
i am paired successfully to home.mycroft.ai
running ./start_mycroft.sh audiotest  works as expected, it records and plays back perfectly

things not working:

when running ./start_mycroft.sh all i get no audio indication ding when saying “hey, mycroft”
no tts is working at all, anywhere - i have never heard mycroft’s beautiful voice and this makes me sad

for reference, i am currently located in shenzhen, running the system through a (very) reliable private hong kong vpn. bonus info: trying to install without this vpn ended in what seemed like a very broken installation. installing with the vpn took much longer, which i can only assume because nothing broke during the process. does mycroft struggle to install properly within china’s firewall?
i am currently compiling the local mimic to rule out any network issues, but my hopefulness is low.
what logs or info can i provide to try and get to the bottom of this?
","
i fixed this by creating and adding the following to /etc/asound.conf
defaults.pcm.card 1
defaults.ctl.card 1

as per these instructions: https://www.alsa-project.org/wiki/setting_the_default_device
i was actually trying to get spotify working which still doesn’t work, but at least this is fixed now

hi @duwudi
i sounds to me that you have an audio problem.  that everything works but audio isnt played.
try the audio-troubleshooting guide
https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/troubleshooting/audio-troubleshooting
the relevant logs are located in /var/log/mycroft/ and could be the audio.log and/or the voice.log - most likely the first one.

i did go there to try and resolve the issue, but since all 3 tests seemed to work - with the exception of mycroft’s voice - i assumed i had a different problem specifically relating to tts.
perhaps it would be better if audiotest used the same method for playing back the recording as mycroft’s tts uses to play speech, this was the cause of my confusion and what led me to believe it wasn’t just a simple audio issue.
in any case, i’ve now learned a lot about audio in linux so i guess that’s my bonus prize for expecting the worst and digging deeper than i needed to! 

it is weird that the audio test works but mycroft fails to play audio. they use the same playback call, to make this as similar as possible…
if you’re using goole tts it would be different though since that receives the data as mp3 and hence uses mpg123 for playback instead of paplay as the default for wavs.

hello,
i think i have a similar problem like this.
i used the audio troublesooting guide and the mycroft-start audiotest -l worked very well but still mycroft is not receiving any input from my blue microphone blue snowball since i don’t see any mic level.
i tried mycroft-config set listener.device_name but the problem is still the same.
does someone have any idea?
best regards
vadi

hi vadi, have you tried changing the play commands listed here:
https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/troubleshooting/audio-troubleshooting#usb-audio-devices

hey gez-mycroft,
thank you for the response. actually i did work out in the end.
i tried it with sudo ‘‘service mycroft-* restart’’ which did not work out at first. after i went to bed, shutdown the pi and  tried it the next day everything was working.
i thougth that adding the ‘‘sudo service mycroft-* restart’’ to the audio troubleshooting guide would be a good idea?
best regards
vadi

good suggestion - added.
also, you can do this without sudo by running
mycroft-start restart all

of if the helper commands aren’t available:
~/mycroft-core/start-mycroft.sh restart all

"
397,how do i send a message to the messagebus,none,"
hello
im reading the messagebus documentation and it says that its possible to send an utterance to the messagebus. which particular function makes that possible?
","
i think the docs are a bit outdated as it describes uses og self.emitter.emit() but i think the right would be to use self.bus.emit
def some_method(self):  
    self.bus.emit(message(""recognizer_loop:utterance"",  
                              {'utterances': [""inject a user utterance""],  
                               'lang': 'en-us'}))  




mycroft



message bus - mycroft
a message bus is mechanism for independent systems to communicate with each other using a set of _messages_ for common commands or notifiers. in the mycroft ecosystem, the messagebus is a websocket and the messages contain a message type with an...






hey andlo,
i was also looking for something similar. i was wondering… does this method store the utterance in the memory? would it be possible to match this utterance against other intents in mycroft core as well?
thanks!

when sending a utterance to the messagebus, and other skills will match and react to it.
so if you add a
self.bus.emit(message(""recognizer_loop:utterance"",  
                              {'utterances': ""play rainy day""],  
                               'lang': 'en-us'}))  

at the end of the weather-skill and the ask mycroft “how is the wether” he will tell you the weatherforcast by using the weather-skill and then play rainy day by using a music skill.

@andlo,  hi, this isnt working currently. the error is: nameerror: name 'message' is not defined. is there another change to the code that has removed the ‘message’ function?


github.com/mycroftai/mycroft-core








'emitter' no longer correct and 'message' not working



        opened 12:45pm - 07 jun 20 utc




          auwsom
        





in the messagebus docs, self.bus.emit should now be used instead of self.emitter.emit. however, doing so results in the error ""nameerror: name...









nope just needed to add that to the docs, thanks for pointing it out
"
398,fallback skill not even attempting to load,support,"
i have two mycrofts, one on virtualbox and the other on native linux.
i’m developing a custom fallback skill, and on vb it loads successfully from opt/mycroft/skills folder. on native linux, it does not even attempt to load, the logs are empty and don’t even mention it, as if it was ignoring the folder whatsoever.
any ideas? permissions error? how would i grant correct permissions in such case?
thanks.
","



 modernonline:

any ideas? permissions error? how would i grant correct permissions in such case?


is there any output in the mycroft-cli console?
permissions: check which permissions and ownership other skills have and apply the same to your skill-folder.

you can view the owner user and group with:
ls -la

and to recursively change the permissions of everything in that directory:
cd /opt/mycroft/skill/my-skill
chown -r user:group *

replacing my-skill, user and group with your own values. often your user and group are the same but best to check another skill as dominik suggested.
"
399,auto start and slow reply,general discussion,"
i installed mycroft on my rasspberry pi 4 1 gb ram version can anyone help me with a way to make it start with the boot of rasspbian and it’s replying slowler than i expected is that because of the ram size or there a way that i can make it reply faster ?
","
i’ve noticed this too i think there’s a python library that needs installing… check the mycroft cli on start up and check if it throwing up any warnings. i haven’t restarted yet so i haven’t seen the effect.
i doubt this will have an effect start up time but should decrease the time it takes to respond.

thank you for your help, i checked it and there is no errors occurred at all !

hey muhammed,
for response time, depending on your mic, you can try the suggestion here:




speed of reacting to the wake word general discussion


    hy 
i use mycroft on a pi3. 
when i call the wake word and i can see that the wake word is recognized in the cli it takes 1.3 seconds till i hear the sound over my 3.5mm headphones to tell me that mycroft is listening. in some video i can see that it can react faster. 
can some one tell me if the pi3 is the bottleneck or what can i improve? 
andy
  


as for auto-starting mycroft, there are many ways to achieve this, but the simplest is probably to add the relevant commands to your ~/.bashrc file.  you can see the picroft .bashrc file here. on the final line we call auto_run.sh which is also in the picroft repo. currently this is a bit of a “do everything” script that could do with some refactoring but hopefully that gives you an idea.

hi, the suggestion on how to decrease the response time, the mentioned method makes it work better but i still couldn’t make the auto start steps i tried it but it didn’t work, i am using it on rassbian.
thank you

iam sorry but i am still can’t make my assitant autostart , can you please help me with clear steps or a video tutorial
thank you.

i tried to edit ./bashrc file and nothing happened at all

hey, what did you add to your ~/.bashrc?
if you just want to start all mycroft services you could add:
~/mycroft-core/start-mycroft.sh all

"
400,configurable skills not showing up in mycroft home skill settings,support,"
hello,
i have installed mycroft on a rasperry pi 3b+ running raspian. i followed the linux installation instructions. my device is successfully paired, the default skills are working, and i have been able to successfully create custom skills. however, in my mycroft home account, no skills show up under skill settings.
i originally noticed this when trying to use the mycroft spotify skill, since it is necessary to authenticate my spotify account. however, i realized that none of my configurable skills (as defined by having a settingsmeta.json or settingsmeta.yaml) are showing up in mycroft home (such as mycroft-alarm, mycroft-weather, etc).
i believe that i can modify the spotify skill, and hardcode my account credentials, however i would not want to hardcode settings for every configurable skill that i install.
any help would be appreciated.
thanks,
edward
","
you should wait a few minutes to skill settings to appear on home.mycroft.ai. you can “force” by restarting mycroft services or even the rpi.
anyway, it eventually will appear on home.mycroft.ai/skills

thanks, i just ended up uninstalling and reinstalling mycroft. i must’ve caused the problems myself by messing with the configuration files.
"
401,messagebus wakeword not working,none,"
so, im trying to have mycroft ‘reprompt’ with the listening tone in one of the fallback skills, unknown-skill, so i can retry my vocal command directly. (ill put a limit of 1-3 attempts to ‘re-recognize’)
the problem is similar to this post (linked below) trying to programmatically trigger the wakeword. they used a cli messagebus command python -m mycroft.messagebus.send ""mycroft.mic.listen"". whereas im trying to provoke the response from within the python ini.py. so i found an entry in the docs for (https://mycroft-ai.gitbook.io/docs/mycroft-technologies/mycroft-core/message-types#recognizer_loop-wakeword) it, but it doesnt seem to be triggering the wakeword.
code added:
self.emitter.emit(message('recognizer_loop:wakeword', {""utterance"": ""homie""}))
error:
attributeerror: 'unknownskill' object has no attribute 'emitter'
so i tried the respective cli command below and its not working as expected either. any ideas?
python3 -m mycroft.messagebus.send 'recognizer_loop:wakeword' '{ ""utterance"": ""homie""}'




is there a way to wake mycroft with a press of the keyboard instead of mycroft listening for 'hey mycroft'? general discussion


    awesome tips thx andlo !
  

also, which of the fallback-skills is most generic to put such a ‘please repeat’ trigger in?
so, im reading more about the fallback in the docs (https://mycroft-ai.gitbook.io/docs/skill-development/skill-types/fallback-skill) and i dont understand why the fallback skill needs to be  ‘removed’ on shutdown.
","
is there a native method for using emitter inside a skill (as the name of the code flavor tab ‘generating message from mycroftskill’ implies) or do i need to use a 3rd party package such as this: https://github.com/jarbasal/local_listener
according to this the docs are all old on the use of ‘emitter’ and should be changed to ‘bus’. i did this and is no longer throwing an error on ‘emitter’




stopping play_wav()? skills


    so for the audioservice i believe you want to be passing it a reference to the messagebus eg: 
self.audio_service = audioservice(self.bus). it then knows where to send the appropriate messages. 
the emitter is what sends messages to the bus. however it’s format has changed over the years as mycroft evolved and became more stable. so instead of self.emitter.emit now we have something like: 
self.bus.emit(message(""speak"", {""utterance"": ""words to be spoken"", ""lang"": ""en-us""})) 
however if you’re wr…
  


@gez-mycroft

have this working, see here:


github.com/mycroftai/mycroft-core








catchall fallback asks to repeat phrase when no intent determined



        opened 01:41pm - 07 jun 20 utc




          auwsom
        





following the docs on fallbacks here: https://mycroft-ai.gitbook.io/docs/skill-development/skill-types/fallback-skill, and the example here: https://github.com/forslund/fallback-meaning, and figuring out the correct syntax by trial and...










dont use my local listener
you want the get_response method
https://mycroft-core.readthedocs.io/en/latest/source/mycroft.html#mycroft.mycroftskill.get_response
if you really want to trigger regular listening, the correct message is “mycroft.mic.listen”

thanks for the link, those docs look more complete. but i was able to solve my issue… the syntax was changed from self.emitter.emit to self.bus. emit, and then i needed to import the message package.
im going to look at your code because it contains some phonemes for basic commands i might be able to use. im hoping to create a section in the main repository where people can add to those, along with python code for basic commands that the kubuntu plasmoid uses, etc.
"
402,convenience functions suggestions,mycroft project,"
i know there are some bigger fish to fry, but as i am getting my feet wet in skill development, i have found a couple of cases where it seems like the “right” solution would be a standard convenience function built into mycroft objects. here are my suggestions:
first, when mycroft speaks an email address, it would be great to have a single function that would modify the string so that it will be pronounced in a “natural” way. there is nuance to this: sometimes the best thing is to spell out certain parts (but obviously rarely the tld, unless it is a country-code), sometimes the domain or the mailbox name are just concatenations of two simple dictionary words (which should just be pronounced, probably). also, smtp actually permits several non-alpha-numeric characters such as hyphens, plus, period, underscore, an several others. note, it is not always practical to put in a pre-modified string. what is the email address is supplied by settings.json?  so something like this would be great:  self.speak('please send email to: ’ + self.pronounce_email(self.settings.get(“support_email”, “”))
maybe a whole list of special pronounce_ methods.
second, while i found the scrolling text display code in the ip address skill and got it to work, this seems like just the sort of thing that should happen in a function named something like self.enclosure.auto_scroll_mouth_text(‘important, longish info here’)  or an optional mode of the already existing mouth_text method activated by an optional additional parameter.  just call it and forget it. let it handle timing and enclosure type awareness and resetting, etc.
last, a single function that when called, checks for the existence of a valid self.settings value, and if none exists, prompts for a value, confirms it and then writes it to settings.json. i presume that not every setting would be a reasonable fit for this, but some things probably don’t really need a trip to home.mycroft.ai (though that option should be kept available).
","
these are great suggestions, @jrwarwick. thank you!
i’m going to ping @forslund to get his opinion here.

thanks for the suggestion and i agree that this would be extremely useful.
we have the mycroft.util.format module that contains these kind of functions (or links to a language specific version). see https://github.com/mycroftai/mycroft-core/blob/dev/mycroft/util/format.py
you’re example would become:
from mycroft.util.format import pronounce_email

...

         self.speak('please send email to: ’ + prounounce_email(self.settings.get(“support_email”, “”))

we’d be happy to accept contributions towards this!

i would love an easy way to temporary switch to a very limited small vocab ( yes, no, 1, 2, 3, 4 ) with local tts. that would help everyone in creating much better and faster (get_)responses, without converse hacking.

thank you, kathy and forslund. i am taking a crack at implementing that pronounce_email method using some pretty clever code found on stackoverflow. one requirement of this method is a simple plaintext wordlist. the first edition of this list is about 1 mb. where should it be stored? my first thought is in /usr/share/dict but perhaps you want things tighter within the mycroft home directory?

@tjoen, do you mean local stt? is the following your proposed scenario?  mycroft intent is activated successfully, but inside the handler, more information is needed. perhaps a series of yes/no questions, and with the limited possible responses of yes/no, a less sophisticated, but locally executing speech-to-text recognition would still have a high probability of correctly distinguishing between responses.

precisely. some way to switch inside the skill to a very limited subset of pocketsphinx words maybe? numbers 1 to 10, yes, no, repeat. might reduce the load on the voice services too?

that sounds like an interesting idea. maybe also “stop”, “cancel”, and “never mind”.

thought about it some more and what would make this new function even better,
is the ability to also set the recording time of the speech.
that would also speed up the skill and avoid extra listening time.
something like:
r = self.get_response_local_stt(‘what.is.your.answer’, validator=none, on_fail=none, num_retries=-1, duration=2 )
let me give an example:
i’ve been testing with a new trivia skill, that asks multiple choice trivia questions, (so you answer a number 1 to 4) . i was thinking to implement multiplayer though a configuration on home.mycroft.ai but having 10 questions for 2 players would result in 20 remote stt requests each containing just one word. also, the default recording time of the get_response()  takes up a lot of extra time when speaking just one word.
https://github.com/tjoen/skill-trivia
even testing with 5 questions, and one player, some of the stt replies take a lot of time.
having a (somewhat less reliable) local function would probably be a lot faster, and result in a much better playable game.

so, i did a small test on the mark 1 and found that it is actually pretty accurate and fast,
https://github.com/tjoen/local-stt-test
this is using the default pocketspihinx and a limited dict and lm.
you’ll have to disable the speech-client to test before testing.
still pretty good results. i will also try it on my aiy-picroft version.

@forslund, my proposed implementation will need a plaintext wordlist. would /mycroft-core/mycroft/util/lang/wordlist_simple_en.txt be an acceptable path for the addition? this file would be under 1 mb.

we’re generally against large files in the mycroft repo. i think the best place, maybe download it in the dev_setup.sh script and a better place for a file like this would maybe be under the …mycroft/res/text/en-us/
though it’s hard to say without the actual content and exactly what your trying to achieve, but i’m looking forward to seeing it!




github



jarbasal/local_listener
local_listener - local pockesphinx listener for mycroft





listen once
capture one utterance
local = locallistener()
print local.listen_once()

listen continuous
local = locallistener()
i = 0
for utterance in local.listen():
    print utterance
    i += 1
    if i > 5:
        local.stop_listening()

it would maybe be good to trigger naptime before this,
self.emitter.emit(message('recognizer_loop:sleep'))

and reactivate after
self.emitter.emit(message('recognizer_loop:wake_up'))

but naptime skill will answer with “i am awake”, not sure how to best handle this? @forslund

@jarbas_ai, hi could you tell me a bit about using this package? im trying to follow the docs to use the ‘emitter’ here: messagebus wakeword not working?
figured it out finally, see here:


github.com/mycroftai/mycroft-core








'emitter' no longer correct and 'message' not working



        opened 12:45pm - 07 jun 20 utc




          auwsom
        





in the messagebus docs, self.bus.emit should now be used instead of self.emitter.emit. however, doing so results in the error ""nameerror: name...









"
403,hooking into the mycroft output from the command line,mycroft project,"
fyi, i couldnt get the plasmoid required for install of the desktop control skill installed on kubuntu 18.10, and created a github issue here: https://github.com/mycroftai/mycroft-core/issues/2600. i’m working to find a direct method of hooking into the mycroft output to the command line. anyone with suggestions, please post, thanks!
working on a solution, so far this works to capture output, extract the command from the voice.log:
tail -f -n 1 /var/log/mycroft/voice.log | grep --line-buffered -po ""(?<=').*?(?=')"" | xargs -t -n1 -d '\n' echo
this will actually execute the spoken command:
tail -f -n 1 /var/log/mycroft/voice.log | grep --line-buffered -po ""(?<=').*?(?=')"" | xargs -t -n1 -d'\n' -i {} sh -c ""{}""
need to make a cli command skill that recognizes a keyword like ‘command’ or ‘bash’ to retrieve the last phrase and excute. also want a loop back two times before giving up after the recording timeout.
will continue development notes in the github issue…
","
i’d use python and the message bus, then you can just pull the text and run it directly instead of the bash fun.

thanks @baconator, could you add a quick example to save me some time searching the docs? …havent used the bus system yet…
actually, i was just about to tackle ‘python -m mycroft.messagebus.send “mycroft.mic.listen”’ from the microft-listen function, but i wasnt sure if i needed to be in the venv or not.
also, i have a couple quick questions maybe you could rescue me from more trial and error… i just tried adding ‘barge-in’ functionality following the link (mycroft not responding during audio output?), but when i paste in the snippet below to mycroft.conf, it complains. ive gone over the syntax several time with no luck. ill post the error in a second, but maybe it is inconsistent indentation like in python or eof?
“listener”: {
“mute_during_output”: false
}

@baconator and @gez-mycroft how strange, when copy and pasting from the forum, link above, the double quotes are not acceptable to the mycroft-config edit user checking. see the difference below. i pasted in a different snippet i saw about disabling online mycroft home in the readme, because i read the warning about port 8181 and am concerned about security, and have not found detailed answers on how to secure the system. i tried ufw and the iptables commands from links i will post below.
 ""skills"": {
    ""blacklisted_skills"": [
    ""mycroft-configuration.mycroftai"",
    ""mycroft-pairing.mycroftai""
  ]
}

but for the double quotes, notice the difference, actually it will probably render both with unaccepted quote font…(but it actually shows them differently) it looks like they are ‘backward’ double quotes:
works:
“listener”: {
“mute_during_output”: false
}
doesnt work:
“listener”: {
“mute_during_output”: false
}
that is from this post:




mycroft not responding during audio output? support


    isn’t the listener by default muted on playback because of those mark-1 troubles? 


putting that section in your custom mycroft.conf and enabling it should do the tick i believe. 

“listener”: { 
“mute_during_output”: false 
}
  



for python mbus commands, install package using: mycroft-pip install mycroft-messagebus-client, after activating the venv with source venv-activate.sh` in  ~/mycroft-core/venv-activate.sh. from: https://mycroft-ai.gitbook.io/docs/mycroft-technologies/mycroft-core/message-bus
then python3 -m mycroft.messagebus.send 'speak' '{ ""utterance"": ""test""}'
from: https://mycroft-ai.gitbook.io/docs/mycroft-technologies/mycroft-core/message-types
python3 -m mycroft.messagebus.send 'mycroft.mic.listen' will trigger as a wakeword.
python3 -m mycroft.messagebus.send 'recognizer_loop:utterance' '{ ""utterances"": ""what is the weather""}'  will trigger as an utterance, but doesnt seem to work. i did remove some of the parameters, but there is no fail or error message. it does not show up in the mycroft-cli-client like the above to commands similarly truncated.
@baconator, @gez-mycroft and @andlo  i dont see how to intercept the utterance…?
i see this using python but it doenst use the messagebus as suggested: catch and print what user said
message.data.get('utterances')[0])   …or does it?
is there a command line equivalent?
git clone https://github.com/mycroftai/skill-speak /opt/mycroft/skills/skill-speak
just created a skill following (https://mycroft-ai.gitbook.io/docs/skill-development/introduction/your-first-skill) using mycroft-msk create and went to dir ll /opt/mycroft/skills/cli-command-skill/ but there is no ‘dialog’ dir as in the docs…?
there is a dialog file in /opt/mycroft/skills/cli-command-skill/locale/en-us/command.cli.dialog. definitely some deviation from the docs for quickstart.
also, is this stored in mycrft repo or in my own? … my own. interesting…
console out in python? https://mycroft-ai.gitbook.io/docs/mycroft-technologies/mycroft-core/message-types#mycroft-debug-log   nope.




how to debug intents? skill support


    poor mans debuging is to insert a lot of log calls (self.log.info or debug) and then watch the log for those. 
you could also enable remote debug adaptor and connect to that from your ide - i made a skill that helps dooing that 


it works from vscode or the theia ide which i also made a skil for
  


https://mycroft-ai.gitbook.io/docs/skill-development/skill-structure/logging#basic-usage
self.log.info(""this is an info level log message."")
bummer, basic skill template not functioning. even after trying to use activate cli-command-skill in the mycroft-cli-client, and having it show up in the skills list. even after restart all.
error    |  1263 | mycroft.skills.skill_loader:_communicate_load_status:287 | skill cli-command-skill failed to load
'inconsistent use of tabs and spaces in indentation`
if i have to restart all services and reload the cli-client, development cycle will be very slow.
after restarting all several times and loading the cli-client, i now receive this error and the initialization messages are different in the cli-client and the custom skill doesnt load. restart all again doesnt help, nor does mycroft-stop all. at a loss here on how to proceed, no stranded processes left to be found. seems like some kind of device issue, maybe internet, but i have that disabled and am working locally. i did get the custom command to load once, and was working on an import error and then a @intent_handler versus #@intent_file_handler between the template skill and the ‘parrot skill’ i snipped some code from for the utterance read.
2020-05-31 21:47:16.067 | warning  | 10696 | mycroft.api:check_remote_pairing:531 | could not get device info: permissionerror(13, 'permission denied')
2020-05-31 21:47:16.522 | info     | 10696 | mycroft.messagebus.load_config:load_message_bus_config:33 | loading message bus configs

skills are automatically reloaded on changes in the __init__.py  so you will not need to restart mycroft after each change. if you want to force a reload while developing you can run touch /opt/mycroft/skills/my_skill/__init__.py (quicker and easier than :deactivate + :activate in the cli)
sounds a bit like your editor might have malformed the example skill? replacing some space indentation with tabs or vice-versa (in python indentation has meaning and will cause trouble if not consistent)
the example you link is using the “messagebus client” a small module that can be used from third-party applications interacting with mycroft so the syntax is a bit simplified.
python3 -m mycroft.messagebus.send 'recognizer_loop:utterance' '{ ""utterances"": ""what is the weather""}'

may be a typo…should probably be
python3 -m mycroft.messagebus.send 'recognizer_loop:utterance' '{ ""utterances"": [""what is the weather""]}'

(will verify this after coffee)
edit: verified, should be a list of utterances as above
/ å

thanks very much for they reply. indeed i did take out the brackets, and didnt realize that was in list/array format as the example in the docs only has one member (‘text’ i believe), which means i can include multiple utterances?.. great! and yes it does work now, thanks.
good to know about the touch ini.py. will try… although i would have thought that saving in the editor would have done the same, will retest… yes, this is working, saving in editor does create changes reflected in cli-client.
still getting the device permission error… tracking down clues… its only upon running the cli-client. restarting would probably fix it, but i have a browser window open with unfinished work in it. would be better to restart whatever is hanging…  also the error is likely related to the fact i have remote home disabled in my config (mentioned above). so the only problem is not getting the skills errors in the cli-client really.
so, i just tried sending an utterance (with correct format) for my custom skill and its not recognizing it. going to use the logs hopefully to debug without the cli-client… i dont see a log that shows the main loading (and errors) for the skills.
looking forward, does it matter which intent handler method i import and use (@intent_handler versus #@intent_file_handler)? i realize there must be differences implied in the names, but both are used in simple skills with similar functionalities and used without the other one (function independently) which leads me to believe they are interchangeable to get a basic skill to start working.
edit: aha! i found i had hastily exited out of the mycroft-config edit user vim window and it was stuck running in the background… knew it was time to go to bed!.. yay, onward!
well, the cli-client is loading (logging) normally, but my custom skill still isnt being loaded. i  would really like to be able to search this logging, but i dont see it anywhere in the log files in the docs which are in /var/log/mycroft/ or even in /var/log/syslog.
seem to have found the issue with cli-client, is related to the symlinking of /mycroft-core to two different installs, one in home dir one on a partition with more spaces and symlinked… thats good, just need to clear that up… it seems that mimic is installed on the big partition and is working there even with the non-symlinked mycroft-core is the one where my custom skill is working… a mess.
…it still would be good to know where the main debug log is stored so im not locked into using the cli-client where i cant search or copy multiple pages…
okay , also a bit confused as to why duplicate skills are in both /opt/mycroft/skills and ~/mycroft-core/skills? it seems i should be editing the one in /opt according to the reply above, but then this must be synced into the /mycroft-core when loaded, because it does have my latest edits…  okay this must be directly symlinked, as it is reflecting changes made to dir names. …i see the whole skills dir is. (i dont know why devs insist on putting files all over the place… in the hopes that another user with their own home dir doesnt want to install their own software? even still symlink the other way, if that it the case…  anyway, i digress).

currently working on this error:
repeat = re.sub('^.*?' + message.data['speak'], '', utterance)#                                                                    
keyerror: 'speak'

code was copied from https://github.com/mycroftai/skill-speak/blob/20.02/init.py
i think the ‘say’ vocab is using a different skill, because when i change the names of both skill-speak which i cloned from github and the mycroft-speak.mycroft, the command still works somehow. this leads me to believe i would get this same keyerror if i could trigger skill-speak.
yeah, changed the trigger vocab in speak.voc for that skill and not triggering. tried to activate the skill again, tried restarting all services, tried touching that skill ini.py and skill not triggering.
oh, for f’s sake, why limit the amount of edits?? is this really hurting anybody as i record my troubleshooting process?? come on… i guess ill have to switch to github where they dont have silly arbitrary whining complaints… was trying to keep this in the community forum, but i guess not… i hope thats just a discourse thing, with a limit that only an exited dev like me could hit. i’ve been working on voice computer control off and on for a few years, and after moving to ubuntu, there was nothing very promising until this.

re my comment from above: ""edit: aha! i found i had hastily exited out of the mycroft-config edit user vim window and it was stuck running in the background… …well, the cli-client is loading (logging) normally, ""
it turns out the config file left open is because the usual way to close nano editor (ctrl-x) is disabled in view mode while looking at the config for default using mycroft-config edit default. this causes the cli-client not to show the starting up portions of the logs.


github.com/mycroftai/mycroft-core








how to exit `mycroft-config edit default` without leaving stopped job



        opened 09:30pm - 06 jun 20 utc




          auwsom
        





all ubuntus? (reporting from kubuntu 18.10)
the config file left open in a running job because the usual way to close nano...









after ironing out some symlinks to installation directories to larger partitions than my main install and home directory, im still not able to get my custom skill running, although it does say it is ‘loaded’ and it does work when i do use the non-symlinked ‘mycroft-core’ directory (but then my mimic installation isnt accessible).
@forslund im currently getting two errors that may be unrelated, but am trying to resolve anyway:
2020-06-06 14:31:37.804 | warning  |  4329 | mycroft.api:check_remote_pairing:531 | could not get device info: permissionerror(13, 'permission denied')
2020-06-06 14:31:38.006 | info     |  4329 | mycroft.messagebus.load_config:load_message_bus_config:33 | loading message bus configs 
ive removed all of the user config i had included to block connection to mycroft.ai (from the link below) and restarted all services but no luck.
any advice on how to track those down?
community.mycroft.ai/t/easiest-way-to-use-mycroft-completely-offline/3741/23
edit: these error lines below occur in the bus.log file first thing on startup. i’m thinking its probably a permissions error, but ive already chowned the entire mycroft-core dir, and dont know which straggler file would be causing this. i will try the config files…
 2020-06-06 14:40:35.786 | info     |  4929 | __main__:main:38 | starting message bus service...
2020-06-06 14:40:35.789 | info     |  4929 | mycroft.messagebus.load_config:load_message_bus_config:33 | loading message bus configs
2020-06-06 14:40:36.230 | warning  |  4929 | mycroft.api:check_remote_pairing:531 | could not get device info: permissionerror(13, 'permission denied') 

well, just before i renamed all the directories for a new install, i thought to try using the dev_setup.sh again, and it resolved the permissions error.
yep, all errors resolve, except this stock-skill error that has been there since install. custom skill still not working yet.
16:00:30.332 | error    | 13269 | mycroft.skills.skill_loader:_create_skill_instance:231 | skill __init__ failed with exception('skill has been disabled by mycroft',) traceback (most recent call last):   file ""/media/abc/iprogs/mycroft-core/mycroft/skills/skill_loader.py"", line 228, in _create_skill_instance     self.instance = skill_module.create_skill()   file ""/opt/mycroft/skills/mycroft-stock.mycroftai/__init__.py"", line 127, in create_skill     return stockskill()   file ""/opt/mycroft/skills/mycroft-stock.mycroftai/__init__.py"", line 87, in __init__     raise exception('skill has been disabled by mycroft') exception: skill has been disabled by mycroft  16:00:30.338 | error    | 13269 | mycroft.skills.skill_loader:_communicate_load_status:287 | skill mycroft-stock.mycroftai failed to load

@forslund, happy to report that  after running dev_setup.sh again, it corrected whatever permissions errors there were… i tried doing it manually so i could learn where everything is located and how the files work together, which i have a basic idea of now… and my new custom skill is working… so i can get back to actually programming functionality instead of troubleshooting the installation and setup, yay!
part of reporting all of the errors on this forum page is for anyone else searching on those errors to maybe find some ideas for a solution.

working example of a cli command skill here: https://github.com/mycroftai/mycroft-core/issues/2600
"
404,ask me anything on r iama with michael lewis,general discussion,"
originally published at:			https://mycroft.ai/blog/askmeanything-on-r-iama-with-michael-lewis/
we did an ama with mycroft ai founder joshua montgomery last year and we had some great questions and conversations. we are doing another ama this weekend with mycroft ai ceo, michael lewis.
do you have a burning question about mycroft? this saturday, mycroft ceo michael lewis is hosting an official reddit ask me anything on r/iama. on saturday, june 6, at 11:00 am cdt, michael will be live on https://www.reddit.com/r/iama to answer your questions about mycroft, privacy, data ownership, open source technology, and more.
if you want to set a reminder through your reddit account you can use the remindmebot.
we are looking forward to having you and answering your questions.
","
the ama is starting now, jump in and ask michael anything:



reddit



r/iama - i am michael lewis. a founder of stellar semiconductor, cryptic...
0 votes and 0 comments so far on reddit






"
405,python behavior in a skill,none,"
i am working on a skill that requires me to mount and unmount a usb stick based on a plug/unplugged event. i am able to successfully detect the insertion and removal of the usb key but mounting is a whole other issue. when i attempt to mount the detected volume the mount is not successful indicating the device may be in use. if i execute the exact python code at the linux cli the drive is detected and successfully mounts. i am attempting this with picroft (venv). i assume this is privilege related but i am at a loss to figure out how to achieve this. any assistance is welcome.
","
https://docs.python.org/3/library/os.path.html#os.path.ismount might be of use?

can you share a code snippet of what you’re doing?

i have this code running in a thread as part of the mycroft skill (getmountpathusbdevice) is the function that will mount the drive
    def start_usb_thread(self, my_id, terminate):
        log.info(""usb monitoring loop started!"")
        while not terminate():  # wait while this interval completes
            time.sleep(1)  # todo make the polling time a variable or make it a separate thread
            # get the status of the connected usb device
            self.status = self.usbdevice.isdeviceconnected()
            # log.info(""checking usb device: "" + str(self.status))
            if self.status != self.prev_status:
                log.info(""usb status changed!"")
                self.prev_status = self.status
                if self.status:  #device inserted
                    log.info(""device inserted!"")
                    device = self.usbdevice.getdevdata()
                    # mount the device and get the path
                    self.path = self.usbdevice.getmountpathusbdevice('mycroft')  #todo add sudo password to websettings
                    log.info(""stat: "" + str(self.status))
                    log.info(""dev: "" + str(device))
                    log.info(""path: "" + str(self.path))
                    log.info(""---------------------------------"")
                    self.speak_dialog('update.library', expect_response=false)
                    self.song_list = self.create_library(self.path)
                    #log.info(str(self.song_list))
                else:
                    # unmount the path
                    self.usbdevice.umountpathusbdevice('mycroft')  #todo add sudo password to websettings
                    log.info(""device removed!"")
                    self.speak_dialog('usb.removed', expect_response=false)
                    self.song_list = []
                    self.path = """"
        self.usbdevice.stoplistener(self.observer)

the code that does the mounting is here
# returns the accesible path of the device on the raspberry pi
# you can change how the path gets calulated.
def getmountpathusbdevice(password):
    sudopassword = password #'mycroft'
    global usbdev_devpath
    if not isdeviceconnected() or usbdev_devpath == none:
        return none
    # check if the dev path exists
    if os.path.exists(usbdev_devpath):
        # create a mount directory
        if not os.path.exists('usb-music'):
            os.makedirs('usb-music')
            while not os.path.exists('usb-music'):
                time.sleep(1)
        #command = ""sudo mount -t auto "" + usbdev_devpath + "" "" + os.getcwd() + '/usb-music'
        command = ""sudo mount -t auto /dev/sdb1 /home/pi/mycroft-core/usb-music""
        p = os.system(command)
        # return the path to the folder from root
        truepath = os.getcwd() + '/usb-music'
        return truepath
    return none

this code does work if i execute it as stand alone python code at the  cl after exiting the mycroft-cli-interface
thanks
"
406,ama with mycroft ceo michael lewis on r iama,general discussion,"
hi all, our ceo michael lewis will be doing an ama on reddit this saturday
come and ask him anything from 11am cdt (utc -5) on saturday june 6.
https://www.reddit.com/r/iama/
and just in case you missed that whole announcement:



mycroft – 12 mar 20



mycroft ai welcomes michael lewis as new ceo - mycroft
today i’m happy to announce that i’m taking off the ceo hat and passing it to michael lewis, who is significantly more qualified to wear it.






",
407,unpaired mycroft and cant pair again,support,"
hi, i unpaired mycroft on the website, and would like to pair it again.
the identity folder is clear apart from pycache.
when i launch the core again, mycroft does not ask to be paired, but instead just wouldn’t connect to any server. what am i doing wrong?
[update] i even downloaded and set up a fresh mycroft core. still the problem persists. the internet connection is fine.
[update2] i completely reset my virtualbox (lol) and now it works. but then my final question is: how do i successfully ‘unpair’ mycroft. say i want to develop some stuff, then unpair it and send the entire core to somebody else so they can pair and test it out? seems like my simple unpairing on mycroft-ai website didn’t do it properly.
thanks.
","
i had the same problem - but it stopped when i reinstalled mycroft 

have a look at the picroft recipe;


github.com


mycroftai/enclosure-picroft/blob/buster/image_recipe.md
# recipe for creating the picroft img

these are the steps followed to create the base image for picroft on raspbian buster.  this was performed on a raspberry pi 3b+ or pi 4

note: at startup picroft will automatically update itself to the latest version of released software, scripts and skills.


### start with the official raspbian image
* download and burn [raspbian buster lite](https://downloads.raspberrypi.org/raspbian_lite_latest).
  <br>_last used 2019-09-26 version_
* install into raspberry pi and boot
  - login: pi
  - password: raspberry

### general configuration
  - ```sudo raspi-config```
  - 1 change user password
      - enter and verify ```mycroft```
  - 2 network options
      - n1 hostname


  this file has been truncated. show original





at the end, they reset everything before releasing the image. i believe you are after the same.

hey there,
can i confirm your ~/.mycroft/identity directory for the user running mycroft was empty?
based on your description, it sounds like when you restarted mycroft it attempted to pair but couldn’t connect to the server is that right?
do you have the logs from it? was it the mycroft servers it couldn’t connect to or the local messagebus?

hi, thanks for your reply.
couldn’t connect to servers. /identity folder was empty.
essentially i unpaired mycroft from the web interface, then tried to pair it again. message bus worked fine. anyways, it sorted after re-installing fully. unfortunately don’t have the logs but i remember on the bus it was “error connecting to servers” and mycroft would reply “i’m having difficulty communicating with the server”.
could it have been a locality issue? i think i was signed up as china but our network traffic goes through hk.
"
408,a few questions about mycroft,general discussion,"
hi there,
first of all i would like to thank everyone for the amazing work on this project, i discovered this voice assistant about a month ago and am rather impressed about how well it works when compared to the other commercial voice assistants. i am currently working on my first skill which i hope to publish within the next couple of weeks which will make it possible to use apple’s find my feature to ring a lost phone via mycroft.
in the meantime i have a few questions and bits of feedback which i would like to provide you with. i am fully open to help to bring these ideas to life, so if someone would be willing to point me in the right direction i would be more that happy to try to get a pull request on the way.
1. when saying the wake word, i often find that i have to wait for the chime to play before giving my request
is there any way that the initiation of the recording could be sped up? i find it rather unnatural to have to pause my sentence between the ‘hey mycroft’ and the beginning of my request. if i do not pause i find that the first word or two of my request is cut off.
2. how could multi-language precise wake words be implemented
i am bilingual in two languages and would find it very practical to be able to speak in two different languages to mycroft (google assistant has a similar functionality where you can add an extra language which it will listen for).
do you think it would be feasible to run two instances of precise in parallel on a raspberry pi to enable the detection of the wake word in two different languages? i suppose we would then have to detect the language used before passing the request. how do you think this could be accomplished?
3. how can conversation context be properly used in a skill
i saw this page in the docs which discusses exactly this, unfortunately none of the example requests such as “how tall is john cleese?” and “where’s he from?” work for my on my default installation. i see how this can be efficient at remembering elements of a request such as the name of the subject or the name of a town to enable a continuous conversation about the subject, but what about other forms of requests. say i ask ‘what’s the weather like in london’, and then i ask ‘how about in new york’. i’m expecting mycroft to give me the weather for new york, but how could i code this into my skill?
4. is it possible to chain commands together
imagine that i wanted to ask mycroft to do two things at once. for instance ‘turn on the lights in the living room and set them to blue’, is this something which could easily be accomplished through mycroft? this would be especially useful for controlling iot devices, but it would be very practical to be able to generally ask mycroft to do at least two things at once in one command.
thanks for having stuck until the end!
","



 jackb:

1. when saying the wake word, i often find that i have to wait for the chime to play before giving my request
is there any way that the initiation of the recording could be sped up? i


you can turn of “the chime” in your mycroft.conf by setting  ""confirm_listening"": false
more advanced: you can reduce the audio subsystems latency a little bit by editing  /etc/pulse/default.pa  and add “tsched=0” to the line “load-module-udev-detect” so that it looks like:
load-module module-udev-detect tsched=0


i do not want to sound too insisting, but it would be a great help if someone with the required knowledge could answer my questions.
i am trying to create my own skill using conversational context and it would be particularly useful is my third question could be answered.
thanks!




mycroft – 21 jun 19



guest blog - all about intents - mycroft intents - mycroft
in this guest blog, mycroft community member jarbas ai explains mycroft intents and the nlu engines that handle them in skills.






hey jack,
to fill in on some of the other questions:
2. multi-lingual precise
currently precise can only utilize a single model at a time. generally a model detects a single wake word (/phrase /sound). there are some community efforts to add multiple wake word support.
the alternative would be to train a single model on both your wake words. smarter people than i might have a better idea on how well that will work…
3. conversational context docs
these are very dated unfortunately. they’re on my list to re-work but it won’t be for a little while.
from memory, context is not currently used in the weather skill but it would be an ideal candidate as you’ve shown.
some skills that i know use context are:

grocery list
deckchair cinema

these might give you a better idea of how it works in a real example.
4. chaining commands
not currently. jarbas had an experimental thing at one stage but i don’t think it’s in a state to use. at the moment each utterance is treated as a single intent.
"
409,gpio pins working on pi 3,support,"
i downloaded the new image of picroft from the mycroft website, and have been able to use it smoothly. however, recently i have been attempting to run gpio pins in my custom skill and have had no success. for testing purposes, i decided to try downloading and running the picroft_example_skill_gpio skill from https://github.com/mycroftai/picroft_example_skill_gpio which i found from some documentation here: https://docs.mycroft.ai/skill.creation/advanced-topics/gpio
i installed the skill by running the command msm install https://github.com/mycroftai/picroft_example_skill_gpio.git
however, when i tried running the skill there was no output coming from the gpio27 pin (or pin 13 if you use the gpio.board layout). specifically, i have been using the ‘turn led on’ command, which is supposed to send a high signal out of the gpio27 pin, however i am only getting a low signal coming out of that pin no matter what command i use. i have a raspberry pi 3, and noticed there was an unanswered post about this same issue in the mycroft forums (mycroft and gpio raspberry pi 3).
has anyone been able to use the gpio pins with their picroft? or gotten this example skill working?
update:
i have been able to figure out that i am unable to locate a module named rpi.gpio
i have made a python file called version.py that contains the lines:

import rpi.gpio as gpio
a = gpio.version
print a

when i attempt running this script that should simply tell me what gpio version i have, i get the following as a result:

traceback (most recent call last):
file ""version.py"", line 1, in <module>
import rpi.gpio as gpio
importerror: no module named rpi.gpio'

i tried adding a pythonpath to my ‘env’, and made this path the same as my original path…this did not work
any ideas?
update 2:
i was able to install pip, and use pip to install python-rpi.gpio, and now my simple version script is working. however, the example skill i downloaded does not recognize the intent anymore. for example, when i say “turn led on” mycroft responds with “sorry i didn’t catch that, please rephrase your request”. it seems like installing the ‘rpi.gpio’ module or pip itself has caused this mycroft skill to not recognize its intent, but that does not seem like a likely cause to me.
any ideas are appriciated
","
hi @dleweyiv, thanks for posting this - and thanks for having a go hacking around the gpio pins! i’m going to tag my colleagues @forslund and @steve.penrod, who are more knowledgeable about rpi and pip and python dependencies.
kind regards,
kathy

hi,
following in your steps installing the gpio module i also ran into trouble. checking the /var/mycroft-skills.log i found
  file ""/usr/local/lib/python2.7/site-packages/mycroft_core-0.9.7-py2.7.egg/mycroft/skills/core.py"", line 126, in load_skill
    skill_descriptor[""name""] + mainmodule, *skill_descriptor[""info""])
  file ""/opt/mycroft/skills/picroft_example_skill_gpio/__init__.py"", line 41, in <module>
    import gpio
  file ""/opt/mycroft/skills/picroft_example_skill_gpio/gpio.py"", line 57, in <module>
    gpio.setup(27,gpio.out)
runtimeerror: no access to /dev/mem.  try running as root!

this was fixed by adding the mycroft user to the gpio group:
sudo useradd mycroft gpio
and then restarting the skills service
/etc/init.d/mycroft-skills stop
/etc/init.d/mycroft-skills start

(this was done using the gpio example skill on the latest picroft image)
hope this helps!
regards
/åke

hello again @forslund ,
thank you for your original reply, that solved my issue with that example skill. however, i seem to be running into another. i have a custom skill made (located here: https://github.com/dleweyiv/tourguideskill). the goal of this custom skill is to be able to answer all questions pertaining to our university that a tour guide would be willing to answer. i thought that once i got the example skill working, i could easily implement it in my custom skill.
the issue i am running into is when i change the header of my inity.py file to include the import gpio then nothing inside the skill works anymore. for example, with import gpio being commented out i could say “hey mycroft, what is the retention rate here?” and it would respond accordingly. however, when i try to add the import gpio line at the top to attempt to import the gpio to my init file, everything stops working (aka when i ask about the retention rate, it asks me to rephrase my request.
to be clear, i am trying to set certain gpio pins in the handlers of some of the intents, but i cannot seem to import the gpio. if you have any insights again, i would greatly appreciate it

the thing here i think is that the gpio.py file is in the local directory (which is not in the path). in the example skill, the skill directory is added to the path before import gpio
see https://github.com/mycroftai/picroft_example_skill_gpio/blob/master/init.py#l30 for how it’s done in the example skill.
basically what you need would be
import sys
from os.path import abspath, dirname
sys.path.append(abspath(dirname(__file__)))
import gpio

let me know if this works, if not i’ll continue to troubleshoot.
/åke

åke,
you are the best. thank you for all your help, i cannot promise i won’t be reaching out again.
thanks again!

glad to be of help, feel free to reach out if you hit more trouble. 

åke,
conceptual question for you: when running a self.speak_dialog command, there always is a delay before the speech starts. are there already flags that exist that mark when the actual speech starts and stops? if not, do you know of any way to retrieve these timings?

it’s a slight delay while the audio is generated and then speech starts but there is no way to get the times reliably right now.
there is however a function called wait_while_speaking that can be imported from mycroft.audio which will wait until speech is complete. not sure if it’s something like that you’re interested in?

hello again åke,
i am working more on my custom skill and cannot seem to figure out some of the strange issues i am having.
when running my custom skill, inside a intent handler i set gpio pins(sometimes i set them multiple times). however, the states of the gpio pins do not always stay constant when i am reading them in using another pi and its gpio pins. when i test this by hooking the outputs of my first pi’s gpio pins to leds on a breadboard, the leds never flicker, causing me to be uncertain whether the output signals are actually fluctuating. do you believe (i know you probably cannot know for sure) the output states of the gpio are fluctuating when i have not changed them? if so, ifs there any way to stop this fluctuation without implementing hardware such as pull-down resistors?
any insights will be appreciated immensely

i have very little experience of the raspberry pi gpio so i’m just guessing (and googling) here.
i would probably recommend having a pull up or pull down resistor between the devices. there seem to be a gpio.setup function that can set the internal pull up/down of the pi.
to really see what’s going on the best would be to use an oscilloscope and monitor the signal.
sorry for not being more helpful.

i have done as you told but it is also not working for me. i am not getting led to turn on. it shows and speaks that led is on but still it’s not on.

hi raj,
have you got the code you are working on in a public repo somewhere like github? if people can have a look at what you’re trying to do, they might be able to give some suggestions.

i am just using picroft gpio example. nothing more. it got solved but got new issues like it automatic takes wakeup word even if i am not saying it and starts recording. when i adjusted the threshold to “1e-20” it took time to load new wake up word then started again looping but with few minutes gap and it than doesn’t take voice commands

hi , i followed ( https://github.com/mycroftai/picroft_example_skill_gpio ) tutorial  but i stopped in "" make docs "" step i don’t know how to figure this out ot how to write this command can anybody help me please?
thank you
"
410,mycroft freenas,support,"
hi !
i’m trying to have mycroft running on a nas. i’m using freenas and when i follow the github steps to install mycroft i encounter this issue :

image733×67 1.22 kb

any idea ?
many thanks !
","
pretty sure freenas still bases on bsd, so there’s probably a few things that won’t work without adjusting.  if you search here there’s a freebsd thread which might help.

thanks for the quick reply.
however, i don’t understand, nothing seems to be working. i don’t realy understand how i’m supposed to use the freesbd patch either… when i run “bash dev_setup.sh”, nothing’s happening.

hmmm.  there’s usually some output when you try and run commands like that.
if you’re not very experienced with bsd or linux, it would probably be easiest to run mycroft on a debian-based linux system, or run picroft on a raspberry pi3/4.

yes, way more confortable with debian !  i’m gonna use omv instead. thx for the help anyway, take care !
"
411,default failure action,general discussion,"
i use mycroft mostly for spotify and a few home assistant integrations.
i find that it is not able to return results from spoitfy very often.
i ask “play artist name” and sometime it responds “just one moment while i look for that”  then plays the artist as expected.
then if i try some other artists i get the same “just one moment while i look for that” then it starts playing ap news, which i don;t even have configured in my skills, i have cbc set up as i am in canada.
is this “ap news” a default response when the request is not processed correctly? is there any way to change it to something else?
thanks
","
hi redacted, thanks for flagging this.
whilst there’s a default news station, you can also call any of the other news stations by name. this is particularly for countries where we have multiple providers. it sounds like “ap news” is matching a bit liberally.
to help us test this out, can you remember any of the queries that triggered it? the more the merrier 

my guess is that it is missing the match in spotify.  can you clarify.  when you call for the music, are you invoking spotify by name or just saying play?
for example: “play hughy lewis and the news on spotify”  should return something like their album “four” from 1987.  i think it’s their undisputed masterpiece.
if you simply say “play hughy lewis and the news” then the play skill will likely pick up on “news” and trigger your news playback.
i’m not sure the disabiguation software is working as intended at the moment.  as gez said above, can you provide some specifics so we can take a peek at the issue?

thanks for the replies, i have been trying to invoke it using "" play artist"" and “play artist on spotify”  both ended up playing ap news. none of them are hughy lewis an the news 
i think i can reproduce the issue, i will try again later and pull the logs.
thanks,

playing wave '/home/pi/mycroft-core/mycroft/res/snd/start_listening.wav' : signed 16 bit little endian, rate 48000 hz, stereo
2020-01-14 14:30:50.408 | info     |   682 | __main__:handle_record_begin:37 | begin recording...
2020-01-14 14:30:53.171 | info     |   682 | __main__:handle_record_end:43 | end recording...
2020-01-14 14:30:54.544 | info     |   682 | __main__:handle_utterance:64 | utterance: ['play tyler childers']


2020-01-14 14:30:40.451 | info     |   676 | spotifyskill | pausing spotify...
removing event mycroft-spotify.forslund:monitorspotify
2020-01-14 14:30:50.432 | info     |   676 | volumeskill | muting!
2020-01-14 14:30:50.550 | info     |   676 | volumeskill | volume before mute: 60
2020-01-14 14:30:50.552 | info     |   676 | volumeskill | 60
0
60
2020-01-14 14:30:56.184 | info     |   676 | playback control skill | resolving player for: tyler childers
2020-01-14 14:30:56.354 | info     |   676 | spotifyskill | handling ""tyler childers"" as a genric query...
2020-01-14 14:30:56.355 | info     |   676 | spotifyskill | checking users playlists
removing event mycroft-playback-control.mycroftai:playquerytimeout
removing event mycroft-playback-control.mycroftai:playquerytimeout
2020-01-14 14:30:56.501 | info     |   676 | spotifyskill | spotify confidence: 0.8235294117647058
2020-01-14 14:30:56.524 | info     |   676 | spotifyskill |               data: {'data': {'collaborative': false, 'description': '', 'external_urls': {'spotify': 'https://open.spotify.com/playlist/37i9dqzf1e4wx5r7lxotnp'}, 'href': 'https://api.spotify.com/v1/playlists/37i9dqzf1e4wx5r7lxotnp', 'id': '37i9dqzf1e4wx5r7lxotnp', 'images': [{'height': none, 'url': 'https://seeded-session-images.scdn.co/v1/img/artist/13zedw6vybf12hyczrr4ev/en', 'width': none}], 'name': 'tyler childers radio', 'owner': {'display_name': 'spotify', 'external_urls': {'spotify': 'https://open.spotify.com/user/spotify'}, 'href': 'https://api.spotify.com/v1/users/spotify', 'id': 'spotify', 'type': 'user', 'uri': 'spotify:user:spotify'}, 'primary_color': none, 'public': false, 'snapshot_id': 'mjyzmty4nzasmdawmdawmdbhmgvhodexnwi1mtewowvkodi2zje4ntuwzmziymfmnq==', 'tracks': {'href': 'https://api.spotify.com/v1/playlists/37i9dqzf1e4wx5r7lxotnp/tracks', 'total': 50}, 'type': 'playlist', 'uri': 'spotify:playlist:37i9dqzf1e4wx5r7lxotnp'}, 'name': 'tyler childers radio', 'type': 'playlist'}
removing event mycroft-spotify.forslund:launch_librespot
removing event mycroft-playback-control.mycroftai:playquerytimeout
2020-01-14 14:31:01.252 | info     |   676 | playback control skill | playing with: mycroft-npr-news.mycroftai
2020-01-14 14:31:01.573 | info     |   676 | severeweatherinformation | severeweatherinformation skill stop
2020-01-14 14:31:01.716 | info     |   676 | playback control skill | audio service status: {}
  % total    % received % xferd  average speed   time    time     time  current
                                 dload  upload   total   spent    left  speed
  5 3165k    5  176k    0     0  27470      0  0:01:57  0:00:06  0:01:51 306992020-01-14 14:31:19.135 | info     |   676 | volumeskill | muting!
2020-01-14 14:31:19.261 | info     |   676 | volumeskill | volume before mute: 60
2020-01-14 14:31:19.265 | info     |   676 | volumeskill | 60
0
60
2020-01-14 14:31:24.157 | info     |   676 | severeweatherinformation | severeweatherinformation skill stop
2020-01-14 14:31:24.272 | info     |   676 | playback control skill | audio service status: {'artist': '', 'album': ''}
2020-01-14 14:31:43.221 | info     |   676 | volumeskill | muting!
2020-01-14 14:31:43.328 | info     |   676 | volumeskill | volume before mute: 60
2020-01-14 14:31:43.329 | info     |   676 | volumeskill | 60
0
60
2020-01-14 14:32:08.194 | info     |   676 | volumeskill | muting!
2020-01-14 14:32:08.298 | info     |   676 | volumeskill | volume before mute: 60
2020-01-14 14:32:08.307 | info     |   676 | volumeskill | 60

high performance mpeg 1.0/2.0/2.5 audio player for layers 1, 2 and 3
        version 1.25.10; written and copyright by michael hipp and others
        free software (lgpl) without any warranty but with best wishes

directory: /tmp/mycroft/cache/tts/googletts/
playing mpeg stream 1 of 1: 593a45a785a12130317b1ce3c97b9cc2.mp3 ...

mpeg 2.0 l iii cbr32 24000 mono

mpg123: death by sigterm

[0:01] decoding of 593a45a785a12130317b1ce3c97b9cc2.mp3 finished.
2020-01-14 14:31:01.519 | info     |   679 | mycroft.audio.speech:mute_and_speak:120 | speak: here is this hour's news on yle.
high performance mpeg 1.0/2.0/2.5 audio player for layers 1, 2 and 3
        version 1.25.10; written and copyright by michael hipp and others
        free software (lgpl) without any warranty but with best wishes



ah yeah,  i see what’s happening.
the skill is searching for a match of the station acronym in the utterance. as it found yle in “tyler” it reported a high confidence level which was apparently higher than spotify’s confidence.
thanks for reporting this, i’ve got a fix in the works so we’ll try get that out to devices as soon as possible.

great! thanks. let me know if you need anything else.

hey, i seem to be having this same problem; no matter what i request it defaults to some news playback. were you able to fix this?
"
412,audio input not working immediately after install,none,"
i dont remember exactly how i resolve this, but it involved digging into the docs and using the audio tests here: https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/troubleshooting/audio-troubleshooting
in my situation, after install and starting the services (all) there was no indicator it was running until i used the mycroft-cli-client. then i could see it responding but couldnt hear it. the audio test showed it was recording me, but not getting into mycroft, so i restarted the services mycroft-start all restart and pulseaudio.
./start-mycroft.sh all
mycroft-start all 

heres my command sequence from history:
 1921  bin/mycroft-start audiotest   (had to add to path manually)
 1922  pactl list sinks short
 1923  pulseaudio -k
 1924  stop-mycroft.sh
 1925  bash stop-mycroft.sh
 1926  ./start-mycroft.sh
 1927  sudo ./start-mycroft.sh  (had copied with root krusader)
 1928  sudo ./start-mycroft.sh all restart 
 1929  pulseaudio --start

add to path permanently:
echo ""export path=$path:$home/mycroft-core/bin"" >>~/.bashrc
source ~/.bashrc
","
hey auwsom,
thanks for posting what worked for you.
i wanted to suggest not running mycroft with sudo. it shouldn’t be necessary and may have unintended effects.
you might have mentioned this in one of your threads already but what os are you running? i’m interested in why the commands didn’t get added to $path for you.
if you find the audio isn’t working in the future can you try just the
./start-mycroft.sh all restart

and see if that alone resolves it?




 auwsom:

mycroft-start all restart


hey @gez-mycroft, so i mentioned in parens that i was using root because i had downloaded to a partition with more space and was (had to?) use  root to copy the dir back into home dir after trying to symlink it, but has possibly unrelated problems. anyway, krusader is a kubuntu file manager, and i always seem to have to use the root permissioned version to interact with other partitions mounted from other oses (havent worked that out yet) or to edit files i created when installing as root.  i slogged through the setup until i had it working enough to not give up now ;), but i should go back and clean up the install a bit.
i’m pretty sure the path setting started writing to the root path profile because of that, or i didnt select it because other settings were writing to the root path profile. the mycroft dir in my home dir is under 500mbs which is fine, but the one in the downloaded partition is 2.2gbs, too much for my backupable home dir and small clonable kubuntu partition. …indeed some necessary files are being used in the non-home dir… yet the symlinking the large dir to the home dir does work.
i used the mycroft-start all restart, is that different than ./start-mycroft.sh all restart? the audio not working was a one time thing, but i thought my install had not worked, because there was no feedback when i started it. so i posted here to help other who might not see there is a cli-client for troubleshooting. the mycoft team might consider turning on debug at the command line directly after install, possibly automatically running a few tests with some feedback, to help people get set up and make sure things are running before troubleshooting individual components which might not be working. then a flag could be set to turn off the install debug.

not very helpfull, i know but…
you said this in the other thread about the firewall;
“as a noob concerned with security,”
believe me! don’t run software as root if it is not intended to be ran as root. fix your file/mount permission issues!!!

good reminder, thanks, yes had already chowned that dir back to $user when troubleshooting another issue… not that much of a noob, except i try to assume that i am relative to intrusion threats to be safe, but good reminder nonetheless and for others too.
as far as krusader as root goes, there are certain things non-root fm wont do, aside from using files and dirs made while installing and partitioning… am using non-root now to be reminded of them and will elaborate…
"
413,wireless conversational picroft,none,"
been having fun with this. you guys created something really special.
after installing it on a pi 4, i added a microphone and a speaker.
then, i loaded a local version of deepspeech and this is running as service now, locally.  it loads from cron and reloads if it goes down.  i made the changes in the config file, but i do not know how to tell which service i am really using.   is there a way to confirm that it is using the service locally?
i added chatscript as a local service now also, also reloading if it goes down. chatscript is an advanced conversational chatbot framework that has a lot of capability built into it.   i added this and a bunch of custom code, so basically, you can talk to it and it remembers you and is somewhat intelligent. everything is stored locally and it learns from you, basically triples.
it looks like the text to speech can be local if i choose the british male voice.  i have to look at this tomorrow. i want to see if i can make this local also. but it looks like everything is going to the mycroft servers and there is no option for running it locally.
i am not at all opposed to supporting this effort, but i want an option to run it without internet if it is not connected.
i got everything to work end to end, but i am not sure which stt and tts i am actually using.
when i put it together, you can talk to a pi microphone and it does stt, sends the text to chatscript. chatscript does the conversational magic and replies with text.  the text is then sent to the text to speech engine. then it goes through the cable to the speaker.
basically, it is a wireless standalone chatbot.
i have some questions. how can i tell which services it is really using.  i guess i can unplug everything and see if it works?
and deepspeech is really slow. i am wondering if anyone has really gotten it to work quickly on a pi.
the chatscript engine is also running on an app called elfchat on android and ios (free), i am just seeing if there’s another use for my code on this platform.
just some ramblings
","
its a bit faster than realtime on a pi4 but depending on your setup you could be waiting for end of voice intent + timed silence and then only presenting it to deepspeech that adds time to intent then intent processing time and finally intent action such as text to speech.
i think in the new current alpha 8.0 the internal kws & vad callback implementation of deepspeech might show its head as its on the roadmap.
it will then be streamlined for stream with only need for audio input as guess it can be used direct.
but from benchmarks its x1.2 realtime still limited to a single core and pi3 drops down to less .5x realtime.
so yeah its not the fastest but also when it receves the audio and how long intent action starts and complete could also be adding what seems in response huge latency.
7.1 is the current stable release as 8.x alpha is now live.

a pi is a slow computer.  deepspeech, even with the tflite model, is going to be less than ideal.  you can check the logs on your instance of deepspeech, should be logging something somewhere.  what config changes did you make?  did you use the streaming or server instance for ds?

yes, i tried the 3 versus the 4, and the 4 is a lot faster. i was replicating the test posted on seed.  while i appreciate her post, i do not see the same results (faster than speech), although she is on an earlier version, v6 and other factors are different.
here are two ways i tested it,

running it from the command line:
(.venv) pi@picroft:~ $ deepspeech --model deepspeech-0.7.0-models.tflite --scorer deepspeech-0.7.0-models.scorer --audio audio/2830-3980-0043.wav

loading model from file deepspeech-0.7.0-models.tflite
tensorflow: v1.15.0-24-gceb46aa
deepspeech: v0.7.1-0-g2e9c281
loaded model in 0.00428s.
loading scorer from files deepspeech-0.7.0-models.scorer
loaded scorer in 0.00109s.
running inference.
experience proves this
inference took 3.650s for 1.975s audio file.

by calling the locally running server
(.venv) pi@picroft:~ $ time curl -x post --data-binary @audio/4507-16021-0012.wav http://localhost:8080/stt
why should one hall on the way
real    0m5.030s
user    0m0.038s
sys     0m0.028s

i will have to look at this later, maybe there is some setting that delays the timing.
also, i was surprised to see that the command line is significantly faster than a running instance of this service. but looking at the load time, i guess this makes sense.
i am not sure i understand this, “how long intent action starts and complete”.
i will try other audio files.

re deepspeech on a pi, not an out of the box solution for you, but check



github



hellochatterbox/speech2text
chatterbox stt engines. contribute to hellochatterbox/speech2text development by creating an account on github.





contains streaming stt for deepspeech and kaldi, which should run in a pi
not integrated in mycroft-core yet  (but should be fairly easy to add support for it)

dunno strange as i was running 7.0 and was getting exactly what they where saying approx x1.2 realtime for the pi4 and < x0.5 for pi3.
but that was just a vanilla deepspeech only install, on the couple of sample wavs they provide and the benchmarks where bang on expected.
if you run with the scorer dunno what it does but you have to run it a couple of times and benches eventually become right.
i was also not all that impressed about single core use as dnn is possible to multithread its just a bit complex.
it was just the though that it might get to be > realtime on a pi3 and pi4 well then you talking.
what is also interesting for the pi4 is the new raspios 64bit release as it could well translate to another 20-30%.
but multithreading seem to be a touchy subject amongst the mozzila developers.
but noobed my comments in there.
“how long intent action starts and complete” yeah not  very sensicial but whatever your input process is, its the time it takes to get a the result to the the user. tts has latency also.

excellent! i don’t suppose that you have the mycroft skill for connecting to the chatscript chatbot in github? i’d love to load both the skill and chatscript on my animatronic and fuss around with it.

charles, direct im me for the code.  it is a series of steps. if we can get it to work on your end, maybe we can publish something for others.  i got it to work, but it took a number of steps.
info @ projectonegames.com
"
414,discord server idea,general discussion,"
hey,
i am quite new to the mycroft community and i find it really exiting to have a open source voice recognition software.
since i am still struggling with some error and already used the forum for that i was thinking about a discord server which is already existing for the raspberry pi.
so my question is, if this might be a considerable idea?
best regards
vadi
","
have you visited chat.mycroft.ai?

yep, i just realised that this post is obsolete 

still good to post it, helps for anyone searching for the discord server in the future 
welcome to mycroft too!
"
415,gnome wins against patent abuser,general discussion,"
originally published at:			https://mycroft.ai/blog/gnome-wins-against-patent-abuser/
an enormous congratulations to the gnome foundation from all of us at mycroft ai! 
in august of 2019, the gnome foundation was served with a lawsuit alleging patent infringement on us patent 9,936,086. this week a settlement was reached between the two parties.
the patent abuser, in this case, rothschild patent imaging llc, claims to have invented…get ready for it…a means of capturing images through a mobile device, then utilizing a wireless receiver and transmitter, filtering said images based on a “topic, theme or individual shown in the respective photographic”, then sending those filtered images to another device.
ridiculous.
the patent abuser claims that the gnome foundation’s photo organizing tool, shotwell, infringed on the patent, and thus gnome needed to “settle for a high five-figure amount.” the gnome foundation took a hard stance and said,
we will stand firm against this baseless attack, not just for gnome and shotwell, but for all free and open source software projects.
 
thankfully, last week the case was settled in a walk-away agreement. gnome received a release and covenant not to be sued for patents held by rothschild patent imaging. with an additional covenant and release for both patents held by rpi and leigh rothschild covering software released under an existing open source initiative approved license (and subsequent versions thereof). 
this is a great win for gnome and for all open source projects. 
gnome was backed by some great attorneys at shearman & sterling, our friends over at the open invention network, and 4,100+ donors pitching in $151,000+ to contribute to the gnome patent troll defense fund. 
this adds to the encouraging news for us here at mycroft ai, and reinforces our confidence in our own patent abuse lawsuit.
","
nice, but invalidating the patent would have been the best result.

i’ve heard a bit of interest from some popular linux podcasts in getting more information on the details, so hoping that we’ll get to hear more of that soon.
"
416,can i run linux commands,support,"
is there a way that i can get mycroft to run some linux programs. i am running linux mint 19 on a laptop. for example:

“hey mycroft. open firefox.”
“hey mycroft. open pithos”

etc.
thanks!
","
the desktop skill can accomplish most of this now.  https://github.com/tree-ind/desktop-control/tree/e3eaa3827f4644d5c4d5fc1b878ba7bb83aae657

thanks. this looks like it requires the kde plasma desktop. is that the case?

i believe so, yes.  (20char padding)

thanks. unfortunately, that will not work for me. i use cinnamon and xfce. is there another way that you know of?

use that skill as a framework to for another skill that works in xfce?

hi there @dennisd,
at the moment the skill-desktop only works on mycroft for plasma.
a separate skill will be required for other distributions, and will need to specifically use the path to executables on that particular distribution; this is one of the drawbacks of mycroft across multiple distributions.
at a code level, we try to provide a way to do this using requirements.sh - which can be used to conditionally import different libraries or binaries for different platforms.
in summary though, this would require a new skill to be developed for xfce / cinnamon.

6 posts were split to a new topic: hooking into the mycroft output from the command line
"
